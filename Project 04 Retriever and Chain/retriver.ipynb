{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever And Chain With Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GLOBAL\\n \\nEDITION\\nThis is a special edition of an established \\ntitle widely used by colleges and universities \\nthroughout the world. Pearson published this \\nexclusive edition for the benefit of students \\noutside the United States and Canada. If \\nyou purchased this book within the United \\nStates or Canada, you should be aware that \\nit has been imported without the approval of \\nthe Publisher or Author. The Global Edition \\nis not supported in the United States and \\nCanada.\\nPearson Global Edition\\nGLOBAL\\n \\nEDITION\\nFor these Global Editions, the editorial team at Pearson has \\ncollaborated with educators across the world to address a \\nwide range of subjects and requirements, equipping students \\nwith the best possible learning tools. This Global Edition \\npreserves the cutting-edge approach and pedagogy of the \\noriginal, but also features alterations, customization, and \\nadaptation from the North American version.\\nD\\nigital Image Processing\\n FOURTH EDITION\\n Rafael C. Gonzalez • Richard E. Woods\\nDigital Image Processing\\nGonzalez \\nWoods\\nFOURTH\\n \\nEDITION\\nGLOBAL\\n \\nEDITION\\nGonzalez_04_1292223049_Final.indd   1\\n11/08/17   5:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 0}),\n",
       " Document(page_content='Support Package for \\nDigital\\n \\nImage Processing\\nYour new textbook provides access to support packages that may include reviews in areas \\nlike probability and vectors, tutorials on topics relevant to the material in the book, an image \\ndatabase, and more. Refer to the Preface in the textbook for a detailed list of resources.\\nFollow the instructions below to register for the Companion Website for Rafael C. Gonzalez and \\nRichard E. Woods’ \\nDigital Image Processing\\n, Fourth Edition, Global Edition.\\n1.\\n \\nGo to \\nwww.ImageProcessingPlace.com\\n2.\\n \\nFind the title of your te\\nxtbook.\\n3.\\n  \\nCl\\nick Support Materials and follow the on-screen instructions to create a login name and \\npassword.\\nUse the login name and password you created during registration to start using the \\ndigital resources that accompany your textbook.\\nIMPORTANT:\\nThis serial code can only be used once. This subscription is not transferrable.\\nGonzalez_04_1292223049_ifc_Final.indd   1\\n11/08/17   5:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 1}),\n",
       " Document(page_content='Processing\\nigital Image\\n4\\nD\\nFOURTH\\nEDITION\\nRafael C. Gonzalez\\nUniversity of Tennessee\\nRichard E. Woods\\nInterapptics\\n330 Hudson Street, New York, NY 10013\\nGlobal Edition\\nDIP4E_GLOBAL_Print_Ready.indb   1\\n7/6/2017   10:55:08 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 2}),\n",
       " Document(page_content='Senior Vice President Courseware Portfolio Management:\\n Marcia J. Horton\\nDirector, Portfolio Management: Engineering, Computer Science & Global Editions:\\n Julian Partridge\\nPortfolio Manager:\\n Julie Bai \\nField Marketing Manager:\\n Demetrius Hall \\nProduct Marketing Manager:\\n Yvonne Vannatta \\nMarketing Assistant:\\n Jon Bryant \\nContent Managing Producer, ECS and Math:\\n Scott Disanno \\nContent Producer:\\n Michelle Bayman \\nProject Manager:\\n Rose Kernan\\nAssistant Project Editor, Global Editions:\\n Vikash Tiwari\\nOperations Specialist:\\n Maura Zaldivar-Garcia \\nManager, Rights and Permissions:\\n Ben Ferrini \\nSenior Manufacturing Controller, Global Editions:\\n Trudy Kimber \\nMedia Production Manager, Global Editions:\\n Vikram Kumar\\nCover Designer:\\n Lumina Datamatics \\nCover Photo:\\n \\nCT image\\n—© zhuravliki.123rf.com/Pearson Asset Library; \\nGram-negative bacteria\\n—© royaltystockphoto.com/\\nShutterstock.com; \\nOrion Nebula\\n—© creativemarc/Shutterstock.com; \\nFingerprints\\n—© Larysa Ray/Shutterstock.com; \\nCancer \\ncells\\n—© Greenshoots Communications/Alamy Stock Photo\\nMATLAB is a registered trademark of The MathWorks, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098.\\nPearson Education Limited\\nEdinburgh Gate\\nHarlow\\nEssex CM20 2JE\\nEngland\\nand Associated Companies throughout the world\\nVisit us on the World Wide Web at: \\nwww.pearsonglobaleditions.com\\n© Pearson Education Limited 2018\\nThe rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them \\nin accordance with the Copyright, Designs and Patents Act 1988.\\nAuthorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4\\n, \\nby Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education © 2018.\\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by \\nany means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the pub\\n-\\nlisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron \\nHouse, 6–10 Kirby Street, London EC1N 8TS.\\nAll trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the \\nauthor or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliatio\\nn \\nwith or endorsement of this book by such owners.\\nBritish Library Cataloguing-in-Publication Data\\nA catalogue record for this book is available from the British Library\\n10 9 8 7 6 5 4 3 2 1\\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-22304-9\\nTypeset by Richard E. Woods\\nPrinted and bound in Malaysia\\nDIP4E_GLOBAL_Print_Ready.indb   2\\n7/6/2017   10:55:08 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 3}),\n",
       " Document(page_content='To Connie, Ralph, and Rob \\nand\\nTo Janice, David, and Jonathan\\nDIP4E_GLOBAL_Print_Ready.indb   3\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 4}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 5}),\n",
       " Document(page_content='Contents\\nPreface  9\\nAcknowledgments  12\\nThe Book Website  13\\nThe DIP4E Support Packages  13\\nAbout the Authors  14\\n1\\n Introduction  17\\nWhat is Digital Image Processing?  18\\nThe Origins of Digital Image Processing  19\\nExamples of Fields that Use Digital Image Processing  23\\nFundamental Steps in Digital Image Processing  41\\nComponents of an Image Processing System  44\\n2\\n Digital Image Fundamentals  47\\nElements of Visual Perception  48\\nLight and the Electromagnetic Spectrum  54\\nImage Sensing and Acquisition  57\\nImage Sampling and Quantization  63\\nSome Basic Relationships Between Pixels  79\\nIntroduction to the Basic Mathematical Tools Used in Digital Image \\nProcessing  \\n83\\n3\\n Intensity Transformations and Spatial  \\nFiltering  119\\nBackground  120\\nSome Basic Intensity Transformation Functions  122\\nHistogram Processing  133\\nFundamentals of Spatial Filtering  153\\nSmoothing (Lowpass) Spatial Filters  164\\nSharpening (Highpass) Spatial Filters  175\\nHighpass, Bandreject, and Bandpass Filters from Lowpass Filters  188\\nCombining Spatial Enhancement Methods  191\\nDIP4E_GLOBAL_Print_Ready.indb   5\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 6}),\n",
       " Document(page_content='6\\n    \\nContents\\n4\\n Filtering in the Frequency \\nDomain  203\\nBackground  204\\nPreliminary Concepts  207\\nSampling and the Fourier Transform of Sampled  \\nFunctions  215\\nThe Discrete Fourier Transform of One Variable  225\\nExtensions to Functions of Two Variables  230\\nSome Properties of the 2-D DFT and IDFT  240\\nThe Basics of Filtering in the Frequency Domain  260\\nImage Smoothing Using Lowpass Frequency Domain  \\nFilters  272\\nImage Sharpening Using Highpass Filters  284\\nSelective Filtering  296\\nThe Fast Fourier Transform  303\\n5\\n Image Restoration \\nand Reconstruction  317\\nA Model of the Image Degradation/Restoration  \\nprocess  318\\nNoise Models  318\\nRestoration in the Presence of Noise Only—Spatial Filtering  327\\nPeriodic Noise Reduction Using Frequency Domain Filtering  340\\nLinear, Position-Invariant Degradations  348\\nEstimating the Degradation Function  352\\nInverse Filtering  356\\nMinimum Mean Square Error (Wiener) Filtering  358\\nConstrained Least Squares Filtering  363\\nGeometric Mean Filter  367\\nImage Reconstruction from Projections  368\\n6\\n Color Image Processing  399\\nColor Fundamentals  400\\nColor Models  405\\nPseudocolor Image Processing  420\\nBasics of Full-Color Image Processing  429\\nColor Transformations  430\\nDIP4E_GLOBAL_Print_Ready.indb   6\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 7}),\n",
       " Document(page_content='Contents\\n    \\n7\\nColor Image Smoothing and Sharpening  442\\nUsing Color in Image Segmentation  445\\nNoise in Color Images  452\\nColor Image Compression  455\\n7\\n Wavelet and Other Image Transforms  463\\nPreliminaries  464\\nMatrix-based Transforms  466\\nCorrelation  478\\nBasis Functions in the Time-Frequency Plane  479\\nBasis Images  483\\nFourier-Related Transforms  484\\nWalsh-Hadamard Transforms  496\\nSlant Transform  500\\nHaar Transform  502\\nWavelet Transforms  \\n504\\n8\\n Image Compression and  \\nWatermarking  539\\nFundamentals  540\\nHuffman Coding  553\\nGolomb Coding  556\\nArithmetic Coding  561\\nLZW Coding  564\\nRun-length Coding  566\\nSymbol-based Coding  572\\nBit-plane Coding  575\\nBlock Transform Coding  576\\nPredictive Coding  \\n594\\nWavelet Coding  \\n614\\nDigital Image Watermarking  \\n624\\n9\\n Morphological Image Processing  635\\nPreliminaries  636\\nErosion and Dilation  638\\nOpening and Closing  644\\nThe Hit-or-Miss Transform  648\\nDIP4E_GLOBAL_Print_Ready.indb   7\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 8}),\n",
       " Document(page_content='8\\n    \\nContents\\nSome Basic Morphological Algorithms  652\\nMorphological Reconstruction  667\\nSummary of Morphological Operations on Binary Images  673\\nGrayscale Morphology  674\\n10\\n Image Segmentation  699\\nFundamentals  \\n700\\nPoint, Line, and Edge Detection  \\n701\\nThresholding  \\n742\\nSegmentation by Region Growing and by Region Splitting and \\nMerging  \\n764\\nRegion Segmentation Using Clustering and  \\nSuperpixels  \\n770\\nRegion Segmentation Using Graph Cuts\\n  \\n777\\nSegmentation Using Morphological Watersheds\\n  \\n786\\nThe Use of Motion in Segmentation  \\n796\\n11\\n Feature Extraction   811\\nBackground  \\n812\\nBoundary Preprocessing  \\n814\\nBoundary Feature Descriptors  \\n831\\nRegion Feature Descriptors  \\n840\\nPrincipal Components as Feature Descriptors  \\n859\\nWhole-Image Features  \\n868\\nScale-Invariant Feature Transform (SIFT)  \\n881\\n12\\n Image Pattern Classification  903\\nBackground  \\n904\\nPatterns and Pattern Classes  \\n906\\nPattern Classification by Prototype Matching  \\n910\\nOptimum (Bayes) Statistical Classifiers  \\n923\\nNeural Networks and Deep Learning  \\n931\\nDeep Convolutional Neural Networks  \\n964\\nSome Additional Details of Implementation  \\n987\\nBibliography  995\\nIndex  1009\\nDIP4E_GLOBAL_Print_Ready.indb   8\\n7/12/2017   10:23:39 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 9}),\n",
       " Document(page_content='Preface\\nWhen something can be read without effort, great effort has gone into its writing.\\nEnrique Jardiel Poncela\\nThis edition of \\nDigital Image Processing\\n is a major revision of the book.\\n As in \\nthe 1977 and 1987 editions by Gonzalez and Wintz, and the 1992, 2002, and 2008  \\neditions by Gonzalez and Woods, this sixth-generation edition was prepared \\nwith students and instructors in mind. The principal objectives of the book \\ncontinue to be to provide an introduction to basic concepts and methodologies \\napplicable to digital image processing, and to develop a foundation that can \\nbe used as the basis for further study and research in this field. To achieve \\nthese objectives, we focused again on material that we believe is fundamental \\nand whose scope of application is not limited to the solution of specialized \\nproblems. The mathematical complexity of the book remains at a level well \\nwithin the grasp of college seniors and first-year graduate students who have \\nintroductory preparation in mathematical analysis, vectors, matrices, probability, \\nstatistics, linear systems, and computer programming. The book website pro-\\nvides tutorials to support readers needing a review of this background material.  \\nOne of the principal reasons this book has been the world leader in its ﬁeld for \\n40 years is the level of attention we pay to the changing educational needs of our \\nreaders. The present edition is based on an extensive survey that involved faculty, \\nstudents, and independent readers of the book in 150 institutions from 30 countries. \\nThe survey revealed a need for coverage of new material that has matured since the \\nlast edition of the book. The principal ﬁndings of the survey indicated a need for: \\n• \\nExpanded coverage of the fundamentals of spatial filtering.\\n• \\nA more comprehensive and cohesive coverage of image transforms.\\n• \\nA more complete presentation of finite differences, with a focus on edge detec-\\ntion.\\n• \\nA discussion of clustering, superpixels, and their use in region segmentation. \\n• \\nCoverage of maximally stable extremal regions.\\n• \\nExpanded coverage of feature extraction to include the Scale Invariant Feature \\nT\\nransform (SIFT).\\n• \\nExpanded coverage of neural networks to include deep neural networks, back-\\npropagation,\\n deep learning, and, especially, deep convolutional neural networks. \\n• \\nMore homework exercises at the end of the chapters.\\nT\\nhe new and reorganized material that resulted in the present edition is our \\nattempt at providing a reasonable balance between rigor, clarity of presentation, \\nand the ﬁndings of the survey. In addition to new material, earlier portions of the \\ntext were updated and clariﬁed. This edition contains 241 new images, 72 new draw-\\nings, and 135 new exercises.\\nDIP4E_GLOBAL_Print_Ready.indb   9\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 10}),\n",
       " Document(page_content='10\\n    \\nPreface\\nNew to This Edition\\nThe highlights of this edition are as follows.\\nChapter 1:\\n Some figures were updated, and parts of the text were rewritten to cor-\\nrespond to changes in later chapters\\n.\\nChapter 2:\\n Many of the sections and examples were rewritten for clarity. We \\nadded 14 new exercises\\n. \\nChapter 3:\\n Fundamental concepts of spatial ﬁltering were rewritten to include a \\ndiscussion on separable ﬁlter kernels\\n, expanded coverage of the properties of low-\\npass Gaussian kernels, and expanded coverage of highpass, bandreject, and band-\\npass ﬁlters, including numerous new examples that illustrate their use. In addition to \\nrevisions in the text, including 6 new examples, the chapter has 59 new images, 2 new \\nline drawings, and 15 new exercises.\\nChapter 4:\\n Several of the sections of this chapter were revised to improve the clar-\\nity of presentation.\\n We replaced dated graphical material with 35 new images and 4 \\nnew line drawings. We added 21 new exercises. \\nChapter 5:\\n Revisions to this chapter were limited to clariﬁcations and a few cor-\\nrections in notation.\\n We added 6 new images and 14 new exercises, \\nChapter 6:\\n Several sections were clarified, and the explanation of the CMY and \\nCMYK color models was expanded,\\n including 2 new images.\\nChapter 7:\\n This is a new chapter that brings together wavelets, several new trans-\\nforms\\n, and many of the image transforms that were scattered throughout the book. \\nThe emphasis of this new chapter is on the presentation of these transforms from a \\nunified point of view.  We added 24 new images, 20 new drawings, and 25 new exer-\\ncises. \\nChapter 8:\\n The material was revised with numerous clarifications and several \\nimprovements to the presentation.\\nChapter 9:\\n Revisions of this chapter included a complete rewrite of several sec-\\ntions\\n, including redrafting of several line drawings. We added 16 new exercises\\nChapter 10:\\n Several of the sections were rewritten for clarity. We updated the \\nchapter by adding coverage of finite differences\\n, \\nK\\n-means clustering, superpixels, \\nand graph cuts. The new topics are illustrated with 4 new examples. In total, we \\nadded 29 new images, 3 new drawings, and 6 new exercises.\\nChapter 11:\\n The chapter was updated with numerous topics, beginning with a more \\ndetailed classification of feature types and their uses\\n. In addition to improvements in \\nthe clarity of presentation, we added coverage of slope change codes, expanded the \\nexplanation of skeletons, medial axes, and the distance transform, and added sev-\\neral new basic descriptors of compactness, circularity, and eccentricity. New mate-\\nrial includes coverage of the Harris-Stephens corner detector, and a presentation of \\nmaximally stable extremal regions. A major addition to the chapter is a comprehen-\\nsive discussion dealing with the Scale-Invariant Feature Transform (SIFT). The new \\nmaterial is complemented by 65 new images, 15 new drawings, and 12 new exercises.\\nDIP4E_GLOBAL_Print_Ready.indb   10\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 11}),\n",
       " Document(page_content='Preface\\n    \\n11\\nChapter 12:\\n This chapter underwent a major revision to include an extensive \\nrewrite of neural networks and deep learning\\n, an area that has grown significantly \\nsince the last edition of the book. We added a comprehensive discussion on fully \\nconnected, deep neural networks that includes derivation of backpropagation start-\\ning from basic principles. The equations of backpropagation were expressed in “tra-\\nditional” scalar terms, and then generalized into a compact set of matrix equations \\nideally suited for implementation of deep neural nets. The effectiveness of fully con-\\nnected networks was demonstrated with several examples that included a compari-\\nson with the Bayes classifier. One of the most-requested topics in the survey was \\ncoverage of deep convolutional neural networks. We added an extensive section on \\nthis, following the same blueprint we used for deep, fully connected nets. That is, we \\nderived the equations of backpropagation for convolutional nets, and showed how \\nthey are different from “traditional” backpropagation. We then illustrated the use of \\nconvolutional networks with simple images, and applied them to large image data-\\nbases of numerals and natural scenes.  The written material is complemented by 23 \\nnew images, 28 new drawings, and 12 new exercises.\\nAlso for the first time, we have created student and faculty support packages that \\ncan be downloaded from the book website. The \\nStudent Support Package\\n contains \\nmany of the original images in the book and answers to selected exercises The \\nFac-\\nulty Support Package\\n contains solutions to all exercises, teaching suggestions, and all \\nthe art in the book in the form of modifiable PowerPoint slides. One support pack-\\nage is made available with every new book, free of charge. \\nThe book website, established during the launch of the 2002 edition, continues to \\nbe a success, attracting more than 25,000 visitors each month. The site was upgraded \\nfor the launch of this edition. For more details on site features and content, see \\nThe \\nBook Website\\n, following the \\nAcknowledgments\\n section.\\nThis edition of \\nDigital Image Processing\\n is a reﬂection of how the educational \\nneeds of our readers have changed since 2008. As is usual in an endeavor such as \\nthis, progress in the ﬁeld continues after work on the manuscript stops. One of the \\nreasons why this book has been so well accepted since it ﬁrst appeared in 1977 is its \\ncontinued emphasis on fundamental concepts that retain their relevance over time. \\nThis approach, among other things, attempts to provide a measure of stability in a \\nrapidly evolving body of knowledge. We have tried to follow the same principle in \\npreparing this edition of the book.\\nR.C.G.\\nR.E.W.\\nDIP4E_GLOBAL_Print_Ready.indb   11\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 12}),\n",
       " Document(page_content='12\\n    \\nAcknowledgments\\nAcknowledgments\\nWe are indebted to a number of individuals in academic circles, industry, and gov-\\nernment who have contributed to this edition of the book. In particular, we wish \\nto extend our appreciation to Hairong Qi and her students, Zhifei Zhang and \\nChengcheng Li, for their valuable review of the material on neural networks, and for \\ntheir help in generating examples for that material. We also want to thank Ernesto \\nBribiesca Correa for providing and reviewing material on slope chain codes, and \\nDirk Padfield for his many suggestions and review of several chapters in the book. \\nWe appreciate Michel Kocher’s many thoughtful comments and suggestions over \\nthe years on how to improve the book. Thanks also to Steve Eddins for his sugges-\\ntions on MATLAB and related software issues.\\nNumerous individuals have contributed to material carried over from the previ-\\nous to the current edition of the book. Their contributions have been important in so \\nmany different ways that we ﬁnd it difﬁcult to acknowledge them in any other way \\nbut alphabetically. We thank Mongi A. Abidi, Yongmin Kim, Bryan Morse, Andrew \\nOldroyd, Ali M. Reza, Edgardo Felipe Riveron, Jose Ruiz Shulcloper, and Cameron \\nH.G. Wright for their many suggestions on how to improve the presentation and/or \\nthe scope of coverage in the book. We are also indebted to Naomi Fernandes at the \\nMathWorks for providing us with MATLAB software and support that were impor-\\ntant in our ability to create many of the examples and experimental results included \\nin this edition of the book.\\nA signiﬁcant percentage of the new images used in this edition (and in some \\ncases their history and interpretation) were obtained through the efforts of indi-\\nviduals whose contributions are sincerely appreciated. In particular, we wish to \\nacknowledge the efforts of Serge Beucher, Uwe Boos, Michael E. Casey, Michael \\nW. Davidson, Susan L. Forsburg, Thomas R. Gest, Daniel A. Hammer, Zhong He, \\nRoger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, \\nRhonda Knighton, Don P . Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, \\nMichael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David \\nK. Wehe, and Robert A. West. We also wish to acknowledge other individuals and \\norganizations cited in the captions of numerous ﬁgures throughout the book for \\ntheir permission to use that material. \\nWe also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for \\ntheir support and signiﬁcant patience during the production of the book.\\nR.C.G.\\nR.E.W.\\nDIP4E_GLOBAL_Print_Ready.indb   12\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 13}),\n",
       " Document(page_content='The Book Website\\nwww.ImageProcessingPlace.com\\nDigital Image Processing\\n is a completely self-contained book.\\n However, the compan-\\nion website offers additional support in a number of important areas.\\nFor the Student or Independent Reader the site contains\\n• \\nReviews in areas such as probability, statistics, vectors, and matrices.\\n• \\nA Tutorials section containing dozens of tutorials on topics relevant to the mate-\\nrial in the book.\\n• \\nAn image database containing all the images in the book, as well as many other \\nimage databases\\n.\\nFor the Instructor the site contains\\n• \\nAn Instructor’s Manual with complete solutions to all the problems.\\n• \\nClassroom presentation materials in modifiable PowerPoint format.\\n• \\nMaterial removed from previous editions, downloadable in convenient PDF \\nformat.\\n• \\nNumerous links to other educational resources.\\nF\\nor the Practitioner the site contains additional specialized topics such as\\n• \\nLinks to commercial sites.\\n• \\nSelected new references.\\n• \\nLinks to commercial image databases.\\nT\\nhe website is an ideal tool for keeping the book current between editions by includ-\\ning new topics, digital images, and other relevant material that has appeared after \\nthe book was published. Although considerable care was taken in the production \\nof the book, the website is also a convenient repository for any errors discovered \\nbetween printings. \\nThe DIP4E Support Packages\\nIn this edition, we created support packages for students and faculty to organize \\nall the classroom support materials available for the new edition of the book into \\none easy download. The Student Support Package contains many of the original \\nimages in the book, and answers to selected exercises, The Faculty Support Package \\ncontains solutions to all exercises, teaching suggestions, and all the art in the book \\nin modifiable PowerPoint slides. One support package is made available with every \\nnew book, free of charge. Applications for the support packages are submitted at \\nthe book website.\\nDIP4E_GLOBAL_Print_Ready.indb   13\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 14}),\n",
       " Document(page_content='About the Authors\\nRAFAEL C. GONZALEZ\\nR. C. Gonzalez received the B.S.E.E. degree from the University of Miami in 1965 \\nand the M.E. and Ph.D. degrees in electrical engineering from the University of \\nFlorida, Gainesville, in 1967 and 1970, respectively. He joined the Electrical and \\nComputer Science Department at the University of Tennessee, Knoxville (UTK) in \\n1970, where he became Associate Professor in 1973, Professor in 1978, and Distin-\\nguished Service Professor in 1984. He served as Chairman of the department from \\n1994 through 1997. He is currently a Professor Emeritus at UTK.\\nGonzalez is the founder of the Image & Pattern Analysis Laboratory and the \\nRobotics & Computer Vision Laboratory at the University of Tennessee. He also \\nfounded Perceptics Corporation in 1982 and was its president until 1992. The last \\nthree years of this period were spent under a full-time employment contract with \\nWestinghouse Corporation, who acquired the company in 1989. \\nUnder his direction, Perceptics became highly successful in image processing, \\ncomputer vision, and laser disk storage technology. In its initial ten years, Perceptics \\nintroduced a series of innovative products, including: The world’s ﬁrst commercially \\navailable computer vision system for automatically reading license plates on moving \\nvehicles; a series of large-scale image processing and archiving systems used by the \\nU.S. Navy at six different manufacturing sites throughout the country to inspect the \\nrocket motors of missiles in the Trident II Submarine Program; the market-leading \\nfamily of imaging boards for advanced Macintosh computers; and a line of trillion-\\nbyte laser disk products.\\nHe is a frequent consultant to industry and government in the areas of pattern \\nrecognition, image processing, and machine learning. His academic honors for work \\nin these ﬁelds include the 1977 UTK College of Engineering Faculty Achievement \\nAward; the 1978 UTK Chancellor’s Research Scholar Award; the 1980 Magnavox \\nEngineering Professor Award; and the 1980 M.E. Brooks Distinguished Professor \\nAward. In 1981 he became an IBM Professor at the University of Tennessee and \\nin 1984 he was named a Distinguished Service Professor there. He was awarded a \\nDistinguished Alumnus Award by the University of Miami in 1985, the Phi Kappa \\nPhi Scholar Award in 1986, and the University of Tennessee’s Nathan W. Dougherty \\nAward for Excellence in Engineering in 1992.\\nHonors for industrial accomplishment include the 1987 IEEE Outstanding Engi-\\nneer Award for Commercial Development in Tennessee; the 1988 Albert Rose \\nNational Award for Excellence in Commercial Image Processing; the 1989 B. Otto \\nWheeley Award for Excellence in Technology Transfer; the 1989 Coopers and \\nLybrand Entrepreneur of the Year Award; the 1992 IEEE Region 3 Outstanding \\nEngineer Award; and the 1993 Automated Imaging Association National Award for \\nTechnology Development.\\nGonzalez is author or co-author of over 100 technical articles, two edited books, \\nand four textbooks in the ﬁelds of pattern recognition, image processing, and robot-\\nics. His books are used in over 1000 universities and research institutions throughout \\nDIP4E_GLOBAL_Print_Ready.indb   14\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 15}),\n",
       " Document(page_content='the world. He is listed in the prestigious \\nMarquis Who’s Who in America\\n, \\nMarquis \\nWho’s Who in Engineering\\n, \\nMarquis Who’s Who in the World\\n, and in 10 other national \\nand international biographical citations. He is the co-holder of two U.S. Patents, and \\nhas been an associate editor of the \\nIEEE Transactions on Systems, Man and Cyber-\\nnetics\\n, and the \\nInternational Journal of Computer and Information Sciences\\n. He is a \\nmember of numerous professional and honorary societies, including Tau Beta Pi, Phi \\nKappa Phi, Eta Kappa Nu, and Sigma Xi. He is a Fellow of the IEEE.\\nRICHARD E. WOODS\\nR. E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical Engineering from \\nthe University of Tennessee, Knoxville in 1975, 1977, and 1980, respectively. He \\nbecame an Assistant Professor of Electrical Engineering and Computer Science in \\n1981 and was recognized as a Distinguished Engineering Alumnus in 1986.\\nA veteran hardware and software developer, Dr. Woods has been involved in \\nthe founding of several high-technology startups, including Perceptics Corporation, \\nwhere he was responsible for the development of the company’s quantitative image \\nanalysis and autonomous decision-making products; MedData Interactive, a high-\\ntechnology company specializing in the development of handheld computer systems \\nfor medical applications; and Interapptics, an internet-based company that designs \\ndesktop and handheld computer applications.\\nDr. Woods currently serves on several nonproﬁt educational and media-related \\nboards, including Johnson University, and was recently a summer English instructor \\nat the Beijing Institute of Technology. He is the holder of a U.S. Patent in the area \\nof digital image processing and has published two textbooks, as well as numerous \\narticles related to digital signal processing. Dr. Woods is a member of several profes-\\nsional societies, including Tau Beta Pi, Phi Kappa Phi, and the IEEE.\\nDIP4E_GLOBAL_Print_Ready.indb   15\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 16}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 17}),\n",
       " Document(page_content='171\\nIntroduction\\nOne picture is worth more than ten thousand words.\\nAnonymous\\nPreview\\nInterest in digital image processing methods stems from two principal application areas: improvement \\nof pictorial information for human interpretation, and processing of image data for tasks such as storage, \\ntransmission, and extraction of pictorial information. This chapter has several objectives: (1) to deﬁne \\nthe scope of the ﬁeld that we call image processing; (2) to give a historical perspective of the origins of \\nthis ﬁeld; (3) to present an overview of the state of the art in image processing by examining some of \\nthe principal areas in which it is applied; (4) to discuss brieﬂy the principal approaches used in digital \\nimage processing; (5) to give an overview of the components contained in a typical, general-purpose \\nimage processing system; and (6) to provide direction to the literature where image processing work is \\nreported. The material in this chapter is extensively illustrated with a range of images that are represen-\\ntative of the images we will be using throughout the book.\\nUpon completion of this chapter, readers should:\\n Understand the concept of a digital image.\\n Have a broad overview of the historical under-\\npinnings of the ﬁeld of digital image process-\\ning.\\n Understand the deﬁnition and scope of digi-\\ntal image processing.\\n Know the fundamentals of the electromag-\\nnetic spectrum and its relationship to image \\ngeneration.\\n Be aware of the different ﬁelds in which digi-\\ntal image processing methods are applied.\\n Be familiar with the basic processes involved \\nin image processing.\\n Be familiar with the components that make \\nup a general-purpose digital image process-\\ning system.\\n Be familiar with the scope of the literature \\nwhere image processing work is reported.\\nDIP4E_GLOBAL_Print_Ready.indb   17\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 18}),\n",
       " Document(page_content='18\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\nAn image may be defined as a two-dimensional function, \\nfx y\\n(,\\n) ,\\n where \\nx\\n and \\ny\\n are \\nspatial\\n (plane) coordinates\\n, and the amplitude of \\nf\\n at any pair of coordinates \\n(,)\\nxy\\n \\nis called the \\nintensity\\n or \\ngra\\ny level\\n of the image at that point. When \\nx\\n, \\ny\\n, and the \\nintensity values of \\nf\\n are all finite, discrete quantities, we call the image a \\ndigital image\\n. \\nThe field of \\ndigital image processing\\n refers to processing digital images by means of \\na digital computer. Note that a digital image is composed of a finite number of ele-\\nments, each of which has a particular location and value. These elements are called \\npicture elements\\n, \\nimage elements\\n, \\npels\\n, and \\npixels\\n. \\nPixel\\n is the term used most widely \\nto denote the elements of a digital image. We will consider these definitions in more \\nformal terms in Chapter 2.\\nVision is the most advanced of our senses, so it is not surprising that images \\nplay the single most important role in human perception. However, unlike humans, \\nwho are limited to the visual band of the electromagnetic (EM) spectrum, imaging \\nmachines cover almost the entire EM spectrum, ranging from gamma to radio waves. \\nThey can operate on images generated by sources that humans are not accustomed \\nto associating with images. These include ultrasound, electron microscopy, and com-\\nputer-generated images. Thus, digital image processing encompasses a wide and var-\\nied ﬁeld of applications.\\nThere is no general agreement among authors regarding where image process-\\ning stops and other related areas, such as \\nimage analysis\\n and \\ncomputer vision\\n, start. \\nSometimes, a distinction is made by deﬁning image processing as a discipline in \\nwhich both the input and output of a process are images. We believe this to be a \\nlimiting and somewhat artiﬁcial boundary. For example, under this deﬁnition, even \\nthe trivial task of computing the average intensity of an image (which yields a sin-\\ngle number) would not be considered an image processing operation. On the other \\nhand, there are ﬁelds such as computer vision whose ultimate goal is to use comput-\\ners to emulate human vision, including learning and being able to make inferences \\nand take actions based on visual inputs. This area itself is a branch of \\nartiﬁcial intel-\\nligence\\n (AI) whose objective is to emulate human intelligence. The ﬁeld of AI is in its \\nearliest stages of infancy in terms of development, with progress having been much \\nslower than originally anticipated. The area of image analysis (also called \\nimage \\nunderstanding\\n) is in between image processing and computer vision.\\nThere are no clear-cut boundaries in the continuum from image processing at \\none end to computer vision at the other. However, one useful paradigm is to con-\\nsider three types of computerized processes in this continuum: low-, mid-, and high-\\nlevel processes. Low-level processes involve primitive operations such as image \\npreprocessing to reduce noise, contrast enhancement, and image sharpening. A low-\\nlevel process is characterized by the fact that both its inputs and outputs are images. \\nMid-level processing of images involves tasks such as segmentation (partitioning \\nan image into regions or objects), description of those objects to reduce them to a \\nform suitable for computer processing, and classiﬁcation (recognition) of individual \\nobjects. A mid-level process is characterized by the fact that its inputs generally \\nare images, but its outputs are attributes extracted from those images (e.g., edges, \\ncontours, and the identity of individual objects). Finally, higher-level processing \\n1.1\\nDIP4E_GLOBAL_Print_Ready.indb   18\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 19}),\n",
       " Document(page_content='1.2\\n  \\nThe Origins of Digital Image Processing\\n    \\n19\\ninvolves “making sense” of an \\nensemble of recognized objects, as in image analysis, \\nand, at the far end of the continuum, performing the cognitive functions normally \\nassociated with human vision.\\nBased on the preceding comments, we see that a logical place of overlap between \\nimage processing and image analysis is the area of recognition of individual regions \\nor objects in an image. Thus, what we call in this book \\ndigital image processing\\n encom-\\npasses processes whose inputs and outputs are images and, in addition, includes pro-\\ncesses that extract attributes from images up to, and including, the recognition of \\nindividual objects. As an illustration to clarify these concepts, consider the area of \\nautomated analysis of text. The processes of acquiring an image of the area con-\\ntaining the text, preprocessing that image, extracting (segmenting) the individual \\ncharacters, describing the characters in a form suitable for computer processing, and \\nrecognizing those individual characters are in the scope of what we call digital image \\nprocessing in this book. Making sense of the content of the page may be viewed as \\nbeing in the domain of image analysis and even computer vision, depending on the \\nlevel of complexity implied by the statement “making sense of.” As will become \\nevident shortly, digital image processing, as we have deﬁned it, is used routinely in a \\nbroad range of areas of exceptional social and economic value. The concepts devel-\\noped in the following chapters are the foundation for the methods used in those \\napplication areas.\\n1.2 THE ORIGINS OF DIGITAL IMAGE PROCESSING  \\nOne of the earliest applications of digital images was in the newspaper industry, \\nwhen pictures were first sent by submarine cable between London and New York. \\nIntroduction of the Bartlane cable picture transmission system in the early 1920s \\nreduced the time required to transport a picture across the Atlantic from more than \\na week to less than three hours. Specialized printing equipment coded pictures for \\ncable transmission, then reconstructed them at the receiving end. Figure 1.1 was \\ntransmitted in this way and reproduced on a telegraph printer fitted with typefaces \\nsimulating a halftone pattern. \\nSome of the initial problems in improving the visual quality of these early digital \\npictures were related to the selection of printing procedures and the distribution of \\n1.2\\nFIGURE 1.1\\n A digital picture produced in 1921 from a coded tape by a telegraph printer with \\nspecial typefaces. (McFarlane.) [References in the bibliography at the end of the book are \\nlisted in alphabetical order by authors’ last names.]\\nDIP4E_GLOBAL_Print_Ready.indb   19\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 20}),\n",
       " Document(page_content='20\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nintensity levels. The printing method used to obtain Fig. 1.1 was abandoned toward \\nthe end of 1921 in favor of a technique based on photographic reproduction made \\nfrom tapes perforated at the telegraph receiving terminal. Figure 1.2 shows an image \\nobtained using this method. The improvements over Fig. 1.1 are evident, both in \\ntonal quality and in resolution.\\nThe early Bartlane systems were capable of coding images in ﬁve distinct levels \\nof gray. This capability was increased to 15 levels in 1929. Figure 1.3 is typical of the \\ntype of images that could be obtained using the 15-tone equipment. During this \\nperiod, introduction of a system for developing a ﬁlm plate via light beams that were \\nmodulated by the coded picture tape improved the reproduction process consider-\\nably.\\nAlthough the examples just cited involve digital images, they are not considered \\ndigital image processing results in the context of our deﬁnition, because digital com-\\nputers were not used in their creation. Thus, the history of digital image processing \\nis intimately tied to the development of the digital computer. In fact, digital images \\nrequire so much storage and computational power that progress in the ﬁeld of digi-\\ntal image processing has been dependent on the development of digital computers \\nand of supporting technologies that include data storage, display, and transmission.\\nFIGURE 1.2\\nA digital picture \\nmade in 1922 \\nfrom a tape \\npunched after \\nthe signals had \\ncrossed the  \\nAtlantic twice. \\n(McFarlane.)\\nFIGURE 1.3\\nUnretouched \\ncable picture of \\nGenerals Pershing \\n(right) and Foch,  \\ntransmitted in \\n1929 from  \\nLondon to New \\nYork by 15-tone \\nequipment. \\n(McFarlane.)\\nDIP4E_GLOBAL_Print_Ready.indb   20\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 21}),\n",
       " Document(page_content='1.2\\n  \\nThe Origins of Digital Image Processing\\n    \\n21\\nThe concept of a computer dates back to the invention of the abacus in Asia \\nMinor, more than 5000 years ago. More recently, there have been developments \\nin the past two centuries that are the foundation of what we call a computer today. \\nHowever, the basis for what we call a \\nmodern\\n digital computer dates back to only \\nthe 1940s, with the introduction by John von Neumann of two key concepts: (1) a \\nmemory to hold a stored program and data, and (2) conditional branching. These \\ntwo ideas are the foundation of a central processing unit (CPU), which is at the heart \\nof computers today. Starting with von Neumann, there were a series of key advanc-\\nes that led to computers powerful enough to be used for digital image processing. \\nBrieﬂy, these advances may be summarized as follows: (1) the invention of the tran-\\nsistor at Bell Laboratories in 1948; (2) the development in the 1950s and 1960s of \\nthe high-level programming languages COBOL (Common Business-Oriented Lan-\\nguage) and FORTRAN (Formula Translator); (3) the invention of the integrated \\ncircuit (IC) at Texas Instruments in 1958; (4) the development of operating systems \\nin the early 1960s; (5) the development of the microprocessor (a single chip consist-\\ning of a CPU, memory, and input and output controls) by Intel in the early 1970s; \\n(6) the introduction by IBM of the personal computer in 1981; and (7) progressive \\nminiaturization of components, starting with large-scale integration (LI) in the late \\n1970s, then very-large-scale integration (VLSI) in the 1980s, to the present use of \\nultra-large-scale integration (ULSI) and experimental nonotechnologies. Concur-\\nrent with these advances were developments in the areas of mass storage and display \\nsystems, both of which are fundamental requirements for digital image processing.\\nThe ﬁrst computers powerful enough to carry out meaningful image processing \\ntasks appeared in the early 1960s. The birth of what we call digital image processing \\ntoday can be traced to the availability of those machines, and to the onset of the \\nspace program during that period. It took the combination of those two develop-\\nments to bring into focus the potential of digital image processing for solving prob-\\nlems of practical signiﬁcance. Work on using computer techniques for improving \\nimages from a space probe began at the Jet Propulsion Laboratory (Pasadena, Cali-\\nfornia) in 1964, when pictures of the moon transmitted by \\nRanger 7\\n were processed \\nby a computer to correct various types of image distortion inherent in the on-board \\ntelevision camera. Figure 1.4 shows the ﬁrst image of the moon taken by \\nRanger \\n7\\n on July 31, 1964 at 9:09 A.M. Eastern Daylight Time (EDT), about 17 minutes \\nbefore impacting the lunar surface (the markers, called \\nreseau marks\\n, are used for \\ngeometric corrections, as discussed in Chapter 2).This also is the ﬁrst image of the \\nmoon taken by a U.S. spacecraft. The imaging lessons learned with \\nRanger 7\\n served \\nas the basis for improved methods used to enhance and restore images from the Sur-\\nveyor missions to the moon, the \\nMariner\\n series of ﬂyby missions to Mars, the \\nApollo \\nmanned ﬂights to the moon, and others.\\nIn parallel with space applications, digital image processing techniques began in \\nthe late 1960s and early 1970s to be used in medical imaging, remote Earth resourc-\\nes observations, and astronomy. The invention in the early 1970s of \\ncomputerized \\naxial tomography\\n (CAT), also called \\ncomputerized tomography\\n (CT) for short, is \\none of the most important events in the application of image processing in medical \\ndiagnosis. Computerized axial tomography is a process in which a ring of detectors \\nDIP4E_GLOBAL_Print_Ready.indb   21\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 22}),\n",
       " Document(page_content='22\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nencircles an object (or patient) and an X-ray source, concentric with the detector \\nring, rotates about the object. The X-rays pass through the object and are collected \\nat the opposite end by the corresponding detectors in the ring. This procedure is \\nrepeated the source rotates. Tomography consists of algorithms that use the sensed \\ndata to construct an image that represents a “slice” through the object. Motion of \\nthe object in a direction perpendicular to the ring of detectors produces a set of \\nsuch slices, which constitute a three-dimensional (3-D) rendition of the inside of the \\nobject. Tomography was invented independently by Sir Godfrey N. Hounsﬁeld and \\nProfessor Allan M. Cormack, who shared the 1979 Nobel Prize in Medicine for their \\ninvention. It is interesting to note that X-rays were discovered in 1895 by Wilhelm \\nConrad Roentgen, for which he received the 1901 Nobel Prize for Physics. These two \\ninventions, nearly 100 years apart, led to some of the most important applications of \\nimage processing today.\\nFrom the 1960s until the present, the ﬁeld of image processing has grown vigor-\\nously. In addition to applications in medicine and the space program, digital image \\nprocessing techniques are now used in a broad range of applications. Computer pro-\\ncedures are used to enhance the contrast or code the intensity levels into color for \\neasier interpretation of X-rays and other images used in industry, medicine, and the \\nbiological sciences. Geographers use the same or similar techniques to study pollu-\\ntion patterns from aerial and satellite imagery. Image enhancement and restoration \\nprocedures are used to process degraded images of unrecoverable objects, or experi-\\nmental results too expensive to duplicate. In archeology, image processing meth-\\nods have successfully restored blurred pictures that were the only available records \\nof rare artifacts lost or damaged after being photographed. In physics and related \\nﬁelds, computer techniques routinely enhance images of experiments in areas such \\nas high-energy plasmas and electron microscopy. Similarly successful applications \\nof image processing concepts can be found in astronomy, biology, nuclear medicine, \\nlaw enforcement, defense, and industry.\\nFIGURE 1.4\\nThe ﬁrst picture \\nof the moon by \\na U.S. spacecraft. \\nRanger 7\\n took \\nthis image on \\nJuly 31, 1964 at \\n9:09 A.M. EDT, \\nabout 17 minutes \\nbefore impacting \\nthe lunar surface. \\n(Courtesy of \\nNASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   22\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 23}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n23\\nThese examples illustrate processing results intended for human interpretation. \\nThe second major area of application of digital image processing techniques men-\\ntioned at the beginning of this chapter is in solving problems dealing with machine \\nperception. In this case, interest is on procedures for extracting information from \\nan image, in a form suitable for computer processing. Often, this information bears \\nlittle resemblance to visual features that humans use in interpreting the content \\nof an image. Examples of the type of information used in machine perception are \\nstatistical moments, Fourier transform coefﬁcients, and multidimensional distance \\nmeasures. Typical problems in machine perception that routinely utilize image pro-\\ncessing techniques are automatic character recognition, industrial machine vision \\nfor product assembly and inspection, military recognizance, automatic processing of \\nﬁngerprints, screening of X-rays and blood samples, and machine processing of aer-\\nial and satellite imagery for weather prediction and environmental assessment. The \\ncontinuing decline in the ratio of computer price to performance, and the expansion \\nof networking and communication bandwidth via the internet, have created unprec-\\nedented opportunities for continued growth of digital image processing. Some of \\nthese application areas will be illustrated in the following section.\\n1.3 EXAMPLES OF FIELDS THAT USE DIGITAL IMAGE PROCESSING  \\nToday, there is almost no area of technical endeavor that is not impacted in some \\nway by digital image processing. We can cover only a few of these applications in the \\ncontext and space of the current discussion. However, limited as it is, the material \\npresented in this section will leave no doubt in your mind regarding the breadth and \\nimportance of digital image processing. We show in this section numerous areas of \\napplication, each of which routinely utilizes the digital image processing techniques \\ndeveloped in the following chapters. Many of the images shown in this section are \\nused later in one or more of the examples given in the book. Most images shown are \\ndigital images. \\nThe areas of application of digital image processing are so varied that some form \\nof organization is desirable in attempting to capture the breadth of this ﬁeld. One \\nof the simplest ways to develop a basic understanding of the extent of image pro-\\ncessing applications is to categorize images according to their source (e.g., X-ray, \\nvisual, infrared, and so on).The principal energy source for images in use today is \\nthe electromagnetic energy spectrum. Other important sources of energy include \\nacoustic, ultrasonic, and electronic (in the form of electron beams used in electron \\nmicroscopy). Synthetic images, used for modeling and visualization, are generated \\nby computer. In this section we will discuss brieﬂy how images are generated in \\nthese various categories, and the areas in which they are applied. Methods for con-\\nverting images into digital form will be discussed in the next chapter.\\nImages based on radiation from the EM spectrum are the most familiar, espe-\\ncially images in the X-ray and visual bands of the spectrum. Electromagnetic waves \\ncan be conceptualized as propagating sinusoidal waves of varying wavelengths, or \\nthey can be thought of as a stream of massless particles, each traveling in a wavelike \\npattern and moving at the speed of light. Each massless particle contains a certain \\namount (or bundle) of energy. Each bundle of energy is called a \\nphoton\\n. If spectral \\n1.3\\nDIP4E_GLOBAL_Print_Ready.indb   23\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 24}),\n",
       " Document(page_content='24\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nbands are grouped according to energy per photon, we obtain the spectrum shown \\nin Fig. 1.5, ranging from gamma rays (highest energy) at one end to radio waves \\n(lowest energy) at the other. The bands are shown shaded to convey the fact that \\nbands of the EM spectrum are not distinct, but rather transition smoothly from one \\nto the other.\\nGAMMA-RAY IMAGING\\nMajor uses of imaging based on gamma rays include nuclear medicine and astro-\\nnomical observations. In nuclear medicine, the approach is to inject a patient with a \\nradioactive isotope that emits gamma rays as it decays. Images are produced from \\nthe emissions collected by gamma-ray detectors. Figure 1.6(a) shows an image of a \\ncomplete bone scan obtained by using gamma-ray imaging. Images of this sort are \\nused to locate sites of bone pathology, such as infections or tumors. Figure 1.6(b) \\nshows another major modality of nuclear imaging called \\npositron emission tomogra-\\nphy\\n (PET). The principle is the same as with X-ray tomography, mentioned briefly \\nin Section 1.2. However, instead of using an external source of X-ray energy, the \\npatient is given a radioactive isotope that emits positrons as it decays. When a pos-\\nitron meets an electron, both are annihilated and two gamma rays are given off. \\nThese are detected and a tomographic image is created using the basic principles of \\ntomography. The image shown in Fig. 1.6(b) is one sample of a sequence that con-\\nstitutes a 3-D rendition of the patient. This image shows a tumor in the brain and \\nanother in the lung, easily visible as small white masses.\\nA star in the constellation of Cygnus exploded about 15,000 years ago, generat-\\ning a superheated, stationary gas cloud (known as the Cygnus Loop) that glows in \\na spectacular array of colors. Figure 1.6(c) shows an image of the Cygnus Loop in \\nthe gamma-ray band. Unlike the two examples in Figs. 1.6(a) and (b), this image was \\nobtained using the natural radiation of the object being imaged. Finally, Fig. 1.6(d) \\nshows an image of gamma radiation from a valve in a nuclear reactor. An area of \\nstrong radiation is seen in the lower left side of the image.\\nX-RAY IMAGING\\nX-rays are among the oldest sources of EM radiation used for imaging. The best \\nknown use of X-rays is medical diagnostics, but they are also used extensively in \\nindustry and other areas, such as astronomy. X-rays for medical and industrial imag-\\ning are generated using an X-ray tube, which is a vacuum tube with a cathode and \\nanode. The cathode is heated, causing free electrons to be released. These electrons \\nflow at high speed to the positively charged anode. When the electrons strike a \\n10\\n/H11002\\n9\\n10\\n/H11002\\n8\\n10\\n/H11002\\n7\\n10\\n/H11002\\n6\\n10\\n/H11002\\n5\\n10\\n/H11002\\n4\\n10\\n/H11002\\n3\\n10\\n/H11002\\n2\\n10\\n0\\n10\\n/H11002\\n1\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\nEnergy of one photon (electron volts)\\nGamma rays X-rays Ultraviolet Visible Infrared Microwaves\\nRadio waves\\nFIGURE 1.5\\n The electromagnetic spectrum arranged according to energy per photon.\\nDIP4E_GLOBAL_Print_Ready.indb   24\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 25}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n25\\nnucleus, energy is released in the form of X-ray radiation. The energy (penetrat-\\ning power) of X-rays is controlled by a voltage applied across the anode, and by a \\ncurrent applied to the filament in the cathode. Figure 1.7(a) shows a familiar chest \\nX-ray generated simply by placing the patient between an X-ray source and a film \\nsensitive to X-ray energy. The intensity of the X-rays is modified by absorption as \\nthey pass through the patient, and the resulting energy falling on the film develops it, \\nmuch in the same way that light develops photographic film. In digital radiography, \\nb a\\nd c\\nFIGURE 1.6\\nExamples of \\ngamma-ray  \\nimaging.  \\n(a) Bone scan.  \\n(b) PET image. \\n(c) Cygnus Loop. \\n(d) Gamma radia-\\ntion (bright spot) \\nfrom a reactor \\nvalve.  \\n(Images  \\ncourtesy of  \\n(a) G.E. Medical \\nSystems; (b) Dr. \\nMichael E. Casey, \\nCTI PET Systems; \\n(c) NASA;  \\n(d) Professors \\nZhong He and \\nDavid K. Wehe,  \\nUniversity of \\nMichigan.) \\nDIP4E_GLOBAL_Print_Ready.indb   25\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 26}),\n",
       " Document(page_content='26\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\ndigital images are obtained by one of two methods: (1) by digitizing X-ray films; or; \\n(2) by having the X-rays that pass through the patient fall directly onto devices (such \\nas a phosphor screen) that convert X-rays to light. The light signal in turn is captured \\nby a light-sensitive digitizing system. We will discuss digitization in more detail in \\nChapters 2 and 4.\\nb\\na\\nd\\nc\\ne\\nFIGURE 1.7\\nExamples of \\nX-ray imaging.  \\n(a) Chest X-ray. \\n(b) Aortic  \\nangiogram.  \\n(c) Head CT.  \\n(d) Circuit boards. \\n(e) Cygnus Loop. \\n(Images courtesy \\nof (a) and (c) Dr. \\nDavid R. Pickens, \\nDept. of  \\nRadiology & \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center; \\n(b) Dr. Thomas \\nR. Gest, Division \\nof Anatomical \\nSciences, Univ. of \\nMichigan Medical \\nSchool;  \\n(d) Mr. Joseph \\nE. Pascente, Lixi, \\nInc.; and  \\n(e) NASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   26\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 27}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n27\\nAngiography is another major application in an area called contrast enhancement \\nradiography. This procedure is used to obtain images of blood vessels, called \\nangio-\\ngrams\\n. A catheter (a small, ﬂexible, hollow tube) is inserted, for example, into an \\nartery or vein in the groin. The catheter is threaded into the blood vessel and guided \\nto the area to be studied. When the catheter reaches the site under investigation, \\nan X-ray contrast medium is injected through the tube. This enhances the contrast \\nof the blood vessels and enables a radiologist to see any irregularities or blockages. \\nFigure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen \\nbeing inserted into the large blood vessel on the lower left of the picture. Note the \\nhigh contrast of the large vessel as the contrast medium ﬂows up in the direction of \\nthe kidneys, which are also visible in the image. As we will discuss further in Chapter 2, \\nangiography is a major area of digital image processing, where image subtraction is \\nused to further enhance the blood vessels being studied.\\nAnother important use of X-rays in medical imaging is computerized axial tomog-\\nraphy (CAT). Due to their resolution and 3-D capabilities, CAT scans revolution-\\nized medicine from the moment they ﬁrst became available in the early 1970s. As \\nnoted in Section 1.2, each CAT image is a “slice” taken perpendicularly through \\nthe patient. Numerous slices are generated as the patient is moved in a longitudinal \\ndirection. The ensemble of such images constitutes a 3-D rendition of the inside of \\nthe body, with the longitudinal resolution being proportional to the number of slice \\nimages taken. Figure 1.7(c) shows a typical CAT slice image of a human head.\\nTechniques similar to the ones just discussed, but generally involving higher \\nenergy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray \\nimage of an electronic circuit board. Such images, representative of literally hundreds \\nof industrial applications of X-rays, are used to examine circuit boards for ﬂaws in \\nmanufacturing, such as missing components or broken traces. Industrial CAT scans \\nare useful when the parts can be penetrated by X-rays, such as in plastic assemblies, \\nand even large bodies, such as solid-propellant rocket motors. Figure 1.7(e) shows an \\nexample of X-ray imaging in astronomy. This image is the Cygnus Loop of Fig. 1.6(c), \\nbut imaged in the X-ray band.\\nIMAGING IN THE ULTRAVIOLET BAND\\nApplications of ultraviolet “light” are varied. They include lithography, industrial \\ninspection, microscopy, lasers, biological imaging, and astronomical observations. \\nWe illustrate imaging in this band with examples from microscopy and astronomy.\\nUltraviolet light is used in ﬂuorescence microscopy, one of the fastest growing \\nareas of microscopy. Fluorescence is a phenomenon discovered in the middle of the \\nnineteenth century, when it was ﬁrst observed that the mineral ﬂuorspar ﬂuoresces \\nwhen ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but \\nwhen a photon of ultraviolet radiation collides with an electron in an atom of a ﬂuo-\\nrescent material, it elevates the electron to a higher energy level. Subsequently, the \\nexcited electron relaxes to a lower level and emits light in the form of a lower-energy \\nphoton in the visible (red) light region. Important tasks performed with a ﬂuores-\\ncence microscope are to use an excitation light to irradiate a prepared specimen, \\nand then to separate the much weaker radiating ﬂuorescent light from the brighter \\nDIP4E_GLOBAL_Print_Ready.indb   27\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 28}),\n",
       " Document(page_content='28\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nexcitation light. Thus, only the emission light reaches the eye or other detector. The \\nresulting ﬂuorescing areas shine against a dark background with sufﬁcient contrast \\nto permit detection. The darker the background of the nonﬂuorescing material, the \\nmore efﬁcient the instrument.\\nFluorescence microscopy is an excellent method for studying materials that can be \\nmade to ﬂuoresce, either in their natural form (primary ﬂuorescence) or when treat-\\ned with chemicals capable of ﬂuorescing (secondary ﬂuorescence). Figures 1.8(a) \\nand (b) show results typical of the capability of ﬂuorescence microscopy. Figure \\n1.8(a) shows a ﬂuorescence microscope image of normal corn, and Fig. 1.8(b) shows \\ncorn infected by “smut,” a disease of cereals, corn, grasses, onions, and sorghum that \\ncan be caused by any one of more than 700 species of parasitic fungi. Corn smut is \\nparticularly harmful because corn is one of the principal food sources in the world. \\nAs another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy \\nregion of the ultraviolet band.\\nIMAGING IN THE VISIBLE AND INFRARED BANDS\\nConsidering that the visual band of the electromagnetic spectrum is the most famil-\\niar in all our activities, it is not surprising that imaging in this band outweighs by far \\nall the others in terms of breadth of application. The infrared band often is used in \\nconjunction with visual imaging, so we have grouped the visible and infrared bands \\nin this section for the purpose of illustration. We consider in the following discus-\\nsion applications in light microscopy, astronomy, remote sensing, industry, and law \\nenforcement.\\nFigure 1.9 shows several examples of images obtained with a light microscope. \\nThe examples range from pharmaceuticals and microinspection to materials char-\\nacterization. Even in microscopy alone, the application areas are too numerous to \\ndetail here. It is not difﬁcult to conceptualize the types of processes one might apply \\nto these images, ranging from enhancement to measurements.\\nb a\\nc\\nFIGURE 1.8\\n Examples of ultraviolet imaging. (a) Normal corn. (b) Corn infected by smut. (c) Cygnus Loop. (Images \\n(a) and (b) courtesy of Dr. Michael W. Davidson, Florida State University, (c) NASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   28\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 29}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n29\\nAnother major area of visual processing is remote sensing, which usually includes \\nseveral bands in the visual and infrared regions of the spectrum. Table 1.1 shows the \\nso-called \\nthematic bands\\n in NASA’s LANDSAT satellites. The primary function of \\nLANDSAT is to obtain and transmit images of the Earth from space, for purposes \\nof monitoring environmental conditions on the planet. The bands are expressed in \\nterms of wavelength, with \\n1\\nm\\nm\\n being equal to \\n10\\n6\\n−\\n m (we will discuss the wave-\\nlength regions of the electromagnetic spectrum in more detail in Chapter 2). Note \\nthe characteristics and uses of each band in Table 1.1.\\nIn order to develop a basic appreciation for the power of this type of multispec-\\ntral imaging, consider Fig. 1.10, which shows one image for each of the spectral bands \\nin Table 1.1. The area imaged is Washington D.C., which includes features such as \\nbuildings, roads, vegetation, and a major river (the Potomac) going though the city. \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 1.9\\nExamples of light  \\nmicroscopy images.  \\n(a) Taxol (antican-\\ncer agent), magni-\\nﬁed \\n250\\n×\\n. \\n(b) Cholesterol—\\n40\\n×\\n.  \\n(c) Microproces-\\nsor—\\n60\\n×\\n.  \\n(d) Nickel oxide \\nthin ﬁlm—\\n600\\n×\\n. \\n(e) Surface of audio \\nCD—\\n1750\\n×\\n.  \\n(f) Organic super\\n-\\nconductor— \\n450\\n×\\n. \\n(Images courtesy of \\nDr\\n. Michael W.  \\nDavidson, Florida \\nState University.) \\nDIP4E_GLOBAL_Print_Ready.indb   29\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 30}),\n",
       " Document(page_content='30\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nImages of population centers are used over time to assess population growth and \\nshift patterns, pollution, and other factors affecting the environment. The differenc-\\nes between visual and infrared image features are quite noticeable in these images. \\nObserve, for example, how well deﬁned the river is from its surroundings in Bands \\n4 and 5.\\nWeather observation and prediction also are major applications of multispectral \\nimaging from satellites. For example, Fig. 1.11 is an image of Hurricane Katrina, one \\nof the most devastating storms in recent memory in the Western Hemisphere. This \\nimage was taken by a National Oceanographic and Atmospheric Administration \\n(NOAA) satellite using sensors in the visible and infrared bands. The eye of the hur-\\nricane is clearly visible in this image.\\nBand No. Name\\nWavelength \\n(\\nM\\nm)\\nCharacteristics and Uses\\n1 Visible blue\\n0.45– 0.52 Maximum water penetration\\n2 Visible green\\n0.53– 0.61 Measures plant vigor\\n3 Visible red\\n0.63– 0.69 Vegetation discrimination\\n4 Near infrared\\n0.78– 0.90 Biomass and shoreline mapping\\n5 Middle infrared 1.55–1.75 Moisture content: soil/vegetation\\n6 Thermal infrared 10.4–12.5 Soil moisture; thermal mapping\\n7 Short-wave infrared 2.09–2.35 Mineral mapping\\nTABLE \\n1.1\\nThematic bands \\nof NASA’s \\nLANDSAT  \\nsatellite.\\n123\\n4567\\nFIGURE 1.10\\n LANDSAT satellite images of the Washington, D.C. area. The numbers refer to the thematic bands in \\nTable 1.1. (Images courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   30\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 31}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n31\\nFigures 1.12 and 1.13 show an application of infrared imaging. These images are \\npart of the Nighttime Lights of the World data set, which provides a global inventory \\nof human settlements. The images were generated by an infrared imaging system \\nmounted on a NOAA/DMSP (Defense Meteorological Satellite Program) satel-\\nlite. The infrared system operates in the band 10.0 to 13.4 \\nm\\nm,\\n and has the unique \\ncapability to observe faint sources of visible\\n, near infrared emissions present on the  \\nEarth’s surface, including cities, towns, villages, gas ﬂares, and ﬁres. Even without \\nformal training in image processing, it is not difﬁcult to imagine writing a computer \\nprogram that would use these images to estimate the relative percent of total electri-\\ncal energy used by various regions of the world.\\nA major area of imaging in the visible spectrum is in automated visual inspection \\nof manufactured goods. Figure 1.14 shows some examples. Figure 1.14(a) is a con-\\ntroller board for a CD-ROM drive. A typical image processing task with products \\nsuch as this is to inspect them for missing parts (the black square on the top, right \\nquadrant of the image is an example of a missing component).\\nFigure 1.14(b) is an imaged pill container. The objective here is to have a machine \\nlook for missing, incomplete, or deformed pills. Figure 1.14(c) shows an application \\nin which image processing is used to look for bottles that are not ﬁlled up to an \\nacceptable level. Figure 1.14(d) shows a clear plastic part with an unacceptable num-\\nber of air pockets in it. Detecting anomalies like these is a major theme of industrial \\ninspection that includes other products, such as wood and cloth. Figure 1.14(e) shows \\na batch of cereal during inspection for color and the presence of anomalies such as \\nburned ﬂakes. Finally, Fig. 1.14(f) shows an image of an intraocular implant (replace-\\nment lens for the human eye). A “structured light” illumination technique was used \\nto highlight deformations toward the center of the lens, and other imperfections. For \\nexample, the markings at 1 o’clock and 5 o’clock are tweezer damage. Most of the \\nother small speckle detail is debris. The objective in this type of inspection is to ﬁnd \\ndamaged or incorrectly manufactured implants automatically, prior to packaging.\\nFIGURE 1.11\\nSatellite image of \\nHurricane Katrina \\ntaken on August \\n29, 2005.  \\n(Courtesy of \\nNOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   31\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 32}),\n",
       " Document(page_content='32\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nFigure 1.15 illustrates some additional examples of image processing in the vis-\\nible spectrum. Figure 1.15(a) shows a thumb print. Images of ﬁngerprints are rou-\\ntinely processed by computer, either to enhance them or to ﬁnd features that aid \\nin the automated search of a database for potential matches. Figure 1.15(b) shows \\nan image of paper currency. Applications of digital image processing in this area \\nFIGURE 1.12\\nInfrared  \\nsatellite images of \\nthe Americas. The \\nsmall shaded map \\nis provided for  \\nreference.  \\n(Courtesy of \\nNOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   32\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 33}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n33\\ninclude automated counting and, in law enforcement, the reading of the serial num-\\nber for the purpose of tracking and identifying currency bills. The two vehicle images \\nshown in Figs. 1.15(c) and (d) are examples of automated license plate reading. The \\nlight rectangles indicate the area in which the imaging system detected the plate. \\nThe black rectangles show the results of automatically reading the plate content by \\nthe system. License plate and other applications of character recognition are used \\nextensively for trafﬁc monitoring and surveillance.\\nIMAGING IN THE MICROWAVE BAND\\nThe principal application of imaging in the microwave band is radar. The unique \\nfeature of imaging radar is its ability to collect data over virtually any region at any \\ntime, regardless of weather or ambient lighting conditions. Some radar waves can \\npenetrate clouds, and under certain conditions, can also see through vegetation, ice, \\nand dry sand. In many cases, radar is the only way to explore inaccessible regions of \\nthe Earth’s surface. An imaging radar works like a flash camera in that it provides \\nits own illumination (microwave pulses) to illuminate an area on the ground and \\nFIGURE 1.13\\nInfrared  \\nsatellite images \\nof the remaining \\npopulated parts \\nof the world. The \\nsmall shaded map \\nis provided for \\nreference.  \\n(Courtesy of \\nNOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   33\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 34}),\n",
       " Document(page_content='34\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\ntake a snapshot image. Instead of a camera lens, a radar uses an antenna and digital \\ncomputer processing to record its images. In a radar image, one can see only the \\nmicrowave energy that was reflected back toward the radar antenna.\\nFigure 1.16 shows a spaceborne radar image covering a rugged mountainous area \\nof southeast Tibet, about 90 km east of the city of Lhasa. In the lower right cor-\\nner is a wide valley of the Lhasa River, which is populated by Tibetan farmers and \\nyak herders, and includes the village of Menba. Mountains in this area reach about \\n5800 m (19,000 ft) above sea level, while the valley ﬂoors lie about 4300 m (14,000 ft) \\nabove sea level. Note the clarity and detail of the image, unencumbered by clouds or \\nother atmospheric conditions that normally interfere with images in the visual band.\\nIMAGING IN THE RADIO BAND\\nAs in the case of imaging at the other end of the spectrum (gamma rays), the major \\napplications of imaging in the radio band are in medicine and astronomy. In medicine, \\nradio waves are used in magnetic resonance imaging (MRI). This technique places a \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 1.14\\n Some examples of manufactured goods checked using digital image processing. (a) Circuit board con-\\ntroller. (b) Packaged pills. (c) Bottles. (d) Air bubbles in a clear plastic product. (e) Cereal. (f) Image of intraocular \\nimplant. (Figure (f) courtesy of Mr. Pete Sites, Perceptics Corporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   34\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 35}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n35\\npatient in a powerful magnet and passes radio waves through the individual’s body \\nin short pulses. Each pulse causes a responding pulse of radio waves to be emitted \\nby the patient’s tissues. The location from which these signals originate and their \\nstrength are determined by a computer, which produces a two-dimensional image \\nof a section of the patient. MRI can produce images in any plane. Figure 1.17 shows \\nMRI images of a human knee and spine.\\nThe rightmost image in Fig. 1.18 is an image of the Crab Pulsar in the radio band. \\nAlso shown for an interesting comparison are images of the same region, but taken \\nin most of the bands discussed earlier. Observe that each image gives a totally dif-\\nferent “view” of the pulsar.\\nOTHER IMAGING MODALITIES\\nAlthough imaging in the electromagnetic spectrum is dominant by far, there are a \\nnumber of other imaging modalities that are also important. Specifically, we discuss \\nb\\na\\nd\\nc\\nFIGURE 1.15\\nSome additional \\nexamples of  \\nimaging in the  \\nvisible spectrum. \\n(a) Thumb print. \\n(b) Paper  \\ncurrency.  \\n(c) and (d) Auto-\\nmated license \\nplate reading.  \\n(Figure (a) \\ncourtesy of the \\nNational  \\nInstitute of  \\nStandards and \\nTechnology.  \\nFigures (c) and \\n(d) courtesy of \\nDr. Juan  \\nHerrera,  \\nPerceptics  \\nCorporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   35\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 36}),\n",
       " Document(page_content='36\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nin this section acoustic imaging, electron microscopy, and synthetic (computer-gen-\\nerated) imaging. \\nImaging using “sound” ﬁnds application in geological exploration, industry, and \\nmedicine. Geological applications use sound in the low end of the sound spectrum \\n(hundreds of Hz) while imaging in other areas use ultrasound (millions of Hz). The \\nmost important commercial applications of image processing in geology are in min-\\neral and oil exploration. For image acquisition over land, one of the main approaches \\nis to use a large truck and a large ﬂat steel plate. The plate is pressed on the \\nground by \\nFIGURE 1.16\\nSpaceborne radar \\nimage of  \\nmountainous \\nregion in  \\nsoutheast Tibet. \\n(Courtesy of \\nNASA.)\\nb a\\nFIGURE 1.17\\n MRI images of a human (a) knee, and (b) spine. (Figure (a) courtesy of Dr. Thom-\\nas R. Gest, Division of Anatomical Sciences, University of Michigan Medical School, and \\n(b) courtesy of Dr. David R. Pickens, Department of Radiology and Radiological Sciences, \\nVanderbilt University Medical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   36\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 37}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n37\\nthe truck, and the truck is vibrated through a frequency spectrum up to 100 Hz. The \\nstrength and speed of the returning sound waves are determined by the composi-\\ntion of the Earth below the surface. These are analyzed by computer, and images are \\ngenerated from the resulting analysis.\\nFor marine image acquisition, the energy source consists usually of two air guns \\ntowed behind a ship. Returning sound waves are detected by hydrophones placed \\nin cables that are either towed behind the ship, laid on the bottom of the ocean, \\nor hung from buoys (vertical cables). The two air guns are alternately pressurized  \\nto ~2000 psi and then set off. The constant motion of the ship provides a transversal \\ndirection of motion that, together with the returning sound waves, is used to gener-\\nate a 3-D map of the composition of the Earth below the bottom of the ocean.\\nFigure 1.19 shows a cross-sectional image of a well-known 3-D model against \\nwhich the performance of seismic imaging algorithms is tested. The arrow points to a \\nhydrocarbon (oil and/or gas) trap. This target is brighter than the surrounding layers \\nbecause the change in density in the target region is larger. Seismic interpreters look \\nfor these “bright spots” to ﬁnd oil and gas. The layers above also are bright, but their \\nbrightness does not vary as strongly across the layers. Many seismic reconstruction \\nalgorithms have difﬁculty imaging this target because of the faults above it.\\nAlthough ultrasound imaging is used routinely in manufacturing, the best known \\napplications of this technique are in medicine, especially in obstetrics, where fetuses \\nare imaged to determine the health of their development. A byproduct of this \\nGamma\\nX-ray\\nOptical\\nInfrared\\nRadio\\nFIGURE 1.18\\n Images of the Crab Pulsar (in the center of each image) covering the electromagnetic spectrum. (Cour-\\ntesy of NASA.)\\nFIGURE 1.19\\nCross-sectional \\nimage of a  \\nseismic model. \\nThe arrow points \\nto a hydrocarbon \\n(oil and/or gas) \\ntrap. (Courtesy of \\nDr. Curtis Ober, \\nSandia National \\nLaboratories.)\\nDIP4E_GLOBAL_Print_Ready.indb   37\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 38}),\n",
       " Document(page_content='38\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nexamination is determining the sex of the baby. Ultrasound images are generated \\nusing the following basic procedure:\\n1. \\nThe ultrasound system (a computer, ultrasound probe consisting of a source, a \\nreceiver\\n, and a display) transmits high-frequency (1 to 5 MHz) sound pulses \\ninto the body.\\n2. \\nThe sound waves travel into the body and hit a boundary between tissues (e.g., \\nbetween ﬂuid and soft tissue\\n, soft tissue and bone). Some of the sound waves \\nare reﬂected back to the probe, while some travel on further until they reach \\nanother boundary and are reﬂected.\\n3. \\nThe reﬂected waves are picked up by the probe and relayed to the computer.\\n4. \\nThe machine calculates the distance from the probe to the tissue or organ bound-\\naries using the speed of sound in tissue (1540 m/s) and the time of each echo’\\ns \\nreturn.\\n5. \\nThe system displays the distances and intensities of the echoes on the screen, \\nforming a two-dimensional image\\n.\\nIn a typical ultrasound image, millions of pulses and echoes are sent and received \\neach second. The probe can be moved along the surface of the body and angled to \\nobtain various views. Figure 1.20 shows several examples of medical uses of ultra-\\nsound. \\nWe continue the discussion on imaging modalities with some examples of elec-\\ntron microscopy. Electron microscopes function as their optical counterparts, except \\nb a\\nd c\\nFIGURE 1.20\\nExamples of \\nultrasound  \\nimaging. (a) A \\nfetus. (b) Another \\nview of the fetus.  \\n(c) Thyroids.  \\n(d) Muscle layers \\nshowing lesion. \\n(Courtesy of \\nSiemens  \\nMedical Systems, \\nInc., Ultrasound \\nGroup.)\\nDIP4E_GLOBAL_Print_Ready.indb   38\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 39}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n39\\nthat they use a focused beam of electrons instead of light to image a specimen. The \\noperation of electron microscopes involves the following basic steps: A stream \\nof electrons is produced by an electron source and accelerated toward the speci-\\nmen using a positive electrical potential. This \\nstream is conﬁned and focused using \\nmetal apertures and magnetic lenses into a thin, monochromatic beam. \\nThis beam is \\nfocused onto the sample using a magnetic lens. Interactions occur inside the irradi-\\nated sample, affecting the electron beam. These interactions and effects are detected \\nand transformed into an image, much in the same way that light is reﬂected from, \\nor absorbed by, objects in a scene. These basic steps are carried out in all electron \\nmicroscopes.\\nA \\ntransmission electron microscope\\n (TEM) works much like a slide projector. A \\nprojector transmits a beam of light through a slide; as the light passes through the \\nslide, it is modulated by the contents of the slide. This transmitted beam is then \\nprojected onto the viewing screen, forming an enlarged image of the slide. TEMs \\nwork in the same way, except that they shine a beam of electrons through a spec-\\nimen (analogous to the slide). The fraction of the beam transmitted through the \\nspecimen is projected onto a phosphor screen. The interaction of the electrons with \\nthe phosphor produces light and, therefore, a viewable image. A \\nscanning electron \\nmicroscope\\n (SEM), on the other hand, actually scans the electron beam and records \\nthe interaction of beam and sample at each location. This produces one dot on a \\nphosphor screen. A complete image is formed by a raster scan of the beam through \\nthe sample, much like a TV camera. The electrons interact with a phosphor screen \\nand produce light. SEMs are suitable for “bulky” samples, while TEMs require very \\nthin samples.\\nElectron microscopes are capable of very high magniﬁcation. While light micros-\\ncopy is limited to magniﬁcations on the order of \\n1000\\n×\\n,\\n electron microscopes can \\nachieve magniﬁcation of \\n10 000 ,\\n×\\n or more. Figure 1.21 shows two SEM images of \\nspecimen failures due to thermal overload.\\nW\\ne conclude the discussion of imaging modalities by looking brieﬂy at images \\nthat are not obtained from physical objects. Instead, they are generated by computer. \\nFractals are striking examples of computer-generated images. Basically, a fractal is \\nnothing more than an iterative reproduction of a basic pattern according to some \\nmathematical rules. For instance, tiling is one of the simplest ways to generate a frac-\\ntal image. A square can be subdivided into four square subregions, each of which can \\nbe further subdivided into four smaller square regions, and so on. Depending on the \\ncomplexity of the rules for ﬁlling each subsquare, some beautiful tile images can be \\ngenerated using this method. Of course, the geometry can be arbitrary. For instance, \\nthe fractal image could be grown radially out of a center point. Figure 1.22(a) shows \\na fractal grown in this way. Figure 1.22(b) shows another fractal (a “moonscape”) \\nthat provides an interesting analogy to the images of space used as illustrations in \\nsome of the preceding sections.\\nA more structured approach to image generation by computer lies in 3-D model-\\ning. This is an area that provides an important intersection between image process-\\ning and computer graphics, and is the basis for many 3-D visualization systems (e.g., \\nﬂight simulators). Figures 1.22(c) and (d) show examples of computer-generated \\nimages. Because the original \\nobject is created in 3-D, images can be generated in any \\nDIP4E_GLOBAL_Print_Ready.indb   39\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 40}),\n",
       " Document(page_content='40\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nperspective from plane projections of the 3-D volume. Images of this type can be \\nused for medical training and for a host of other applications, such as criminal foren-\\nsics and special effects.\\nb a\\nFIGURE 1.21\\n (a) \\n250\\n×\\n SEM image of a tungsten ﬁlament following thermal failure (note the \\nshattered pieces on the lower left).\\n (b) \\n2500\\n×\\n SEM image of a damaged integrated circuit. \\nThe white ﬁbers are oxides resulting from thermal destruction. (Figure (a) courtesy of Mr. \\nMichael Shaffer, Department of Geological Sciences, University of Oregon, Eugene; (b) cour-\\ntesy of Dr. J. M. Hudak, McMaster University, Hamilton, Ontario, Canada.) \\nb a\\nd c\\nFIGURE 1.22\\n(a) and (b) Fractal \\nimages.  \\n(c) and (d) Images \\ngenerated from \\n3-D computer \\nmodels of the \\nobjects shown. \\n(Figures (a) and \\n(b) courtesy of \\nMs. Melissa D. \\nBinde,  \\nSwarthmore \\nCollege; (c) and \\n(d) courtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   40\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 41}),\n",
       " Document(page_content='1.4\\n  \\nFundamental Steps in Digital Image Processing\\n    \\n41\\n1.4 FUNDAMENTAL STEPS IN DIGITAL IMAGE PROCESSING  \\nIt is helpful to divide the material covered in the following chapters into the two \\nbroad categories defined in Section 1.1: methods whose input and output are images, \\nand methods whose inputs may be images, but whose outputs are attributes extract-\\ned from those images. This organization is summarized in Fig. 1.23. The diagram \\ndoes not imply that every process is applied to an image. Rather, the intention is to \\nconvey an idea of all the methodologies that can be applied to images for different \\npurposes, and possibly with different objectives. The discussion in this section may \\nbe viewed as a brief overview of the material in the remainder of the book. \\nImage acquisition\\n is the ﬁrst process in Fig. 1.23. The discussion in Section 1.3 \\ngave some hints regarding the origin of digital images. This topic will be considered \\nin much more detail in Chapter 2, where we also introduce a number of basic digital \\nimage concepts that are used throughout the book. Acquisition could be as simple as \\nbeing given an image that is already in digital form. Generally, the image acquisition \\nstage involves preprocessing, such as scaling.\\nImage enhancement\\n is the process of manipulating an image so the result is more \\nsuitable than the original for a speciﬁc application. The word \\nspeciﬁc\\n is important \\nhere, because it establishes at the outset that enhancement techniques are problem \\noriented. Thus, for example, a method that is quite useful for enhancing X-ray images \\nmay not be the best approach for enhancing satellite images taken in the infrared \\nband of the electromagnetic spectrum.\\nThere is no general “theory” of image enhancement. When an image is processed \\nfor visual interpretation, the viewer is the ultimate judge of how well a particular \\n1.4\\nKnowledge base\\nCHAPTER 7\\nWavelets and\\nother image\\ntransforms\\nOutputs of these processes generally are images\\nCHAPTER 5\\nImage\\nrestoration\\nCHAPTERS 3 & 4\\nImage\\nfiltering and \\nenhancement\\nProblem\\ndomain\\nOutputs of these processes generally are image attributes\\nCHAPTER 8\\nCompression and\\nwatermarking\\nCHAPTER 2\\nImage\\nacquisition\\nCHAPTER 9\\nMorphological\\nprocessing\\n    CHAPTERS 10 \\nSegmentation\\n  CHAPTER 11\\nFeature \\nextraction\\nCHAPTER 12\\nImage \\npattern\\nclassification\\nWavelets and\\nmultiresolution\\nprocessing\\nColor Image\\nProcessing\\nCHAPTER 6\\nFIGURE 1.23\\nFundamental \\nsteps in digital \\nimage processing. \\nThe chapter(s) \\nindicated in the \\nboxes is where \\nthe material \\ndescribed in the \\nbox is discussed.\\nDIP4E_GLOBAL_Print_Ready.indb   41\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 42}),\n",
       " Document(page_content='42\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nmethod works. Enhancement techniques are so varied, and use so many different \\nimage processing approaches, that it is difﬁcult to assemble a meaningful body of \\ntechniques suitable for enhancement in one chapter without extensive background \\ndevelopment. For this reason, and also because beginners in the ﬁeld of image pro-\\ncessing generally ﬁnd enhancement applications visually appealing, interesting, and \\nrelatively simple to understand, we will use image enhancement as examples when \\nintroducing new concepts in parts of Chapter 2 and in Chapters 3 and 4. The mate-\\nrial in the latter two chapters span many of the methods used traditionally for image \\nenhancement. Therefore, using examples from image enhancement to introduce new \\nimage processing methods developed in these early chapters not only saves having \\nan extra chapter in the book dealing with image enhancement but, more importantly, \\nis an effective approach for introducing newcomers to the details of processing tech-\\nniques early in the book. However, as you will see in progressing through the rest \\nof the book, the material developed in Chapters 3 and 4 is applicable to a much \\nbroader class of problems than just image enhancement.\\nImage restoration\\n is an area that also deals with improving the appearance of \\nan image. However, unlike enhancement, which is subjective, image restoration \\nis objective, in the sense that restoration techniques tend to be based on mathe-\\nmatical or probabilistic models of image degradation. Enhancement, on the other \\nhand, is based on human subjective preferences regarding what constitutes a “good” \\nenhancement result.\\nColor image processing\\n is an area that has been gaining in importance because of \\nthe signiﬁcant increase in the use of digital images over the internet. Chapter 6 cov-\\ners a number of fundamental concepts in color models and basic color processing \\nin a digital domain. Color is used also as the basis for extracting features of interest \\nin an image.\\nWavelets\\n are the foundation for representing images in various degrees of reso-\\nlution. In particular, this material is used in the book for image data compression \\nand for pyramidal representation, in which images are subdivided successively into \\nsmaller regions. The material in Chapters 4 and 5 is based mostly on the Fourier \\ntransform. In addition to wavelets, we will also discuss in Chapter 7 a number of \\nother transforms that are used routinely in image processing.\\nCompression\\n, as the name implies, deals with techniques for reducing the storage \\nrequired to save an image, or the bandwidth required to transmit it. Although stor-\\nage technology has improved signiﬁcantly over the past decade, the same cannot be \\nsaid for transmission capacity. This is true particularly in uses of the internet, which \\nare characterized by signiﬁcant pictorial content. Image compression is familiar \\n(perhaps inadvertently) to most users of computers in the form of image ﬁle exten-\\nsions, such as the jpg ﬁle extension used in the JPEG (Joint Photographic Experts \\nGroup) image compression standard.\\nMorphological\\n processing deals with tools for extracting image components that \\nare useful in the representation and description of shape. The material in this chap-\\nter begins a transition from processes that output images to processes that output \\nimage attributes, as indicated in Section 1.1.\\nSegmentation\\n partitions an image into its constituent parts or objects. In gen-\\neral, autonomous segmentation is one of the most difﬁcult tasks in digital image \\nDIP4E_GLOBAL_Print_Ready.indb   42\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 43}),\n",
       " Document(page_content='1.4\\n  \\nFundamental Steps in Digital Image Processing\\n    \\n43\\nprocessing. A rugged segmentation procedure brings the process a long way toward \\nsuccessful solution of imaging problems that require objects to be identiﬁed indi-\\nvidually. On the other hand, weak or erratic segmentation algorithms almost always \\nguarantee eventual failure. In general, the more accurate the segmentation, the \\nmore likely automated object classiﬁcation is to succeed.\\nFeature extraction\\n almost always follows the output of a segmentation stage, which \\nusually is raw pixel data, constituting either the boundary of a region (i.e., the set \\nof pixels separating one image region from another) or all the points in the region \\nitself. Feature extraction consists of feature detection and feature description. \\nFea-\\nture detection\\n refers to ﬁnding the features in an image, region, or boundary. \\nFeature \\ndescription\\n assigns quantitative attributes to the detected features. For example, we \\nmight detect corners in a region, and describe those corners by their orientation \\nand location; both of these descriptors are quantitative attributes. Feature process-\\ning methods discussed in this chapter are subdivided into three principal categories, \\ndepending on whether they are applicable to boundaries, regions, or whole images. \\nSome features are applicable to more than one category. Feature descriptors should \\nbe as insensitive as possible to variations in parameters such as scale, translation, \\nrotation, illumination, and viewpoint. \\nImage pattern classiﬁcation\\n is the process that assigns a label (e.g., “vehicle”) to an \\nobject based on its feature descriptors. In the last chapter of the book, we will discuss  \\nmethods of image pattern classiﬁcation ranging from “classical” approaches such as \\nminimum-distance\\n, \\ncorrelation\\n, and \\nBayes classiﬁers\\n, to more modern approaches \\nimplemented using \\ndeep neural networks\\n. In particular, we will discuss in detail \\ndeep\\n \\nconvolutional neural networks\\n, which are ideally suited for image processing work.\\nSo far, we have said nothing about the need for prior knowledge or about the \\ninteraction between the knowledge base and the processing modules in Fig. 1.23. \\nKnowledge\\n about a problem domain is coded into an image processing system in the \\nform of a knowledge database. This knowledge may be as simple as detailing regions \\nof an image where the information of interest is known to be located, thus limiting \\nthe search that has to be conducted in seeking that information. The knowledge base \\ncan also be quite complex, such as an interrelated list of all major possible defects \\nin a materials inspection problem, or an image database containing high-resolution \\nsatellite images of a region in connection with change-detection applications. In \\naddition to guiding the operation of each processing module, the knowledge base \\nalso controls the interaction between modules. This distinction is made in Fig. 1.23 \\nby the use of double-headed arrows between the processing modules and the knowl-\\nedge base, as opposed to single-headed arrows linking the processing modules.\\nAlthough we do not discuss image display explicitly at this point, it is important to \\nkeep in mind that viewing the results of image processing can take place at the out-\\nput of any stage in Fig. 1.23. We also note that not all image processing applications \\nrequire the complexity of interactions implied by Fig. 1.23. In fact, not even all those \\nmodules are needed in many cases. For example, image enhancement for human \\nvisual interpretation seldom requires use of any of the other stages in Fig. 1.23. In \\ngeneral, however, as the complexity of an image processing task increases, so does \\nthe number of processes required to solve the problem.\\nDIP4E_GLOBAL_Print_Ready.indb   43\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 44}),\n",
       " Document(page_content='44\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\n1.5 COMPONENTS OF AN IMAGE PROCESSING SYSTEM  \\nAs recently as the mid-1980s, numerous models of image processing systems being \\nsold throughout the world were rather substantial peripheral devices that attached \\nto equally substantial host computers. Late in the 1980s and early in the 1990s, the \\nmarket shifted to image processing hardware in the form of single boards designed \\nto be compatible with industry standard buses and to ﬁt into engineering work-\\nstation cabinets and personal computers. In the late 1990s and early 2000s, a new \\nclass of add-on boards, called graphics processing units (GPUs) were introduced for \\nwork on 3-D applications, such as games and other 3-D graphics applications. It was \\nnot long before GPUs found their way into image processing applications involving \\nlarge-scale matrix implementations, such as training deep convolutional networks. \\nIn addition to lowering costs, the market shift from substantial peripheral devices to \\nadd-on processing boards also served as a catalyst for a signiﬁcant number of new \\ncompanies specializing in the development of software written speciﬁcally for image \\nprocessing. \\nThe trend continues toward miniaturizing and blending of general-purpose small \\ncomputers with specialized image processing hardware and software. Figure 1.24 \\nshows the basic components comprising a typical general-purpose system used for \\ndigital image processing. The function of each component will be discussed in the \\nfollowing paragraphs, starting with image sensing.\\nTwo subsystems are required to acquire digital images. The ﬁrst is a physical \\nsen-\\nsor\\n that responds to the energy radiated by the object we wish to image. The second, \\ncalled a \\ndigitizer\\n, is a device for converting the output of the physical sensing device \\ninto digital form. For instance, in a digital video camera, the sensors (CCD chips) \\nproduce an electrical output proportional to light intensity. The digitizer converts \\nthese outputs to digital data. These topics will be covered in Chapter 2.\\nSpecialized image processing hardware usually consists of the digitizer just men-\\ntioned, plus hardware that performs other primitive operations, such as an \\narithme-\\ntic logic unit\\n (ALU), that performs arithmetic and logical operations in parallel on \\nentire images. One example of how an ALU is used is in averaging images as quickly \\nas they are digitized, for the purpose of noise reduction. This type of hardware some-\\ntimes is called a \\nfront-end subsystem\\n, and its most distinguishing characteristic is \\nspeed. In other words, this unit performs functions that require fast data through-\\nputs (e.g., digitizing and averaging video images at 30 frames/s) that the typical main \\ncomputer cannot handle. One or more GPUs (see above) also are common in image \\nprocessing systems that perform intensive matrix operations.\\nThe \\ncomputer\\n in an image processing system is a general-purpose computer and \\ncan range from a PC to a supercomputer. In dedicated applications, sometimes cus-\\ntom computers are used to achieve a required level of performance, but our interest \\nhere is on general-purpose image processing systems. In these systems, almost any \\nwell-equipped PC-type machine is suitable for off-line image processing tasks.\\nSoftware\\n for image processing consists of specialized modules that perform \\nspeciﬁc tasks. A well-designed package also includes the capability for the user to \\nwrite code that, as a minimum, utilizes the specialized modules. More sophisticated \\n1.5\\nDIP4E_GLOBAL_Print_Ready.indb   44\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 45}),\n",
       " Document(page_content='1.5\\n  \\nComponents of an Image Processing System\\n45\\nsoftware packages allow the integration of those modules and general-purpose \\nsoftware commands from at least one computer language. Commercially available \\nimage processing software, such as the well-known MATLAB\\n®\\n Image Processing \\nToolbox, is also common in a well-equipped image processing system. \\nMass storage\\n is a must in image processing applications. An image of size \\n1024 1024\\n×\\npixels, in which the intensity of each pixel is an 8-bit quantity,  requires one megabyte  \\nof storage space if the image is not compressed.\\n When dealing with image databases \\nthat contain thousands, or even millions, of images, providing adequate storage in \\nan image processing system can be a challenge. Digital storage for image processing \\napplications falls into three principal categories: (1) short-term storage for use dur-\\ning processing; (2) on-line storage for relatively fast recall; and (3) archival storage, \\ncharacterized by infrequent access. Storage is measured in bytes (eight bits), Kbytes \\n(10\\n3\\n bytes), Mbytes (\\n10\\n6\\n bytes), Gbytes (\\n10\\n9\\n bytes), and Tbytes (\\n10\\n12\\n bytes).\\noud\\nud\\nCloud\\nImage displays\\nComputer\\nMass storage\\nHardcopy\\nSpecialized\\nimage processing\\nhardware\\nImage sensors\\nProblem\\ndomain\\nImage processing\\nsoftware\\nNetwork\\nCloud\\nFIGURE 1.24\\nComponents of a \\ngeneral-purpose \\nimage processing \\nsystem. \\nDIP4E_GLOBAL_Print_Ready.indb   45\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 46}),\n",
       " Document(page_content='46\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nOne method of providing short-term storage is computer memory. Another is by \\nspecialized boards, called \\nframe buffers\\n, that store one or more images and can be \\naccessed rapidly, usually at video rates (e.g., at 30 complete images per second). The \\nlatter method allows virtually instantaneous image \\nzoom\\n, as well as \\nscroll\\n (vertical \\nshifts) and \\npan\\n (horizontal shifts). Frame buffers usually are housed in the special-\\nized image processing hardware unit in Fig. 1.24. On-line storage generally takes \\nthe form of magnetic disks or optical-media storage. The key factor characterizing \\non-line storage is frequent access to the stored data. Finally, archival storage is char-\\nacterized by massive storage requirements but infrequent need for access. Magnetic \\ntapes and optical disks housed in “jukeboxes” are the usual media for archival appli-\\ncations.\\nImage displays\\n in use today are mainly color, ﬂat screen monitors. Monitors are \\ndriven by the outputs of image and graphics display cards that are an integral part of \\nthe computer system. Seldom are there requirements for image display applications \\nthat cannot be met by display cards and GPUs available commercially as part of the \\ncomputer system. In some cases, it is necessary to have stereo displays, and these are \\nimplemented in the form of headgear containing two small displays embedded in \\ngoggles worn by the user.\\nHardcopy\\n devices for recording images include laser printers, ﬁlm cameras, heat-\\nsensitive devices, ink-jet units, and digital units, such as optical and CD-ROM disks. \\nFilm provides the highest possible resolution, but paper is the obvious medium of \\nchoice for written material. For presentations, images are displayed on ﬁlm trans-\\nparencies or in a digital medium if image projection equipment is used. The latter \\napproach is gaining acceptance as the standard for image presentations.\\nNetworking\\n and \\ncloud\\n communication are almost default functions in any com-\\nputer system in use today. Because of the large amount of data inherent in image \\nprocessing applications, the key consideration in image transmission is \\nbandwidth\\n. In \\ndedicated networks, this typically is not a problem, but communications with remote \\nsites via the internet are not always as efﬁcient. Fortunately, transmission bandwidth \\nis improving quickly as a result of optical ﬁber and other broadband technologies. \\nImage data compression continues to play a major role in the transmission of large \\namounts of image data.\\nSummary, References, and Further Reading\\n  \\nThe main purpose of the material presented in this chapter is to provide a sense of perspective about the origins \\nof digital image processing and, more important, about current and future areas of application of this technology. \\nAlthough the coverage of these topics in this chapter was necessarily incomplete due to space limitations, it should \\nhave left you with a clear impression of the breadth and practical scope of digital image processing. As we proceed \\nin the following chapters with the development of image processing theory and applications, numerous examples \\nare provided to keep a clear focus on the utility and promise of these techniques. Upon concluding the study of the \\nﬁnal chapter, a reader of this book will have arrived at a level of understanding that is the foundation for most of \\nthe work currently underway in this ﬁeld. \\nIn past editions, we have provided a long list of journals and books to give readers an idea of the breadth of the \\nimage processing literature, and where this literature is reported. The list has been updated, and it has become so \\nextensive that it is more practical to include it in the book website: \\nwww.ImageProcessingPlace.com\\n, in the section \\nentitled \\nPublications\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   46\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 47}),\n",
       " Document(page_content='472\\nDigital Image Fundamentals\\nPreview\\nThis chapter is an introduction to a number of basic concepts in digital image processing that are used \\nthroughout the book. Section 2.1 summarizes some important aspects of the human visual system, includ-\\ning image formation in the eye and its capabilities for brightness adaptation and discrimination. Section \\n2.2 discusses light, other components of the electromagnetic spectrum, and their imaging characteristics. \\nSection 2.3 discusses imaging sensors and how they are used to generate digital images. Section 2.4 intro-\\nduces the concepts of uniform image sampling and intensity quantization. Additional topics discussed \\nin that section include digital image representation, the effects of varying the number of samples and \\nintensity levels in an image, the concepts of spatial and intensity resolution, and the principles of image \\ninterpolation. Section 2.5 deals with a variety of basic relationships between pixels. Finally, Section 2.6 \\nis an introduction to the principal mathematical tools we use throughout the book. A second objective \\nof that section is to help you begin developing a “feel” for how these tools are used in a variety of basic \\nimage processing tasks. \\nUpon completion of this chapter, readers should:\\n Have an understanding of some important \\nfunctions and limitations of human vision.\\n Be familiar with the electromagnetic energy \\nspectrum, including basic properties of light.\\n Know how digital images are generated and \\nrepresented.\\n Understand the basics of image sampling and \\nquantization.\\n Be familiar with spatial and intensity resolu-\\ntion and their effects on image appearance.\\n Have an understanding of basic geometric \\nrelationships between image pixels.\\n Be familiar with the principal mathematical \\ntools used in digital image processing.\\n Be able to apply a variety of introductory dig-\\nital image processing techniques.\\nThose who wish to succeed must ask the right preliminary \\nquestions.\\nAristotle\\nDIP4E_GLOBAL_Print_Ready.indb   47\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 48}),\n",
       " Document(page_content='48\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n2.1 ELEMENTS OF VISUAL PERCEPTION  \\nAlthough the field of digital image processing is built on a foundation of mathemat-\\nics, human intuition and analysis often play a role in the choice of one technique \\nversus another, and this choice often is made based on subjective, visual judgments. \\nThus, developing an understanding of basic characteristics of human visual percep-\\ntion as a first step in our journey through this book is appropriate. In particular, our \\ninterest is in the elementary mechanics of how images are formed and perceived \\nby humans. We are interested in learning the physical limitations of human vision \\nin terms of factors that also are used in our work with digital images. Factors such \\nas how human and electronic imaging devices compare in terms of resolution and \\nability to adapt to changes in illumination are not only interesting, they are also \\nimportant from a practical point of view.\\nSTRUCTURE OF THE HUMAN EYE\\nFigure 2.1 shows a simplified cross section of the human eye. The eye is nearly a \\nsphere (with a diameter of about 20 mm) enclosed by three membranes: the \\ncornea\\n \\nand \\nsclera\\n outer cover; the \\nchoroid\\n; and the \\nretina\\n. The cornea is a tough, transparent \\ntissue that covers the anterior surface of the eye. Continuous with the cornea, the \\nsclera is an opaque membrane that encloses the remainder of the optic globe.\\nThe choroid lies directly below the sclera. This membrane contains a network of \\nblood vessels that serve as the major source of nutrition to the eye. Even superﬁcial \\n2.1\\nRetina\\nBlind spot\\nSclera\\nChoroid\\nNerve & sheath\\nFovea\\nVitreous humor\\nVisual axis\\nCiliary fibers\\nCiliary muscle\\nIris\\nCornea\\nLens\\nAnterior chamber\\nCiliary body\\nFIGURE 2.1\\nSimpliﬁed  \\ndiagram of a  \\ncross section of \\nthe human eye.\\nDIP4E_GLOBAL_Print_Ready.indb   48\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 49}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n    \\n49\\ninjury to the choroid can lead to severe eye damage as a result of inﬂammation that \\nrestricts blood ﬂow. The choroid coat is heavily pigmented, which helps reduce the \\namount of extraneous light entering the eye and the backscatter within the optic \\nglobe. At its anterior extreme, the choroid is divided into the \\nciliary body\\n and the \\niris\\n. The latter contracts or expands to control the amount of light that enters the eye. \\nThe central opening of the iris (the \\npupil\\n) varies in diameter from approximately 2 \\nto 8 mm. The front of the iris contains the visible pigment of the eye, whereas the \\nback contains a black pigment.\\nThe \\nlens\\n consists of concentric layers of ﬁbrous cells and is suspended by ﬁbers \\nthat attach to the ciliary body. It is composed of 60% to 70% water, about 6% fat, \\nand more protein than any other tissue in the eye. The lens is colored by a slightly \\nyellow pigmentation that increases with age. In extreme cases, excessive clouding of \\nthe lens, referred to as \\ncataracts\\n, can lead to poor color discrimination and loss of \\nclear vision. The lens absorbs approximately 8% of the visible light spectrum, with \\nhigher absorption at shorter wavelengths. Both infrared and ultraviolet light are \\nabsorbed by proteins within the lens and, in excessive amounts, can damage the eye.\\nThe innermost membrane of the eye is the \\nretina\\n, which lines the inside of the \\nwall’s entire posterior portion. When the eye is focused, light from an object is \\nimaged on the retina. Pattern vision is afforded by discrete light receptors distrib-\\nuted over the surface of the retina. There are two types of receptors: \\ncones\\n and \\nrods\\n. \\nThere are between 6 and 7 million cones in each eye. They are located primarily in \\nthe central portion of the retina, called the \\nfovea\\n, and are highly sensitive to color. \\nHumans can resolve ﬁne details because each cone is connected to its own nerve end. \\nMuscles rotate the eye until the image of a region of interest falls on the fovea. Cone \\nvision is called \\nphotopic\\n or \\nbright-light\\n vision.\\nThe number of rods is much larger: Some 75 to 150 million are distributed over \\nthe retina. The larger area of distribution, and the fact that several rods are connect-\\ned to a single nerve ending, reduces the amount of detail discernible by these recep-\\ntors. Rods capture an overall image of the ﬁeld of view. They are not involved in \\ncolor vision, and are sensitive to low levels of illumination. For example, objects that \\nappear brightly colored in daylight appear as colorless forms in moonlight because \\nonly the rods are stimulated. This phenomenon is known as \\nscotopic\\n or \\ndim-light\\n \\nvision.\\nFigure 2.2\\n \\nshows the density of rods and cones for a cross section of the right eye, \\npassing through the region where the optic nerve emerges from the eye\\n. The absence \\nof receptors in this area causes the so-called \\nblind spot\\n (see Fig. 2.1). Except for this \\nregion, the distribution of receptors is radially symmetric about the fovea. Receptor \\ndensity is measured in degrees from the visual axis. Note in Fig. 2.2 that cones are \\nmost dense in the center area of the fovea, and that rods increase in density from \\nthe center out to approximately 20° off axis. Then, their density decreases out to the \\nperiphery of the retina.\\nThe fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, \\nso it has an area of approximately 1.77 \\nmm\\n2\\n.\\n As Fig. 2.2 shows, the density of cones \\nin that area of the retina is on the order of 150,000 elements per \\nmm\\n2\\n. Based on \\nthese ﬁgures, the number of cones in the fovea, which is the region of highest acuity \\nDIP4E_GLOBAL_Print_Ready.indb   49\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 50}),\n",
       " Document(page_content='50\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nin the eye, is about 265,000 elements. Modern electronic imaging chips exceed this \\nnumber by a large factor. While the ability of humans to integrate intelligence and \\nexperience with vision makes purely quantitative comparisons somewhat superﬁcial, \\nkeep in mind for future discussions that electronic imaging sensors can easily exceed \\nthe capability of the eye in resolving image detail.\\nIMAGE FORMATION IN THE EYE\\nIn an ordinary photographic camera, the lens has a fixed focal length. Focusing at \\nvarious distances is achieved by varying the distance between the lens and the imag-\\ning plane, where the film (or imaging chip in the case of a digital camera) is located. \\nIn the human eye, the converse is true; the distance between the center of the lens \\nand the imaging sensor (the retina) is fixed, and the focal length needed to achieve \\nproper focus is obtained by varying the shape of the lens. The fibers in the ciliary \\nbody accomplish this by flattening or thickening the lens for distant or near ob-\\njects, respectively. The distance between the center of the lens and the retina along \\nthe visual axis is approximately 17 mm. The range of focal lengths is approximately \\n14 mm to 17 mm, the latter taking place when the eye is relaxed and focused at dis-\\ntances greater than about 3 m. The geometry in Fig. 2.3 illustrates how to obtain the \\ndimensions of an image formed on the retina. For example, suppose that a person \\nis looking at a tree 15 m high at a distance of 100 m. Letting \\nh\\n denote the height \\nof that object in the retinal image, the geometry of Fig. 2.3 yields \\n15 100 17\\n=\\nh\\n or \\nh\\n=\\n25\\n.\\n mm.\\n As indicated earlier in this section, the retinal image is focused primar-\\nily on the region of the fovea.\\n Perception then takes place by the relative excitation \\nof light receptors, which transform radiant energy into electrical impulses that ulti-\\nmately are decoded by the brain.\\nBRIGHTNESS ADAPTATION AND DISCRIMINATION\\nBecause digital images are displayed as sets of discrete intensities, the eye’s abil-\\nity to discriminate between different intensity levels is an important consideration \\nFIGURE 2.2\\nDistribution of \\nrods and cones in \\nthe retina.\\nBlind spot\\nCones\\nRods\\nNo. of rods or cones per mm\\n2\\nDegrees from visual axis (center of fovea)\\n180,000\\n135,000\\n90,000\\n45,000\\n80\\n/H11034\\n60\\n/H11034\\n40\\n/H11034\\n20\\n/H11034\\n0\\n/H11034\\n20\\n/H11034\\n40\\n/H11034\\n60\\n/H11034\\n80\\n/H11034\\nDIP4E_GLOBAL_Print_Ready.indb   50\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 51}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n51\\nin presenting image processing results. The range of light intensity levels to which \\nthe human visual system can adapt is enormous—on the order of \\n10\\n10\\n— from the \\nscotopic threshold to the glare limit. Experimental evidence indicates that \\nsubjec-\\ntive brightness\\n (intensity as perceived by the human visual system) is a logarithmic \\nfunction of the light intensity incident on the eye. Figure 2.4, a plot of light inten-\\nsity versus subjective brightness, illustrates this characteristic. The long solid curve \\nrepresents the range of intensities to which the visual system can adapt. In photopic \\nvision alone, the range is about \\n10\\n6\\n. The transition from scotopic to photopic vision \\nis gradual over the approximate range from 0.001 to 0.1 millilambert (\\n−\\n3\\n to \\n−\\n1\\n mL \\nin the log scale),\\n as the double branches of the adaptation curve in this range show.\\nThe key point in interpreting the impressive dynamic range depicted in Fig. 2.4 \\nis that the visual system cannot operate over such a range \\nsimultaneously\\n. Rather, it \\naccomplishes this large variation by changing its overall sensitivity, a phenomenon \\nknown as \\nbrightness adaptation\\n. The total range of distinct intensity levels the eye \\ncan discriminate simultaneously is rather small when compared with the total adap-\\ntation range. For a given set of conditions, the current sensitivity level of the visual \\nsystem is called the \\nbrightness adaptation level\\n, which may correspond, for example, \\nFIGURE 2.3\\nGraphical  \\nrepresentation of \\nthe eye looking at \\na palm tree. Point \\nC \\nis the focal  \\ncenter of the lens.\\n15 m\\nC\\n17 mm\\n100 m\\nFIGURE 2.4\\nRange of subjec-\\ntive brightness \\nsensations  \\nshowing a  \\nparticular  \\nadaptation level, \\nB\\na\\n.\\nGlare limit\\nSubjective brightness\\nAdaptation range\\nScotopic\\nthreshold\\nLog of intensity (mL)\\nScotopic\\nPhotopic\\n/H11002\\n6\\n/H11002\\n4\\n/H11002\\n20 24\\nB\\na\\nB\\nb\\nDIP4E_GLOBAL_Print_Ready.indb   51\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 52}),\n",
       " Document(page_content='52\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nto brightness \\nB\\na\\n in Fig. 2.4. The short intersecting curve represents the range of sub-\\njective brightness that the eye can perceive when adapted to \\nthis\\n level. This range is \\nrather restricted, having a level \\nB\\nb\\n at, and below which, all stimuli are perceived as \\nindistinguishable blacks. The upper portion of the curve is not actually restricted but, \\nif extended too far, loses its meaning because much higher intensities would simply \\nraise the adaptation level higher than \\nB\\na\\n.\\nThe ability of the eye to discriminate between \\nc\\nhanges\\n in light intensity at any \\nspeciﬁc adaptation level is of considerable interest. A classic experiment used to \\ndetermine the capability of the human visual system for brightness discrimination \\nconsists of having a subject look at a ﬂat, uniformly illuminated area large enough to \\noccupy the entire ﬁeld of view. This area typically is a diffuser, such as opaque glass, \\nilluminated from behind by a light source, \\nI\\n, with variable intensity. To this ﬁeld is \\nadded an increment of illumination, \\n/H9004\\nI\\n, in the form of a short-duration ﬂash that \\nappears as a circle in the center of the uniformly illuminated ﬁeld,\\n as Fig. 2.5 shows.\\nIf \\n/H9004\\nI\\n is not bright enough, the subject says “no,” indicating no perceivable change. \\nAs \\n/H9004\\nI\\n gets stronger, the subject may give a positive response of “ye\\ns,” \\nindicating a \\nperceived change\\n. Finally, when \\n/H9004\\nI\\n is strong enough, the subject will give a response \\nof \\n“yes” all the time. The quantity \\n/H9004\\nII\\nc\\n, where \\n/H9004\\nI\\nc\\n is the increment of illumination \\ndiscriminable 50% of the time with background illumination \\nI\\n, is called the \\nWeber \\nratio\\n. A small value of \\n/H9004\\nII\\nc\\n means that a small percentage change in intensity is \\ndiscriminable. This represents “good” brightness discrimination. Conversely, a large \\nvalue of \\n/H9004\\nII\\nc\\n means that a large percentage change in intensity is required for the \\neye to detect the change. This represents “poor” brightness discrimination.\\nA plot of \\n/H9004\\nII\\nc\\n as a function of \\nlog\\nI\\n has the characteristic shape shown in Fig. 2.6. \\nT\\nhis curve shows that brightness discrimination is poor (the Weber ratio is large) at \\nlow levels of illumination, and it improves signiﬁcantly (the Weber ratio decreases) \\nas background illumination increases. The two branches in the curve reﬂect the fact \\nthat at low levels of illumination vision is carried out by the rods, whereas, at high \\nlevels, vision is a function of cones.\\nIf the background illumination is held constant and the intensity of the other \\nsource, instead of ﬂashing, is now allowed to vary incrementally from never being \\nperceived to always being perceived, the typical observer can discern a total of one \\nto two dozen different intensity changes. Roughly, this result is related to the num-\\nber of different intensities a person can see at any one\\n point \\nor\\n small area\\n in a mono-\\nchrome image. This does not mean that an image can be represented by \\nsuch a \\nsmall \\nnumber of intensity values because, as the eye roams about the image, the average \\nFIGURE 2.5  \\nBasic\\nexperimental  \\nsetup used to \\ncharacterize \\nbrightness  \\ndiscrimination.\\nI\\nI\\n \\n/H9004\\nI\\n+\\nDIP4E_GLOBAL_Print_Ready.indb   52\\n6/16/2017   2:02:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 53}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n    \\n53\\nbackground changes, thus allowing a \\ndifferent\\n set of incremental changes to be detect-\\ned at each new adaptation level. The net result is that the eye is capable of a broader \\nrange of \\noverall\\n intensity discrimination. In fact, as we will show in Section 2.4, the eye \\nis capable of detecting objectionable effects in monochrome images whose overall \\nintensity is represented by fewer than approximately two dozen levels.\\nTwo phenomena demonstrate that perceived brightness is not a simple function \\nof intensity. The ﬁrst is based on the fact that the visual system tends to undershoot \\nor overshoot around the boundary of regions of different intensities. Figure 2.7(a) \\nshows a striking example of this phenomenon. Although the intensity of the stripes \\nFIGURE 2.6\\nA typical plot of \\nthe Weber ratio \\nas a function of \\nintensity.\\n/H11002\\n1.5\\n/H11002\\n2.0\\n/H11002\\n4\\n/H11002\\n3\\n/H11002\\n2\\n/H11002\\n10\\nlog \\nI\\nlog \\n/H9004\\nI\\nc\\n/\\nI\\n1234\\n/H11002\\n1.0\\n/H11002\\n0.5\\n0.5\\n1.0\\n0\\nActual intensity\\nPerceived intensity\\nFIGURE 2.7\\nIllustration of the \\nMach band effect. \\nPerceived  \\nintensity is not a \\nsimple function of \\nactual intensity.\\nb\\na\\nc\\nDIP4E_GLOBAL_Print_Ready.indb   53\\n6/16/2017   2:02:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 54}),\n",
       " Document(page_content='54\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nis constant [see Fig. 2.7(b)], we actually perceive a brightness pattern that is strongly \\nscalloped near the boundaries, as Fig. 2.7(c) shows. These perceived scalloped bands \\nare called \\nMach bands\\n after Ernst Mach, who ﬁrst described the phenomenon in 1865.\\nThe second phenomenon, called \\nsimultaneous contrast\\n, is that a region’s per-\\nceived brightness does not depend only on its intensity, as Fig. 2.8 demonstrates. All \\nthe center squares have exactly the same intensity, but each appears to the eye to \\nbecome darker as the background gets lighter. A more familiar example is a piece of \\npaper that looks white when lying on a desk, but can appear totally black when used \\nto shield the eyes while looking directly at a bright sky.\\nOther examples of human perception phenomena are \\noptical illusions\\n, in which \\nthe eye ﬁlls in nonexisting details or wrongly perceives geometrical properties of \\nobjects. Figure 2.9 shows some examples. In Fig. 2.9(a), the outline of a square is \\nseen clearly, despite the fact that no lines deﬁning such a ﬁgure are part of the image. \\nThe same effect, this time with a circle, can be seen in Fig. 2.9(b); note how just a few \\nlines are sufﬁcient to give the illusion of a complete circle. The two horizontal line \\nsegments in Fig. 2.9(c) are of the same length, but one appears shorter than the other. \\nFinally, all long lines in Fig. 2.9(d) are equidistant and parallel. Yet, the crosshatching \\ncreates the illusion that those lines are far from being parallel.\\n2.2 LIGHT AND THE ELECTROMAGNETIC SPECTRUM  \\nThe electromagnetic spectrum was introduced in Section 1.3. We now consider this \\ntopic in more detail. In 1666, Sir Isaac Newton discovered that when a beam of \\nsunlight passes through a glass prism, the emerging beam of light is not white but \\nconsists instead of a continuous spectrum of colors ranging from violet at one end \\nto red at the other. As Fig. 2.10 shows, the range of colors we perceive in visible light \\nis a small portion of the electromagnetic spectrum. On one end of the spectrum are \\nradio waves with wavelengths billions of times longer than those of visible light. On \\nthe other end of the spectrum are gamma rays with wavelengths millions of times \\nsmaller than those of visible light. We showed examples in Section 1.3 of images in \\nmost of the bands in the EM spectrum.\\n2.2\\nb a\\nc\\nFIGURE 2.8\\n Examples of simultaneous contrast. All the inner squares have the same intensity, \\nbut they appear progressively darker as the background becomes lighter.\\nDIP4E_GLOBAL_Print_Ready.indb   54\\n6/16/2017   2:02:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 55}),\n",
       " Document(page_content='2.2\\n  \\nLight and the Electromagnetic Spectrum\\n    \\n55\\nThe electromagnetic spectrum can be expressed in terms of wavelength, frequency, \\nor energy. Wavelength (\\nl\\n) and frequency (\\nn\\n) are related by the expression \\n \\nl\\nn\\n=\\nc\\n \\n(2-1)\\nwhere \\nc\\n is the speed of light (\\n2\\n998 10\\n8\\n.\\n*\\n \\nm/s). Figure 2.11 shows a schematic repre-\\nsentation of one wavelength.\\n \\nThe energy of the various components of the electromagnetic spectrum is given \\nby the expression\\n \\nEh\\n=\\nn\\n \\n(2-2)\\nwhere \\nh\\n is Planck’\\ns constant. The units of wavelength are meters, with the terms \\nmicrons\\n (denoted \\nm\\nm\\n and equal to \\n10\\n6\\n−\\n m) and \\nnanometers\\n (denoted nm and equal \\nto 10\\n9\\n−\\n m) being used just as frequently. Frequency is measured in \\nHertz\\n (Hz), with \\none Hz being equal to one cycle of a sinusoidal wave per second. A commonly used \\nunit of energy is the \\nelectron-volt\\n.\\nElectromagnetic waves can be visualized as propagating sinusoidal waves with \\nwavelength \\nl\\n (Fig. 2.11), or they can be thought of as a stream of massless particles, \\nb a\\nd c\\nFIGURE 2.9  \\nSome \\nwell-known  \\noptical illusions.\\nDIP4E_GLOBAL_Print_Ready.indb   55\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 56}),\n",
       " Document(page_content='56\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\neach traveling in a wavelike pattern and moving at the speed of light. Each mass-\\nless particle contains a certain amount (or bundle) of energy, called a \\nphoton\\n. We \\nsee from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency \\n(shorter wavelength) electromagnetic phenomena carry more energy per photon. \\nThus, radio waves have photons with low energies, microwaves have more energy \\nthan radio waves, infrared still more, then visible, ultraviolet, X-rays, and ﬁnally \\ngamma rays, the most energetic of all. High-energy electromagnetic radiation, espe-\\ncially in the X-ray and gamma ray bands, is particularly harmful to living organisms. \\nLight is a type of electromagnetic radiation that can be sensed by the eye. The \\nvisible (color) spectrum is shown expanded in Fig. 2.10 for the purpose of discussion \\n(we will discuss color in detail in Chapter 6). The visible band of the electromag-\\nnetic spectrum spans the range from approximately 0.43 \\nm\\nm\\n (violet) to about 0.79 \\nm\\nm\\n (red). For convenience, the color spectrum is divided into six broad regions: \\nviolet,\\n blue, green, yellow, orange, and red. No color (or other component of the \\nRadio waves\\nMicrowaves\\nInfrared\\nVisible spectrum\\nUltraviolet\\nGamma rays X-rays\\n0.4 \\n/H11003\\n 10\\n/H11002\\n6\\n0.5 \\n/H11003\\n 10\\n/H11002\\n6\\n0.6 \\n/H11003\\n 10\\n/H11002\\n6\\n0.7 \\n/H11003\\n 10\\n/H11002\\n6\\nInfrared\\nUltraviolet Violet Blue Green Yellow Red\\nOrange\\n10\\n5\\n10\\n6\\n10\\n7\\n10\\n8\\n10\\n9\\n10\\n10\\n10\\n11\\n10\\n12\\n10\\n13\\n10\\n14\\n10\\n15\\n10\\n16\\n10\\n17\\n10\\n18\\n10\\n19\\n10\\n20\\n10\\n21\\nFrequency (Hz)\\n10\\n/H11002\\n9\\n10\\n/H11002\\n8\\n10\\n/H11002\\n7\\n10\\n/H11002\\n6\\n10\\n/H11002\\n5\\n10\\n/H11002\\n4\\n10\\n/H11002\\n3\\n10\\n/H11002\\n2\\n10\\n/H11002\\n1\\n1\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\nEnergy of one photon (electron volts)\\n10\\n3\\n10\\n2\\n10\\n1\\n1\\n10\\n/H11002\\n1\\n10\\n/H11002\\n2\\n10\\n/H11002\\n3\\n10\\n/H11002\\n4\\n10\\n/H11002\\n5\\n10\\n/H11002\\n6\\n10\\n/H11002\\n7\\n10\\n/H11002\\n8\\n10\\n/H11002\\n9\\n10\\n/H11002\\n10\\n10\\n/H11002\\n11\\n10\\n/H11002\\n12\\nWavelength (meters)\\nFIGURE 2.10\\n  The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanations, but note \\nthat it encompasses a very narrow range of the total EM spectrum.\\nl\\nFIGURE 2.11\\nGraphical  \\nrepresentation of \\none wavelength.\\nDIP4E_GLOBAL_Print_Ready.indb   56\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 57}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n    \\n57\\nelectromagnetic spectrum) ends abruptly; rather, each range blends smoothly into \\nthe next, as Fig. 2.10 shows.\\nThe colors perceived in an object are determined by the nature of the light \\nreﬂect-\\ned\\n by the object. A body that reﬂects light relatively balanced in all visible wave-\\nlengths appears white to the observer. However, a body that favors reﬂectance in \\na limited range of the visible spectrum exhibits some shades of color. For example, \\ngreen objects reﬂect light with wavelengths primarily in the 500 to 570 nm range, \\nwhile absorbing most of the energy at other wavelengths.\\nLight that is void of color is called \\nmonochromatic\\n (or \\nachromatic\\n) light. The \\nonly attribute of monochromatic light is its intensity. Because the intensity of mono-\\nchromatic light is perceived to vary from black to grays and ﬁnally to white, the \\nterm \\ngray level\\n is used commonly to denote monochromatic intensity (we use the \\nterms \\nintensity\\n and \\ngray level\\n interchangeably in subsequent discussions). The range \\nof values of monochromatic light from black to white is usually called the \\ngray scale\\n, \\nand monochromatic images are frequently referred to as \\ngrayscale images\\n.\\nChromatic\\n (color) light spans the electromagnetic energy spectrum from approxi-\\nmately 0.43 to 0.79 \\nm\\nm,\\n as noted previously. In addition to frequency, three other \\nquantities are used to describe a chromatic light source:\\n radiance, luminance, and \\nbrightness. \\nRadiance\\n is the total amount of energy that ﬂows from the light source, \\nand it is usually measured in watts (W). \\nLuminance\\n, measured in lumens (lm), gives \\na measure of the amount of energy an observer \\nperceives\\n from a light source. For \\nexample, light emitted from a source operating in the far infrared region of the \\nspectrum could have signiﬁcant energy (radiance), but an observer would hardly \\nperceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, \\nbrightness\\n is a subjective descriptor of light perception that is practically impossible \\nto measure. It embodies the achromatic notion of intensity and is one of the key fac-\\ntors in describing color sensation.\\nIn principle, if a sensor can be developed that is capable of detecting energy \\nradiated in a band of the electromagnetic spectrum, we can image events of inter-\\nest in that band. Note, however, that the wavelength of an electromagnetic wave \\nrequired to “see” an object must be of the same size as, or smaller than, the object. \\nFor example, a water molecule has a diameter on the order of \\n10\\n10\\n−\\n m. Thus, to study \\nthese molecules, we would need a source capable of emitting energy in the far (high-\\nenergy) ultraviolet band or soft (low-energy) X-ray bands. \\nAlthough imaging is based predominantly on energy from electromagnetic wave \\nradiation, this is not the only method for generating images. For example, we saw in \\nSection 1.3 that sound reﬂected from objects can be used to form ultrasonic images. \\nOther sources of digital images are electron beams for electron microscopy, and \\nsoftware for generating synthetic images used in graphics and visualization.\\n2.3 IMAGE SENSING AND ACQUISITION  \\nMost of the images in which we are interested are generated by the combination of \\nan “illumination” source and the reflection or absorption of energy from that source \\nby the elements of the “scene” being imaged. We enclose \\nillumination\\n and \\nscene\\n \\nin quotes to emphasize the fact that they are considerably more general than the \\n2.3\\nDIP4E_GLOBAL_Print_Ready.indb   57\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 58}),\n",
       " Document(page_content='58\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nfamiliar situation in which a visible light source illuminates a familiar 3-D scene. For \\nexample, the illumination may originate from a source of electromagnetic energy, \\nsuch as a radar, infrared, or X-ray system. But, as noted earlier, it could originate \\nfrom less traditional sources, such as ultrasound or even a computer-generated illu-\\nmination pattern. Similarly, the scene elements could be familiar objects, but they \\ncan just as easily be molecules, buried rock formations, or a human brain. Depend-\\ning on the nature of the source, illumination energy is reflected from, or transmitted \\nthrough, objects. An example in the first category is light reflected from a planar \\nsurface. An example in the second category is when X-rays pass through a patient’s \\nbody for the purpose of generating a diagnostic X-ray image. In some applications, \\nthe reflected or transmitted energy is focused onto a photo converter (e.g., a phos-\\nphor screen) that converts the energy into visible light. Electron microscopy and \\nsome applications of gamma imaging use this approach. \\nFigure 2.12 shows the three principal sensor arrangements used to transform inci-\\ndent energy into digital images. The idea is simple: Incoming energy is transformed \\ninto a voltage by a combination of the input electrical power and sensor material \\nthat is responsive to the type of energy being detected. The output voltage wave-\\nform is the response of the sensor, and a digital quantity is obtained by digitizing that \\nresponse. In this section, we look at the principal modalities for image sensing and \\ngeneration. We will discuss image digitizing in Section 2.4.\\nIMAGE ACQUISITION USING A SINGLE SENSING ELEMENT\\nFigure 2.12(a) shows the components of a single sensing element. A familiar sensor \\nof this type is the photodiode, which is constructed of silicon materials and whose \\noutput is a voltage proportional to light intensity. Using a filter in front of a sensor \\nimproves its selectivity. For example, an optical green-transmission filter favors light \\nin the green band of the color spectrum. As a consequence, the sensor output would \\nbe stronger for green light than for other visible light components.\\nIn order to generate a 2-D image using a single sensing element, there has to \\nbe relative displacements in both the \\nx\\n- and \\ny\\n-directions between the sensor and \\nthe area to be imaged. Figure 2.13 shows an arrangement used in high-precision \\nscanning, where a ﬁlm negative is mounted onto a drum whose mechanical rotation \\nprovides displacement in one dimension. The sensor is mounted on a lead screw \\nthat provides motion in the perpendicular direction. A light source is contained \\ninside the drum. As the light passes through the ﬁlm, its intensity is modiﬁed by \\nthe ﬁlm density before it is captured by the sensor. This \"modulation\" of the light \\nintensity causes corresponding variations in the sensor voltage, which are ultimately \\nconverted to image intensity levels by digitization. \\nThis method is an inexpensive way to obtain high-resolution images because \\nmechanical motion can be controlled with high precision. The main disadvantages \\nof this method are that it is slow and not readily portable. Other similar mechanical \\narrangements use a ﬂat imaging bed, with the sensor moving in two linear direc-\\ntions. These types of mechanical digitizers sometimes are referred to as \\ntransmission\\n \\nmicrodensitometers\\n. Systems in which light is reﬂected from the medium, instead \\nof passing through it, are called \\nreﬂection microdensitometers\\n. Another example \\nof imaging with a single sensing element places a laser source coincident with the \\nDIP4E_GLOBAL_Print_Ready.indb   58\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 59}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n59\\nSensing material\\nVoltage waveform out\\nFilter\\nEnergy\\nPower in\\nHousing\\nb\\na\\nc\\nFIGURE 2.12\\n(a) Single sensing \\nelement. \\n(b) Line sensor.  \\n(c) Array sensor.\\nSensor\\nLinear motion\\nOne image line out\\nper increment of rotation\\nand full linear displacement\\nof sensor from left to right\\nFilm\\nRotation\\nFIGURE 2.13\\nCombining a \\nsingle sensing \\nelement with \\nmechanical  \\nmotion to  \\ngenerate a 2-D \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   59\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 60}),\n",
       " Document(page_content='60\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nsensor. Moving mirrors are used to control the outgoing beam in a scanning pattern \\nand to direct the reﬂected laser signal onto the sensor. \\nIMAGE ACQUISITION USING SENSOR STRIPS\\nA geometry used more frequently than single sensors is an in-line sensor strip, as in \\nFig. 2.12(b). The strip provides imaging elements in one direction. Motion perpen-\\ndicular to the strip provides imaging in the other direction, as shown in Fig. 2.14(a). \\nThis arrangement is used in most flat bed scanners. Sensing devices with 4000 or \\nmore in-line sensors are possible. In-line sensors are used routinely in airborne \\nimaging applications, in which the imaging system is mounted on an aircraft that \\nflies at a constant altitude and speed over the geographical area to be imaged. One-\\ndimensional imaging sensor strips that respond to various bands of the electromag-\\nnetic spectrum are mounted perpendicular to the direction of flight. An imaging \\nstrip gives one line of an image at a time, and the motion of the strip relative to \\nthe scene completes the other dimension of a 2-D image. Lenses or other focusing \\nschemes are used to project the area to be scanned onto the sensors.\\nSensor strips in a ring conﬁguration are used in medical and industrial imaging \\nto obtain cross-sectional (“slice”) images of 3-D objects, as Fig. 2.14(b) shows. A \\nrotating X-ray source provides illumination, and X-ray sensitive sensors opposite \\nthe source collect the energy that passes through the object. This is the basis for \\nmedical and industrial computerized axial tomography (CAT) imaging, as indicated \\nin Sections 1.2 and 1.3. The output of the sensors is processed by reconstruction \\nalgorithms whose objective is to transform the sensed data into meaningful cross-\\nsectional images (see Section 5.11). In other words, images are not obtained directly \\nSensor strip\\nLinear \\nmotion\\nImaged area\\nOne image line out per\\nincrement of linear motion\\nImage\\nreconstruction\\n3-D object\\nLinear motion\\nSensor ring\\nX-ray source\\nCross-sectional images\\nof 3-D object\\nSource\\nrotation\\nb a\\nFIGURE 2.14\\n(a) Image  \\nacquisition using \\na linear sensor \\nstrip. (b) Image \\nacquisition using \\na circular sensor \\nstrip.\\nDIP4E_GLOBAL_Print_Ready.indb   60\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 61}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n    \\n61\\nfrom the sensors by motion alone; they also require extensive computer process-\\ning. A 3-D digital volume consisting of stacked images is generated as the object is \\nmoved in a direction perpendicular to the sensor ring. Other modalities of imaging \\nbased on the CAT principle include magnetic resonance imaging (MRI) and posi-\\ntron emission tomography (PET). The illumination sources, sensors, and types of \\nimages are different, but conceptually their applications are very similar to the basic \\nimaging approach shown in Fig. 2.14(b).\\nIMAGE ACQUISITION USING SENSOR ARRAYS\\nFigure 2.12(c) shows individual sensing elements arranged in the form of a 2-D array. \\nElectromagnetic and ultrasonic sensing devices frequently are arranged in this man-\\nner. This is also the predominant arrangement found in digital cameras. A typical \\nsensor for these cameras is a CCD (charge-coupled device) array, which can be \\nmanufactured with a broad range of sensing properties and can be packaged in rug-\\nged arrays of \\n4000 4000\\n*\\n elements or more. CCD sensors are used widely in digital \\ncameras and other light-sensing instruments\\n. The response of each sensor is pro-\\nportional to the integral of the light energy projected onto the surface of the sensor, \\na property that is used in astronomical and other applications requiring low noise \\nimages. Noise reduction is achieved by letting the sensor integrate the input light \\nsignal over minutes or even hours. Because the sensor array in Fig. 2.12(c) is two-\\ndimensional, its key advantage is that a complete image can be obtained by focusing \\nthe energy pattern onto the surface of the array. Motion obviously is not necessary, \\nas is the case with the sensor arrangements discussed in the preceding two sections.\\nFigure 2.15 shows the principal manner in which array sensors are used. This \\nﬁgure shows the energy from an illumination source being reﬂected from a scene \\n(as mentioned at the beginning of this section, the energy also could be transmit-\\nted through the scene). The ﬁrst function performed by the imaging system in Fig. \\n2.15(c) is to collect the incoming energy and focus it onto an image plane. If the illu-\\nmination is light, the front end of the imaging system is an optical lens that projects \\nthe viewed scene onto the focal plane of the lens, as Fig. 2.15(d) shows. The sensor \\narray, which is coincident with the focal plane, produces outputs proportional to the \\nintegral of the light received at each sensor. Digital and analog circuitry sweep these \\noutputs and convert them to an analog signal, which is then digitized by another sec-\\ntion of the imaging system. The output is a digital image, as shown diagrammatically \\nin Fig. 2.15(e). Converting images into digital form is the topic of Section 2.4.\\nA SIMPLE IMAGE FORMATION MODEL\\nAs introduced in Section 1.1, we denote images by two-dimensional functions of the \\nform \\nfx y\\n(,\\n)\\n. The value of \\nf\\n at spatial coordinates \\n(,)\\nxy\\n is a scalar quantity whose \\nphysical meaning is determined by the source of the image\\n, and whose values are \\nproportional to energy radiated by a physical source (e.g., electromagnetic waves). \\nAs a consequence, \\nfx y\\n(,\\n)\\n must be nonnegative\\n†\\n and finite; that is,\\n†\\n  Image intensities can become negative during processing, or as a result of interpretation. For example, in radar \\nimages, objects moving toward the radar often are interpreted as having negative velocities while objects moving \\naway are interpreted as having positive velocities. Thus, a velocity image might be coded as having both positive \\nand negative values. When storing and displaying images, we normally scale the intensities so that the smallest \\nnegative value becomes 0 (see Section 2.6 regarding intensity scaling).\\nIn some cases, the source \\nis imaged directly, as \\nin obtaining images of \\nthe sun.\\nDIP4E_GLOBAL_Print_Ready.indb   61\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='62\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n \\n0\\n≤<\\nfx\\ny\\n(,)\\n/H11009\\n \\n(2-3)\\nFunction \\nfx y\\n(,\\n)\\n is characterized by two components: (1) the amount of source illu-\\nmination incident on the scene being viewed,\\n and (2) the amount of illumination \\nreflected by the objects in the scene. Appropriately, these are called the \\nillumination\\n \\nand \\nreflectance\\n components, and are denoted by \\nixy\\n(,\\n)\\n and \\nrxy\\n(,\\n)\\n, respectively. The \\ntwo functions combine as a product to form \\nfx y\\n(,\\n)\\n:\\n \\nfx y i x yrx y\\n(,\\n) (,)(,)\\n=\\n \\n(2-4)\\nwhere\\n \\n0\\n≤<\\nix\\ny\\n(,)\\n/H11009\\n \\n(2-5)\\nand\\n \\n01\\n≤≤\\nrx\\ny\\n(,)\\n \\n(2-6)\\nThus, reflectance is bounded by 0 (total absorption) and 1 (total reflectance). The \\nnature of \\nixy\\n(,\\n)\\n is determined by the illumination source, and \\nrxy\\n(,\\n)\\n is determined \\nby the characteristics of the imaged objects\\n. These expressions are applicable also \\nto images formed via transmission of the illumination through a medium, such as a \\nIllumination (energy)\\nsource\\nImaging system\\n(Internal) image plane\\nOutput (digitized) image\\nScene\\nb\\na\\nd\\nc\\ne\\nFIGURE 2.15\\n  An example of digital image acquisition. (a) Illumination (energy) source. (b) A scene. (c) Imaging \\nsystem. (d) Projection of the scene onto the image plane. (e) Digitized image.\\nDIP4E_GLOBAL_Print_Ready.indb   62\\n6/16/2017   2:02:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 63}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n63\\nchest X-ray. In this case, we would deal with a \\ntransmissivity\\n instead of a \\nreflectivity\\n \\nfunction, but the limits would be the same as in Eq. (2-6), and the image function \\nformed would be modeled as the product in Eq. (2-4).\\nEXAMPLE 2.1 :  Some typical values of illumination and reﬂectance.\\nThe following numerical quantities illustrate some typical values of illumination and reﬂectance for \\nvisible light. On a clear day, the sun may produce in excess of \\n90 000\\n,\\n lm/m\\n2\\n of illumination on the sur-\\nface of the earth. This value decreases to less than \\n10 000\\n,\\n lm/m\\n2\\n on a cloudy day. On a clear evening, a \\nfull moon yields about \\n01\\n.\\n lm/m\\n2\\n of illumination. The typical illumination level in a commercial ofﬁce \\nis about \\n1 000\\n,\\n lm/m\\n2\\n. Similarly, the following are typical values of \\nrxy\\n(,\\n)\\n: 0.01 for black velvet, 0.65 for \\nstainless steel,\\n 0.80 for ﬂat-white wall paint, 0.90 for silver-plated metal, and 0.93 for snow. \\nLet the intensity (gray level) of a monochrome image at any coordinates \\n(,)\\nxy\\n \\nbe denoted by \\n \\n/\\n=\\nfx\\ny\\n(,)\\n \\n(2-7)\\nFrom Eqs. (2-4) through (2-6) it is evident that \\n/\\n lies in the range\\n \\nLL\\nmin\\nmax\\n≤≤\\n/\\n \\n(2-8)\\nIn theory, the requirement on \\nL\\nmin\\n is that it be nonnegative, and on \\nL\\nmax\\n that it \\nbe finite. In practice, \\nLi r\\nmin min min\\n=\\n and \\nLi r\\nmax max max\\n=\\n. From Example 2.1, using \\naverage office illumination and reflectance values as guidelines\\n, we may expect \\nL\\nmin\\n≈\\n10\\n and \\nL\\nmax\\n≈\\n1000\\n to be typical indoor values in the absence of additional \\nillumination.\\n The units of these quantities are \\nlum/m\\n2\\n.\\n However, actual units sel-\\ndom are of interest,\\n except in cases where photometric measurements are being \\nperformed.\\nThe interval \\n[, ]\\nmin max\\nLL\\n is called the \\nintensity\\n (or \\ngra\\ny\\n) \\nscale\\n. Common practice is \\nto shift this interval numerically to the interval \\n[,] ,\\n01\\n or \\n[, ] ,\\n0\\nC\\n where \\n/\\n=\\n0\\n is consid-\\nered black and \\n/\\n=\\n1 (or \\n)\\nC\\n is considered white on the scale. All intermediate values \\nare shades of gray varying from black to white\\n.\\n2.4  IMAGE SAMPLING AND QUANTIZATION  \\nAs discussed in the previous section, there are numerous ways to acquire images, but \\nour objective in all is the same: to generate digital images from sensed data. The out-\\nput of most sensors is a continuous voltage waveform whose amplitude and spatial \\nbehavior are related to the physical phenomenon being sensed. To create a digital \\nimage, we need to convert the continuous sensed data into a digital format. This \\nrequires two processes: \\nsampling\\n and \\nquantization\\n.\\nBASIC CONCEPTS IN SAMPLING AND QUANTIZATION\\nFigure 2.16(a) shows a continuous image \\nf\\n that we want to convert to digital form. \\nAn image may be continuous with respect to the \\nx\\n- and \\ny\\n-coordinates, and also in \\n2.4\\nThe discussion of sam-\\npling in this section is of \\nan intuitive nature. We \\nwill discuss this topic in \\ndepth in Chapter 4.\\nDIP4E_GLOBAL_Print_Ready.indb   63\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 64}),\n",
       " Document(page_content='64\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\namplitude. To digitize it, we have to sample the function in both coordinates and \\nalso in amplitude. Digitizing the coordinate values is called \\nsampling\\n. Digitizing the \\namplitude values is called \\nquantization\\n.\\nThe one-dimensional function in Fig. 2.16(b) is a plot of amplitude (intensity \\nlevel) values of the continuous image along the line segment \\nAB\\n in Fig. 2.16(a). The \\nrandom variations are due to image noise. To sample this function, we take equally \\nspaced samples along line \\nAB\\n, as shown in Fig. 2.16(c). The samples are shown as \\nsmall dark squares superimposed on the function, and their (discrete) spatial loca-\\ntions are indicated by corresponding tick marks in the bottom of the ﬁgure. The set \\nof dark squares constitute the \\nsampled\\n function. However, the \\nvalues\\n of the sam-\\nples still span (vertically) a continuous range of intensity values. In order to form a \\ndigital function, the intensity values also must be converted (\\nquantized\\n) into \\ndiscrete\\n \\nquantities. The vertical gray bar in Fig. 2.16(c) depicts the intensity scale divided \\ninto eight discrete intervals, ranging from black to white. The vertical tick marks \\nindicate the speciﬁc value assigned to each of the eight intensity intervals. The con-\\ntinuous intensity levels are quantized by assigning one of the eight values to each \\nsample, depending on the vertical proximity of a sample to a vertical tick mark. The \\ndigital samples resulting from both sampling and quantization are shown as white \\nsquares in Fig. 2.16(d). Starting at the top of the continuous image and carrying out \\nthis procedure downward, line by line, produces a two-dimensional digital image. \\nIt is implied in Fig. 2.16 that, in addition to the number of discrete levels used, the \\naccuracy achieved in quantization is highly dependent on the noise content of the \\nsampled signal. \\nb a\\nd c\\nFIGURE 2.16\\n(a) Continuous \\nimage. (b) A \\nscan line show-\\ning intensity \\nvariations along \\nline \\nAB\\n in the \\ncontinuous image. \\n(c) Sampling and \\nquantization.  \\n(d) Digital scan \\nline. (The black \\nborder in (a) is \\nincluded for  \\nclarity. It is not \\npart of the image).\\nAB\\nAB\\nSampling\\nAB\\nAB\\nQuantization\\nDIP4E_GLOBAL_Print_Ready.indb   64\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 65}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n65\\nIn practice, the method of sampling is determined by the sensor arrangement \\nused to generate the image. When an image is generated by a single sensing element \\ncombined with mechanical motion, as in Fig. 2.13, the output of the sensor is quan-\\ntized in the manner described above. However, spatial sampling is accomplished by \\nselecting the number of individual mechanical increments at which we activate the \\nsensor to collect data. Mechanical motion can be very exact so, in principle, there is \\nalmost no limit on how ﬁne we can sample an image using this approach. In practice, \\nlimits on sampling accuracy are determined by other factors, such as the quality of \\nthe optical components used in the system.\\nWhen a sensing strip is used for image acquisition, the number of sensors in the \\nstrip establishes the samples in the resulting image in one direction, and mechanical \\nmotion establishes the number of samples in the other. Quantization of the sensor \\noutputs completes the process of generating a digital image.\\nWhen a sensing array is used for image acquisition, no motion is required. The \\nnumber of sensors in the array establishes the limits of sampling in both directions. \\nQuantization of the sensor outputs is as explained above. Figure 2.17 illustrates this \\nconcept. Figure 2.17(a) shows a continuous image projected onto the plane of a 2-D \\nsensor. Figure 2.17(b) shows the image after sampling and quantization. The quality \\nof a digital image is determined to a large degree by the number of samples and dis-\\ncrete intensity levels used in sampling and quantization. However, as we will show \\nlater in this section, image content also plays a role in the choice of these parameters.\\nREPRESENTING DIGITAL IMAGES\\nLet \\nfst\\n(,\\n)\\n represent a \\ncontinuous\\n image function of two continuous variables\\n, \\ns\\n and \\nt\\n. We convert this function into a \\ndigital image\\n by sampling and quantization, as \\nexplained in the previous section. Suppose that we sample the continuous image \\ninto a digital image, \\nfx y\\n(,\\n)\\n, containing \\nM\\n rows and \\nN\\n columns\\n, where \\n(,)\\nxy\\n are \\ndiscrete coordinates\\n. For notational clarity and convenience, we use integer values \\nfor these discrete coordinates: \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, ,\\n…\\n. Thus, \\nfor example\\n, the value of the digital image at the origin is \\nf\\n(,\\n)\\n00\\n, and its value at \\nthe next coordinates along the first row is \\nf\\n(,\\n)\\n01\\n. Here, the notation (0, 1) is used \\nb a\\nFIGURE 2.17\\n(a) Continuous \\nimage projected \\nonto a sensor \\narray. (b) Result \\nof image sampling \\nand quantization.\\nDIP4E_GLOBAL_Print_Ready.indb   65\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 66}),\n",
       " Document(page_content='66\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nto denote the second sample along the first row. It \\ndoes not\\n mean that these are \\nthe values of the physical coordinates when the image was sampled. In general, the \\nvalue of a digital image at any coordinates \\n(,)\\nxy\\n is denoted \\nfx y\\n(,\\n)\\n, where \\nx\\n and \\ny\\n \\nare integers\\n. When we need to refer to specific coordinates \\n(, )\\nij\\n, we use the notation \\nfij\\n(,\\n) ,\\n where the arguments are integers. The section of the real plane spanned by \\nthe coordinates of an image is called the \\nspatial domain\\n,\\n with \\nx\\n and \\ny\\n being referred \\nto as \\nspatial variables\\n or \\nspatial coordinates\\n.\\nFigure 2.18\\n \\nshows three ways of representing \\nfx y\\n(,\\n)\\n. Figure 2.18(a) is a plot of \\nthe function,\\n with two axes determining spatial location and the third axis being the \\nvalues of \\nf\\n as a function of \\nx \\nand \\ny\\n. This representation is useful when working with \\ngrayscale sets whose elements are expressed as triplets of the form \\n(,,)\\nxy\\nz\\n, where \\nx\\n and \\ny\\n are spatial coordinates and \\nz\\n is the value of \\nf\\n at coordinates \\n(,) .\\nxy\\n We will \\nwork with this representation brieﬂy in Section 2.6.\\nT\\nhe representation in Fig. 2.18(b) is more common, and it shows \\nfx y\\n(,\\n)\\n as it would \\nappear on a computer display or photograph.\\n Here, the intensity of each point in the \\ndisplay is proportional to the value of \\nf\\n at that point. In this ﬁgure, there are only \\nthree equally spaced intensity values. If the intensity is normalized to the interval \\n[,] ,\\n01\\n then each point in the image has the value 0, 0.5, or 1. A monitor or printer con-\\nverts these three values to black,\\n gray, or white, respectively, as in Fig. 2.18(b). This \\ntype of representation includes color images, and allows us to view results at a glance.\\nAs Fig. 2.18(c) shows, the third representation is an array (matrix) composed of \\nthe numerical values of \\nfx y\\n(,\\n)\\n. This is the representation used for computer process-\\ning\\n. In equation form, we write the representation of an \\nMN\\n*\\n numerical array as\\n \\nfx y\\nff\\nf N\\nff f N\\nfM\\n(,)\\n(,) (,) (, )\\n(, ) (,) (, )\\n(,\\n=\\n−\\n−\\n−\\n00 01\\n0 1\\n10 11\\n1 1\\n1\\n/midhorizellipsis\\n/midhorizellipsis\\n/vertellipsis/vertellipsis /vertellipsis\\n0\\n01 1 1 1\\n)( , ) ( , )\\nfM fM N\\n−− −\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n/midhorizellipsis\\n \\n(2-9)\\nThe right side of this equation is a digital image represented as an array of real \\nnumbers\\n. Each element of this array is called an \\nimage element\\n, \\npicture element\\n, \\npixel\\n, \\nor \\npel\\n. We use the terms \\nimage\\n and \\npixel\\n throughout the book to denote a digital \\nimage and its elements. Figure 2.19 shows a graphical representation of an image \\narray, where the \\nx\\n- and \\ny\\n-axis are used to denote the rows and columns of the array. \\nSpeciﬁc pixels are values of the array at a ﬁxed pair of coordinates. As mentioned \\nearlier, we generally use \\nfij\\n(,\\n)\\n when referring to a pixel with coordinates \\n(, ) .\\nij\\nWe can also represent a digital image in a traditional matrix form:\\n \\nA\\n=\\n⎡\\n⎣\\n−\\n−\\n−− − −\\naa a\\naa a\\naa a\\nN\\nN\\nMM M N\\n00 01\\n0 1\\n10 11\\n1 1\\n10 11\\n11\\n,, ,\\n,, ,\\n,, ,\\n/midhorizellipsis\\n/midhorizellipsis\\n/vertellipsis/vertellipsis /vertellipsis\\n/midhorizellipsis\\n⎢ ⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(2-10)\\nClearly, \\naf i j\\nij\\n=\\n(, ) ,\\n so Eqs. (2-9) and (2-10) denote identical arrays.\\nDIP4E_GLOBAL_Print_Ready.indb   66\\n6/16/2017   2:02:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 67}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n67\\nAs Fig. 2.19 shows, we deﬁne the \\norigin\\n of an image at the top left corner. This is \\na convention based on the fact that many image displays (e.g., TV monitors) sweep \\nan image starting at the top left and moving to the right, one row at a time. More \\nimportant is the fact that the ﬁrst element of a matrix is by convention at the top \\nleft of the array. Choosing the origin of \\nfx y\\n(,\\n)\\n at that point makes sense mathemati-\\ncally because digital images in reality are matrices\\n. In fact, as you will see, sometimes \\nwe use \\nx\\n and \\ny\\n interchangeably in equations with the \\nrows\\n (\\nr\\n) and \\ncolumns\\n (\\nc\\n) of a \\nmatrix.\\nIt is important to note that the representation in Fig. 2.19, in which the positive \\nx\\n-axis extends downward and the positive \\ny\\n-axis extends to the right, is precisely the \\nright-handed Cartesian coordinate system with which you are familiar,\\n†\\n but shown \\nrotated by 90\\n°\\n so that the origin appears on the top, left.\\n†\\n Recall that a right-handed coordinate system is such that, when the index of the right hand points in the direc-\\ntion of the positive \\nx\\n-axis and the middle ﬁnger points in the (perpendicular) direction of the positive \\ny\\n-axis, the \\nthumb points up. As Figs. 2.18 and 2.19 show, this indeed is the case in our image coordinate system. In practice, \\nyou will also ﬁnd implementations based on a left-handed system, in which the \\nx\\n- and \\ny\\n-axis are interchanged \\nfrom the way we show them in Figs. 2.18 and 2.19. For example, MATLAB uses a left-handed system for image \\nprocessing. Both systems are perfectly valid, provided they are used consistently.\\nx\\ny\\nf\\n(\\nx, y\\n)\\n.5\\ny\\nx\\nOrigin\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1 1\\n1\\n.5\\n.5\\n.5\\n.5\\n.5\\n.5\\nb\\na\\nc\\nFIGURE 2.18\\n(a) Image plotted \\nas a surface.  \\n(b) Image displayed \\nas a visual intensity \\narray. (c) Image \\nshown as a 2-D nu-\\nmerical array. (The \\nnumbers 0, .5, and \\n1 represent black, \\ngray, and white, \\nrespectively.)\\nDIP4E_GLOBAL_Print_Ready.indb   67\\n6/16/2017   2:02:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 68}),\n",
       " Document(page_content='68\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe \\ncenter\\n of an \\nMN\\n×\\n digital image with origin at \\n(,)\\n00\\n and range to \\n(,)\\nMN\\n−−\\n11\\nis obtained by dividing \\nM\\n and \\nN\\n by 2 and rounding \\ndo\\nwn\\n to the nearest integer. \\nThis operation sometimes is denoted using the ﬂoor operator, \\nJK\\ni\\n,\\n as shown in Fig. \\n2.19.\\n This holds true for \\nM\\n and \\nN\\n even \\nor\\n odd. For example, the center of an image \\nof size \\n1023 1024\\n×\\n is at \\n(, ) .\\n511\\n512\\n Some programming languages (e.g., MATLAB) \\nstart indexing at 1 instead of at 0.\\n The center of an image in that case is found at \\n(,) ( ) , ( ) .\\nxy M N\\ncc\\n=+ +\\n()\\nfloor floor\\n21 21\\nTo express sampling and quantization in more formal mathematical terms, let \\nZ\\n and \\nR\\n denote the set of integers and the set of real numbers\\n, respectively. The \\nsampling process may be viewed as partitioning the \\nxy\\n-plane into a grid, with the \\ncoordinates of the center of each cell in the grid being a pair of elements from the \\nCartesian product \\nZ\\n2\\n (also denoted \\nZZ\\n×\\n)\\n which, as you may recall, is the set of \\nall ordered pairs of elements \\n(, )\\nzz\\nij\\n with \\nz\\ni\\n and \\nz\\nj\\n being integers from set \\nZ\\n. Hence, \\nfx y\\n(,\\n)\\n is a digital image if \\n(,)\\nxy\\n are integers from \\nZ\\n2\\n and \\nf\\n is a function that assigns \\nan intensity value (that is, a real number from the set of real numbers, \\nR\\n) to each \\ndistinct pair of coordinates \\n(,)\\nxy\\n. This functional assignment is the quantization pro-\\ncess described earlier\\n. If the intensity levels also are integers, then\\nRZ\\n=\\n,\\n and a \\ndigital image becomes a 2-D function whose coordinates and amplitude values are \\nintegers\\n. This is the representation we use in the book.\\nImage digitization requires that decisions be made regarding the values for \\nM\\n, \\nN\\n, \\nand for the number, \\nL\\n, of discrete intensity levels. There are no restrictions placed \\non \\nM\\n and \\nN\\n, other than they have to be positive integers. However, digital storage \\nand quantizing hardware considerations usually lead to the number of intensity lev-\\nels, \\nL\\n, being an integer power of two; that is\\nL\\nk\\n=\\n2\\n(2-11)\\nwhere \\nk\\n is an integer\\n. We assume that the discrete levels are equally spaced and that \\nthey are integers in the range \\n[, ]\\n01\\nL\\n−\\n. \\nThe \\nﬂoor\\n of \\nz\\n, sometimes \\ndenoted \\nJ\\nz\\nK\\n, is the largest \\ninteger that is less than \\nor equal to \\nz\\n. The \\nceiling\\n \\nof \\nz\\n, denoted \\nL\\nz\\nM\\n, is the \\nsmallest integer that is \\ngreater than or equal \\nto \\nz\\n.\\nSee Eq. (2-41) in  \\nSection 2.6 for a formal \\ndeﬁnition of the  \\nCartesian product.\\nFIGURE 2.19\\nCoordinate  \\nconvention used \\nto represent digital \\nimages. Because \\ncoordinate values \\nare integers, there \\nis a one-to-one \\ncorrespondence \\nbetween \\nx\\n and \\ny\\n \\nand the rows (\\nr\\n) \\nand columns (\\nc\\n) of \\na matrix.\\nOrigin\\n0\\nN \\n-\\n1\\n-\\n1\\nM\\n0\\ny\\nx\\ni\\nj \\npixel \\nf\\n(\\ni\\n, \\nj\\n)\\nImage \\nf\\n(\\nx\\n, \\ny\\n)\\n1\\n1\\n2\\nCenter\\nThe coordinates of the \\nimage center are\\nx\\nc\\ny\\nc\\nx\\nc\\n, \\ny\\nc\\n  =  \\nN\\n2\\nQR\\nfloor\\nM\\n2\\nQR\\nfloor ,\\na\\nb\\nB A\\nDIP4E_GLOBAL_Print_Ready.indb   68\\n6/16/2017   2:02:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 69}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n69\\nSometimes, the range of values spanned by the gray scale is referred to as the \\ndynamic range\\n, a term used in different ways in different ﬁelds. Here, we deﬁne the \\ndynamic range of an imaging system to be the ratio of the maximum measurable \\nintensity to the minimum detectable intensity level in the system. As a rule, the \\nupper limit is determined by \\nsaturation\\n and the lower limit by \\nnoise\\n, although noise \\ncan be present also in lighter intensities. Figure 2.20 shows examples of saturation \\nand slight visible noise. Because the darker regions are composed primarily of pixels \\nwith the minimum detectable intensity, the background in Fig. 2.20 is the noisiest \\npart of the image; however, dark background noise typically is much harder to see. \\nThe dynamic range establishes the lowest and highest intensity levels that a system \\ncan represent and, consequently, that an image can have. Closely associated with this \\nconcept is \\nimage contrast\\n, which we deﬁne as the difference in intensity between \\nthe highest and lowest intensity levels in an image. The \\ncontrast ratio\\n is the ratio of \\nthese two quantities. When an appreciable number of pixels in an image have a high \\ndynamic range, we can expect the image to have high contrast. Conversely, an image \\nwith low dynamic range typically has a dull, washed-out gray look. We will discuss \\nthese concepts in more detail in Chapter 3.\\nThe number, \\nb\\n, of bits required to store a digital image is\\n \\nbMNk\\n=\\n**\\n \\n(2-12)\\nWhen \\nMN\\n=\\n, this equation becomes\\n \\nbN k\\n=\\n2\\n \\n(2-13)\\nNoise\\nSaturation\\nFIGURE 2.20\\nAn image exhibit-\\ning saturation and \\nnoise. Saturation \\nis the highest val-\\nue beyond which \\nall intensity values \\nare clipped (note \\nhow the entire \\nsaturated area has \\na high, constant \\nintensity level). \\nVisible noise in \\nthis case appears \\nas a grainy texture \\npattern. The dark \\nbackground is \\nnoisier, but the \\nnoise is difﬁcult \\nto see.\\nDIP4E_GLOBAL_Print_Ready.indb   69\\n6/16/2017   2:02:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 70}),\n",
       " Document(page_content='70\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nFigure 2.21 shows the number of megabytes required to store square images for \\nvarious values of \\nN\\n and \\nk\\n (as usual, one byte equals 8 bits and a megabyte equals \\n10\\n6\\n bytes). \\nWhen an image can have \\n2\\nk\\n possible intensity levels, it is common practice to \\nrefer to it as a “\\nk\\n-bit image,” (e,g., a 256-level image is called an \\n8-bit image\\n). Note \\nthat storage requirements for large 8-bit images (e.g., \\n10 000 10 000\\n,,\\n*\\n pixels) are \\nnot insigniﬁcant.\\nLINEAR VS. COORDINATE INDEXING\\nThe convention discussed in the previous section, in which the location of a pixel is \\ngiven by its 2-D coordinates, is referred to as \\ncoordinate indexing\\n, or \\nsubscript index-\\ning\\n. Another type of indexing used extensively in programming image processing \\nalgorithms is \\nlinear indexing\\n, which consists of a 1-D string of nonnegative integers \\nbased on computing offsets from coordinates \\n(,)\\n00\\n. There are two principal types of \\nlinear indexing\\n, one is based on a row scan of an image, and the other on a column scan.\\nFigure 2.22 illustrates the principle of linear indexing based on a column scan. \\nThe idea is to scan an image column by column, starting at the origin and proceeding \\ndown and then to the right. The linear index is based on counting pixels as we scan \\nthe image in the manner shown in Fig. 2.22. Thus, a scan of the ﬁrst (leftmost) column \\nyields linear indices 0 through \\nM\\n−\\n1\\n. A scan of the second column yields indices \\nM\\n \\nthrough \\n21\\nM\\n−\\n, and so on, until the last pixel in the last column is assigned the linear \\nindex value \\nMN\\n−\\n1\\n. Thus, a linear index, denoted by \\na\\n, has one of \\nMN\\n possible \\nvalues:\\n \\n012 1\\n,,\\n, ,\\n…\\nMN\\n−\\n, as Fig. 2.22 shows. The important thing to notice here is \\nthat each pixel is assigned a linear index value that identiﬁes it uniquely\\n.\\nThe formula for generating linear indices based on a column scan is straightfor-\\nward and can be determined by inspection. For any pair of coordinates \\n(, )\\nxy\\n, the \\ncorresponding linear index value is\\n \\na\\n=+\\nMy\\nx\\n \\n(2-14)\\nN\\n*\\n10\\n3\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n123456789 1 0\\nk\\n = 8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n0\\nMegabytes (\\n    \\n            )\\n*\\nb\\n8\\n10\\n6\\nFIGURE 2.21\\nNumber of  \\nmegabytes \\nrequired to store \\nimages for  \\nvarious values of \\nN\\n and \\nk\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   70\\n6/16/2017   2:02:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 71}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n71\\nConversely, the coordinate indices for a given linear index value \\na\\n are given by the \\nequations\\n†\\nxM\\n=\\na\\nmod\\n(2-15)\\nand\\n \\nyx M\\n=\\n()\\na\\n-\\n(2-16)\\nRecall that \\na\\nmo\\nd\\nM\\n means “the remainder of the division of \\na\\n by \\nM\\n.\\n” This is a \\nformal way of stating that row numbers repeat themselves at the start of every col-\\numn. Thus, when \\na\\n=\\n0, the remainder of the division of 0 by \\nM\\n is 0,\\n so \\nx\\n=\\n0\\n. When \\na\\n=\\n1,\\n the remainder is 1, and so \\nx\\n=\\n1\\n. You can see that \\nx\\n will continue to be equal \\nto \\na\\n until \\na\\n=−\\nM\\n1\\n. When \\na\\n=\\nM\\n (which is at the beginning of the second column), \\nthe remainder is 0,\\n and thus \\nx\\n=\\n0\\n again, and it increases by 1 until the next column \\nis reached,\\n when the pattern repeats itself. Similar comments apply to Eq. (2-16). See \\nProblem 2.11 for a derivation of the preceding two equations.\\nSPATIAL AND INTENSITY RESOLUTION\\nIntuitively, \\nspatial resolution\\n is a measure of the smallest discernible detail in an \\nimage. Quantitatively, spatial resolution can be stated in several ways, with \\nline \\npairs per unit distance\\n, and \\ndots (pixels) per unit distance\\n being common measures. \\nSuppose that we construct a chart with alternating black and white vertical lines, \\neach of width \\nW\\n units (\\nW\\n can be less than 1). The width of a \\nline pair\\n is thus 2\\nW\\n, and \\nthere are \\nW\\n2\\n line pairs per unit distance. For example, if the width of a line is 0.1 mm, \\nthere are 5 line pairs per unit distance (i.e\\n., per mm). A widely used definition of \\nimage resolution is the largest number of \\ndiscernible\\n line pairs per unit distance (e.g., \\n100 line pairs per mm). Dots per unit distance is a measure of image resolution used \\nin the printing and publishing industry. In the U.S., this measure usually is expressed \\nas \\ndots per inch\\n (dpi). To give you an idea of quality, newspapers are printed with a \\n†\\nWhen working with modular number systems, it is more accurate to write \\nxM\\n≡\\na\\nmo\\nd\\n, where the symbol \\n≡\\nmeans \\ncongruence\\n.\\n However, our interest here is just on converting from linear to coordinate indexing, so we \\nuse the more familiar equal sign.\\nx\\ny\\nImage\\n f\\n(\\nx\\n, \\ny\\n)\\n(0, 0)  \\nα\\n = 0\\n(\\nM\\n \\n-\\n 1, 0)  \\nα\\n = \\nM\\n \\n-\\n 1\\n(\\nM\\n \\n-\\n 1, \\nN\\n \\n-\\n 1)  \\nα\\n = \\nMN\\n \\n-\\n 1\\n(0, 1)  \\nα\\n = \\nM\\n(0, 2)  \\nα\\n = 2\\nM\\n(\\nM\\n \\n-\\n 1, 1)  \\nα\\n = 2\\nM\\n \\n-\\n 1\\nI\\nma\\ng\\ne\\n \\nf\\n(\\nf\\nf\\nx\\n,\\n \\ny\\n)\\nFIGURE 2.22\\nIllustration of  \\ncolumn scanning \\nfor generating  \\nlinear indices. \\nShown are several \\n2-D coordinates (in \\nparentheses) and \\ntheir corresponding \\nlinear indices.\\nDIP4E_GLOBAL_Print_Ready.indb   71\\n6/16/2017   2:02:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 72}),\n",
       " Document(page_content='72\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nresolution of 75 dpi, magazines at 133 dpi, glossy brochures at 175 dpi, and the book \\npage at which you are presently looking was printed at 2400 dpi. \\nTo be meaningful, measures of spatial resolution must be stated with respect to \\nspatial units. Image size by itself does not tell the complete story. For example, to say \\nthat an image has a resolution of \\n1024 1024\\n*\\n pixels is not a meaningful statement \\nwithout stating the spatial dimensions encompassed by the image\\n. Size by itself is \\nhelpful only in making comparisons between imaging capabilities. For instance, a \\ndigital camera with a 20-megapixel CCD imaging chip can be expected to have a \\nhigher capability to resolve detail than an 8-megapixel camera, assuming that both \\ncameras are equipped with comparable lenses and the comparison images are taken \\nat the same distance.\\nIntensity resolution\\n similarly refers to the smallest \\ndiscernible\\n change in inten-\\nsity level. We have considerable discretion regarding the number of spatial samples \\n(pixels) used to generate a digital image, but this is not true regarding the number \\nof intensity levels. Based on hardware considerations, the number of intensity levels \\nusually is an integer power of two, as we mentioned when discussing Eq. (2-11). The \\nmost common number is 8 bits, with 16 bits being used in some applications in which \\nenhancement of speciﬁc intensity ranges is necessary. Intensity quantization using \\n32 bits is rare. Sometimes one ﬁnds systems that can digitize the intensity levels of \\nan image using 10 or 12 bits, but these are not as common. \\nUnlike spatial resolution, which must be based on a per-unit-of-distance basis to \\nbe meaningful, it is common practice to refer to the number of bits used to quan-\\ntize intensity as the “\\nintensity resolution\\n.” For example, it is common to say that an \\nimage whose intensity is quantized into 256 levels has 8 bits of intensity resolution. \\nHowever, keep in mind that \\ndiscernible\\n changes in intensity are inﬂuenced also by \\nnoise and saturation values, and by the capabilities of human perception to analyze \\nand interpret details in the context of an entire scene (see Section 2.1). The following \\ntwo examples illustrate the effects of spatial and intensity resolution on discernible \\ndetail. Later in this section, we will discuss how these two parameters interact in \\ndetermining perceived image quality.\\nEXAMPLE 2.2 : Effects of reducing the spatial resolution of a digital image.\\nFigure 2.23 shows the effects of reducing the spatial resolution of an image. The images in Figs. 2.23(a) \\nthrough (d) have resolutions of 930, 300, 150, and 72 dpi, respectively. Naturally, the lower resolution \\nimages are smaller than the original image in (a). For example, the original image is of size \\n2136 2140\\n*\\n \\npixels\\n, but the 72 dpi image is an array of only \\n165 166\\n*\\n pixels. In order to facilitate comparisons, all the \\nsmaller images were zoomed back to the original size (the method used for zooming will be discussed \\nlater in this section).\\n This is somewhat equivalent to “getting closer” to the smaller images so that we can \\nmake comparable statements about visible details. \\nThere are some small visual differences between Figs. 2.23(a) and (b), the most notable being a slight \\ndistortion in the seconds marker pointing to 60 on the right side of the chronometer. For the most part, \\nhowever, Fig. 2.23(b) is quite acceptable. In fact, 300 dpi is the typical minimum image spatial resolution \\nused for book publishing, so one would not expect to see much difference between these two images. \\nFigure 2.23(c) begins to show visible degradation (see, for example, the outer edges of the chronometer \\nDIP4E_GLOBAL_Print_Ready.indb   72\\n6/16/2017   2:02:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 73}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n73\\ncase and compare the seconds marker with the previous two images). The numbers also show visible \\ndegradation. Figure 2.23(d) shows degradation that is visible in most features of the image. When print-\\ning at such low resolutions, the printing and publishing industry uses a number of techniques (such as \\nlocally varying the pixel size) to produce much better results than those in Fig. 2.23(d). Also, as we will \\nshow later in this section, it is possible to improve on the results of Fig. 2.23 by the choice of interpola-\\ntion method used.\\nEXAMPLE 2.3 :  Effects of varying the number of intensity levels in a digital image.\\nFigure 2.24(a) is a \\n774 640\\n×\\n CT projection image, displayed using 256 intensity levels (see Chapter 1 \\nregarding CT images).\\n The objective of this example is to reduce the number of intensities of the image \\nfrom 256 to 2 in integer powers of 2, while keeping the spatial resolution constant. Figures 2.24(b) \\nthrough (d) were obtained by reducing the number of intensity levels to 128, 64, and 32, respectively (we \\nwill discuss in Chapter 3 how to reduce the number of levels). \\nb a\\nd c\\nFIGURE 2.23\\nEffects of  \\nreducing spatial \\nresolution. The \\nimages shown \\nare at:  \\n(a) 930 dpi,  \\n(b) 300 dpi,  \\n(c) 150 dpi, and \\n(d) 72 dpi.\\nDIP4E_GLOBAL_Print_Ready.indb   73\\n6/16/2017   2:02:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 74}),\n",
       " Document(page_content='74\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe 128- and 64-level images are visually identical for all practical purposes. However, the 32-level image \\nin Fig. 2.24(d) has a set of almost imperceptible, very ﬁne ridge-like structures in areas of constant inten-\\nsity. These structures are clearly visible in the 16-level image in Fig. 2.24(e). This effect, caused by using \\nan insufﬁcient number of intensity levels in smooth areas of a digital image, is called \\nfalse contouring\\n, so \\nnamed because the ridges resemble topographic contours in a map. False contouring generally is quite \\nobjectionable in images displayed using 16 or fewer uniformly spaced intensity levels, as the images in \\nFigs. 2.24(e)-(h) show. \\nAs a very rough guideline, and assuming integer powers of 2 for convenience, images of size \\n256 256\\n*\\n \\npixels with 64 intensity levels\\n, and printed on a size format on the order of \\n55\\n*\\n cm, are about the lowest \\nspatial and intensity resolution images that can be expected to be reasonably free of objectionable sam-\\npling distortions and false contouring\\n.\\nb a\\nd c\\nFIGURE 2.24\\n(a) 774 \\n×\\n 640, \\n256-level image. \\n(b)-(d) Image  \\ndisplayed in 128, \\n64, and 32 inten-\\nsity levels, while  \\nkeeping the  \\nspatial resolution  \\nconstant.  \\n(Original image \\ncourtesy of the \\nDr. David R.  \\nPickens,  \\nDepartment of \\nRadiology & \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   74\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 75}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n75\\nThe results in Examples 2.2 and 2.3 illustrate the effects produced on image qual-\\nity by varying spatial and intensity resolution independently. However, these results \\ndid not consider any relationships that might exist between these two parameters. \\nAn early study by Huang [1965] attempted to quantify experimentally the effects on \\nimage quality produced by the interaction of these two variables. The experiment \\nconsisted of a set of subjective tests. Images similar to those shown in Fig. 2.25 were \\nused. The woman’s face represents an image with relatively little detail; the picture \\nof the cameraman contains an intermediate amount of detail; and the crowd picture \\ncontains, by comparison, a large amount of detail. \\nSets of these three types of images of various sizes and intensity resolution were \\ngenerated by varying \\nN\\n and \\nk\\n [see Eq. (2-13)]. Observers were then asked to rank \\nf e\\nh\\ng\\nFIGURE 2.24\\n(\\nContinued\\n) \\n(e)-(h) Image \\ndisplayed in 16, 8, \\n4, and 2 intensity \\nlevels. \\nDIP4E_GLOBAL_Print_Ready.indb   75\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 76}),\n",
       " Document(page_content='76\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nthem according to their subjective quality. Results were summarized in the form of \\nso-called \\nisopreference curves\\n in the \\nNk\\n-plane. (Figure 2.26 shows average isopref-\\nerence curves representative of the types of images in Fig. 2.25.) Each point in the \\nNk\\n-plane represents an image having values of \\nN\\n and \\nk\\n equal to the coordinates \\nof that point. Points lying on an isopreference curve correspond to images of equal \\nsubjective quality. It was found in the course of the experiments that the isoprefer-\\nence curves tended to shift right and upward, but their shapes in each of the three \\nimage categories were similar to those in Fig. 2.26. These results were not unexpect-\\ned, because a shift up and right in the curves simply means larger values for \\nN\\n and \\nk\\n, \\nwhich implies better picture quality.\\nb a\\nc\\nFIGURE 2.25\\n (a) Image with a low level of detail. (b) Image with a medium level of detail. (c) Image with a relatively \\nlarge amount of detail. (Image (b) courtesy of the Massachusetts Institute of Technology.)\\nFace\\n256\\n128\\n64\\n32\\n4\\n5\\nk\\nN\\nCrowd\\nCameraman\\nFIGURE 2.26\\nRepresentative  \\nisopreference \\ncurves for the \\nthree types of  \\nimages in  \\nFig. 2.25.\\nDIP4E_GLOBAL_Print_Ready.indb   76\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 77}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n77\\nObserve that isopreference curves tend to become more vertical as the detail in \\nthe image increases. This result suggests that for images with a large amount of detail \\nonly a few intensity levels may be needed. For example, the isopreference curve in \\nFig. 2.26 corresponding to the crowd is nearly vertical. This indicates that, for a ﬁxed \\nvalue of \\nN\\n, the perceived quality for this type of image is nearly independent of the \\nnumber of intensity levels used (for the range of intensity levels shown in Fig. 2.26). \\nThe perceived quality in the other two image categories remained the same in some \\nintervals in which the number of samples was increased, but the number of intensity \\nlevels actually decreased. The most likely reason for this result is that a decrease in \\nk\\n \\ntends to increase the apparent contrast, a visual effect often perceived as improved \\nimage quality.\\nIMAGE INTERPOLATION\\nInterpolation is used in tasks such as zooming, shrinking, rotating, and geometrically \\ncorrecting digital images. Our principal objective in this section is to introduce inter-\\npolation and apply it to image resizing (shrinking and zooming), which are basically \\nimage resampling methods. Uses of interpolation in applications such as rotation \\nand geometric corrections will be discussed in Section 2.6.\\nInterpolation\\n is the process of using known data to estimate values at unknown \\nlocations. We begin the discussion of this topic with a short example. Suppose that \\nan image of size \\n500 500\\n*\\n pixels has to be enlarged 1.5 times to \\n750 750\\n*\\n pixels. A \\nsimple way to visualize zooming is to create an imaginary \\n750 750\\n*\\n grid with the \\nsame pixel spacing as the original image\\n, then shrink it so that it exactly overlays the \\noriginal image. Obviously, the pixel spacing in the shrunken \\n750 750\\n*\\n grid will be \\nless than the pixel spacing in the original image\\n. To assign an intensity value to any \\npoint in the overlay, we look for its closest pixel in the underlying original image and \\nassign the intensity of that pixel to the new pixel in the \\n750 750\\n*\\n grid. When intensi-\\nties have been assigned to all the points in the overlay grid,\\n we expand it back to the \\nspeciﬁed size to obtain the resized image.\\nThe method just discussed is called \\nnearest neighbor interpolation\\n because it \\nassigns to each new location the intensity of its nearest neighbor in the original \\nimage (see Section 2.5 regarding neighborhoods). This approach is simple but, it has \\nthe tendency to produce undesirable artifacts, such as severe distortion of straight \\nedges. A more suitable approach is \\nbilinear interpolation\\n, in which we use the four \\nnearest neighbors to estimate the intensity at a given location. Let \\n(,)\\nxy\\n denote the \\ncoordinates of the location to which we want to assign an intensity value (think of \\nit as a point of the grid described previously),\\n and let \\nv\\n(,\\n)\\nxy\\n denote that intensity \\nvalue\\n. For bilinear interpolation, the assigned value is obtained using the equation\\n \\nv\\n(,\\n)\\nxy a x b y c x y d\\n=\\n++ +\\n \\n(2-17)\\nwhere the four coefficients are determined from the four equations in four \\nunknowns that can be written using the \\nfour\\n nearest neighbors of point \\n(,)\\nxy\\n. \\nBilinear interpolation gives much better results than nearest neighbor interpolation,\\n \\nwith a modest increase in computational burden.\\nContrary to what the \\nname suggests, bilinear \\ninterpolation is \\nnot\\n a \\nlinear operation because \\nit involves multiplication \\nof coordinates (which is \\nnot a linear operation). \\nSee Eq. (2-17).\\nDIP4E_GLOBAL_Print_Ready.indb   77\\n6/16/2017   2:02:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 78}),\n",
       " Document(page_content='78\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe next level of complexity is \\nbicubic interpolation\\n, which involves the sixteen \\nnearest neighbors of a point. The intensity value assigned to point \\n(,)\\nxy\\n is obtained \\nusing the equation\\n \\nv\\n(,\\n)\\nxy axy\\nij\\nij\\nj i\\n=\\n= =\\n∑ ∑\\n0\\n3\\n0\\n3\\n  \\n(2-18)\\nThe sixteen coefficients are determined from the sixteen equations with six-\\nteen unknowns that can be written using the sixteen nearest neighbors of point \\n(,)\\nxy\\n . Observe that Eq. (2-18) reduces in form to Eq. (2-17) if the limits of both \\nsummations in the former equation are 0 to 1.\\n Generally, bicubic interpolation does \\na better job of preserving fine detail than its bilinear counterpart. Bicubic interpola-\\ntion is the standard used in commercial image editing applications, such as Adobe \\nPhotoshop and Corel Photopaint.\\nAlthough images are displayed with integer coordinates, it is possible during pro-\\ncessing to work with \\nsubpixel accuracy\\n by increasing the size of the image using \\ninterpolation to “ﬁll the gaps” between pixels in the original image.\\nEXAMPLE 2.4 :  Comparison of interpolation approaches for image shrinking and zooming.\\nFigure 2.27(a) is the same as Fig. 2.23(d), which was obtained by reducing the resolution of the 930 dpi \\nimage in Fig. 2.23(a) to 72 dpi (the size shrank from \\n2136 2140\\n*\\n to \\n165 166\\n*\\n pixels) and then zooming \\nthe reduced image back to its original size\\n. To generate Fig. 2.23(d) we used nearest neighbor interpola-\\ntion both to shrink and zoom the image. As noted earlier, the result in Fig. 2.27(a) is rather poor. Figures \\n2.27(b) and (c) are the results of repeating the same procedure but using, respectively, bilinear and bicu-\\nbic interpolation for both shrinking and zooming. The result obtained by using bilinear interpolation is a \\nsigniﬁcant improvement over nearest neighbor interpolation, but the resulting image is blurred slightly. \\nMuch sharper results can be obtained using bicubic interpolation, as Fig. 2.27(c) shows. \\n \\nFIGURE 2.27\\n (a) Image reduced to 72 dpi and zoomed back to its original 930 dpi using nearest neighbor interpolation. \\nThis ﬁgure is the same as Fig. 2.23(d). (b) Image reduced to 72 dpi and zoomed using bilinear interpolation. (c) Same \\nas (b) but using bicubic interpolation.\\nb a\\nc\\nDIP4E_GLOBAL_Print_Ready.indb   78\\n6/16/2017   2:02:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 79}),\n",
       " Document(page_content='2.5\\n  \\nSome Basic Relationships Between Pixels\\n    \\n79\\nIt is possible to use more neighbors in interpolation, and there are more complex \\ntechniques, such as using \\nsplines\\n or \\nwavelets\\n, that in some instances can yield better \\nresults than the methods just discussed. While preserving ﬁne detail is an exception-\\nally important consideration in image generation for 3-D graphics (for example, see \\nHughes and Andries [2013]), the extra computational burden seldom is justiﬁable \\nfor general-purpose digital image processing, where bilinear or bicubic interpola-\\ntion typically are the methods of choice.\\n2.5 SOME BASIC RELATIONSHIPS BETWEEN PIXELS  \\nIn this section, we discuss several important relationships between pixels in a digital \\nimage. When referring in the following discussion to particular pixels, we use lower-\\ncase letters, such as \\np\\n and \\nq\\n.\\nNEIGHBORS OF A PIXEL\\nA pixel \\np\\n at coordinates \\n(,)\\nxy\\n has two horizontal and two vertical neighbors with \\ncoordinates\\n \\n( , ), ( , ), ( , ), ( , )\\nxy\\nxy x y x y\\n+− +−\\n11 11\\nThis set of pixels, called the 4\\n-neighbors\\n of \\np\\n,\\n is denoted \\nNp\\n4\\n()\\n.\\nT\\nhe four \\ndiagonal\\n neighbors of \\np\\n have coordinates\\n \\n(, ) , (, ) , (, ) , (, )\\nxy\\nxy xy xy\\n++ +− −+ −−\\n11 11 11 11\\nand are denoted \\nNp\\nD\\n()\\n. These neighbors, together with the 4-neighbors, are called \\nthe 8-\\nneighbors\\n of \\np\\n,\\n denoted by \\nNp\\n8\\n()\\n. The set of image locations of the neighbors \\nof a point \\np\\n is called the \\nneighborhood\\n of \\np\\n.\\n The neighborhood is said to be \\nclosed\\n if \\nit contains \\np\\n. Otherwise, the neighborhood is said to be \\nopen\\n.\\nADJACENCY, CONNECTIVITY, REGIONS, AND BOUNDARIES\\nLet \\nV\\n be the set of intensity values used to define adjacency. In a binary image, \\nV\\n=\\n{}\\n1\\n if we are referring to adjacency of pixels with value 1. In a grayscale image, \\nthe idea is the same\\n, but set \\nV\\n typically contains more elements. For example, if we \\nare dealing with the adjacency of pixels whose values are in the range 0 to 255, set \\nV\\n \\ncould be any subset of these 256 values. We consider three types of adjacency:\\n1. \\n4-\\nadjacency\\n.\\n Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are 4-adjacent if \\nq\\n is in the \\nset \\nNp\\n4\\n() .\\n2. \\n8-\\nadjacency\\n.\\n Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are 8-adjacent if \\nq\\n is in the \\nset \\nNp\\n8\\n()\\n.\\n3. \\nm-adjacency \\n(also called \\nmix\\ned adjacency\\n). Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are \\nm\\n-adjacent if\\n2.5\\nDIP4E_GLOBAL_Print_Ready.indb   79\\n6/16/2017   2:02:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 80}),\n",
       " Document(page_content='80\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n(a) \\nq\\n is in \\nNp\\n4\\n()\\n, \\nor\\n(b) \\nq\\n is in \\nNp\\nD\\n()\\n \\nand\\n the set \\nNp Nq\\n44\\n() ()\\n¨\\n has no pixels whose values are \\nfrom \\nV\\n.\\nMixed adjacenc\\ny is a modification of 8-adjacency, and is introduced to eliminate the \\nambiguities that may result from using 8-adjacency. For example, consider the pixel \\narrangement in Fig. 2.28(a) and let \\nV\\n=\\n{}\\n1\\n. The three pixels at the top of Fig. 2.28(b) \\nshow multiple (ambiguous) 8-adjacenc\\ny, as indicated by the dashed lines. This ambi-\\nguity is removed by using \\nm\\n-adjacency, as in Fig. 2.28(c). In other words, the center \\nand upper-right diagonal pixels are not \\nm\\n-adjacent because they do not satisfy con-\\ndition (b).\\nA \\ndigital\\n \\npath\\n (or \\ncurve\\n) from pixel \\np\\n with coordinates \\n(, )\\nxy\\n00\\n to pixel \\nq\\n with \\ncoordinates \\n(, )\\nxy\\nnn\\n is a sequence of distinct pixels with coordinates\\n \\n( , ), ( , ), , ( , )\\nxy\\nxy xy\\nnn\\n00 11\\n…\\nwhere points \\n(,)\\nxy\\nii\\n and \\n(,)\\nxy\\nii\\n−−\\n11\\n are adjacent for \\n1\\n≤≤\\nin\\n. In this case, \\nn\\n is the \\nlength\\n of the path.\\n If \\n(,)(, )\\nxy\\nxy\\nnn\\n00\\n=\\n the path is a \\nclosed\\n path.\\n We can define 4-, 8-, \\nor \\nm\\n-paths, depending on the type of adjacency specified. For example, the paths in \\nFig. 2.28(b) between the top right and bottom right points are 8-paths, and the path \\nin Fig. 2.28(c) is an \\nm\\n-path.\\nLet \\nS\\n represent a subset of pixels in an image. Two pixels \\np\\n and \\nq\\n are said to be \\nconnected in\\n \\nS\\n if there exists a path between them consisting entirely of pixels in \\nS\\n. \\nFor any pixel \\np\\n in \\nS\\n, the set of pixels that are connected to it in \\nS\\n is called a \\nconnected \\ncomponent\\n of \\nS\\n. If it only has one component, and that component is connected, \\nthen \\nS\\n is called a \\nconnected set\\n.\\nLet \\nR\\n represent a subset of pixels in an image. We call \\nR\\n a \\nregion\\n of the image if \\nR\\n \\nis a connected set. Two regions, \\nR\\ni\\n and \\nR\\nj\\n are said to be \\nadjacent\\n if their union forms \\na connected set. Regions that are not adjacent are said to be \\ndisjoint\\n. We consider 4- \\nand 8-adjacency when referring to regions. For our deﬁnition to make sense, the type \\nof adjacency used must be speciﬁed. For example, the two regions of 1’s in Fig. 2.28(d) \\nare adjacent only if 8-adjacency is used (according to the deﬁnition in the previous \\nWe use the symbols \\n¨ \\nand \\n´\\n to denote set \\nintersection and union, \\nrespectively. Given sets \\nA\\n and \\nB\\n, recall that \\ntheir intersection is the \\nset of elements that \\nare members of both \\nA\\n and \\nB\\n. The union of \\nthese two sets is the set \\nof elements that are \\nmembers of \\nA\\n, of \\nB\\n, or \\nof both. We will discuss \\nsets in more detail in \\nSection 2.6.\\n0\\n11\\n0\\n1\\n0\\n00\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n111\\n1\\n0\\n1\\n0\\n1\\n0\\nR\\ni\\nR\\nj\\n00\\n1\\n11\\n1\\n111\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n11\\n0\\n1\\n0\\n00\\n1\\n0\\n0\\n0\\n11\\n1\\n0\\n0\\n1\\nb a\\nc\\ne d\\nf\\nFIGURE 2.28\\n (a) An arrangement of pixels. (b) Pixels that are 8-adjacent (adjacency is shown by dashed lines). \\n \\n(c) \\nm\\n-adjacency. (d) Two regions (of 1’s) that are 8-adjacent. (e) The circled point is on the boundary of the 1-valued \\npixels only if 8-adjacency between the region and background is used. (f) The inner boundary of the 1-valued region \\ndoes not form a closed path, but its outer boundary does.\\nDIP4E_GLOBAL_Print_Ready.indb   80\\n6/16/2017   2:02:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 81}),\n",
       " Document(page_content='2.5\\n  \\nSome Basic Relationships Between Pixels\\n    \\n81\\nparagraph, a 4-path between the two regions does not exist, so their union is not a \\nconnected set).\\nSuppose an image contains \\nK\\n disjoint regions, \\nRk K\\nk\\n,, , , ,\\n=\\n12\\n…\\n none of which \\ntouches the image border\\n.\\n†\\n Let \\nR\\nu\\n denote the union of all the \\nK\\n regions, and let \\nR\\nu\\nc\\n()\\n denote its complement (recall that the \\ncomplement\\n of a set \\nA\\n is the set of \\npoints that are not in \\nA\\n). We call all the points in \\nR\\nu\\n the \\nforeground\\n, and all the \\npoints in \\nR\\nu\\nc\\n()\\n the \\nbackground\\n of the image.\\nThe \\nboundary\\n (also called the \\nborder\\n or \\ncontour\\n) of a region \\nR\\n is the set of pixels in \\nR\\n that are adjacent to pixels in the complement of \\nR\\n. Stated another way, the border \\nof a region is the set of pixels in the region that have at least one background neigh-\\nbor. Here again, we must specify the connectivity being used to deﬁne adjacency. For \\nexample, the point circled in Fig. 2.28(e) is not a member of the border of the 1-val-\\nued region if 4-connectivity is used between the region and its background, because \\nthe only possible connection between that point and the background is diagonal. \\nAs a rule, adjacency between points in a region and its background is deﬁned using \\n8-connectivity to handle situations such as this.\\nThe preceding deﬁnition sometimes is referred to as the \\ninner border\\n of the \\nregion to distinguish it from its \\nouter border\\n, which is the corresponding border in \\nthe background. This distinction is important in the development of border-follow-\\ning algorithms. Such algorithms usually are formulated to follow the outer boundary \\nin order to guarantee that the result will form a closed path. For instance, the inner \\nborder of the 1-valued region in Fig. 2.28(f) is the region itself. This border does not \\nsatisfy the deﬁnition of a closed path. On the other hand, the outer border of the \\nregion does form a closed path around the region.\\nIf \\nR\\n happens to be an entire image, then its \\nboundary\\n (or \\nborder\\n) is deﬁned as the \\nset of pixels in the ﬁrst and last rows and columns of the image. This extra deﬁnition \\nis required because an image has no neighbors beyond its border. Normally, when \\nwe refer to a region, we are referring to a subset of an image, and any pixels in the \\nboundary of the region that happen to coincide with the border of the image are \\nincluded implicitly as part of the region boundary.\\nThe concept of an \\nedge\\n is found frequently in discussions dealing with regions \\nand boundaries. However, there is a key difference between these two concepts. The \\nboundary of a ﬁnite region forms a closed path and is thus a “global” concept. As we \\nwill discuss in detail in Chapter 10, edges are formed from pixels with derivative val-\\nues that exceed a preset threshold. Thus, an edge is a “local” concept that is based on \\na measure of intensity-level discontinuity at a point. It is possible to link edge points \\ninto edge segments, and sometimes these segments are linked in such a way that \\nthey correspond to boundaries, but this is not always the case. The one exception in \\nwhich edges and boundaries correspond is in binary images. Depending on the type \\nof connectivity and edge operators used (we will discuss these in Chapter 10), the \\nedge extracted from a binary region will be the same as the region boundary. This is \\n†\\n  We make this assumption to avoid having to deal with special cases. This can be done without loss of generality \\nbecause if one or more regions touch the border of an image, we can simply pad the image with a 1-pixel-wide \\nborder of background values.\\nDIP4E_GLOBAL_Print_Ready.indb   81\\n6/16/2017   2:02:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 82}),\n",
       " Document(page_content='82\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nintuitive. Conceptually, until we arrive at Chapter 10, it is helpful to think of edges \\nas intensity discontinuities, and of boundaries as closed paths.\\nDISTANCE MEASURES\\nFor pixels \\np\\n, \\nq\\n, and \\ns\\n, with coordinates \\n(,)\\nxy\\n, \\n(,)\\nu\\nv\\n, and \\n(, ) ,\\nw\\nz\\n respectively, \\nD\\nis a \\ndistance function\\n or \\nmetric\\n if\\n(a) \\nD\\npq D pq p q\\n(,) ( (,) )\\n≥\\n00\\n==\\niff\\n,\\n(b) \\nDp\\nq Dqp\\n(,) (,)\\n=\\n, and\\n(c) \\nDps Dpq Dqs\\n(,) (,) (,)\\n.\\n≤+\\nThe \\nEuclidean distance\\n between \\np\\n and \\nq\\n is defined as\\n \\nDp q x y\\ne\\n(,) ( ) ( )\\n=−+ −\\n⎡\\n⎣\\n⎤\\n⎦\\nuv\\n22\\n1\\n2\\n \\n(2-19)\\nFor this distance measure, the pixels having a distance less than or equal to some \\nvalue \\nr\\n from \\n(,)\\nxy\\n are the points contained in a disk of radius \\nr\\n centered at \\n(,)\\nxy\\n.\\nT\\nhe \\nD\\n4\\n \\ndistance\\n, (called the \\ncity-block distance\\n) between \\np\\n and \\nq\\n is deﬁned as\\n \\nDp q x y\\n4\\n(,)\\n=− −\\nuv\\n+\\n \\n(2-20)\\nIn this case, pixels having a \\nD\\n4\\n distance from \\n(,)\\nxy\\n that is less than or equal to some \\nvalue \\nd\\n form a diamond centered at \\n(,)\\nxy\\n. For example, the pixels with \\nD\\n4\\n distance \\n≤2\\n \\nfrom \\n(,)\\nxy\\n (the center point) form the following contours of constant distance:\\n \\n2\\n212\\n2101\\n2\\n212\\n2\\nThe pixels with \\nD\\n4\\n1\\n=\\n are the 4-neighbors of \\n(,)\\nxy\\n.\\nT\\nhe \\nD\\n8\\n \\ndistance\\n (called the \\nchessboard distance\\n) between \\np\\n and \\nq\\n is deﬁned as\\n \\nDp q x y\\n8\\n( , ) max( , )\\n=− −\\nuv\\n \\n(2-21)\\nIn this case, the pixels with \\nD\\n8\\n distance from \\n(,)\\nxy\\n less than or equal to some value \\nd\\n \\nform a square centered at \\n(,)\\nxy\\n. For example, the pixels with \\nD\\n8\\n distance \\n≤2\\n form \\nthe following contours of constant distance:\\n \\n22222\\n21112\\n2101\\n2\\n21112\\n22222\\nThe pixels with \\nD\\n8\\n1\\n=\\n are the 8-neighbors of the pixel at \\n(,)\\nxy\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   82\\n6/16/2017   2:02:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 83}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n83\\nNote that the \\nD\\n4\\n and \\nD\\n8\\n distances between \\np\\n and \\nq\\n are independent of any paths \\nthat might exist between these points because these distances involve only the coor-\\ndinates of the points. In the case of \\nm\\n-adjacency, however, the \\nD\\nm\\n distance between \\ntwo points is deﬁned as the shortest \\nm\\n-path between the points. In this case, the \\ndistance between two pixels will depend on the values of the pixels along the path, \\nas well as the values of their neighbors. For instance, consider the following arrange-\\nment of pixels and assume that \\np\\n, \\np\\n2\\n, and \\np\\n4\\n have a value of 1, and that \\np\\n1\\n and \\np\\n3\\n \\ncan be 0 or 1:\\n \\npp\\npp\\np\\n34\\n12\\nSuppose that we consider adjacency of pixels valued 1 (i.e.,\\nV\\n=\\n{}\\n1\\n). If \\np\\n1\\n and \\np\\n3\\n are 0, \\nthe length of the shortest \\nm\\n-path (the \\nD\\nm\\n distance) between \\np\\n and \\np\\n4\\n is 2. If \\np\\n1\\n is 1, \\nthen \\np\\n2\\n and \\np\\n will no longer be \\nm\\n-adjacent (see the definition of \\nm\\n-adjacency given \\nearlier) and the length of the shortest \\nm\\n-path becomes 3 (the path goes through the \\npoints \\npp p p\\n124\\n). Similar comments apply if \\np\\n3\\n is 1 (and \\np\\n1\\n is 0); in this case, the \\nlength of the shortest \\nm\\n-path also is 3. Finally, if both \\np\\n1\\n and \\np\\n3\\n are 1, the length of \\nthe shortest \\nm\\n-path between \\np\\n and \\np\\n4\\n is 4. In this case, the path goes through the \\nsequence of points \\npp p p p\\n1234\\n.\\n2.6 INTRODUCTION TO THE BASIC MATHEMATICAL TOOLS USED IN \\nDIGITAL IMAGE PROCESSING \\nThis section has two principal objectives: (1) to introduce various mathematical \\ntools we use throughout the book; and (2) to help you begin developing a “feel” for \\nhow these tools are used by applying them to a variety of basic image-processing \\ntasks, some of which will be used numerous times in subsequent discussions. \\nELEMENTWISE VERSUS MATRIX OPERATIONS\\nAn \\nelementwise operation\\n involving one or more images is carried out on a \\npixel-by-\\npixel \\nbasis. We mentioned earlier in this chapter that images can be viewed equiva-\\nlently as matrices. In fact, as you will see later in this section, there are many situ-\\nations in which operations between images are carried out using matrix theory. It \\nis for this reason that a clear distinction must be made between elementwise and \\nmatrix operations. For example, consider the following \\n22\\n*\\n images (matrices):\\n \\naa\\naa\\nbb\\nbb\\n11 12\\n21 22\\n11 12\\n21 22\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nand\\nThe \\nelementwise product\\n (often denoted using the symbol \\n}\\n or \\nz\\n)\\n of these two \\nimages is\\n \\naa\\naa\\nbb\\nbb\\nab\\nab\\nab a\\n11 12\\n21 22\\n11 12\\n21 22\\n11 11 12 12\\n21 21 2\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n}\\n2\\n22 2\\nb\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n2.6\\nYou may ﬁnd it helpful \\nto download and study \\nthe review material \\ndealing with probability, \\nvectors, linear algebra, \\nand linear systems. The \\nreview is available in the \\nTutorials section of the \\nbook website. \\nThe elementwise product \\nof two matrices is also \\ncalled the \\nHadamar\\nd \\nproduct\\n of the matrices.\\nThe symbol \\n| \\nis often \\nused to denote \\nelement-\\nwise division\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   83\\n6/16/2017   2:02:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 84}),\n",
       " Document(page_content='84\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThat is, the elementwise product is obtained by multiplying pairs of \\ncorresponding\\n \\npixels. On the other hand, the \\nmatrix product\\n of the images is formed using the rules \\nof matrix multiplication:\\n \\naa\\naa\\nbb\\nbb\\nab\\nab ab a\\n11 12\\n21 22\\n11 12\\n21 22\\n11 11 12 21 11 12\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n++\\n1 12 22\\n21 11 22 21 21 12 22 22\\nb\\nab ab ab ab\\n++\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nWe assume elementwise operations throughout the book, unless stated otherwise. \\nF\\nor example, when we refer to raising an image to a power, we mean that each indi-\\nvidual pixel is raised to that power; when we refer to dividing an image by another, \\nwe mean that the division is between corresponding pixel pairs, and so on. The terms \\nelementwise addition\\n and \\nsubtraction\\n of two images are redundant because these are \\nelementwise operations by deﬁnition. However, you may see them used sometimes \\nto clarify notational ambiguities. \\nLINEAR VERSUS NONLINEAR OPERATIONS\\nOne of the most important classifications of an image processing method is whether \\nit is linear or nonlinear. Consider a general operator, \\n/H5108\\n,\\n that produces an output \\nimage\\n, \\ngxy\\n(,\\n)\\n, from a given input image, \\nfx y\\n(,\\n)\\n:\\n \\n/H5108\\nfx\\ny g x y\\n(,) (,)\\n[]\\n=\\n \\n(2-22)\\nGiven two arbitrary constants, \\na\\n and \\nb\\n,\\n and two arbitrary images \\nfx y\\n1\\n(,)\\n and \\nfx y\\n2\\n(,) ,\\n/H5108\\n is said to be a \\nlinear operator\\n if\\n \\n/H5108/H5108\\n/H5108\\na\\nf xy b f xy a f xy b f xy\\nag x y bg\\n12 1 2\\n12\\n(,) (,) (,) (,)\\n(,) (\\n+\\n[]\\n=\\n[]\\n+\\n[]\\n=+\\nx\\nxy\\n,)\\n \\n(2-23)\\nThis equation indicates that the output of a linear operation applied to the sum of \\ntwo inputs is the same as performing the operation individually on the inputs and \\nthen summing the results\\n. In addition, the output of a linear operation on a con-\\nstant multiplied by an input is the same as the output of the operation due to the \\noriginal input multiplied by that constant. The first property is called the property \\nof \\nadditivity,\\n and the second is called the property of \\nhomogeneity\\n. By definition, an \\noperator that fails to satisfy Eq. (2-23) is said to be \\nnonlinear\\n.\\nAs an example, suppose that \\n/H5108\\n is the sum operator, \\nΣ\\n.\\n The function performed \\nby this operator is simply to sum its inputs. To test for linearity, we start with the left \\nside of Eq. (2-23) and attempt to prove that it is equal to the right side:\\n \\na f xy b f xy a f xy b f xy\\na\\nf xy b f xy\\n12 1 2\\n12\\n(,) (,) (,) (,)\\n(,) (,)\\n+\\n[]\\n=+\\n=+\\n∑∑\\n∑\\n∑ ∑ ∑\\n=+\\na g xy b g xy\\n12\\n(,) (,)\\n \\nwhere the first step follows from the fact that summation is distributive\\n. So, an \\nexpansion of the left side is equal to the right side of Eq. (2-23), and we conclude \\nthat the sum operator is linear.\\nThese are image  \\nsummations, not the \\nsums of all the elements \\nof an image. \\nDIP4E_GLOBAL_Print_Ready.indb   84\\n6/16/2017   2:02:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 85}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n85\\nOn the other hand, suppose that we are working with the max operation, whose \\nfunction is to ﬁnd the maximum value of the pixels in an image. For our purposes \\nhere, the simplest way to prove that this operator is nonlinear is to ﬁnd an example \\nthat fails the test in Eq. (2\\n-23). Consider the following two images\\n \\nff\\n12\\n02\\n23\\n65\\n47\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nand\\nand suppose that we let \\na\\n=\\n1\\n and \\nb\\n=−\\n1\\n. To test for linearity, we again start with the \\nleft side of Eq.\\n (2-23):\\n \\nmax ( ) ( ) max\\n1\\n02\\n23\\n1\\n65\\n47\\n63\\n24\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n+−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n=\\n−−\\n−−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫ ⎫\\n⎬\\n⎭\\n=−\\n2\\nWorking next with the right side, we obtain\\n \\n() m a x ( ) m a x\\n( )\\n1\\n02\\n23\\n1\\n65\\n47\\n31\\n7 4\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n+−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n=+\\n−= −\\nThe left and right sides of Eq. (2-23) are not equal in this case, so we have proved \\nthat the max operator is nonlinear\\n.\\nAs you will see in the next three chapters, linear operations are exceptionally impor-\\ntant because they encompass a large body of theoretical and practical results that are \\napplicable to image processing. \\nThe scope of nonlinear operations is considerably \\nmore limited. However, you will encounter in the following chapters several nonlin-\\near image processing operations whose performance far exceeds what is achievable \\nby their linear counterparts.\\nARITHMETIC OPERATIONS\\nArithmetic operations between two images \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n are denoted as\\n \\nsxy f xy gxy\\ndx\\ny f xy gxy\\npxy f xy gx\\n(,) (,) (,)\\n(,) (,) (,)\\n(,) (,) (,\\n=+\\n=−\\n=×\\ny y\\nxy f xy gxy\\n)\\n(,) (,) (,)\\nv\\n=\\n÷\\n \\n(2-24)\\nThese are elementwise operations which, as noted earlier in this section, means \\nthat they are performed between corresponding pixel pairs in \\nf\\n and \\ng\\n for \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As usual, \\nM\\n and \\nN\\n are the row and \\ncolumn sizes of the images\\n. Clearly, \\ns\\n, \\nd\\n, \\np\\n, and \\nv\\n are images of size \\nMN\\n×\\n also. \\nNote that image arithmetic in the manner just defined involves images of the same \\nsize\\n. The following examples illustrate the important role of arithmetic operations \\nin digital image processing.\\nDIP4E_GLOBAL_Print_Ready.indb   85\\n6/16/2017   2:02:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 86}),\n",
       " Document(page_content='86\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nEXAMPLE 2.5 :  Using image addition (averaging) for noise reduction.\\nSuppose that \\ngxy\\n(,\\n)\\n is a corrupted image formed by the addition of noise, \\nh\\n(, )\\nxy\\n, to a \\nnoiseless\\n image \\nfx y\\n(,\\n)\\n \\n; that is,\\n \\ngxy fxy xy\\n(,\\n) (, ) (, )\\n=+\\nh\\n \\n(2-25)\\nwhere the assumption is that at every pair of coordinates \\n(,)\\nxy\\n the noise is uncorrelated\\n†\\n and has \\nzero average value. We assume also that the noise and image values are uncorrelated (this is a typical \\nassumption for additive noise). The objective of the following procedure is to reduce the noise content \\nof the output image by adding a set of noisy input images, \\ngx y\\ni\\n(,).\\n{}\\n This is a technique used frequently \\nfor image enhancement.\\nIf the noise satisﬁes the constraints just stated, it can be shown (Problem 2.26) that if an image \\ngxy\\n(,)\\n \\nis formed by averaging \\nK\\n different noisy images\\n,\\n \\ngxy\\nK\\ngx y\\ni\\ni\\nK\\n(,) (,)\\n=\\n=\\n∑\\n1\\n1\\n \\n(2-26)\\nthen it follows that \\n \\nEg x y fx y\\n(,) (,)\\n{}\\n=\\n \\n(2-27)\\nand\\n \\nss\\nh\\ngx y\\nx y\\nK\\n(,) (,)\\n22\\n1\\n=\\n \\n(2-28)\\nwhere \\nEg x y\\n(,)\\n{}\\n is the expected value of \\ngxy\\n(,)\\n, and \\ns\\ngx y\\n(,)\\n2\\n and \\ns\\nh\\n(,)\\nxy\\n2\\n are the variances of \\ngxy\\n(,)\\n and \\nh\\n(,\\n)\\nxy\\n, respectively, all at coordinates \\n(,)\\nxy\\n. These variances are arrays of the same size as the input \\nimage\\n, and there is a scalar variance value for each pixel location. \\nThe standard deviation (square root of the variance) at any point \\n(,)\\nxy\\n in the average image is\\n \\nss\\nh\\ngx y\\nx y\\nK\\n(,)\\n(,)\\n=\\n1\\n \\n(2-29)\\nAs \\nK\\n increases\\n, Eqs. (2-28) and (2-29) indicate that the variability (as measured by the variance or the \\nstandard deviation) of the pixel values at each location \\n(,)\\nxy\\n decreases. Because \\nEg x y fx y\\n(,) (,) ,\\n{}\\n=\\n \\nthis means that \\ngxy\\n(,)\\n approaches the noiseless image \\nfx y\\n(,\\n)\\n as the number of noisy images used in the \\naveraging process increases\\n. In order to avoid blurring and other artifacts in the output (average) image, \\nit is necessary that the images \\ngx y\\ni\\n(,)\\n be \\nregistered\\n (i.e\\n., spatially aligned).\\nAn important application of image averaging is in the ﬁeld of astronomy, where imaging under \\nvery low light levels often cause sensor noise to render individual images virtually useless for analysis \\n(lowering the temperature of the sensor helps reduce noise). Figure 2.29(a) shows an 8-bit image of the \\nGalaxy Pair NGC 3314, in which noise corruption was simulated by adding to it Gaussian noise with \\nzero mean and a standard deviation of 64 intensity levels. This image, which is representative of noisy \\nastronomical images taken under low light conditions, is useless for all practical purposes. Figures \\n2.29(b) through (f) show the results of averaging 5, 10, 20, 50, and 100 images, respectively. We see from \\nFig. 2.29(b) that an average of only 10 images resulted in some visible improvement.\\n \\nAccording to Eq. \\n†\\n The variance of a random variable \\nz\\n with mean \\nz  \\nis deﬁned as \\nEz z\\n{( ) }\\n−\\n2\\n, where \\nE\\n{}\\n/H17033\\n is the expected value of the argument. The covari-\\nance of two random variables \\nz\\ni\\n and \\nz\\nj\\n is deﬁned as \\nEz z z z\\nii jj\\n{( )( )}.\\n−−\\n If the variables are uncorrelated, their covariance is 0, and vice \\nversa.\\n (Do not confuse correlation and statistical independence. If two random variables are statistically independent, their correlation is \\nzero. However, the converse is not true in general.)\\nDIP4E_GLOBAL_Print_Ready.indb   86\\n6/16/2017   2:02:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 87}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n87\\n(2-29), the standard deviation of the noise in Fig. 2.29(b) is less than half \\n(. )\\n150 4 5\\n=\\n the standard \\ndeviation of the noise in F\\nig. 2.29(a), or \\n(. ) ( )\\n04\\n5 6 4 2 9\\n≈\\n intensity levels. Similarly, the standard devia-\\ntions of the noise in F\\nigs. 2.29(c) through (f) are 0.32, 0.22, 0.14, and 0.10 of the original, which translates \\napproximately into 20, 14, 9, and 6 intensity levels, respectively. We see in these images a progression \\nof more visible detail as the standard deviation of the noise decreases. The last two images are visually \\nidentical for all practical purposes. This is not unexpected, as the difference between the standard devia-\\ntions of their noise level is only about 3 intensity levels According to the discussion in connection with \\nFig. 2.5, this difference is below what a human generally is able to detect.\\nEXAMPLE 2.6 :  Comparing images using subtraction.\\nImage subtraction is used routinely for enhancing differences between images. For example, the image \\nin Fig. 2.30(b) was obtained by setting to zero the least-signiﬁcant bit of every pixel in Fig. 2.30(a). \\nVisually, these images are indistinguishable. However, as Fig. 2.30(c) shows, subtracting one image from \\nb a\\nc\\ne d\\nf\\nFIGURE 2.29\\n (a) Image of Galaxy Pair NGC 3314 corrupted by additive Gaussian noise. (b)-(f) Result of averaging \\n5, 10, 20, 50, and 1,00 noisy images, respectively. All images are of size \\n566 598\\n×\\n pixels, and all were scaled so that \\ntheir intensities would span the full [0, 255] intensity scale. (Original image courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   87\\n6/16/2017   2:02:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 88}),\n",
       " Document(page_content='88\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nthe other clearly shows their differences. Black (0) values in the difference image indicate locations \\nwhere there is no difference between the images in Figs. 2.30(a) and (b). \\nWe saw in Fig. 2.23 that detail was lost as the resolution was reduced in the chronometer image \\nshown in Fig. 2.23(a). A vivid indication of image change as a function of resolution can be obtained \\nby displaying the differences between the original image and its various lower-resolution counterparts. \\nFigure 2.31(a) shows the difference between the 930 dpi and 72 dpi images. As you can see, the dif-\\nferences are quite noticeable. The intensity at any point in the difference image is proportional to the \\nmagnitude of the numerical difference between the two images at that point. Therefore, we can analyze \\nwhich areas of the original image are affected the most when \\nresolution is reduced. The \\nnext two images \\nin Fig. 2.31 show proportionally less overall intensities, indicating smaller differences between the 930 dpi \\nimage and 150 dpi and 300 dpi images, as expected. \\nb a\\nc\\nFIGURE 2.30\\n (a) Infrared image of the Washington, D.C. area. (b) Image resulting from setting to zero the least \\nsigniﬁcant bit of every pixel in (a). (c) Difference of the two images, scaled to the range [0, 255] for clarity. (Original \\nimage courtesy of NASA.)\\nb a\\nc\\nFIGURE 2.31\\n (a) Difference between the 930 dpi and 72 dpi images in Fig. 2.23. (b) Difference between the 930 dpi and \\n150 dpi images. (c) Difference between the 930 dpi and 300 dpi images.\\nDIP4E_GLOBAL_Print_Ready.indb   88\\n6/16/2017   2:02:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 89}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n89\\nAs a ﬁnal illustration, we discuss brieﬂy an area of medical imaging called \\nmask mode radiography\\n, a \\ncommercially successful and highly beneﬁcial use of image subtraction. Consider image differences of \\nthe form\\n \\ngxy f xy hxy\\n(,\\n) (,) (,)\\n=−\\n \\n(2-30)\\nIn this case \\nhxy\\n(,\\n)\\n, the \\nmask\\n,\\n is an X-ray image of a region of a patient’s body captured by an intensiﬁed \\nTV camera (instead of traditional X-ray ﬁlm) located opposite an X-ray source. The procedure consists \\nof injecting an X-ray contrast medium into the patient’s bloodstream, taking a series of images called \\nlive images\\n [samples of which are denoted as \\nfx y\\n(,\\n)\\n] of the same anatomical region as \\nhxy\\n(,\\n)\\n, and sub-\\ntracting the mask from the series of incoming live images after injection of the contrast medium.\\n The net \\neffect of subtracting the mask from each sample live image is that the areas that are different between \\nfx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n appear in the output image, \\ngxy\\n(,\\n)\\n, as enhanced detail. Because images can be cap-\\ntured at \\nTV rates, this procedure outputs a video showing how the contrast medium propagates through \\nthe various arteries in the area being observed.\\nFigure 2.32(a) shows a mask X-ray image of the top of a patient’s head prior to injection of an iodine \\nmedium into the bloodstream, and Fig. 2.32(b) is a sample of a live image taken after the medium was \\nb a\\nd c\\nFIGURE 2.32\\n  \\nDigital  \\nsubtraction  \\nangiography.  \\n(a) Mask image. \\n(b) A live image. \\n(c) Difference \\nbetween (a) and \\n(b). (d) Enhanced \\ndifference image. \\n(Figures (a) and \\n(b) courtesy of \\nthe Image  \\nSciences  \\nInstitute,  \\nUniversity \\nMedical Center, \\nUtrecht, The \\nNetherlands.)\\nDIP4E_GLOBAL_Print_Ready.indb   89\\n6/16/2017   2:02:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 90}),\n",
       " Document(page_content='90\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\ninjected. Figure 2.32(c) is the difference between (a) and (b). Some ﬁne blood vessel structures are vis-\\nible in this image. The difference is clear in Fig. 2.32(d), which was obtained by sharpening the image and \\nenhancing its contrast (we will discuss these techniques in the next chapter). Figure 2.32(d) is a “snap-\\nshot” of how the medium is propagating through the blood vessels in the subject’s brain.\\nEXAMPLE 2.7 :  Using image multiplication and division for shading correction and for masking.\\nAn important application of image multiplication (and division) is \\nshading correction\\n. Suppose that an \\nimaging sensor produces images that can be modeled as the product of a “perfect image,” denoted by \\nfx y\\n(,\\n)\\n, times a shading function, \\nhxy\\n(,\\n)\\n; that is, \\ngxy f xyhxy\\n(,\\n) (,)(,)\\n=\\n. If \\nhxy\\n(,\\n)\\n is known or can be \\nestimated,\\n we can obtain \\nfx y\\n(,\\n)\\n (or an estimate of it) by multiplying the sensed image by the inverse of \\nhxy\\n(,\\n)\\n (i.e., dividing \\ng\\n by \\nh \\nusing\\n \\nelementwise division).\\n If access to the imaging system is possible, we \\ncan obtain a good approximation to the shading function by imaging a target of constant intensity. When \\nthe sensor is not available, we often can estimate the shading pattern directly from a shaded image using \\nthe approaches discussed in Sections 3.5 and 9.8. Figure 2.33 shows an example of shading correction \\nusing an estimate of the shading pattern. The corrected image is not perfect because of errors in the \\nshading pattern (this is typical), but the result deﬁnitely is an improvement over the shaded image in Fig. \\n2.33 (a). See Section 3.5\\n \\nfor a discussion of how we estimated Fig. 2.33 (b). Another use of image mul-\\ntiplication is in \\nmasking\\n,\\n also called \\nregion of interest\\n (ROI), operations. As Fig. 2.34 shows, the process \\nconsists of multiplying a given image by a mask image that has 1’s in the ROI and 0’s elsewhere. There \\ncan be more than one ROI in the mask image, and the shape of the ROI can be arbitrary.\\nA few comments about implementing image arithmetic operations are in order \\nbefore we leave this section. In practice, most images are displayed using 8 bits (even \\n24-bit color images consist of three separate 8-bit channels). Thus, we expect image \\nvalues to be in the range from 0 to 255. When images are saved in a standard image \\nformat, such as TIFF or JPEG, conversion to this range is automatic. When image \\nvalues exceed the allowed range, clipping or scaling becomes necessary. For example, \\nthe values in the difference of two 8-bit images can range from a minimum of \\n−\\n255 \\nb a\\nc\\nFIGURE 2.33\\n Shading correction. (a) Shaded test pattern. (b) Estimated shading pattern. (c) Product of (a) by the \\nreciprocal of (b). (See Section 3.5 for a discussion of how (b) was estimated.)\\nDIP4E_GLOBAL_Print_Ready.indb   90\\n6/16/2017   2:02:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 91}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n91\\nto a maximum of 255, and the values of the sum of two such images can range from 0 \\nto 510. When converting images to eight bits, many software applications simply set \\nall negative values to 0 and set to 255 all values that exceed this limit. Given a digital \\nimage \\ng\\n resulting from one or more arithmetic (or other) operations, an approach \\nguaranteeing that the full range of a values is “captured” into a ﬁxed number of bits \\nis as follows. First, we perform the operation\\n \\ngg g\\nm\\n=−\\nmin( )\\n \\n(2-31)\\nwhich creates an image whose minimum value is 0. Then, we perform the operation\\n \\ngK g g\\nsm m\\n=\\n[]\\nmax( )\\n \\n(2-32)\\nwhich creates a scaled image, \\ng\\ns\\n,\\n whose values are in the range [0, \\nK\\n].\\n When working \\nwith 8-bit images, setting \\nK\\n=\\n255\\n gives us a scaled image whose intensities span the \\nfull 8-bit scale from 0 to 255.\\n  Similar comments apply to 16-bit images or higher. This \\napproach can be used for all arithmetic operations. When performing division, we \\nhave the extra requirement that a small number should be added to the pixels of the \\ndivisor image to avoid division by 0.\\nSET AND LOGICAL OPERATIONS\\nIn this section, we discuss the basics of set theory. We also introduce and illustrate \\nsome important set and logical operations.\\nBasic Set Operations\\nA \\nset\\n is a collection of distinct objects. If \\na\\n is an \\nelement\\n of set \\nA\\n, then we write\\n \\naA\\n∈\\n  \\n(2-33)\\nSimilarly, if \\na\\n is not an element of \\nA\\n we write\\n \\naA\\nx\\n \\n(2-34)\\nThe set with no elements is called the \\nnull\\n or \\nempty set\\n,\\n and is denoted by \\n∅\\n.\\nThese are elementwise \\nsubtraction and division.\\nb a\\nc\\nFIGURE 2.34\\n (a) Digital dental X-ray image. (b) ROI mask for isolating teeth with ﬁllings (white corresponds to 1 and \\nblack corresponds to 0). (c) Product of (a) and (b).\\nDIP4E_GLOBAL_Print_Ready.indb   91\\n6/16/2017   2:02:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 92}),\n",
       " Document(page_content='92\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nA set is denoted by the contents of two braces: \\n{} .\\ni\\n For example, the expression\\n \\nCc c d d D\\n==\\n{}\\n-H\\n,\\nmeans that \\nC\\n is the set of elements\\n, \\nc\\n, such that \\nc\\n is formed by multiplying each of \\nthe elements of set \\nD\\n by \\n−\\n1\\n. \\nIf every element of a set \\nA\\n is also an element of a set \\nB\\n,\\n then \\nA\\n is said to be a \\nsubset\\n of \\nB\\n, denoted as\\n \\nAB\\n8\\n \\n(2-35)\\nThe \\nunion\\n of two sets \\nA\\n and \\nB\\n,\\n denoted as\\n \\nCA B\\n=\\n´\\n \\n(2-36)\\nis a set \\nC\\n consisting of elements belonging \\neither\\n to \\nA\\n,\\n to \\nB\\n, \\nor\\n to \\nboth\\n. Similarly, the \\nintersection\\n of two sets \\nA\\n and \\nB\\n, denoted by\\n \\nDA B\\n=\\n¨\\n \\n(2-37)\\nis a set \\nD\\n consisting of elements belonging to \\nboth\\n \\nA\\n and \\nB\\n.\\n Sets \\nA\\n and \\nB\\n are said to \\nbe \\ndisjoint\\n or \\nmutually\\n \\nexclusive\\n if they have no elements in common, in which case,\\n \\nAB\\n¨\\n=∅\\n \\n(2-38)\\nThe \\nsample space\\n, \\nÆ\\n,\\n (also called the \\nset\\n \\nuniverse\\n) is the set of all possible set \\nelements in a given application.\\n By deﬁnition, these set elements are members of \\nthe sample space for that application. For example, if you are working with the set \\nof real numbers, then the sample space is the real line, which contains all the real \\nnumbers. In image processing, we typically deﬁne \\nÆ\\n to be the rectangle containing \\nall the pixels in an image\\n.\\nThe \\ncomplement\\n of a set \\nA\\n is the set of elements that are not in \\nA\\n:\\n \\nAA\\nc\\n=\\n{}\\nww\\nx\\n \\n(2-39)\\nThe \\ndifference\\n of two sets \\nA\\n and \\nB\\n,\\n denoted \\nAB\\n−\\n, is defined as\\n \\nAB A B A B\\nc\\n−=\\n{}\\n=\\nww w\\nHx ¨\\n,\\n \\n(2-40)\\nThis is the set of elements that belong to \\nA\\n,\\n but not to \\nB\\n. We can define \\nA\\nc\\n in terms \\nof \\nÆ\\n and the set difference operation; that is, \\nAA\\nc\\n=−\\nÆ\\n. Table 2.1 shows several \\nimportant set properties and relationships\\n.\\nFigure 2.35\\n \\nshows diagrammatically (in so-called \\nV\\nenn diagrams\\n) some of the set \\nrelationships in Table 2.1. The shaded areas in the various figures correspond to the \\nset operation indicated above or below the figure. Figure 2.35(a) shows the sample \\nset, \\nÆ\\n.\\n As no earlier, this is the set of all possible elements in a given application. Fig-\\nure 2.35(b) shows that the complement of a set \\nA\\n is the set of all elements in \\nÆ\\n \\nthat \\nare not in \\nA\\n,\\n which agrees with our earlier deﬁnition. Observe that Figs. 2.35(e) and \\n(g) are identical, which proves the validity of Eq. (2-40) using Venn diagrams. This \\nDIP4E_GLOBAL_Print_Ready.indb   92\\n6/16/2017   2:02:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 93}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n93\\nis an example of the usefulness of Venn diagrams for proving equivalences between \\nset relationships. \\nWhen applying the concepts just discussed to image processing, we let sets repre-\\nsent objects (regions) in a binary image, and the elements of the sets are the \\n(,)\\nxy\\n \\ncoordinates of those objects\\n. For example, if we want to know whether two objects, \\nA\\n and \\nB,\\n of a binary image overlap, all we have to do is compute \\nAB\\n¨\\n. If the result \\nis not the empty set,\\n we know that some of the elements of the two objects overlap. \\nKeep in mind that the only way that the operations illustrated in Fig. 2.35 can make \\nsense in the context of image processing is if the images containing the sets are \\nbinary, in which case we can talk about set membership based on coordinates, the \\nassumption being that all members of the sets have the same intensity value (typi-\\ncally denoted by 1). We will discuss set operations involving binary images in more \\ndetail in the following section and in Chapter 9.\\nThe preceding concepts are not applicable when dealing with grayscale images, \\nbecause we have not deﬁned yet a mechanism for assigning intensity values to the \\npixels resulting from a set operation. In Sections 3.8 and 9.6 we will deﬁne the union \\nand intersection operations for grayscale values as the maximum and minimum of \\ncorresponding pixel pairs, respectively. We deﬁne the \\ncomplement\\n of a grayscale \\nimage as the pairwise differences between a constant and the intensity of every pixel \\nin the image. The fact that we deal with corresponding pixel pairs tells us that gray-\\nscale set operations are elementwise operations, as deﬁned earlier. The following \\nexample is a brief illustration of set operations involving grayscale images. We will \\ndiscuss these concepts further in the two sections just mentioned.\\nDescription\\nExpressions\\nOperations between the \\nsample space and null sets\\nÆÆ Æ\\n´ Æ Æ\\n¨\\ncc\\n=∅ ∅ = ∅= ∅=∅\\n;; ;\\nUnion and intersection with \\nthe null and sample space sets\\nAA A A A A\\n´¨\\n´ Æ Æ ¨ Æ\\n∅= ∅=∅ = =\\n;;;\\nUnion and intersection of a \\nset with itself\\nAAA AAA\\n´¨\\n==\\n;\\nUnion and intersection of a \\nset with its complement\\nAA AA\\ncc\\n´Æ ¨\\n== ∅\\n;\\nCommutative laws\\nABBA\\nAB\\nBA\\n´´\\n¨¨\\n=\\n=\\nAssociative laws\\n() ()\\n()\\n()\\nAB C A BC\\nAB CA BC\\n´´ ´´\\n¨¨ ¨¨\\n=\\n=\\nDistributive laws\\n() () ()\\n()\\n() ()\\nAB C AC BC\\nAB C AC BC\\n´¨ ¨´¨\\n¨´ ´¨´\\n=\\n=\\nDeMorgan’s laws\\n \\n()\\n()\\nAB\\nA B\\nAB A B\\ncc c\\ncc c\\n´¨\\n¨´\\n=\\n=\\nTABLE \\n2.1\\nSome important \\nset operations \\nand relationships.\\nDIP4E_GLOBAL_Print_Ready.indb   93\\n6/16/2017   2:02:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 94}),\n",
       " Document(page_content='94\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nEXAMPLE 2.8 :  Illustration of set operations involving grayscale images.\\nLet the elements of a grayscale image be represented by a set \\nA\\n whose elements are triplets of the form \\n(, , )\\nxy\\nz\\n, where \\nx\\n and \\ny\\n are spatial coordinates\\n, and \\nz\\n denotes intensity values. We deﬁne the \\ncomplement \\nof \\nA\\n as the set \\n \\nAx y K z x y z A\\nc\\n=−\\n{}\\n(, , ) (, , )\\nH\\n \\nwhich is the set of pixels of \\nA\\n whose intensities have been subtracted from a constant \\nK\\n.\\n This constant \\nis equal to the maximum intensity value in the image, \\n21\\nk\\n−\\n, where \\nk\\n is the number of bits used to \\nrepresent \\nz\\n.\\n Let \\nA\\n denote the 8-bit grayscale image in Fig. 2.36(a), and suppose that we want to form \\nthe negative of \\nA\\n using grayscale set operations. The negative is the set complement, and this is an 8-bit \\nimage, so all we have to do is let \\nK\\n=\\n255 in the set deﬁned above:\\n \\nAx y z x y z A\\nc\\n=−\\n{}\\n(, , ) (, , )\\n255\\nH\\nFigure 2.36(b) shows the result. We show this only for illustrative purposes. Image negatives generally \\nare computed using an intensity transformation function,\\n as discussed later in this section.\\nA\\nc\\nAB\\n¨\\nA\\nA\\nB\\nAB\\n−\\nB\\nc\\nB\\nC\\nA\\nAB\\nc\\n¨\\nAB C\\n¨´\\n()\\n´\\nAB\\nΩ\\nB\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 2.35\\n Venn diagrams corresponding to some of the set operations in Table 2.1. The results of the operations, \\nsuch as \\nA\\nc\\n,\\n are shown shaded. Figures (e) and (g) are the same, proving via Venn diagrams that \\nAB AB\\nc\\n−=\\n¨\\n[see Eq. (2-40)].\\nDIP4E_GLOBAL_Print_Ready.indb   94\\n6/16/2017   2:02:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 95}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n95\\nThe \\nunion\\n of two grayscale sets \\nA\\n and \\nB\\n with the same number of elements is deﬁned as the set\\n \\nAB a baA b B\\nz\\n´H\\nH\\n=\\n{}\\nmax( , ) ,\\nwhere it is understood that the max operation is applied to pairs of corresponding elements. If \\nA\\n and \\nB\\n \\nare grayscale images of the same size\\n, we see that their the union is an array formed from the maximum \\nintensity between pairs of spatially corresponding elements. As an illustration, suppose that \\nA\\n again \\nrepresents the image in Fig. 2.36(a), and let \\nB\\n denote a rectangular array of the same size as \\nA\\n, but in \\nwhich all values of \\nz\\n are equal to 3 times the mean intensity, \\nz\\n,\\n of the elements of \\nA\\n.\\n Figure 2.36(c) shows \\nthe result of performing the set union, in which all values exceeding \\n3\\nz\\n appear as values from \\nA\\n and all \\nother pixels have value \\n3\\nz\\n, which is a mid-gray value.\\nBefore leaving the discussion of sets\\n, we introduce some additional concepts that \\nare used later in the book. The \\nCartesian product\\n of two sets \\nX\\n and \\nY\\n, denoted \\nXY\\n×\\n,\\n is the set of \\nall\\n possible ordered pairs whose ﬁrst component is a member of \\nX\\n and whose second component is a member of \\nY\\n.\\n In other words,\\n \\nXY x y x X y Y\\n*= H H\\n(, ) a n d\\n{}\\n \\n(2-41)\\nFor example, if \\nX\\n is a set of \\nM\\n equally spaced values on the \\nx\\n-axis and \\nY\\n is a set of \\nN\\n \\nequally spaced values on the \\ny\\n-axis\\n, we see that the Cartesian product of these two \\nsets define the coordinates of an \\nM\\n-by-\\nN\\n rectangular array (i.e., the coordinates of \\nan image). As another example, if \\nX\\n and \\nY\\n denote the specific \\nx\\n- and \\ny\\n-coordinates \\nof a group of 8-connected, 1-valued pixels in a binary image, then set \\nXY\\n×\\n repre-\\nsents the region (object) comprised of those pixels\\n.\\nWe follow convention \\nin using the symbol \\n×\\n \\nto denote the Cartesian \\nproduct. This is not to \\nbe confused with our \\nuse of the same symbol \\nthroughout the book \\nto denote the size of \\nan \\nM\\n-by-\\nN\\n image (i.e., \\nM\\n \\n×\\n \\nN\\n).\\nb a\\nc\\nFIGURE 2.36\\nSet operations  \\ninvolving grayscale \\nimages. (a) Original  \\nimage. (b) Image \\nnegative obtained \\nusing grayscale set  \\ncomplementation. \\n(c) The union of \\nimage (a) and a \\nconstant image. \\n(Original image \\ncourtesy of G.E. \\nMedical Systems.)\\nDIP4E_GLOBAL_Print_Ready.indb   95\\n6/16/2017   2:02:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 96}),\n",
       " Document(page_content='96\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nA \\nrelation\\n (or, more precisely, a \\nbinary relation\\n) on a set \\nA\\n is a collection of \\nordered pairs of elements from \\nA\\n. That is, a binary relation is a subset of the Carte-\\nsian product \\nAA\\n×\\n. A binary relation between \\ntwo\\n sets\\n, \\nA\\n and \\nB\\n, is a subset of \\nAB\\n×\\n.\\nA \\npartial or\\nder\\n on a set \\nS\\n is a relation \\n*\\n on \\nS\\n such that \\n*\\n is:\\n(a) \\nreﬂexive:\\n for any \\naS\\nH\\n, \\naa\\n*\\n;\\n(b) \\ntransitive:\\n for any \\nabc S\\n,,\\nH\\n, \\nab\\n*\\n and \\nbc\\n*\\n implies that \\nac\\n*\\n;\\n(c) \\nantisymmetric:\\n for any \\nab S\\n,,\\nH\\n \\nab\\n*\\n and \\nba\\n*\\n implies that \\nab\\n=\\n.\\nwhere, for example, \\nab\\n*\\n reads “\\na\\n is related to \\nb\\n.\\n” This means that \\na\\n and \\nb\\n are in set \\n*\\n, which itself is a subset of \\nSS\\n×\\n according to the preceding definition of a relation. \\nA set with a partial order is called a \\npartially or\\ndered set\\n.\\nLet the symbol \\nU\\n denote an ordering relation. An expression of the form\\n \\naaa a\\nn\\n123\\nUUUU\\n/midhorizellipsis\\nreads: \\na\\n1\\n precedes \\na\\n2\\n or is the same as \\na\\n2\\n, \\na\\n2\\n precedes \\na\\n3\\n or is the same as \\na\\n3\\n, and so on. \\nWhen working with numbers, the symbol \\nU\\n typically is replaced by more traditional \\nsymbols\\n. For example, the set of real numbers ordered by the relation “less than or \\nequal to” (denoted by \\n≤\\n)\\n is a partially ordered set (see Problem 2.33). Similarly, the \\nset of natural numbers, paired with the relation “divisible by” (denoted by \\n÷\\n),\\n is a \\npartially ordered set.\\nOf more interest to us later in the book are strict orderings\\n. A \\nstrict ordering\\n on a \\nset \\nS\\n is a relation \\n*\\n on \\nS\\n,\\n such that \\n*\\n is:\\n(a) \\nantireﬂexive:\\n for any \\naS aa\\nH\\n,;\\n¬\\n*\\n(b) \\ntransitive:\\n for any \\nabc S\\n,,\\n,\\nH\\n \\nab\\n*\\n and \\nbc\\n*\\n implies that \\nac\\n*\\n.\\nwhere \\n¬\\naa\\n*\\n means that \\na\\n is \\nnot related\\n to \\na\\n.\\n Let the symbol \\nE\\n denote a strict \\nordering relation.\\n An expression of the form\\n \\naaa a\\nn\\n123\\nEEEE\\n/midhorizellipsis\\nreads \\na\\n1\\n precedes \\na\\n2\\n, \\na\\n2\\n precedes \\na\\n3\\n,\\n and so on. A set with a strict ordering is called \\na \\nstrict-or\\ndered set\\n. \\nAs an example, consider the set composed of the English alphabet of lowercase \\nletters, \\nSa b c z\\n=\\n{}\\n,,, ,\\n/midhorizellipsis\\n. Based on the preceding deﬁnition, the ordering\\n \\nabc z\\nEE\\nE E\\n/midhorizellipsis\\nis strict because no member of the set can precede itself (antireflexivity) and, for any \\nthree letters in \\nS\\n,\\n if the first precedes the second, and the second precedes the third, \\nthen the first precedes the third (transitivity). Similarly, the set of integers paired \\nwith the relation “less than (<)” is a strict-ordered set. \\nLogical Operations\\nLogical operations deal with TRUE (typically denoted by 1) and FALSE (typically \\ndenoted by 0) variables and expressions. For our purposes, this means binary images \\nDIP4E_GLOBAL_Print_Ready.indb   96\\n6/16/2017   2:02:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 97}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n97\\ncomposed of \\nforeground\\n (1-valued) pixels, and a \\nbackground\\n composed of 0-valued \\npixels. \\nWe work with set and logical operators on binary images using one of two basic \\napproaches: (1) we can use the \\ncoordinates\\n of individual regions of foreground pix-\\nels in a single image as sets, or (2) we can work with one or more images of the same \\nsize and perform logical operations between corresponding pixels in those arrays.\\nIn the ﬁrst category, a binary image can be viewed as a Venn diagram in which \\nthe coordinates of individual regions of 1-valued pixels are treated as sets. The \\nunion of these sets with the set composed of 0-valued pixels comprises the set uni-\\nverse, \\nÆ\\n.\\n In this representation, we work with single images using all the set opera-\\ntions deﬁned in the previous section.\\n For example, given a binary image with two \\n1-valued regions, \\nR\\n1\\n and \\nR\\n2\\n,\\n we can determine if the regions overlap (i.e., if they \\nhave at least one pair of coordinates in common) by performing the set intersec-\\ntion operation \\nRR\\n12\\n¨\\n (see Fig. 2.35). In the second approach, we perform logical \\noperations on the pixels of one binary image\\n, or on the corresponding pixels of two \\nor more binary images of the same size. \\nLogical operators can be deﬁned in terms of truth tables, as Table 2.2 shows for \\ntwo logical variables \\na\\n and \\nb\\n. The logical AND operation (also denoted \\n¿\\n)\\n yields a 1 \\n(TR\\nUE) only when both \\na\\n \\nand\\n \\nb\\n are 1. Otherwise, it yields 0 (FALSE). Similarly, \\nthe logical OR \\n(\\n¡\\n)\\n yields 1 when both \\na\\n \\nor\\n \\nb\\n or \\nboth\\n are 1,\\n and 0 otherwise. The \\nNOT \\n()\\n/H11011\\n operator is self explanatory. When applied to two binary images, AND \\nand OR operate on pairs of corresponding pixels between the images\\n. That is, they \\nare elementwise operators (see the deﬁnition of elementwise operators given earlier \\nin this chapter) in this context. The operators AND, OR, and NOT are \\nfunctionally \\ncomplete\\n, in the sense that they can be used as the basis for constructing any other \\nlogical operator. \\nFigure 2.37 illustrates the logical operations deﬁned in Table 2.2 using the second \\napproach discussed above. The NOT of binary image \\nB\\n1\\n is an array obtained by \\nchanging all 1-valued pixels to 0, and vice versa. The AND of \\nB\\n1\\n and \\nB\\n2\\n contains a \\n1 at all spatial locations where the corresponding elements of \\nB\\n1\\n and \\nB\\n2\\n are 1; the \\noperation yields 0’s elsewhere. Similarly, the OR of these two images is an array \\nthat contains a 1 in locations where the corresponding elements of \\nB\\n1\\n, \\nor B\\n2\\n, \\nor \\nboth\\n,\\n are 1. The array contains 0’s elsewhere. The result in the fourth row of Fig. 2.37 \\ncorresponds to the set of 1-valued pixels in \\nB\\n1\\n \\nbut not in \\nB\\n2\\n.\\n \\nT\\nhe last row in the \\nﬁgure is the XOR (exclusive OR) operation, which yields 1 in the locations where \\nthe corresponding elements of \\nB\\n1\\n \\nor\\n \\nB\\n2\\n,\\n \\n(but \\nnot\\n \\nboth\\n) are 1.\\n Note that the logical \\nab\\nab\\nAND\\nab\\nO\\nR NOT(\\na\\n)\\n0 0 001\\n0\\n1\\n011\\n1 0 010\\n1 1 110\\nTABLE \\n2.2\\nTruth table  \\ndeﬁning the \\nlogical operators \\nAND\\n() ,\\n¿\\n  \\nOR\\n() ,\\n¡\\n and  \\nNO\\nT\\n() .\\n/H11011\\nDIP4E_GLOBAL_Print_Ready.indb   97\\n6/16/2017   2:02:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 98}),\n",
       " Document(page_content='98\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nexpressions in the last two rows of Fig. 2.37 were constructed using operators from \\nTable 2.2; these are examples of the functionally complete nature of these operators.\\nWe can arrive at the same results in Fig. 2.37 using the ﬁrst approach discussed \\nabove. To do this, we begin by labeling the individual 1-valued regions in each of \\nthe two images (in this case there is only one such region in each image). Let \\nA\\n \\nand \\nB \\ndenote the \\nset of coordinates\\n of all the 1-valued pixels in images \\nB\\n1\\n and \\nB\\n2\\n,\\nrespectively. Then we form a \\nsingle\\n array by ORing the two images\\n, while keeping \\nthe labels \\nA\\n and \\nB\\n. The result would look like the array \\nBB\\n12\\nOR\\n in Fig. 2.37, but \\nwith the two white regions labeled \\nA\\n and \\nB\\n.\\n In other words, the resulting array \\nwould look like a Venn diagram. With reference to the Venn diagrams and set opera-\\ntions deﬁned in the previous section, we obtain the results in the rightmost column \\nof Fig. 2.37 using set operations as follows: \\nAB\\nc\\n=\\nNOT( ),\\n1\\n \\nAB B B\\n¨\\n=\\n12\\nAND ,\\nAB B B\\n´\\n=\\n12\\nOR ,\\n and similarly for the other results in Fig. 2.37. We will make \\nextensive use in Chapter 9 of the concepts developed in this section.\\nSPATIAL OPERATIONS\\nSpatial operations are performed directly on the pixels of an image. We classify \\nspatial operations into three broad categories: (1) single-pixel operations, (2) neigh-\\nborhood operations, and (3) geometric spatial transformations.\\nFIGURE 2.37\\nIllustration of \\nlogical operations \\ninvolving  \\nforeground \\n(white) pixels. \\nBlack represents \\nbinary 0’s and \\nwhite binary 1’s. \\nThe dashed lines \\nare shown for  \\nreference only. \\nThey are not part \\nof the result. \\nNOT\\nNOT(\\nB\\n1\\n)\\nB\\n1\\n AND \\nB\\n2\\nB\\n1\\n OR \\nB\\n2\\nB\\n1\\n AND [NOT (\\nB\\n2\\n)]\\nB\\n1\\n XOR \\nB\\n2\\nAND\\nB\\n1\\nB\\n1\\nB\\n2\\nOR\\nXOR\\nAND-\\nNOT\\nDIP4E_GLOBAL_Print_Ready.indb   98\\n6/16/2017   2:02:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 99}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n99\\nSingle-Pixel Operations\\nThe simplest operation we perform on a digital image is to alter the intensity of its \\npixels individually using a transformation function, \\nT\\n, of the form:\\n \\nsT z\\n=\\n()\\n \\n(2-42)\\nwhere \\nz\\n is the intensity of a pixel in the original image and \\ns\\n is the (mapped) inten-\\nsity of the corresponding pixel in the processed image\\n. For example, Fig. 2.38 shows \\nthe transformation used to obtain the \\nnegative\\n (sometimes called the \\ncomplement\\n) \\nof an 8-bit image. This transformation could be used, for example, to obtain the \\nnegative image in Fig. 2.36, instead of using sets. \\nNeighborhood Operations\\nLet \\nS\\nxy\\n denote the set of coordinates of a neighborhood (see Section 2.5 regarding \\nneighborhoods) centered on an arbitrary point \\n(,)\\nxy\\n in an image, \\nf\\n.\\n Neighborhood \\nprocessing generates a corresponding pixel at the same coordinates in an output \\n(processed) image, \\ng\\n, such that the value of that pixel is determined by a specified \\noperation on the neighborhood of pixels in the input image with coordinates in the \\nset \\nS\\nxy\\n. For example, suppose that the specified operation is to compute the average \\nvalue of the pixels in a rectangular neighborhood of size \\nmn\\n×\\n centered on\\n \\n(,\\n)\\nxy\\n \\n.\\n \\nThe coordinates of pixels in this region are the elements of set \\nS\\nxy\\n. Figures 2.39(a) \\nand (b) illustrate the process. We can express this averaging operation as\\n \\ngxy\\nmn\\nfrc\\nrc S\\nxy\\n(,)\\n( ,)\\n(, )\\n=\\n∑\\n1\\nH\\n \\n(2-43)\\nwhere \\nr\\n and \\nc\\n are the row and column coordinates of the pixels whose coordinates \\nare in the set \\nS\\nxy\\n. Image \\ng\\n is created by varying the coordinates \\n(,)\\nxy\\n so that the \\ncenter of the neighborhood moves from pixel to pixel in image \\nf\\n,\\n and then repeat-\\ning the neighborhood operation at each new location. For instance, the image in \\nFig. 2.39(d) was created in this manner using a neighborhood of size \\n41 41\\n×\\n. The \\nOur use of the word \\n“negative” in this context \\nrefers to the digital \\nequivalent of a  \\nphotographic negative, \\nnot to the numerical \\nnegative of the pixels in \\nthe image.\\ns\\n \\n/H11005\\n \\nT\\n(\\nz\\n)\\nz\\ns\\n0\\n0\\n255\\nz\\n0\\n255\\nFIGURE 2.38\\nIntensity  \\ntransformation \\nfunction used to \\nobtain the digital \\nequivalent of \\nphotographic \\nnegative of an \\n8-bit image..\\nDIP4E_GLOBAL_Print_Ready.indb   99\\n6/16/2017   2:02:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 100}),\n",
       " Document(page_content='100\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nnet effect is to perform local blurring in the original image. This type of process is \\nused, for example, to eliminate small details and thus render “blobs” correspond-\\ning to the largest regions of an image. We will discuss neighborhood processing in \\nChapters 3 and 5, and in several other places in the book. \\nGeometric Transformations\\nWe use geometric transformations modify the spatial arrangement of pixels in an \\nimage. These transformations are called \\nrubber-sheet transformations\\n because they \\nmay be viewed as analogous to “printing” an image on a rubber sheet, then stretch-\\ning or shrinking the sheet according to a predefined set of rules. Geometric transfor-\\nmations of digital images consist of two basic operations: \\nThe value of this pixel\\nis the average value of the\\npixels in \\nS\\nxy\\nImage \\nf\\nImage \\ng\\n(\\nx, y\\n)\\n(\\nx, y\\n)\\nS\\nxy\\nm\\nn\\nb a\\nd c\\nFIGURE 2.39\\nLocal averaging  \\nusing neighbor-\\nhood processing. \\nThe procedure is  \\nillustrated in (a) \\nand (b) for a  \\nrectangular  \\nneighborhood.  \\n(c) An aortic  \\nangiogram (see  \\nSection 1.3).  \\n(d) The result of  \\nusing Eq. (2-43) \\nwith \\nmn\\n==\\n41. \\nT\\nhe images are \\nof size \\n790 686\\n×\\n \\npixels\\n. (Original  \\nimage courtesy \\nof Dr. Thomas R. \\nGest, Division of  \\nAnatomical  \\nSciences,  \\nUniversity of \\nMichigan Medical \\nSchool.)\\nDIP4E_GLOBAL_Print_Ready.indb   100\\n6/16/2017   2:02:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 101}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n101\\n1. \\nSpatial transformation of coordinates. \\n2. \\nIntensity interpolation that assigns intensity values to the spatially transformed \\npixels\\n. \\nThe transformation of coordinates may be expressed as\\n \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nx\\ny\\nx\\ny\\ntt\\ntt\\nx\\ny\\nT\\n11 12\\n21 22\\n \\n(2-44)\\nwhere \\n(,)\\nxy\\n are pixel coordinates in the original image and \\n(,)\\n′′\\nxy\\n are the \\ncorresponding pixel coordinates of the transformed image\\n. For example, the \\ntransformation \\n(,) ( , )\\n′′\\n=\\nxy\\nxy\\n22\\n shrinks the original image to half its size in both \\nspatial directions\\n. \\nOur interest is in so-called \\nafﬁne transformations\\n, which include scaling, translation, \\nrotation, and shearing. The key characteristic of an afﬁne transformation in 2-D is \\nthat it preserves points, straight lines, and planes. Equation (2-44) can be used to \\nexpress the transformations just mentioned, except translation, which would require \\nthat a constant 2-D vector be added to the right side of the equation. However, it is \\npossible to use homogeneous coordinates to express all four afﬁne transformations \\nusing a single \\n33\\n×\\n matrix in the following general form: \\n \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\nx\\ny\\nx\\ny\\naa\\na\\naaa\\n11 0 0 1\\n11 12 13\\n21 22 23\\nA\\n⎦ ⎦\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\ny\\n1\\n \\n(2-45)\\nThis transformation can \\nscale\\n, \\nrotate\\n, \\ntranslate\\n,\\n or \\nsheer\\n an image, depending on the \\nvalues chosen for the elements of matrix \\nA\\n. Table 2.3 shows the matrix values used \\nto implement these transformations. A significant advantage of being able to per-\\nform all transformations using the unified representation in Eq. (2-45) is that it pro-\\nvides the framework for concatenating a sequence of operations. For example, if we \\nwant to resize an image, rotate it, and move the result to some location, we simply \\nform a \\n33\\n×\\n matrix equal to the product of the scaling, rotation, and translation \\nmatrices from \\nTable 2.3 (see Problems 2.36 and 2.37).\\nThe preceding transformation moves the coordinates of pixels in an image to new \\nlocations. To complete the process, we have to assign intensity values to those loca-\\ntions. This task is accomplished using \\nintensity interpolation\\n. We already discussed \\nthis topic in Section 2.4. We began that discussion with an example of zooming an \\nimage and discussed the issue of intensity assignment to new pixel locations. Zoom-\\ning is simply scaling, as detailed in the second row of Table 2.3, and an analysis simi-\\nlar to the one we developed for zooming is applicable to the problem of assigning \\nintensity values to the relocated pixels resulting from the other transformations in \\nTable 2.3. As in Section 2.4, we consider nearest neighbor, bilinear, and bicubic inter-\\npolation techniques when working with these transformations.\\nWe can use Eq. (2-45) in two basic ways. The ﬁrst, is a \\nforward mapping\\n, which \\nconsists of scanning the pixels of the input image and, at each location \\n(,) ,\\nxy\\n com-\\nDIP4E_GLOBAL_Print_Ready.indb   101\\n6/16/2017   2:02:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 102}),\n",
       " Document(page_content='102\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nputing the spatial location \\n(,)\\n′′\\nxy\\n of the corresponding pixel in the output image \\nusing Eq.\\n (2-45) directly. A problem with the forward mapping approach is that two \\nor more pixels in the input image can be transformed to the same location in the \\noutput image, raising the question of how to combine multiple output values into a \\nsingle output pixel value. In addition, it is possible that some output locations may \\nnot be assigned a pixel at all. The second approach, called \\ninverse mapping\\n, scans \\nthe output pixel locations and, at each location \\n(,) ,\\n′′\\nxy\\n computes the corresponding \\nlocation in the input image using \\n(,) ( , ) .\\nxy\\n=\\n′′\\n−\\nAx y\\n1\\n It then interpolates (using one \\nof the techniques discussed in Section 2.4) among the nearest input pixels to deter-\\nmine the intensity of the output pixel value. Inverse mappings are more efﬁcient to \\nimplement than forward mappings, and are used in numerous commercial imple-\\nmentations of spatial transformations (for example, MATLAB uses this approach).\\nTransformation\\nName\\nAffine Matrix, A\\nCoordinate\\nEquations\\nExample\\nIdentity\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nx\\n′\\ny\\n′\\nxx\\n=\\n′\\nyy\\n=\\n′\\nTranslation\\ny\\nyy t\\n=+\\n′\\nx\\nxx t\\n=+\\n′\\n10\\n01\\n001\\nx\\ny\\nt\\nt\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nShear (vertical)\\n10\\n010\\n001\\ns\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nv\\nyy\\n=\\n′\\nxx s y\\n=+\\n′\\nv\\nShear (horizontal)\\n10 0\\n10\\n00 1\\nh\\ns\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nxx\\n=\\n′\\nh\\nys x y\\n=+\\n′\\nScaling/Reflection\\n(For reflection, set one \\nscaling factor to \\n−\\n1\\nand the other to 0)\\nc\\nx\\n0\\n0\\n0\\nc\\ny\\n0\\n0\\n0\\n1\\nx\\nxc x\\n=\\n′\\ny\\nyc y\\n=\\n′\\nRotation (about the\\norigin)\\n0\\ncos \\nu\\n/H11002\\nsin \\nu\\nsin \\nu\\ncos \\nu\\n0\\n00 1\\ncos sin\\nxx y\\n=−\\n′\\nuu\\nsin cos\\nyx y\\n=+\\n′\\nuu\\nx\\n′\\ny\\n′\\nx\\n′\\nx\\n′\\nx\\n′\\nx\\n′\\ny\\n′\\ny\\n′\\ny\\n′\\ny\\n′\\nTABLE \\n2.3\\nAfﬁne  \\ntransformations \\nbased on  \\nEq. (2-45).\\nDIP4E_GLOBAL_Print_Ready.indb   102\\n6/16/2017   2:02:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 103}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n103\\nEXAMPLE 2.9 :  Image rotation and intensity interpolation.\\nThe objective of this example is to illustrate image rotation using an afﬁne transform. Figure 2.40(a) \\nshows a simple image and Figs. 2.40(b)–(d) are the results (using inverse mapping) of rotating the \\noriginal image by \\n−\\n21\\n°\\n (in Table 2.3, clockwise angles of rotation are negative). Intensity assignments \\nwere computed using nearest neighbor\\n, bilinear, and bicubic interpolation, respectively. A key issue in \\nimage rotation is the preservation of straight-line features. As you can see in the enlarged edge sections \\nin Figs. 2.40(f) through (h), nearest neighbor interpolation produced the most jagged edges and, as in \\nSection 2.4, bilinear interpolation yielded signiﬁcantly improved results. As before, using bicubic inter-\\npolation produced slightly better results. In fact, if you compare the progression of enlarged detail in \\nFigs. 2.40(f) to (h), you can see that the transition from white (255) to black (0) is smoother in the last \\nﬁgure because the edge region has more values, and the distribution of those values is better balanced. \\nAlthough the small intensity differences resulting from bilinear and bicubic interpolation are not always \\nnoticeable in human visual analysis, they can be important in processing image data, such as in auto-\\nmated edge following in rotated images.\\nThe size of the spatial rectangle needed to contain a rotated image is larger than the rectangle of the \\noriginal image, as Figs. 2.41(a) and (b) illustrate. We have two options for dealing with this: (1) we can \\ncrop the rotated image so that its size is equal to the size of the original image, as in Fig. 2.41(c), or we \\ncan keep the larger image containing the full rotated original, an Fig. 2.41(d). We used the ﬁrst option in \\nFig. 2.40 because the rotation did not cause the object of interest to lie outside the bounds of the original \\nrectangle. The areas in the rotated image that do not contain image data must be ﬁlled with some value, 0 \\n(black) being \\nthe most common. Note that counterclockwise angles of rotation are considered positive. \\nThis is a result of the way in which our image coordinate system is set up (see Fig. 2.19), and the way in \\nwhich rotation is deﬁned in Table 2.3.\\nImage Registration\\nImage registration is an important application of digital image processing used to \\nalign two or more images of the same scene. In image registration, we have avail-\\nable an \\ninput\\n image and a \\nreference\\n image. The objective is to transform the input \\nimage geometrically to produce an output image that is aligned (registered) with the \\nreference image. Unlike the discussion in the previous section where transformation \\nfunctions are known, the geometric transformation needed to produce the output, \\nregistered image generally is not known, and must be estimated.\\nExamples of image registration include aligning two or more images taken at \\napproximately the same time, but using different imaging systems, such as an MRI \\n(magnetic resonance imaging) scanner and a PET (positron emission tomography) \\nscanner. Or, perhaps the images were taken at different times using the same instru-\\nments, such as satellite images of a given location taken several days, months, or even \\nyears apart. In either case, combining the images or performing quantitative analysis \\nand comparisons between them requires compensating for geometric distortions \\ncaused by differences in viewing angle, distance, orientation, sensor resolution, shifts \\nin object location, and other factors. \\nDIP4E_GLOBAL_Print_Ready.indb   103\\n6/16/2017   2:02:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 104}),\n",
       " Document(page_content='104\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nOne of the principal approaches for solving the problem just discussed is to use \\ntie \\npoints\\n (also called \\ncontrol points\\n). These are corresponding points whose locations \\nare known precisely in the input and reference images. Approaches for selecting tie \\npoints range from selecting them interactively to using algorithms that detect these \\npoints automatically. Some imaging systems have physical artifacts (such as small \\nmetallic objects) embedded in the imaging sensors. These produce a set of known \\npoints (called \\nreseau marks \\nor\\n ﬁducial marks\\n) directly on all images captured by the \\nsystem. These known points can then be used as guides for establishing tie points.\\nThe problem of estimating the transformation function is one of modeling. For \\nexample, suppose that we have a set of four tie points each in an input and a refer-\\nence image. A simple model based on a bilinear approximation is given by\\n \\nxc c c c\\n=+\\n+ +\\n12 3 4\\nvwv w\\n \\n(2-46)\\nand\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 2.40\\n (a) A \\n541 421\\n×\\n image of the letter T. (b) Image rotated \\n−\\n21\\n°\\n using nearest-neighbor interpolation for \\nintensity assignments\\n. (c) Image rotated \\n−\\n21\\n°\\n using bilinear interpolation. (d) Image rotated \\n−\\n21\\n°\\n using bicubic \\ninterpolation.\\n (e)-(h) Zoomed sections (each square is one pixel, and the numbers shown are intensity values).\\n45\\n154\\n247\\n0\\n0\\n255\\n255\\n255\\n255\\n0\\n00\\n77\\n168\\n255\\n00\\n255\\nDIP4E_GLOBAL_Print_Ready.indb   104\\n6/16/2017   2:02:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 105}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n105\\nx\\n \\n¿\\ny\\n \\n¿\\ny\\n \\n¿\\nx\\n \\n¿\\ny\\n \\n¿\\nx\\n \\n¿\\ny\\nx\\nOrigin  \\nImage \\nf\\n(\\nx\\n, \\ny\\n)\\nPositive\\nangle of\\nrotation\\nPos\\ni\\nt\\ni\\nve\\nan\\ngl\\ne of\\nr\\notatio\\nn\\nb a\\nd c\\nFIGURE 2.41\\n(a) A digital  \\nimage.  \\n(b) Rotated image \\n(note the  \\ncounterclockwise \\ndirection for a \\npositive angle of \\nrotation).  \\n(c) Rotated image \\ncropped to ﬁt the \\nsame area as the \\noriginal image.  \\n(d) Image  \\nenlarged to  \\naccommodate \\nthe entire rotated \\nimage.\\nyc c c c\\n=+ + +\\n56 7 8\\nvwv w\\n \\n(2-47)\\nDuring the estimation phase, \\n(, )\\nvw\\n and \\n(,)\\nxy\\n are the coordinates of tie points in the \\ninput and reference images\\n, respectively. If we have four pairs of corresponding tie \\npoints in both images, we can write eight equations using Eqs. (2-46) and (2-47) and \\nuse them to solve for the eight unknown coefficients, \\nc\\n1\\n through \\nc\\n8\\n. \\nOnce we have the coefﬁcients, Eqs. (2-46) and (2-47) become our vehicle for trans-\\nforming all the pixels in the input image. The result is the desired registered image. \\nAfter the coefﬁcients have been computed, we let \\n(\\nv,\\nw)\\n denote the coordinates of \\neach pixel in the input image\\n, and \\n(,)\\nxy\\n become the corresponding coordinates of the \\noutput image\\n. The \\nsame set of coefﬁcients, \\nc\\n1\\n through \\nc\\n8\\n,\\n are used in computing all \\ncoordinates \\n(,)\\nxy\\n; we just step through all \\n(, )\\nvw\\n in the input image to generate the \\ncorresponding \\n(,)\\nxy\\n in the output, registered image. If the tie points were selected \\ncorrectly\\n, this new image should be registered with the reference image, within the \\naccuracy of the bilinear approximation model.\\nIn situations where four tie points are insufﬁcient to obtain satisfactory regis-\\ntration, an approach used frequently is to select a larger number of tie points and \\nthen treat the quadrilaterals formed by groups of four tie points as subimages. The \\nsubimages are processed as above, with all the pixels within a quadrilateral being \\ntransformed using the coefﬁcients determined from the tie points corresponding \\nto that quadrilateral. Then we move to another set of four tie points and repeat the \\nDIP4E_GLOBAL_Print_Ready.indb   105\\n6/16/2017   2:02:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 106}),\n",
       " Document(page_content='106\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nprocedure until all quadrilateral regions have been processed. It is possible to use \\nmore complex regions than quadrilaterals, and to employ more complex models, \\nsuch as polynomials ﬁtted by least squares algorithms. The number of control points \\nand sophistication of the model required to solve a problem is dependent on the \\nseverity of the geometric distortion. Finally, keep in mind that the transformations \\ndeﬁned by Eqs. (2-46) and (2-47), or any other model for that matter, only map the \\nspatial coordinates of the pixels in the input image. We still need to perform inten-\\nsity interpolation using any of the methods discussed previously to assign intensity \\nvalues to the transformed pixels.\\nEXAMPLE 2.10 :  Image registration.\\nFigure 2.42(a) shows a reference image and Fig. 2.42(b) shows the same image, but distorted geometri-\\ncally by vertical and horizontal shear. Our objective is to use the reference image to obtain tie points \\nand then use them to register the images. The tie points we selected (manually) are shown as small white \\nsquares near the corners of the images (we needed only four tie points because the distortion is linear \\nshear in both directions). Figure 2.42(c) shows the registration result obtained using these tie points in \\nthe procedure discussed in the preceding paragraphs. Observe that registration was not perfect, as is \\nevident by the black edges in Fig. 2.42(c). The difference image in Fig. 2.42(d) shows more clearly the \\nslight lack of registration between the reference and corrected images. The reason for the discrepancies \\nis error in the manual selection of the tie points. It is difﬁcult to achieve perfect matches for tie points \\nwhen distortion is so severe.\\nVECTOR AND MATRIX OPERATIONS\\nMultispectral image processing is a typical area in which vector and matrix opera-\\ntions are used routinely. For example, you will learn in Chapter 6 that color images \\nare formed in RGB color space by using red, green, and blue component images, as \\nFig. 2.43 illustrates. Here we see that \\neach\\n pixel of an RGB image has three compo-\\nnents, which can be organized in the form of a column vector\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nz\\nz\\nz\\n1\\n2\\n3\\n \\n(2-48)\\nwhere \\nz\\n1\\n is the intensity of the pixel in the red image, and \\nz\\n2\\n and \\nz\\n3\\n are the corre-\\nsponding pixel intensities in the green and blue images, respectively. Thus, an RGB \\ncolor image of size \\nMN\\n×\\n can be represented by three component images of this \\nsize\\n, or by a total of \\nMN\\n vectors of size \\n31\\n×\\n.\\n A general multispectral case involving \\nn\\n component images (e\\n.g., see Fig. 1.10) will result in \\nn\\n-dimensional vectors:\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nz\\nz\\nz\\nn\\n1\\n2\\n/vertellipsis\\n \\n(2-49)\\nRecall that an \\nn\\n-dimensional vector \\ncan be thought of as a \\npoint in \\nn\\n-dimensional \\nEuclidean space.\\nDIP4E_GLOBAL_Print_Ready.indb   106\\n6/16/2017   2:02:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 107}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n107\\n We will use this type of vector representation throughout the book.\\nThe \\ninner product\\n (also called the \\ndot product\\n) of two \\nn\\n-dimensional column vec-\\ntors \\na\\n and \\nb\\n is deﬁned as\\n \\nab a b\\ni≜\\n/midhorizellipsis\\nT\\nnn\\nii\\ni\\nn\\nab ab ab\\nab\\n=++ +\\n=\\n=\\n∑\\n11 22\\n1\\n \\n(2-50)\\nwhere \\nT\\n indicates the transpose\\n. The \\nEuclidean vector norm\\n, denoted by \\nz\\n,\\n is \\ndefined as the square root of the inner product:\\n \\nzz z\\n=\\n(\\n)\\nT\\n1\\n2\\n \\n(2-51)\\nThe product \\nab\\nT\\n is called \\nthe \\nouter product\\n \\nof \\na\\n \\nand \\nb\\n. It is a matrix of \\nsize \\nn \\n× \\nn\\n. \\nb a\\nd c\\nFIGURE 2.42\\nImage  \\nregistration. \\n(a) Reference \\nimage. (b) Input \\n(geometrically \\ndistorted image). \\nCorresponding tie \\npoints are shown \\nas small white \\nsquares near the \\ncorners.  \\n(c) Registered \\n(output) image \\n(note the errors \\nin the border). \\n(d) Difference \\nbetween (a) and \\n(c), showing more \\nregistration errors.\\nDIP4E_GLOBAL_Print_Ready.indb   107\\n6/16/2017   2:02:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 108}),\n",
       " Document(page_content='108\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nComponent image 3 (Blue)\\nComponent image 2 (Green)\\nComponent image 1 (Red)\\nz\\n \\n/H11005\\nz\\n1\\nz\\n2\\nz\\n3\\nFIGURE 2.43\\nForming a vector \\nfrom  \\ncorresponding \\npixel values in \\nthree RGB  \\ncomponent  \\nimages.\\nWe recognize this expression as the length of vector \\nz\\n.\\nWe can use vector notation to express several of the concepts discussed earlier. \\nFor example, the Euclidean distance, \\nD\\n(,\\n)\\nza\\n, between points (vectors) \\nz\\n and \\na\\n in \\nn\\n-dimensional space is deﬁned as the Euclidean vector norm:\\n \\n \\nD\\nza za za\\nT\\nnn\\n(, )\\n() ( )\\n()\\nz a z a zaza\\n=−= −\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n−−\\n⎡\\n1\\n2\\n11\\n2\\n22\\n22\\n++ +\\n/midhorizellipsis\\n⎣ ⎣\\n⎤\\n⎦\\n1\\n2\\n \\n(2-52)\\nThis is a generalization of the 2-D Euclidean distance defined in Eq. (2-19). \\nAnother advantage of pixel vectors is in linear transformations\\n, represented as\\n \\nwA z a\\n=−\\n()\\n(2-53)\\nwhere \\nA\\n is a matrix of size \\nmn\\n×\\n,\\n and \\nz\\n and \\na\\n are column vectors of size \\nn\\n×\\n1.\\nAs noted in Eq. (2-10), entire images can be treated as matrices (or, equivalently, \\nas vectors),\\n a fact that has important implication in the solution of numerous image \\nprocessing problems. For example, we can express an image of size \\nMN\\n×\\n as a col-\\numn vector of dimension \\nMN\\n×\\n1\\n by letting the ﬁrst \\nM\\n elements of the vector equal \\nthe ﬁrst column of the image\\n, the next \\nM\\n elements equal the second column, and \\nso on. With images formed in this manner, we can express a broad range of linear \\nprocesses applied to an image by using the notation\\n \\ngH f n\\n=+\\n \\n(2-54)\\nwhere \\nf\\n is an \\nMN\\n×\\n1\\n vector representing an input image, \\nn\\n is an \\nMN\\n×\\n1\\n vector rep-\\nresenting an \\nMN\\n×\\n noise pattern, \\ng\\n is an \\nMN\\n×\\n1\\n vector representing a processed \\nimage\\n, and \\nH\\n is an \\nMN MN\\n×\\n matrix representing a linear process applied to the \\ninput image (see the discussion earlier in this chapter regarding linear processes).\\n \\nIt is possible, for example, to develop an entire body of generalized techniques for \\nimage restoration starting with Eq. (2-54), as we discuss in Section 5.9. We will men-\\ntion the use of matrices again in the following section, and show other uses of matri-\\nces for image processing in numerous chapters in the book.\\nDIP4E_GLOBAL_Print_Ready.indb   108\\n6/16/2017   2:02:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 109}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n109\\nIMAGE TRANSFORMS\\nAll the image processing approaches discussed thus far operate directly on the pixels \\nof an input image; that is, they work directly in the spatial domain. In some cases, \\nimage processing tasks are best formulated by transforming the input images, carry-\\ning the specified task in a \\ntransform domain\\n, and applying the inverse transform to \\nreturn to the spatial domain. You will encounter a number of different transforms \\nas you proceed through the book. A particularly important class of 2-D \\nlinear trans-\\nforms\\n, denoted \\nT\\n(,\\n)\\nuv\\n, can be expressed in the general form\\n \\nTf\\nx\\ny\\nr\\nx\\ny\\ny\\nN\\nx\\nM\\n( ,) ( ,) ( ,,,)\\nuv uv\\n=\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1\\n- -\\n \\n(2-55)\\nwhere \\nfx y\\n(,\\n)\\n is an input image, \\nrxy\\n(\\n,,,)\\nuv\\n is called a \\nforwar\\nd transformation ker-\\nnel\\n, and Eq. (2-55) is evaluated for \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n and \\nv\\n=−\\n012\\n1\\n,,, ,\\n…\\nN\\n \\n. As \\nbefore, \\nx\\n and \\ny\\n are spatial variables, while \\nM\\n and \\nN\\n are the row and column dimen-\\nsions of \\nf\\n. Variables \\nu\\n \\nand \\nv\\n are called the \\ntransform variables\\n. \\nT\\n(,\\n)\\nuv\\n is called the \\nforwar\\nd transform\\n of \\nfx y\\n(,\\n)\\n. Given \\nT\\n(,\\n) ,\\nuv\\n we can recover \\nfx y\\n(,\\n)\\n using the \\ninverse \\ntransform \\nof \\nT\\n(,\\n) :\\nuv\\n \\nfx y T s xy\\nN M\\n( ,) ( ,) ( ,,,)\\n=\\n= =\\n∑ ∑\\nuv uv\\nv u\\n0\\n1\\n0\\n1\\n- -\\n \\n(2-56)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, ,\\n…\\n, where \\nsxy\\n(\\n,,,)\\nuv\\n is called an \\ninverse \\ntransformation kernel\\n.\\n Together, Eqs. (2-55) and (2-56) are called a \\ntransform pair\\n.\\nFigure 2.44 shows the basic steps for performing image processing in the linear \\ntransform domain. First, the input image is transformed, the transform is then modi-\\nﬁed by a predeﬁned operation and, ﬁnally, the output image is obtained by computing \\nthe inverse of the modiﬁed transform. Thus, we see that the process goes from the \\nspatial domain to the transform domain, and then back to the spatial domain.\\nThe forward transformation kernel is said to be \\nseparable\\n if\\n \\nrxy\\nr y\\n(\\n,,, )( ,)\\nuv ) = r ( x , u v\\n1\\n2\\n \\n(2-57)\\nIn addition, the kernel is said to be \\nsymmetric\\n if \\nrx\\n1\\n(,)\\nu\\n is functionally equal to \\nry\\n2\\n(,)\\nv\\n \\n, so that\\n \\nrxy r x r y\\n(\\n,,,) ( ,)( ,)\\nuv u v\\n=\\n11\\n \\n(2-58)\\nIdentical comments apply to the inverse kernel.\\nT\\nhe nature of a transform is determined by its kernel. A transform of particular \\nimportance in digital image processing is the \\nFourier transform\\n, which has the fol-\\nlowing forward and inverse kernels:\\n \\nrxy e\\nju x M y N\\n( ,,,)\\n()\\nuv\\nv\\n=\\n−+\\n2\\np\\n \\n(2-59)\\nand\\n \\nsxy\\nMN\\ne\\nju x M y N\\n( ,,,)\\n()\\nuv\\nv\\n=\\n+\\n1\\n2\\np\\n \\n(2-60)\\nDIP4E_GLOBAL_Print_Ready.indb   109\\n6/16/2017   2:02:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 110}),\n",
       " Document(page_content='110\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nrespectively, where \\nj\\n=−\\n1\\n, so these kernels are complex functions. Substituting the \\npreceding kernels into the general transform formulations in Eqs\\n. (2-55) and (2-56) \\ngives us the \\ndiscrete Fourier transform pair\\n:\\n \\nTf\\nx\\ny\\ne\\nju x M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nv\\n=\\n−+\\n= =\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n- -\\n(2-61)\\nand\\n \\nfx y\\nMN\\nTe\\nju x M y N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n= =\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nuv\\nv\\nv u\\np\\n- -\\n \\n(2-62)\\nIt can be shown that the Fourier kernels are separable and symmetric (Problem 2.39), \\nand that separable and symmetric kernels allow 2-D transforms to be computed using \\n1-D transforms (see Problem 2.40).\\n The preceding two equations are of fundamental \\nimportance in digital image processing, as you will see in Chapters 4 and 5.\\nEXAMPLE 2.11 :  Image processing in the transform domain.\\nFigure 2.45(a) shows an image corrupted by periodic (sinusoidal) interference. This type of interference \\ncan be caused, for example, by a malfunctioning imaging system; we will discuss it in Chapter 5. In the \\nspatial domain, the interference appears as waves of intensity. In the frequency domain, the interference \\nmanifests itself as bright bursts of intensity, whose location is determined by the frequency of the sinu-\\nsoidal interference (we will discuss these concepts in much more detail in Chapters 4 and 5). Typi-\\ncally, the bursts are easily observable in an image of the magnitude of the Fourier transform, \\nT\\n(,) .\\nuv\\n \\nW\\nith reference to the diagram in Fig. 2.44, the corrupted image is \\nfx y\\n(,\\n) ,\\n the transform in the leftmost \\nbox is the F\\nourier transform, and Fig. 2.45(b) is \\nT\\n(,)\\nuv\\n displayed as an image. The bright dots shown \\nare the bursts of intensity mentioned above\\n. Figure 2.45(c) shows a mask image (called a \\nﬁlter\\n) with \\nwhite and black representing 1 and 0, respectively. For this example, the operation in the second box of \\nFig. 2.44 is to multiply the ﬁlter by the transform to remove the bursts associated with the interference. \\nFigure 2.45(d) shows the ﬁnal result, obtained by computing the inverse of the modiﬁed transform. The \\ninterference is no longer visible, and previously unseen image detail is now made quite clear. Observe, \\nfor example, the ﬁducial marks (faint crosses) that are used for image registration, as discussed earlier.\\nWhen the forward and inverse kernels of a transform are separable and sym-\\nmetric, and \\nfx y\\n(,\\n)\\n is a square image of size \\nMM\\n×\\n, Eqs. (2-55) and (2-56) can be \\nexpressed in matrix form:\\nThe exponential terms \\nin the Fourier transform \\nkernels can be expanded \\nas sines and cosines of \\nvarious frequencies. As \\na result, the domain of \\nthe Fourier transform \\nis called the \\nfrequency \\ndomain\\n.\\nT\\n(\\nu\\n, \\nv\\n)\\nTransform\\nOperation\\nR\\nInverse\\ntransform\\nTransform domain\\nR\\n[\\nT\\n(\\nu\\n, \\nv\\n)]\\nf\\n(\\nx\\n, \\ny\\n)\\ng\\n(\\nx\\n, \\ny\\n)\\nSpatial\\ndomain\\nSpatial\\ndomain\\nFIGURE 2.44\\nGeneral approach \\nfor working in the \\nlinear transform \\ndomain.\\nDIP4E_GLOBAL_Print_Ready.indb   110\\n6/16/2017   2:02:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 111}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n111\\n \\nTA F A\\n=\\n \\n(2-63)\\n \\nwhere \\nF\\n is an \\nMM\\n×\\n matrix containing the elements of \\nfx y\\n(,\\n)\\n [see Eq. (2-9)], \\nA\\n is \\nan \\nMM\\n×\\n matrix with elements \\nar i j\\nij\\n=\\n1\\n(, ) ,\\n and \\nT\\n is an \\nMM\\n×\\n transform matrix \\nwith elements \\nT\\n(,\\n) ,\\nuv\\n for \\nu,v\\n=−\\n012\\n1\\n,,, , .\\n…\\nM\\nTo obtain the inverse transform, we pre- and post-multiply Eq. (2-63) by an \\ninverse transformation matrix \\nB\\n:\\n \\nBTB BAFAB\\n=\\n \\n(2-64)\\nIf \\nBA\\n=\\n−\\n1\\n,\\n \\nFB T B\\n=\\n \\n(2-65)\\nindicating that \\nF\\n or\\n, equivalently, \\nfx y\\n(,\\n)\\n, can be recovered completely from its \\nforward transform.\\n If \\nB\\n is not equal to \\nA\\n−\\n1\\n, Eq. (2-65) yields an approximation:\\n \\nˆ\\nF\\nBAFAB\\n=\\n \\n(2-66)\\nIn addition to the Fourier transform, a number of important transforms, including \\nthe \\nW\\nalsh\\n, \\nHadamard\\n, \\ndiscrete\\n \\ncosine\\n, \\nHaar\\n, and \\nslant\\n transforms, can be expressed \\nin the form of Eqs. (2-55) and (2-56), or, equivalently, in the form of Eqs. (2-63) and \\n(2-65). We will discuss these and other types of image transforms in later chapters. \\nb a\\nd c\\nFIGURE 2.45\\n(a) Image  \\ncorrupted by  \\nsinusoidal  \\ninterference.  \\n(b) Magnitude of \\nthe Fourier  \\ntransform  \\nshowing the \\nbursts of energy \\ncaused by the \\ninterference \\n(the bursts were \\nenlarged for \\ndisplay purposes). \\n(c) Mask used \\nto eliminate the \\nenergy bursts.  \\n(d) Result of  \\ncomputing the \\ninverse of the \\nmodiﬁed Fourier \\ntransform.  \\n(Original  \\nimage courtesy of \\nNASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   111\\n6/16/2017   2:02:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 112}),\n",
       " Document(page_content='112\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nIMAGE INTENSITIES AS RANDOM VARIABLES\\nWe treat image intensities as random quantities in numerous places in the book. For \\nexample, let \\nzi L\\ni\\n,, , , ,,\\n=−\\n012 1\\n…\\n denote the values of all possible intensities in an \\nMN\\n×\\n digital image. The probability, \\npz\\nk\\n() ,\\n of intensity level \\nz\\nk\\n occurring in the im-\\nage is estimated as\\n \\npz\\nn\\nMN\\nk\\nk\\n()\\n=\\n \\n(2-67)\\nwhere \\nn\\nk\\n is the number of times that intensity \\nz\\nk\\n occurs in the image and \\nMN\\n is the \\ntotal number of pixels. Clearly,\\n \\npz\\nk\\nk\\nL\\n()\\n=\\n−\\n∑\\n=\\n0\\n1\\n1  \\n(2-68)\\nOnce we have \\npz\\nk\\n() ,\\n we can determine a number of important image characteristics. \\nF\\nor example, the mean (average) intensity is given by\\n \\nmz p z\\nkk\\nk\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(2-69)\\nSimilarly, the variance of the intensities is\\n \\ns\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\nzm p z\\nkk\\nk\\nL\\n \\n(2-70)\\nThe variance is a measure of the spread of the values of \\nz\\n about the mean,\\n so it is \\na useful measure of image contrast. In general, the \\nn\\nth central moment of random \\nvariable \\nz\\n about the mean is defined as\\n \\nm\\nnk\\nn\\nk\\nk\\nL\\nzz m p z\\n() ( ) ( )\\n=−\\n=\\n−\\n∑\\n0\\n1\\n \\n(2-71)\\nWe see that \\nm\\n0\\n1\\n() ,\\nz\\n=\\n \\nm\\n1\\n0\\n() ,\\nz\\n=\\n and \\nms\\n2\\n2\\n() .\\nz\\n=\\n Whereas the mean and variance \\nhave an immediately obvious relationship to visual properties of an image\\n, higher-\\norder moments are more subtle. For example, a positive third moment indicates \\nthat the intensities are biased to values higher than the mean, a negative third mo-\\nment would indicate the opposite condition, and a zero third moment would tell us \\nthat the intensities are distributed approximately equally on both sides of the mean. \\nThese features are useful for computational purposes, but they do not tell us much \\nabout the appearance of an image in general.\\nAs you will see in subsequent chapters, concepts from probability play a central \\nrole in a broad range of image processing applications. For example, Eq. (2-67) is \\nutilized in Chapter 3 as the basis for image enhancement techniques based on his-\\ntograms. In Chapter 5, we use probability to develop image restoration algorithms, \\nin Chapter 10 we use probability for image segmentation, in Chapter 11 we use it \\nto describe texture, and in Chapter 12 we use probability as the basis for deriving \\noptimum pattern recognition algorithms.\\nYou may ﬁnd it useful \\nto  consult the tutorials \\nsection in the book \\nwebsite for a brief review \\nof probability.\\nDIP4E_GLOBAL_Print_Ready.indb   112\\n6/16/2017   2:02:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 113}),\n",
       " Document(page_content=' \\n  \\nSummary, References, and Further Reading\\n    \\n113\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n2.1 \\nIf you use a sheet of white paper to shield your \\neyes when looking directly at the sun,\\n the side of \\nthe sheet facing you appears black. Which of the \\nvisual processes discussed in Section 2.1 is respon-\\nsible for this?\\n2.2 * \\nUsing the background information provided in \\nSection 2.1,\\n and thinking purely in geometrical \\nterms, estimate the diameter of the smallest \\nprinted dot that the eye can discern if the page \\non which the dot is printed is 0.2 m away from the \\neyes. Assume for simplicity that the visual system \\nceases to detect the dot when the image of the dot \\non the fovea becomes smaller than the diameter \\nof one receptor (cone) in that area of the retina. \\nAssume further that the fovea can be modeled as \\na square array of dimension \\n15\\n.m\\nm\\n on the side, \\nand that the cones and spaces between the cones \\nare distributed uniformly throughout this array\\n.\\n2.3 \\nAlthough it is not shown in Fig. 2.10, alternating \\ncurrent is part of the electromagnetic spectrum.\\n \\nCommercial alternating current in the United \\nStates has a frequency of 60 Hz. What is the wave-\\nlength in kilometers of this component of the \\nspectrum?\\n2.4 \\nYou are hired to design the front end of an imag-\\ning system for studying the shapes of cells\\n, bacteria, \\nviruses, and proteins. The front end consists in \\nthis case of the illumination source(s) and cor-\\nresponding imaging camera(s).The diameters of \\ncircles required to fully enclose individual speci-\\nmens in each of these categories are 50, 1, 0.1, and \\n00 1\\n.\\nm\\nm,\\n respectively. In order to perform auto-\\nmated analysis\\n, the smallest detail discernible on a \\nspecimen must be \\n0 001 .\\nm\\nm.\\n \\n(a) * \\nCan you solve the imaging aspects of this \\nproblem with a single sensor and camera? \\nIf your answer is yes\\n, specify the illumina-\\ntion wavelength band and the type of camera \\nneeded. By “type,” we mean the band of the \\nelectromagnetic spectrum to which the cam-\\nera is most sensitive (e.g., infrared).\\n(b) \\nIf your answer in (a) is no, what type of illu-\\nmination sources and corresponding imaging \\nsensors would you recommend? Specify the \\nlight sources and cameras as requested in \\npart (a).\\n Use the minimum number of illumi-\\nnation sources and cameras needed to solve \\nthe problem. (\\nHint:\\n From the discussion in \\nSummary, References, and Further Reading\\n \\nThe material in this chapter is the foundation for the remainder of the book. For additional reading on visual per-\\nception, see Snowden et al. [2012], and the classic book by Cornsweet [1970]. Born and Wolf [1999] discuss light in \\nterms of electromagnetic theory. A basic source for further reading on image sensing is Trussell and Vrhel [2008]. \\nThe image formation model discussed in Section 2.3 is from Oppenheim et al. [1968]. The IES Lighting Handbook \\n[2011] is a reference for the illumination and reﬂectance values used in that section. The concepts of image sampling \\nintroduced in Section 2.4 will be covered in detail in Chapter 4. The discussion on experiments dealing with the \\nrelationship between image quality and sampling is based on results from Huang [1965]. For further reading on the \\ntopics discussed in Section 2.5, see Rosenfeld and Kak [1982], and Klette and Rosenfeld [2004].\\nSee Castleman [1996] for additional reading on linear systems in the context of image processing. The method of \\nnoise reduction by image averaging was ﬁrst proposed by Kohler and Howell [1963]. See Ross [2014] regarding the \\nexpected value of the mean and variance of the sum of random variables. See Schröder [2010] for additional read-\\ning on logic and sets. For additional reading on geometric spatial transformations see Wolberg [1990] and Hughes \\nand Andries [2013]. For further reading on image registration see Goshtasby [2012]. Bronson and Costa [2009] is a \\ngood reference for additional reading on vectors and matrices. See Chapter 4 for a detailed treatment of the Fourier \\ntransform, and Chapters 7, 8, and 11 for details on other image transforms. For details on the software aspects of \\nmany of the examples in this chapter, see Gonzalez, Woods, and Eddins [2009]. \\nDIP4E_GLOBAL_Print_Ready.indb   113\\n6/16/2017   2:02:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='114\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nSection 2.2, the illumination required to “see” \\nan object must have a wavelength the same \\nsize or smaller than the object.)\\n2.5 \\nYou are preparing a report and have to insert in it \\nan image of size \\n2048 2048\\n×\\n pixels.\\n(a) * \\nAssuming no limitations on the printer, what \\nwould the resolution in line pairs per mm \\nhave to be for the image to ﬁt in a space of \\nsize \\n55\\n×\\n cm?\\n(b) \\nWhat would the resolution have to be in dpi \\nfor the image to ﬁt in \\n22\\n×\\n inches?\\n2.6 * \\nA CCD camera chip of dimensions \\n77\\n×\\n mm and \\n1024 1024\\n×\\n sensing elements, is focused on a \\nsquare\\n, ﬂat area, located 0.5 m away. The camera \\nis equipped with a 35-mm lens. How many line \\npairs per mm will this camera be able to resolve? \\n(\\nHint:\\n Model the imaging process as in Fig. 2.3, \\nwith the focal length of the camera lens substitut-\\ning for the focal length of the eye.)\\n2.7 \\nAn automobile manufacturer is automating the \\nplacement of certain components on the bumpers \\nof a limited-edition line of sports cars\\n. The com-\\nponents are color-coordinated, so the assembly  \\nrobots need to know the color of each car in order \\nto select the appropriate bumper component. \\nModels come in only four colors: blue, green, red, \\nand white. You are hired to propose a solution \\nbased on imaging. How would you solve the prob-\\nlem of determining the color of each car, keeping \\nin mind that cost is the most important consider-\\nation in your choice of components?\\n2.8 * \\nSuppose that a given automated imaging applica-\\ntion requires a minimum resolution of 5 line pairs \\nper mm to be able to detect features of interest \\nin objects viewed by the camera.\\n The distance \\nbetween the focal center of the camera lens and \\nthe area to be imaged is 1 m. The area being \\nimaged is \\n05 5\\n.\\n×0\\n.\\n m. You have available a 200 \\nmm lens\\n, and your job is to pick an appropriate \\nCCD imaging chip. What is the minimum number \\nof sensing elements and square size, \\ndd\\n×\\n,\\n of the \\nCCD chip that will meet the requirements of this \\napplication? (\\nHint:\\n Model the imaging process \\nas in Fig. 2.3, and assume for simplicity that the \\nimaged area is square.)\\n2.9 \\nA common measure of transmission for digital \\ndata is the \\nbaud rate\\n,\\n deﬁned as symbols (bits in \\nour case) per second. As a minimum, transmission \\nis accomplished in packets consisting of a start \\nbit, a byte (8 bits) of information, and a stop bit. \\nUsing these facts, answer the following:\\n(a) * \\nHow many seconds would it take to transmit \\na sequence of 500 images of size \\n1024 1024\\n×\\n \\npixels with 256 intensity levels using a 3 \\nM-baud \\n(10\\n6\\nbits/sec)\\n baud modem? (This \\nis a representative medium speed for a DSL \\n(Digital Subscriber Line) residential line\\n.\\n(b) \\nWhat would the time be using a 30 G-baud \\n(10\\n9\\nbits/sec)\\n modem? (This is a represen-\\ntative medium speed for a commercial line.)\\n2.10 * \\nHigh-deﬁnition television (HDTV) generates \\nimages with 1125 horizontal \\nTV lines interlaced \\n(i.e., where every other line is “painted” on the \\nscreen in each of two ﬁelds, each ﬁeld being \\n16 0 t h\\n of a second in duration). The width-to-\\nheight aspect ratio of the images is 16:9.\\n The \\nfact that the number of horizontal lines is ﬁxed \\ndetermines the vertical resolution of the images. \\nA company has designed a system that extracts \\ndigital images from HDTV video. The resolution \\nof each horizontal line in their system is propor-\\ntional to vertical resolution of HDTV , with the \\nproportion being the width-to-height ratio of the \\nimages. Each pixel in the color image has 24 bits \\nof intensity, 8 bits each for a red, a green, and a \\nblue component image. These three “primary” \\nimages form a color image. How many bits would \\nit take to store the images extracted from a two-\\nhour HDTV movie?\\n2.11 \\nWhen discussing linear indexing in Section 2.4, \\nwe arrived at the linear index in Eq.\\n (2-14) by \\ninspection. The same argument used there can be \\nextended to a 3-D array with coordinates \\nx\\n, \\ny\\n, and \\nz\\n, and corresponding dimensions \\nM\\n, \\nN\\n, and \\nP\\n. The \\nlinear index for any \\n(,,)\\nxy\\nz\\n is\\nsx M y N z\\n=+ +\\n()\\nStart with this expression and\\n(a) * \\nDerive Eq. (2-15).\\n(b) \\nDerive Eq. (2-16).\\n2.12 * \\nSuppose that a ﬂat area with center at \\n(,)\\nxy\\n00\\n is \\nDIP4E_GLOBAL_Print_Ready.indb   114\\n6/16/2017   2:02:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n115\\nilluminated by a light source with intensity distri-\\nbution\\n \\nixy K e\\nxx yy\\n(,)\\n[( ) ( ) ]\\n=\\n−− +−\\n0\\n2\\n0\\n2\\nAssume for simplicity that the reﬂectance of \\nthe area is constant and equal to 1.0, and let \\nK\\n=\\n255.\\n If the intensity of the resulting image is \\nquantized using \\nk\\n bits, and the eye can detect an \\nabrupt change of eight intensity levels between \\nadjacent pixels, what is the highest value of \\nk\\n that \\nwill cause visible false contouring?\\n2.13 \\nSketch the image in Problem 2.12 for \\nk\\n=\\n2.\\n2.14 \\nConsider the two image subsets, \\nS\\n1\\n and \\nS\\n2\\n in the \\nfollowing ﬁgure. With reference to Section 2.5, \\nand assuming that \\nV\\n=\\n{}\\n1,\\n determine whether \\nthese two subsets are:\\n(a) * \\n 4-adjacent.\\n(b) \\n 8-adjacent. \\n(c) \\nm\\n-adjacent.\\n \\n1\\nS\\n2\\nS\\n00\\n0 0 0 0 0\\n01\\n1\\n11\\n1 0 0 1 0\\n00\\n0\\n10\\n1 1 0 1 0\\n00\\n0\\n00\\n0 0 1 1 1\\n00\\n0\\n01\\n0 0 1 1 1\\n01\\n1\\n2.15 * \\nDevelop an algorithm for converting a one-pixel-\\nthick 8-path to a 4-path.\\n2.16 \\nDevelop an algorithm for converting a one-pixel-\\nthick \\nm\\n-path to a 4-path.\\n2.17 \\nRefer to the discussion toward the end of Sec-\\ntion 2.5,\\n where we deﬁned the background of an \\nimage as \\n() ,\\nR\\nu\\nc\\n the complement of the union of \\nall the regions in the image. In some applications, \\nit is advantageous to deﬁne the background as the \\nsubset of pixels of \\n()\\nR\\nu\\nc\\n that are not \\nhole\\n pixels \\n(informally, think of holes as sets of background \\npixels surrounded by foreground pixels). How \\nwould you modify the deﬁnition to exclude hole \\npixels from \\n()\\nR\\nu\\nc\\n?\\n An answer such as “the back-\\nground is the subset of pixels of \\n()\\nR\\nu\\nc\\n that are not \\nhole pixels” is not acceptable. (\\nHint:\\n Use the con-\\ncept of connectivity.)\\n2.18 \\nConsider the image segment shown in the ﬁgure \\nthat follows\\n.\\n(a) * \\nAs in Section 2.5, let \\nV\\n=\\n{,\\n}\\n01\\n be the set \\nof intensity values used to deﬁne adjacenc\\ny. \\nCompute the lengths of the shortest 4-, 8-, \\nand \\nm\\n-path between \\np \\nand \\nq\\n in the follow-\\ning image. If a particular path does not exist \\nbetween these two points, explain why.\\n3121\\n2202\\n1211\\n1012\\n(\\np\\n)\\n(\\nq\\n)\\n(b) \\nRepeat (a) but using \\nV\\n=\\n{,\\n} .\\n12\\n2.19 \\nConsider two points \\np\\n and \\nq\\n.\\n(a) * \\nState the condition(s) under which the \\nD\\n4\\n \\ndistance between \\np\\n and \\nq\\n is equal to the \\nshortest 4-path between these points.\\n(b) \\nIs this path unique?\\n2.20 \\nRepeat problem 2.19 for the \\nD\\n8\\n distance.\\n2.21 \\nConsider two \\none-dimensional\\n images \\nf\\n and \\ng\\n of \\nthe same size\\n. What has to be true about the ori-\\nentation of these images for the elementwise and \\nmatrix products discussed in Section 2.6 to make \\nsense? Either of the two images can be ﬁrst in \\nforming the product.\\n2.22 * \\nIn the next chapter, we will deal with operators \\nwhose function is to compute the sum of pixel val-\\nues in a small subimage area,\\n \\nS\\nxy\\n,\\n \\nas in Eq.\\n (2-43). \\nShow that these are linear operators.\\n2.23 \\nRefer to Eq. (2-24) in answering the following: \\n(a) * \\nShow that image summation is a linear opera-\\ntion.\\n(b) \\nShow that image subtraction is a linear oper-\\nation.\\n(c) * \\nShow that image multiplication in a nonlinear \\noperation.\\n(d) \\nShow that image division is a nonlinear opera-\\ntion.\\n2.24 \\nThe median, \\nz\\n,\\n of a set of numbers is such that \\nDIP4E_GLOBAL_Print_Ready.indb   115\\n6/16/2017   2:03:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 116}),\n",
       " Document(page_content='116\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nhalf the values in the set are below \\nz\\n and the oth-\\ner half are above it.\\n For example, the median of \\nthe set of values \\n{,,, , , , }\\n23\\n82 02 12 53 1\\n is 20. Show \\nthat an operator that computes the median of a \\nsubimage area,\\n \\nS\\n, is nonlinear. (\\nHint:\\n It is sufﬁ-\\ncient to show that \\nz\\n fails the linearity test for a \\nsimple numerical example\\n.)\\n2.25 * \\nShow that image averaging can be done recur-\\nsively\\n. That is, show that if \\nak\\n()\\nis the average of \\nk\\n images\\n, then the average of \\nk\\n+\\n1\\n images can \\nbe obtained from the already-computed average\\n, \\nak\\n()\\n,\\n and the new image, \\nf\\nk\\n+\\n1\\n. \\n2.26\\nWith reference to Example 2.5:\\n(a) * \\nProve the validity of Eq. (2-27).\\n(b) \\nProve the validity of Eq. (2-28).\\nF\\nor part (b) you will need the following facts from \\nprobability: (1) the variance of a constant times a \\nrandom variable is equal to the constant squared \\ntimes the variance of the random variable. (2) The \\nvariance of the sum of uncorrelated random vari-\\nables is equal to the sum of the variances of the \\nindividual random variables.\\n2.27 \\nConsider two 8-bit images whose intensity levels \\nspan the full range from 0 to 255.\\n(a) * \\nDiscuss the limiting effect of repeatedly sub-\\ntracting image (2) from image (1).\\n Assume \\nthat the results have to be represented also \\nin eight bits.\\n(b) \\nWould reversing the order of the images \\nyield a different result?\\n2.28 * \\nImage subtraction is used often in industrial appli-\\ncations for detecting missing components in prod-\\nuct assembly\\n. The approach is to store a “golden” \\nimage that corresponds to a correct assembly; this \\nimage is then subtracted from incoming images of \\nthe same product. \\nIdeally\\n, the differences would \\nbe zero if the new products are assembled cor-\\nrectly. Difference images for products with miss-\\ning components would be nonzero in the area \\nwhere they differ from the golden image. What \\nconditions do you think have to be met in prac-\\ntice for this method to work?\\n2.29 \\nWith reference to Eq. (2-32),\\n(a) * \\nGive a general formula for the value of \\nK\\n \\nas a function of the number of bits\\n, \\nk\\n, in an \\nimage, such that \\nK\\n results in a scaled image \\nwhose intensities span the full \\nk\\n-bit range.\\n(b) \\nFind \\nK \\nfor 16- and 32-bit images\\n. \\n2.30 \\nGive Venn diagrams for the following expres-\\nsions:\\n(a) * \\n()\\n( ) .\\nAC ABC\\n¨¨ ¨\\n−\\n(b) \\n() () .\\nAC BC\\n¨´¨\\n(c) \\nBA BA B C\\n−−\\n[]\\n() ( )\\n¨¨ ¨\\n(d) \\nBB A C A C\\n−=\\n∅\\n¨´\\n¨\\n() ;\\n.\\nGiven that \\n2.31 \\nUse Venn diagrams to prove the validity of the \\nfollowing expressions:\\n(a) * \\n()\\n()\\n()\\nAB AC ABC A BC\\n¨´ ¨ ¨ ¨ ¨´\\n−\\n[]\\n=\\n(b) \\n()\\nABC A B C\\nc ccc\\n´´ ¨ ¨\\n=\\n(c) \\n() ()\\nAC B B A C\\nc\\n´¨\\n=−−\\n(d) \\n()\\nABC A B C\\nc ccc\\n¨¨ ´ ´\\n=\\n2.32 \\nGive expressions (in terms of sets \\nA\\n, \\nB\\n,\\n and \\nC\\n)\\n \\nfor the sets shown shaded in the following ﬁgures. \\nThe shaded areas in each ﬁgure constitute one set, \\nso give only one expression for each of the four \\nﬁgures.\\n(a)* (b) (c) (d)\\nA\\nB\\nC\\n2.33 \\nWith reference to the discussion on sets in Section \\n2.6,\\n do the following:\\n(a) * \\nLet \\nS\\n be a set of real numbers ordered by the \\nrelation \\n“less than or equal to” \\n() .\\n≤\\n Show \\nthat \\nS\\n is a partially ordered set; that is, show \\nthat the reﬂexive, transitive, and antisymmet-\\nric properties hold.\\n(b) * \\nShow that changing the relation “less than or \\nequal to”\\n to “less than” \\n()\\n<\\n produces a strict \\nordered set.\\n(c) \\nNow let \\nS\\n be the set of lower\\n-case letters in \\nthe English alphabet. Show that, under \\n() ,\\n<\\nS\\n is a strict ordered set.\\n2.34 \\nFor any nonzero integers \\nm\\n and \\nn\\n,\\n we say that \\nm\\n \\nDIP4E_GLOBAL_Print_Ready.indb   116\\n6/16/2017   2:03:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 117}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n117\\nis divisible by \\nn\\n, written \\nmn\\n, if there exists an \\ninteger \\nk\\n such that \\nkn m\\n=\\n.\\n For example, 42 (\\nm\\n) \\nis divisible by 7 (\\nn\\n) because there exists an inte-\\nger \\nk\\n=\\n6\\n such that \\nkn m\\n=\\n.\\n Show that the set of \\npositive integers is a partially ordered set under \\nthe relation “divisible by.” In other words, do the \\nfollowing:\\n(a) * \\nShow that the property of reﬂectivity holds \\nunder this relation.\\n(b) \\nShow that the property of transitivity holds.\\n(c) \\nShow that anti symmetry holds.\\n2.35 \\nIn general, what would the resulting image, \\ngxy\\n(,\\n) ,\\n \\nlook like if we modiﬁed Eq.\\n (2-43), as follows:\\ngxy\\nmn\\nTfr c\\nrc S\\nxy\\n(,)\\n( ,)\\n(,)\\n=\\n[]\\n∑\\n1\\nH\\nwhere \\nT\\n is the intensity transformation function \\nin Fig. 2.38(b)?\\n2.36 \\nWith reference to Table 2.3, provide single, com-\\nposite transformation functions for performing \\nthe following operations:\\n(a) * \\nScaling and translation.\\n(b) * \\nScaling, translation, and rotation.\\n(c) \\nVertical shear, scaling, translation, and rota-\\ntion.\\n(d) \\nDoes the order of multiplication of the indi-\\nvidual matrices to produce a single transfor\\n-\\nmations make a difference? Give an example \\nbased on a scaling/translation transforma-\\ntion to support your answer.\\n2.37 \\nWe know from Eq. (2-45) that an afﬁne transfor-\\nmation of coordinates is given by \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\nx\\ny\\nx\\ny\\naa\\na\\naaa\\n11 0 0 1\\n11 12 13\\n21 22 23\\nA\\n⎦ ⎦\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\ny\\n1\\nwhere \\n(,)\\n′′\\nxy\\n are the transformed coordinates, \\n(,)\\nxy\\n are the original coordinates, and the ele-\\nments of \\nA\\n are given in \\nTable 2.3 for various \\ntypes of transformations. The inverse transforma-\\ntion, \\nA\\n−\\n1\\n,\\n to go from the transformed back to the \\noriginal coordinates is just as important for per\\n-\\nforming inverse mappings.\\n(a) * \\nFind the inverse scaling transformation.\\n(b) \\nFind the inverse translation transformation.\\n(c) \\nFind the inverse vertical and horizontal \\nshearing transformations\\n.\\n(d) * \\nFind the inverse rotation transformation.\\n(e) * \\nShow a composite inverse translation/rota-\\ntion transformation.\\n2.38 \\nWhat are the equations, analogous to Eqs. (2-46) \\nand (2-47),\\n that would result from using triangu-\\nlar instead of quadrilateral regions?\\n2.39 \\nDo the following.\\n(a) * \\nProve that the Fourier kernel in Eq. (2-59) is \\nseparable and symmetric\\n.\\n(b) \\nRepeat (a) for the kernel in Eq. (2-60).\\n2.40 * \\nShow that 2-D transforms with separable, sym-\\nmetric kernels can be computed by:\\n (1) comput-\\ning 1-D transforms along the individual rows (col-\\numns) of the input image; and (2) computing 1-D \\ntransforms along the columns (rows) of the result \\nfrom step (1).\\n2.41 \\nA plant produces miniature polymer squares that \\nhave to undergo 100% visual inspection.\\n Inspec-\\ntion is semi-automated. At each inspection sta-\\ntion, a robot places each polymer square over an \\noptical system that produces a magniﬁed image \\nof the square. The image completely ﬁlls a view-\\ning screen of size \\n80 80\\n×\\n mm.\\n Defects appear as \\ndark circular blobs\\n, and the human inspector’s job \\nis to look at the screen and reject any sample that \\nhas one or more dark blobs with a diameter of 0.8 \\nmm or greater, as measured on the scale of the \\nscreen. The manufacturing manager believes that \\nif she can ﬁnd a way to fully automate the process, \\nproﬁts will increase by 50%, and success in this \\nproject will aid her climb up the corporate ladder. \\nAfter extensive investigation, the manager decides \\nthat the way to solve the problem is to view each \\ninspection screen with a CCD TV camera and feed \\nthe output of the camera into an image processing \\nsystem capable of detecting the blobs, measuring \\ntheir diameter, and activating the accept/reject \\nbutton previously operated by a human inspec-\\ntor. She is able to ﬁnd a suitable system, provided \\nthat the smallest defect occupies an area of at \\nleast \\n22\\n×\\n pixels in the digital image. The manager \\nhires you to help her specify the camera and lens \\nDIP4E_GLOBAL_Print_Ready.indb   117\\n6/16/2017   2:03:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='118\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nsystem to satisfy this requirement, using off-the-\\nshelf components\\n. Available off-the-shelf lenses \\nhave focal lengths that are integer multiples of \\n25 mm or 35 mm, up to 200 mm. Available cam-\\neras yield image sizes of \\n512 512\\n×\\n, \\n1024 1024\\n×\\n, \\nor \\n2048 2048\\n×\\n pixels. The \\nindividual\\n imaging \\nelements in these cameras are squares measuring \\n88\\n×\\n m,\\nm\\n and the spaces between imaging ele-\\nments are \\n2 m.\\nm\\nFor this application, the cameras \\ncost much more than the lenses\\n, so you should use \\nthe lowest-resolution camera possible, consistent \\nwith a suitable lens. As a consultant, you have \\nto provide a written recommendation, showing \\nin reasonable detail the analysis that led to your \\nchoice of components. Use the imaging geometry \\nsuggested in Problem 2.6.\\nDIP4E_GLOBAL_Print_Ready.indb   118\\n6/16/2017   2:03:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 119}),\n",
       " Document(page_content='1193\\nIntensity Transformations and \\nSpatial Filtering\\nPreview\\nThe term \\nspatial domain\\n refers to the image plane itself, and image processing methods in this category \\nare based on direct manipulation of pixels in an image. This is in contrast to image processing in a trans-\\nform domain which, as we will discuss in Chapters 4 and 6, involves ﬁrst transforming an image into the \\ntransform domain, doing the processing there, and obtaining the inverse transform to bring the results \\nback into the spatial domain. Two principal categories of spatial processing are intensity transforma-\\ntions and  spatial ﬁltering. Intensity transformations operate on single pixels of an image for tasks such \\nas contrast manipulation and image thresholding. Spatial ﬁltering performs operations on the neighbor-\\nhood of every pixel in an image. Examples of spatial ﬁltering include image smoothing and sharpening. \\nIn the sections that follow, we discuss a number of “classical” techniques for intensity transformations \\nand spatial ﬁltering.  \\nUpon completion of this chapter, readers should:\\n Understand the meaning of spatial domain \\nprocessing, and how it differs from transform \\ndomain processing.\\n Be familiar with the principal techniques used \\nfor intensity transformations.\\n Understand the physical meaning of image \\nhistograms and how they can be manipulated \\nfor image enhancement.\\n Understand the mechanics of spatial ﬁltering, \\nand how spatial ﬁlters are formed.\\n Understand the principles of spatial convolu-\\ntion and correlation.\\n Be familiar with the principal types of spatial \\nﬁlters, and how they are applied.\\n Be aware of the relationships between spatial \\nﬁlters, and the fundamental role of lowpass \\nﬁlters. \\n Understand how to use combinations of \\nenhancement methods in cases where a single \\napproach is insufﬁcient.\\nIt makes all the difference whether one sees darkness through \\nthe light or brightness through the shadows.\\nDavid Lindsay\\nDIP4E_GLOBAL_Print_Ready.indb   119\\n6/16/2017   2:03:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 120}),\n",
       " Document(page_content='120\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n3.1 BACKGROUND \\nAll the image processing techniques discussed in this chapter are implemented in \\nthe spatial domain, which we know from the discussion in Section 2.4 is the plane \\ncontaining the pixels of an image. Spatial domain techniques operate directly on the \\npixels of an image, as opposed, for example, to the frequency domain (the topic of \\nChapter 4) in which operations are performed on the Fourier transform of an image, \\nrather than on the image itself. As you will learn in progressing through the book, \\nsome image processing tasks are easier or more meaningful to implement in the \\nspatial domain, while others are best suited for other approaches. \\nTHE BASICS OF INTENSITY TRANSFORMATIONS AND SPATIAL  \\nFILTERING\\nThe spatial domain processes we discuss in this chapter are based on the expression\\n \\ngxy T f xy\\n(,\\n) (,)\\n=\\n[]\\n \\n(3-1)\\nwhere \\nfx y\\n(,\\n)\\n is an input image, \\ngxy\\n(,\\n)\\n is the output image, and \\nT\\n is an operator on \\nf\\n \\ndefined over a \\nneighborhood of point \\n(,)\\nxy\\n. The operator can be applied to the pix-\\nels of a single image (our principal focus in this chapter) or to the pixels of a set of \\nimages\\n, such as performing the elementwise sum of a sequence of images for noise \\nreduction, as discussed in Section 2.6. Figure 3.1 shows the basic implementation of \\nEq. (3-1) on a single image. The point \\n(,)\\nxy\\n00\\n shown is an arbitrary location in the \\nimage, and the small region shown is a \\nneighborhood\\n of \\n(,) ,\\nxy\\n00\\n as explained in Sec-\\ntion 2.6. Typically, the neighborhood is rectangular, centered on \\n(,)\\nxy\\n00\\n, and much \\nsmaller in size than the image.\\nThe process that Fig. 3.1 illustrates consists of moving the center of the neighbor-\\nhood from pixel to pixel, and applying the operator \\nT\\n to the pixels in the neighbor-\\nhood to yield an output value at that location. Thus, for any speciﬁc location \\n(,) ,\\nxy\\n00\\n \\n3.1\\nFIGURE 3.1\\nA \\n33\\n×\\n  \\nneighborhood \\nabout a point \\n(,)\\nxy\\n00\\n in an image. \\nThe neighborhood \\nis moved from \\npixel to pixel in the \\nimage to generate \\nan output image.  \\nRecall from  \\nChapter 2 that the \\nvalue of a pixel at \\nlocation \\n(,)\\nxy\\n00\\n is\\nfx y\\n(,\\n) ,\\n00\\n the value \\nof the image at that \\nlocation.\\nOrigin\\n00\\n3 3 neighborhood \\nof point ( , )\\nxy\\n×\\nImage \\nf\\ny \\nx\\nx\\n0\\ny\\n0\\n00\\nPixel [its value is ( , )]\\nfx y\\nDIP4E_GLOBAL_Print_Ready.indb   120\\n6/16/2017   2:03:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 121}),\n",
       " Document(page_content='3.1\\n  \\nBackground\\n    \\n121\\nthe value of the output image \\ng\\n at those coordinates is equal to the result of apply-\\ning \\nT\\n to the neighborhood with origin at \\n(,)\\nxy\\n00\\n in \\nf\\n.  For example, suppose that \\nthe neighborhood is a square of size \\n33\\n×\\n and that operator \\nT\\n is deﬁned as \\n“com-\\npute the average intensity of the pixels in the neighborhood.” Consider an arbitrary \\nlocation in an image, say \\n(,) .\\n100\\n150\\n The result at that location in the output image, \\ng\\n(,\\n) ,\\n100 150\\n is the sum of \\nf\\n(,\\n)\\n100 150\\n and its 8-neighbors, divided by 9. The center of \\nthe neighborhood is then moved to the next adjacent location and the procedure \\nis repeated to generate the next value of the output image \\ng\\n.\\n Typically, the process \\nstarts at the top left of the input image and proceeds pixel by pixel in a horizontal \\n(vertical) scan, one row (column) at a time. We will discuss this type of neighbor-\\nhood processing beginning in Section 3.4.\\nThe smallest possible neighborhood is of size \\n11\\n×\\n.\\n In this case, \\ng\\n depends only \\non the value of \\nf\\n at a single point \\n(,)\\nxy\\n and \\nT\\n in Eq.\\n (3-1) becomes an \\nintensity\\n (also \\ncalled a \\ngray-level,\\n or \\nmapping\\n) \\ntransformation function\\n of the form\\n \\nsT r\\n=\\n()\\n \\n(3-2)\\nwhere, for simplicity in notation, we use \\ns\\n and \\nr\\n to denote\\n, respectively, the intensity \\nof \\ng\\n and \\nf\\n at any point \\n(,) .\\nxy\\n For example, if \\nTr\\n()\\n has the form in Fig. 3.2(a), the \\nresult of applying the transformation to every pixel in \\nf\\n to generate the correspond-\\ning pixels in \\ng\\n would be to produce an image of higher contrast than the original,\\n by \\ndarkening the intensity levels below \\nk\\n and brightening the levels above \\nk\\n. In this \\ntechnique, sometimes called \\ncontrast stretching\\n (see Section 3.2), values of \\nr\\n lower \\nthan \\nk\\n reduce (darken) the values of \\ns\\n, toward black. The opposite is true for values \\nof \\nr\\n higher than \\nk\\n. Observe how an intensity value \\nr\\n0\\n is mapped to obtain the cor-\\nresponding value \\ns\\n0\\n.\\n In the limiting case shown in Fig. 3.2(b), \\nTr\\n()\\n produces a two-\\nlevel (binary) image\\n. A mapping of this form is called a \\nthresholding\\n \\nfunction\\n. Some \\nfairly simple yet powerful processing approaches can be formulated with intensity \\ntransformation functions. In this chapter, we use intensity transformations princi-\\npally for image enhancement. In Chapter 10, we will use them for image segmenta-\\ntion. Approaches whose results depend only on the intensity at a point sometimes \\nare called \\npoint processing\\n techniques, as opposed to the \\nneighborhood processing\\n \\ntechniques discussed in the previous paragraph.\\nDepending on the size \\nof a neighborhood and \\nits location, part of the \\nneighborhood may lie \\noutside the image. There \\nare two solutions to this: \\n(1) to ignore the values \\noutside the image, or \\n(2) to pad image, as \\ndiscussed in Section 3.4.  \\nThe second approach is \\npreferred.\\nb a\\nFIGURE 3.2\\nIntensity  \\ntransformation \\nfunctions.  \\n(a) Contrast  \\nstretching  \\nfunction.  \\n(b) Thresholding \\nfunction.\\nk\\nk\\nr\\n0\\n s\\n0\\n \\n/H11005\\n \\nT\\n(\\nr\\n0\\n)\\nDark Light\\nDark Light\\nDark Light\\nDark Light\\nr \\nr\\ns\\n \\n/H11005\\n \\nT\\n(\\nr\\n)\\ns\\n \\n/H11005\\n \\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nDIP4E_GLOBAL_Print_Ready.indb   121\\n6/16/2017   2:03:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 122}),\n",
       " Document(page_content='122\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nABOUT THE EXAMPLES IN THIS CHAPTER\\nAlthough intensity transformation and spatial filtering methods span a broad range \\nof applications, most of the examples in this chapter are applications to image \\nenhancement. \\nEnhancement\\n is the process of manipulating an image so that the \\nresult is more suitable than the original for a specific application. The word \\nspecific\\n \\nis important, because it establishes at the outset that enhancement techniques are \\nproblem-oriented. Thus, for example, a method that is quite useful for enhancing \\nX-ray images may not be the best approach for enhancing infrared images. There is \\nno general “theory” of image enhancement. When an image is processed for visual \\ninterpretation, the viewer is the ultimate judge of how well a particular method \\nworks. When dealing with machine perception, enhancement is easier to quantify. \\nFor example, in an automated character-recognition system, the most appropriate \\nenhancement method is the one that results in the best recognition rate, leaving \\naside other considerations such as computational requirements of one method \\nversus another. Regardless of the application or method used, image enhancement \\nis one of the most visually appealing areas of image processing. Beginners in image \\nprocessing generally find enhancement applications interesting and relatively sim-\\nple to understand. Therefore, using examples from image enhancement to illustrate \\nthe spatial processing methods developed in this chapter not only saves having an \\nextra chapter in the book dealing with image enhancement but, more importantly, is \\nan effective approach for introducing newcomers to image processing techniques in \\nthe spatial domain. As you progress through the remainder of the book, you will find \\nthat the material developed in this chapter has a scope that is much broader than \\njust image enhancement.\\n3.2 SOME BASIC INTENSITY TRANSFORMATION FUNCTIONS \\nIntensity transformations are among the simplest of all image processing techniques. \\nAs noted in the previous section, we denote the values of pixels, before and after \\nprocessing, by \\nr\\n and \\ns\\n, respectively. These values are related by a transformation \\nT\\n, \\nas given in Eq. (3-2), that maps a pixel value \\nr\\n into a pixel value \\ns\\n. Because we deal \\nwith digital quantities, values of an intensity transformation function typically are \\nstored in a table, and the mappings from \\nr\\n to \\ns\\n are implemented via table lookups. \\nFor an 8-bit image, a lookup table containing the values of \\nT\\n will have 256 entries.\\nAs an introduction to intensity transformations, consider Fig. 3.3, which shows \\nthree basic types of functions used frequently in image processing: linear (negative \\nand identity transformations), logarithmic (log and inverse-log transformations), \\nand power-law (\\nn\\nth power and \\nn\\nth root transformations). The identity function is \\nthe trivial case in which the input and output intensities are identical.\\nIMAGE NEGATIVES \\nThe negative of an image with intensity levels in the range \\n[, ]\\n01\\nL\\n−\\n is obtained by \\nusing the negative transformation function shown in F\\nig. 3.3, which has the form:\\n \\nsL r\\n=−\\n−\\n1  \\n(3-3)\\n3.2\\nDIP4E_GLOBAL_Print_Ready.indb   122\\n6/16/2017   2:03:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 123}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n123\\nReversing the intensity levels of a digital image in this manner produces the \\nequivalent of a photographic negative. This type of processing is used, for example, \\nin enhancing white or gray detail embedded in dark regions of an image, especially \\nwhen the black areas are dominant in size. Figure 3.4 shows an example. The origi-\\nnal image is a digital mammogram showing a small lesion. Despite the fact that the \\nvisual content is the same in both images, some viewers find it easier to analyze the \\nfine details of the breast tissue using the negative image.\\nIdentity\\n0\\nL\\n/\\n4L\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n \\n1\\nInput intensity levels, \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL\\n \\n/H11002\\n \\n1\\nOutput intensity levels, \\ns\\nLog\\nNegative\\nn\\nth power\\nn\\nth root\\nInverse log\\n(exponential)\\nFIGURE 3.3\\nSome basic  \\nintensity  \\ntransformation  \\nfunctions. Each \\ncurve was scaled  \\nindependently\\n so \\nthat all curves \\nwould ﬁt in the \\nsame graph. Our  \\ninterest here is \\non the \\nshapes\\n of \\nthe curves, not \\non their relative \\nvalues. \\nb a\\nFIGURE 3.4\\n(a) A  \\ndigital  \\nmammogram.  \\n(b) Negative \\nimage obtained \\nusing Eq. (3-3). \\n(Image (a)  \\nCourtesy of \\nGeneral Electric \\nMedical Systems.)\\nDIP4E_GLOBAL_Print_Ready.indb   123\\n6/16/2017   2:03:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 124}),\n",
       " Document(page_content='124\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nLOG TRANSFORMATIONS\\nThe general form of the log transformation in Fig. 3.3 is\\n \\nsc r\\n=+\\nlog\\n( )\\n1  \\n(3-4)\\nwhere \\nc\\n is a constant and it is assumed that \\nr\\n≥\\n0.\\n The shape of the log curve in Fig. 3.3 \\nshows that this transformation maps a narrow range of low intensity values in the \\ninput into a wider range of output levels\\n. For example, note how input levels in the \\nrange \\n[, ]\\n04\\nL\\n map to output levels to the range \\n[, ] .\\n03 4\\nL\\n Conversely, higher values \\nof input levels are mapped to a narrower range in the output.\\n We use a transformation \\nof this type to expand the values of dark pixels in an image, while compressing the \\nhigher-level values. The opposite is true of the inverse log (exponential) transformation.\\nAny curve having the general shape of the log function shown in Fig. 3.3 would \\naccomplish this spreading/compressing of intensity levels in an image, but the pow-\\ner-law transformations discussed in the next section are much more versatile for \\nthis purpose. The log function has the important characteristic that it compresses \\nthe dynamic range of pixel values. An example in which pixel values have a large \\ndynamic range is the Fourier spectrum, which we will discuss in Chapter 4. It is not \\nunusual to encounter spectrum values that range from 0 to \\n10\\n6\\n or higher. Processing \\nnumbers such as these presents no problems for a computer, but image displays can-\\nnot reproduce faithfully such a wide range of values. The net effect is that intensity \\ndetail can be lost in the display of a typical Fourier spectrum.\\nFigure 3.5(a) shows a Fourier spectrum with values in the range 0 to \\n15 1 0\\n6\\n..\\n×\\n \\nW\\nhen these values are scaled linearly for display in an 8-bit system, the brightest \\npixels dominate the display, at the expense of lower (and just as important) values \\nof the spectrum. The effect of this dominance is illustrated vividly by the relatively \\nsmall area of the image in Fig. 3.5(a) that is not perceived as black. If, instead of \\ndisplaying the values in this manner, we ﬁrst apply Eq. (3-4) (with \\nc\\n=\\n1\\n in this case) \\nto the spectrum values\\n, then the range of values of the result becomes 0 to 6.2. Trans-\\nforming values in this way enables a greater range of intensities to be shown on the \\ndisplay. Figure 3.5(b) shows the result of scaling the intensity range linearly to the \\nb a\\nFIGURE 3.5\\n(a) Fourier  \\nspectrum  \\ndisplayed as a  \\ngrayscale image. \\n(b) Result of  \\napplying the log \\ntransformation \\nin Eq. (3-4) with \\nc\\n=\\n1.\\n Both images \\nare scaled to the \\nrange [0, 255].\\nDIP4E_GLOBAL_Print_Ready.indb   124\\n6/16/2017   2:03:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 125}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n125\\ninterval \\n[, ]\\n0\\n255\\n and showing the spectrum in the same 8-bit display. The level of \\ndetail visible in this image as compared to an unmodiﬁed display of the spectrum \\nis evident from these two images\\n. Most of the Fourier spectra in image processing \\npublications, including this book, have been scaled in this manner.\\nPOWER-LAW (GAMMA) TRANSFORMATIONS\\nPower-law transformations have the form\\n \\nsc r\\n=\\ng\\n \\n(3-5)\\nwhere \\nc\\n and \\ng\\n are positive constants. Sometimes Eq. (3-5) is written as \\nsc r\\n=+\\n()\\ne\\ng\\n \\nto account for offsets (that is, a measurable output when the input is zero). However, \\noffsets typically are an issue of display calibration, and as a result they are normally \\nignored in Eq. (3-5). Figure 3.6 shows plots of \\ns\\n as a function of \\nr\\n for various values \\nof \\ng\\n.\\n As with log transformations, power-law curves with fractional values of \\ng\\n map \\na narrow range of dark input values into a wider range of output values, with the \\nopposite being true for higher values of input levels. Note also in Fig. 3.6 that a fam-\\nily of transformations can be obtained simply by varying \\ng\\n.\\n Curves generated with \\nvalues of \\ng\\n>\\n1\\n have exactly the opposite effect as those generated with values of \\ng\\n<\\n1.\\n When \\nc\\n==\\ng\\n1 Eq. (3-5) reduces to the identity transformation.\\nT\\nhe response of many devices used for image capture, printing, and display obey \\na power law. By convention, the exponent in a power-law equation is referred to as \\ngamma\\n [hence our use of this symbol in Eq. (3-5)]. The process used to correct these \\npower-law response phenomena is called \\ngamma correction\\n or \\ngamma encoding\\n. \\nFor example, cathode ray tube (CRT) devices have an intensity-to-voltage response \\nthat is a power function, with exponents varying from approximately 1.8 to 2.5. As \\nthe curve for \\ng\\n=\\n25\\n.\\n in Fig. 3.6 shows, such display systems would tend to produce \\ng\\n \\n/H11005\\n 0.04\\ng\\n/H11005\\n 0.10\\ng\\n/H11005\\n 0.20\\ng\\n/H11005\\n 0.40\\ng\\n/H11005\\n 0.67\\ng\\n/H11005\\n 1\\ng\\n/H11005\\n 1.5\\ng\\n/H11005\\n 2.5\\ng\\n/H11005\\n 5.0\\ng\\n/H11005\\n 10.0\\ng\\n/H11005\\n 25.0\\n0\\nL\\n/\\n4L\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n \\n1\\nInput intensity levels,\\n \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL\\n \\n/H11002\\n \\n1\\nOutput intensity levels, \\ns\\nFIGURE 3.6\\nPlots of the  \\ngamma equation \\nsc r\\n=\\ng\\n for various \\nvalues of \\ng\\n (\\nc\\n = 1 \\nin all cases).\\n Each \\ncurve was scaled  \\nindependently\\n so \\nthat all curves \\nwould ﬁt in the \\nsame graph. Our  \\ninterest here is \\non the \\nshapes\\n of \\nthe curves, not \\non their relative \\nvalues.\\nDIP4E_GLOBAL_Print_Ready.indb   125\\n6/16/2017   2:03:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 126}),\n",
       " Document(page_content='126\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nimages that are darker than intended. Figure 3.7 illustrates this effect. Figure 3.7(a) \\nis an image of an intensity ramp displayed in a monitor with a gamma of 2.5. As \\nexpected, the output of the monitor appears darker than the input, as Fig. 3.7(b) \\nshows.\\nIn this case, gamma correction consists of using the transformation \\nsr r\\n==\\n125 04\\n..\\n \\nto preprocess the image before inputting it into the monitor. Figure 3.7(c) is the result. \\nWhen input into the same monitor, the gamma-corrected image produces an output \\nthat is close in appearance to the original image, as Fig. 3.7(d) shows. A similar analysis \\nas above would apply to other imaging devices, such as scanners and printers, the dif-\\nference being the device-dependent value of gamma (Poynton [1996]).\\nEXAMPLE 3.1 : Contrast enhancement using power-law intensity transformations.\\nIn addition to gamma correction, power-law transformations are useful for general-purpose contrast \\nmanipulation. Figure 3.8(a) shows a magnetic resonance image (MRI) of a human upper thoracic spine \\nwith a fracture dislocation. The fracture is visible in the region highlighted by the circle. Because the \\nimage is predominantly dark, an expansion of intensity levels is desirable. This can be accomplished \\nusing a power-law transformation with a fractional exponent. The other images shown in the ﬁgure were \\nobtained by processing Fig. 3.8(a) with the power-law transformation function of Eq. (3-5). The values \\nSometimes, a higher \\ngamma makes the  \\ndisplayed image look \\nbetter to viewers than \\nthe original because of \\nan increase in contrast. \\nHowever, the objective \\nof gamma correction is to \\nproduce a \\nfaithful\\n display \\nof an input image.\\nb a\\nd c\\nFIGURE 3.7\\n(a) Intensity ramp \\nimage. (b) Image \\nas viewed on a \\nsimulated monitor \\nwith a gamma of \\n2.5. (c) Gamma- \\ncorrected image. \\n(d) Corrected \\nimage as viewed \\non the same \\nmonitor. Compare \\n(d) and (a).\\nOriginal image as viewed on a monitor with\\na gamma of 2.5\\nOriginal image\\nGamma Correction\\nGamma-corrected image\\nGamma-corrected image as viewed on the\\nsame monitor\\nDIP4E_GLOBAL_Print_Ready.indb   126\\n6/16/2017   2:03:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 127}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n127\\nof gamma corresponding to images (b) through (d) are 0.6, 0.4, and 0.3, respectively (\\nc\\n=\\n1\\n in all cases). \\nObserve that as gamma decreased from 0.6 to 0.4,\\n more detail became visible. A further decrease of \\ngamma to 0.3 enhanced a little more detail in the background, but began to reduce contrast to the point \\nwhere the image started to have a very slight “washed-out” appearance, especially in the background. \\nThe best enhancement in terms of contrast and discernible detail was obtained with \\ng\\n=\\n04\\n..\\n A value of \\ng\\n=\\n03\\n.\\n is an approximate limit below which contrast in this particular image would be reduced to an \\nunacceptable level.\\nEXAMPLE 3.2 : Another illustration of power-law transformations.\\nFigure 3.9(a) shows the opposite problem of that presented in Fig. 3.8(a). The image to be processed \\nb a\\nd c\\nFIGURE 3.8\\n(a) Magnetic \\nresonance  \\nimage (MRI) of a \\nfractured human \\nspine (the region \\nof the fracture is \\nenclosed by the \\ncircle).  \\n(b)–(d) Results of  \\napplying the  \\ntransformation  \\nin Eq. (3-5) \\nwith \\nc\\n=\\n1 and \\ng\\n=\\n06\\n.,\\n 0.4, and \\n0.3,\\n respectively. \\n(Original image \\ncourtesy of Dr. \\nDavid R. Pickens, \\nDepartment of \\nRadiology and \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   127\\n6/16/2017   2:03:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 128}),\n",
       " Document(page_content='128\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nb a\\nd c\\nFIGURE 3.9\\n(a) Aerial image. \\n(b)–(d) Results \\nof applying the \\ntransformation \\nin Eq. (3-5) with \\ng\\n=\\n30\\n.,\\n 4.0, and \\n5.0, respectively. \\n(\\nc\\n=\\n1\\n in all cases.) \\n \\n(Original image \\ncourtesy of \\nN\\nASA.)\\nnow has a washed-out appearance, indicating that a compression of intensity levels is desirable. This can \\nbe accomplished with Eq. (3-5) using values of \\ng\\n greater than 1. The results of processing Fig. 3.9(a) with \\ng\\n=\\n30\\n.,\\n 4.0, and 5.0 are shown in Figs. 3.9(b) through (d), respectively. Suitable results were obtained \\nusing gamma values of 3.0 and 4.0.\\n The latter result has a slightly more appealing appearance because it \\nhas higher contrast. This is true also of the result obtained with \\ng\\n=\\n50\\n..\\n For example, the airport runways \\nnear the middle of the image appears clearer in F\\nig. 3.9(d) than in any of the other three images. \\nPIECEWISE LINEAR TRANSFORMATION FUNCTIONS\\nAn approach complementary to the methods discussed in the previous three sec-\\ntions is to use piecewise linear functions. The advantage of these functions over those \\ndiscussed thus far is that the form of piecewise functions can be arbitrarily complex. \\nIn fact, as you will see shortly, a practical implementation of some important trans-\\nformations can \\nbe formulated only as piecewise linear functions. The main disadvan-\\ntage of these functions is that their specification requires considerable user input.\\nDIP4E_GLOBAL_Print_Ready.indb   128\\n6/16/2017   2:03:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 129}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n129\\nContrast Stretching\\nLow-contrast images can result from poor illumination, lack of dynamic range in the \\nimaging sensor, or even the wrong setting of a lens aperture during image acquisi-\\ntion. \\nContrast stretching\\n expands the range of intensity levels in an image so that it \\nspans the ideal full intensity range of the recording medium or display device.\\nFigure 3.10(a) shows a typical transformation used for contrast stretching. The \\nlocations of points \\n(, )\\nrs\\n11\\n and \\n(, )\\nrs\\n22\\n control the shape of the transformation function. \\nIf \\nrs\\n11\\n=\\n and \\nrs\\n22\\n=\\n the transformation is a linear function that produces no changes \\nin intensity\\n. If \\nrr\\n12\\n=\\n, \\ns\\n1\\n0\\n=\\n,\\n and \\nsL\\n2\\n1\\n=−\\n the transformation becomes a \\nthreshold-\\ning function\\n that creates a binary image [see F\\nig. 3.2(b)]. Intermediate values of \\n(, )\\nrs\\n11\\n \\nand \\n(,)\\nsr\\n22\\n produce various degrees of spread in the intensity levels of the output \\nimage, thus affecting its contrast. In general, \\nrr\\n12\\n≤\\n and \\nss\\n12\\n≤\\n is assumed so that \\nthe function is single valued and monotonically increasing\\n. This preserves the order \\nof intensity levels, thus preventing the creation of intensity artifacts. Figure 3.10(b) \\nshows an 8-bit image with low contrast. Figure 3.10(c) shows the result of contrast \\nstretching, obtained by setting \\n(, ) ( ,)\\nmin\\nrs r\\n11\\n0\\n=\\n and \\n(, ) ( , ) ,\\nmax\\nrs r L\\n22\\n1\\n=−\\n where \\nr\\nmin\\n and \\nr\\nmax\\n denote the minimum and maximum intensity levels in the input image, \\n0\\nL\\n/\\n4\\nL\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n 1\\nInput intensities, \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL \\n/H11002\\n 1\\nOutput intensities, \\ns\\n(\\nr\\n2\\n, \\ns\\n2\\n)\\n(\\nr\\n1\\n, \\ns\\n1\\n)\\nT\\n(\\nr\\n)\\nb a\\nd c\\nFIGURE 3.10\\nContrast stretching.  \\n(a) Piecewise linear \\ntransformation \\nfunction. (b) A low-\\ncontrast electron \\nmicroscope image \\nof pollen, magniﬁed \\n700 times.  \\n(c) Result of  \\ncontrast stretching. \\n(d) Result of  \\nthresholding.  \\n(Original image \\ncourtesy of Dr.  \\nRoger Heady, \\nResearch School of \\nBiological Sciences, \\nAustralian National \\nUniversity,  \\nCanberra,  \\nAustralia.)\\nDIP4E_GLOBAL_Print_Ready.indb   129\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 130}),\n",
       " Document(page_content='130\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nrespectively. The transformation stretched the intensity levels linearly to the full \\nintensity range, \\n[, ] .\\n01\\nL\\n−\\n Finally, Fig. 3.10(d) shows the result of using the thresh-\\nolding function,\\n with \\n(, ) ( ,)\\nrs\\nm\\n11\\n0\\n=\\n and \\n(, ) (, ) ,\\nrs\\nm L\\n22\\n1\\n=−\\n where \\nm\\n is the mean \\nintensity level in the image\\n. \\nIntensity-Level Slicing\\nThere are applications in which it is of interest to highlight a specific range of inten-\\nsities in an image. Some of these applications include enhancing features in satellite \\nimagery, such as masses of water, and enhancing flaws in X-ray images. The method, \\ncalled \\nintensity-level slicing\\n, can be implemented in several ways, but most are varia-\\ntions of two basic themes. One approach is to display in one value (say, white) all the \\nvalues in the range of interest and in another (say, black) all other intensities. This \\ntransformation, shown in Fig. 3.11(a), produces a binary image. The second approach, \\nbased on the transformation in Fig. 3.11(b), brightens (or darkens) the desired range \\nof intensities, but leaves all other intensity levels in the image unchanged.\\nEXAMPLE 3.3 : Intensity-level slicing.\\nFigure 3.12(a) is an aortic angiogram near the kidney area (see Section 1.3 for details on this image). The \\nobjective of this example is to use intensity-level slicing to enhance the major blood vessels that appear \\nlighter than the background, as a result of an injected contrast medium. Figure 3.12(b) shows the result \\nof using a transformation of the form in Fig. 3.11(a). The selected band was near the top of the intensity \\nscale because the range of interest is brighter than the background. The net result of this transformation \\nis that the blood vessel and parts of the kidneys appear white, while all other intensities are black. This \\ntype of enhancement produces a binary image, and is useful for studying the shape characteristics of the \\nﬂow of the contrast medium (to detect blockages, for example).\\nIf interest lies in the actual intensity values of the region of interest, we can use the transformation of \\nthe form shown in Fig. 3.11(b). Figure 3.12(c) shows the result of using such a transformation in which \\na band of intensities in the mid-gray region around the mean intensity was set to black, while all other \\nintensities were left unchanged. Here, we see that the gray-level tonality of the major blood vessels and \\npart of the kidney area were left intact. Such a result might be useful when interest lies in measuring the \\nactual ﬂow of the contrast medium as a function of time in a sequence of images.\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\n0\\nAB\\n L \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\ns \\ns \\nr \\nr \\nL \\n/H11002\\n 1\\n0\\nAB\\nL \\n/H11002\\n 1 \\nb a\\nFIGURE 3.11\\n(a) This transfor-\\nmation function \\nhighlights range \\n[,]\\nAB\\n and reduces \\nall other intensities \\nto a lower level.\\n \\n(b) This function \\nhighlights range \\n[,]\\nAB\\n and leaves \\nother intensities \\nunchanged.\\nDIP4E_GLOBAL_Print_Ready.indb   130\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 131}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n131\\nBit-Plane Slicing\\nPixel values are integers composed of bits. For example, values in a 256-level gray-\\nscale image are composed of 8 bits (one byte). Instead of highlighting intensity-level \\nranges, as 3.3, we could highlight the contribution made to total image appearance \\nby specific bits. As Fig. 3.13 illustrates, an 8-bit image may be considered as being \\ncomposed of eight one-bit planes, with plane 1 containing the lowest-order bit of all \\npixels in the image, and plane 8 all the highest-order bits.\\nFigure 3.14(a) shows an 8-bit grayscale image and Figs. 3.14(b) through (i) are \\nits eight one-bit planes, with Fig. 3.14(b) corresponding to the highest-order bit. \\nObserve that the four higher-order bit planes, especially the ﬁrst two, contain a sig-\\nniﬁcant amount of the visually-signiﬁcant data. The lower-order planes contribute \\nto more  subtle  intensity details  in the image. The original  image has a gray border \\nwhose intensity is 194. Notice that the corresponding borders of some of the bit \\nb a\\nc\\nFIGURE 3.12\\n  (a) Aortic angiogram. (b) Result of using a slicing transformation of the type illustrated in Fig. 3.11(a), \\nwith the range of intensities of interest selected in the upper end of the gray scale. (c) Result of using the transfor-\\nmation in Fig. 3.11(b), with the selected range set near black, so that the grays in the area of the blood vessels and \\nkidneys were preserved. (Original image courtesy of Dr. Thomas R. Gest, University of Michigan Medical School.) \\nOne 8-bit byte\\nBit plane 8\\n(most significant)\\nBit plane 1\\n(least significant)\\nFIGURE 3.13\\nBit-planes of an \\n8-bit image.\\nDIP4E_GLOBAL_Print_Ready.indb   131\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 132}),\n",
       " Document(page_content='132\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nplanes are black (0), while others are white (1). To see why, consider a pixel in, say, \\nthe middle  of the lower border of Fig. 3.14(a). The corresponding pixels in the bit \\nplanes, starting with the highest-order plane, have values 1 1 0 0 0 0 1 0, which is the \\nbinary representation of decimal 194. The value of any pixel in the original image  \\ncan be similarly reconstructed from its corresponding binary-valued pixels in the bit \\nplanes by converting an 8-bit binary sequence to decimal.\\nThe binary image for the 8th bit plane of an 8-bit image can be obtained by thresh-\\nolding the input image with a transformation function that maps to 0 intensity values \\nbetween 0 and 127, and maps to 1 values between 128 and 255. The binary image in \\nFig. 3.14(b) was obtained in this manner. It is left as an exercise (see Problem 3.3) to \\nobtain the transformation functions for generating the other bit planes.\\nDecomposing an image into its bit planes is useful for analyzing the relative \\nimportance of each bit in the image, a process that aids in determining the adequacy \\nof the number of bits used to quantize the image. Also, this type of decomposition \\nis useful for image compression (the topic of Chapter 8), in which fewer than all \\nplanes are used in reconstructing an image. For example, Fig. 3.15(a) shows an image \\nreconstructed using bit planes 8 and 7 of the preceding decomposition. The recon-\\nstruction is done by multiplying the pixels of the \\nn\\nth plane by the constant \\n2\\n1\\nn\\n−\\n.\\n This \\nconverts the \\nn\\nth signiﬁcant binary bit to decimal.\\n Each bit plane is multiplied by the \\ncorresponding constant, and all resulting planes are added to obtain the grayscale \\nimage. Thus, to obtain Fig. 3.15(a), we multiplied bit plane 8 by 128, bit plane 7 by 64, \\nand added the two planes. Although the main features of the original image were \\nrestored, the reconstructed image appears ﬂat, especially in the background. This \\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 3.14\\n (a) An 8-bit gray-scale image of size \\n550 1192\\n×\\n pixels. (b) through (i) Bit planes 8 through 1, with bit \\nplane 1 corresponding to the least signiﬁcant bit.\\n Each bit plane is a binary image..\\nDIP4E_GLOBAL_Print_Ready.indb   132\\n6/16/2017   2:03:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 133}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n133\\nis not surprising, because two planes can produce only four distinct intensity  lev-\\nels. Adding plane 6 to the reconstruction helped the situation, as Fig. 3.15(b) shows. \\nNote that the background of this image has perceptible false contouring. This effect \\nis reduced signiﬁcantly by adding the 5th plane to the reconstruction, as Fig. 3.15(c) \\nillustrates. Using more planes in the reconstruction would not contribute signiﬁcant-\\nly to the appearance of this image. Thus, we conclude that, in this example, storing \\nthe four highest-order bit planes would allow us to reconstruct the original image \\nin acceptable detail. Storing these four planes instead of the original image requires \\n50% less storage.\\n3.3 HISTOGRAM PROCESSING \\nLet \\nr\\nk\\n,\\n for\\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n denote the intensities of an \\nL\\n-level digital image\\n, \\nfx y\\n(,\\n)\\n. The \\nunnormaliz\\ned histogram\\n of \\nf\\n is defined as\\n \\nhr n k L\\nkk\\n()\\n, , , ,\\n== −\\nf o r 012 1\\n…\\n \\n(3-6)\\nwhere \\nn\\nk\\n is the number of pixels in \\nf\\n with intensity \\nr\\nk\\n,\\n and the subdivisions of the \\nintensity scale are called \\nhistogram bins\\n.\\n Similarly, the \\nnormalized histogram\\n of \\nf\\n is \\ndefined as\\n \\npr\\nhr\\nMN\\nn\\nMN\\nk\\nkk\\n()\\n()\\n==\\n \\n(3-7)\\nwhere, as usual, \\nM\\n and \\nN\\n are the number of image rows and columns\\n, respectively. \\nMostly, we work with normalized histograms, which we refer to simply as \\nhistograms \\nor \\nimage histograms\\n. The sum of \\npr\\nk\\n()\\n for all values of \\nk\\n is always 1.\\n The components \\nof \\npr\\nk\\n()\\n are estimates of the probabilities of intensity levels occurring in an image. \\nAs you will learn in this section,\\n histogram manipulation is a fundamental tool in \\nimage processing. Histograms are simple to compute and are also suitable for fast \\nhardware implementations, thus making histogram-based techniques a popular tool \\nfor real-time image processing. \\nHistogram shape is related to image appearance. For example, Fig. 3.16 shows \\nimages with four basic intensity characteristics: dark, light, low contrast, and high \\ncontrast; the image histograms are also shown. We note in the dark image that the \\nmost populated histogram bins are concentrated on the lower (dark) end of the \\nintensity scale. Similarly, the most populated bins of the light image are biased \\ntoward the higher end of the scale. An image with low contrast has a narrow histo-\\n3.3\\nb a\\nc\\nFIGURE 3.15\\n Image  reconstructed from bit planes: (a) 8 and 7; (b) 8, 7, and 6; (c) 8, 7, 6, and 5.\\nDIP4E_GLOBAL_Print_Ready.indb   133\\n6/16/2017   2:03:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 134}),\n",
       " Document(page_content='134\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\ngram located typically toward the middle of the intensity scale, as Fig. 3.16(c) shows. \\nFor a monochrome image, this implies a dull, washed-out gray look. Finally, we see \\nthat the components of the histogram of the high-contrast image cover a wide range \\nof the intensity scale, and the distribution of pixels is not too far from uniform, with \\nfew bins being much higher than the others. Intuitively, it is reasonable to conclude \\nthat an image whose pixels tend to occupy the entire range of possible intensity lev-\\nels and, in addition, tend to be distributed uniformly, will have an appearance of high \\ncontrast and will exhibit a large variety of gray tones. The net effect will be an image \\nthat shows a great deal of gray-level detail and has a high dynamic range. As you will \\nsee shortly, it is possible to develop a transformation function that can achieve this \\neffect automatically, using only the histogram of an input image.\\nHISTOGRAM EQUALIZATION\\nAssuming initially continuous intensity values, let the variable \\nr\\n denote the intensi-\\nties of an image to be processed. As usual, we assume that \\nr\\n is in the range \\n[, ] ,\\n01\\nL\\n−\\n \\nwith \\nr\\n=\\n0\\n representing black and \\nrL\\n=−\\n1\\n representing white. For \\nr\\n satisfying these \\nconditions\\n, we focus attention on transformations (intensity mappings) of the form\\n \\nsT r rL\\n=−\\n()\\n0 1\\n≤≤\\n \\n(3-8)\\nHistogram of \\nhigh-contrast image\\nHistogram of \\nlow-contrast image\\nHistogram of \\ndark image\\nHistogram of \\nlight image\\nb\\na\\nc\\nd\\nFIGURE 3.16\\n Four image types and their corresponding histograms. (a) dark; (b) light; (c) low contrast; (d) high con-\\ntrast. The horizontal axis of the histograms are values of \\nr\\nk\\n and the vertical axis are values of \\np\\nr\\nk\\n() .\\nDIP4E_GLOBAL_Print_Ready.indb   134\\n6/16/2017   2:03:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 135}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n135\\nthat produce an output intensity value, \\ns\\n, for a given intensity value \\nr\\n in the input \\nimage. We assume that \\n(a) \\nTr\\n()\\n is a monotonic\\n†\\n increasing function in the interval \\n01\\n≤≤\\nrL\\n−\\n; and\\n(b) \\n01\\n≤≤\\nTr\\nL\\n()\\n−\\n for \\n01\\n≤≤\\nrL\\n−\\n.\\nIn some formulations to be discussed shortly, we use the inverse transformation\\n \\nrT s sL\\n=−\\n−\\n1\\n01\\n()\\n≤≤\\n \\n(3-9)\\nin which case we change condition (a) to:\\n(a\\n/H11032\\n)\\n \\nTr\\n()\\n is a \\nstrictly\\n monotonic increasing function in the interval \\n01\\n≤≤\\nrL\\n−\\n.\\nThe condition in (a) that \\nTr\\n()\\n be monotonically increasing guarantees that output \\nintensity values will never be less than corresponding input values\\n, thus preventing \\nartifacts created by reversals of intensity. Condition (b) guarantees that the range of \\noutput intensities is the same as the input. Finally, condition \\n(a )\\n/H11032\\n guarantees that the \\nmappings from \\ns\\n back to \\nr\\n will be one-to-one\\n, thus preventing ambiguities. \\nFigure 3.17(a) shows a function that satisﬁes conditions (a) and (b). Here, we see \\nthat it is possible for multiple input values to map to a single output value and still \\nsatisfy these two conditions. That is, a monotonic transformation function performs \\na one-to-one or many-to-one mapping. This is perfectly ﬁne when mapping from \\nr\\n \\nto \\ns\\n. However, Fig. 3.17(a) presents a problem if we wanted to recover the values of \\nr\\n uniquely from the mapped values (inverse mapping can be visualized by revers-\\ning the direction of the arrows). This would be possible for the inverse mapping \\nof \\ns\\nk\\n in Fig. 3.17(a), but the inverse mapping of \\ns\\nq\\n is a range of values, which, of \\ncourse, prevents us in general from recovering the original value of \\nr\\n that resulted \\n†\\n A function \\nTr\\n()\\n is a \\nmonotonic increasing\\n function if \\nTr Tr\\n()\\n()\\n21\\n≥\\n for \\nrr\\n21\\n>\\n. \\nTr\\n()\\n is a \\nstrictly monotonic increas-\\ning\\n function if \\nTr Tr\\n()\\n()\\n21\\n>\\n for \\nrr\\n21\\n>\\n. Similar deﬁnitions apply to a monotonic decreasing function.\\nSingle\\nvalue,\\n s\\nk\\nr\\nk\\ns\\nk\\nSingle\\nvalue,\\n s\\nq\\nSingle\\nvalue\\nMultiple\\nvalues\\nr \\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\nr \\nT\\n(\\nr\\n)\\n. . .\\nb a\\nFIGURE 3.17\\n(a) Monotonic  \\nincreasing function, \\nshowing how  \\nmultiple values can \\nmap to a single  \\nvalue. (b) Strictly  \\nmonotonic increas-\\ning function. This is \\na one-to-one map-\\nping, both ways.\\nDIP4E_GLOBAL_Print_Ready.indb   135\\n6/16/2017   2:03:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 136}),\n",
       " Document(page_content='136\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nin \\ns\\nq\\n.\\n As Fig. 3.17(b) shows, requiring that \\nTr\\n()\\n be strictly monotonic guarantees \\nthat the inverse mappings will be \\nsingle valued\\n (i.e\\n., the mapping is one-to-one in \\nboth directions).This is \\na theoretical requirement that will allow us to derive some \\nimportant histogram processing techniques later in this chapter. Because images are \\nstored using integer intensity values, we are forced to round all results to their near-\\nest integer values. This often results in strict monotonicity not being satisﬁed, which \\nimplies inverse transformations that may not be unique\\n. Fortunately, this problem is \\nnot difﬁcult to handle in the discrete case, as Example 3.7 in this section illustrates.\\nThe intensity of an image may be viewed as a random variable in the interval \\n[, ] .\\n01\\nL\\n−\\n Let \\npr\\nr\\n()\\n and \\nps\\ns\\n()\\n denote the PDFs of intensity values \\nr\\n and \\ns\\n in two dif-\\nferent images\\n. The subscripts on \\np\\n indicate that \\np\\nr\\n and \\np\\ns\\n are different functions. A \\nfundamental result from probability theory is that if \\npr\\nr\\n()\\n and \\nTr\\n()\\n are known, and \\nTr\\n()\\n is continuous and differentiable over the range of values of interest, then the \\nPDF of the transformed (mapped) variable \\ns\\n can be obtained as\\n \\nps pr\\ndr\\nds\\nsr\\n() ()\\n=\\n \\n(3-10)\\nThus, we see that the PDF of the output intensity variable, \\ns\\n,\\n is determined by the \\nPDF of the input intensities and the transformation function used [recall that \\nr\\n and \\ns\\n are related by \\nTr\\n()\\n]. \\nA transformation function of particular importance in image processing is\\n \\nsT r L p d\\nr\\nr\\n== −\\n() ( ) ( )\\n1\\n0\\n2\\nww\\n \\n(3-11)\\nwhere \\nw\\n is a dummy variable of integration. The integral on the right side is the \\ncumulative distribution function\\n (CDF) of random variable \\nr\\n.\\n Because PDFs always \\nare positive, and the integral of a function is the area under the function, it follows \\nthat the transformation function of Eq. (3-11) satisfies condition (a). This is because \\nthe area under the function cannot decrease as \\nr\\n increases. When the upper limit in \\nthis equation is \\nrL\\n=−\\n()\\n1\\n the integral evaluates to 1, as it must for a PDF.  Thus, the \\nmaximum value of \\ns\\n is \\nL\\n−\\n1,\\n and condition (b) is satisfied also.\\nW\\ne use Eq. (3-10) to ﬁnd the \\nps\\ns\\n()\\n corresponding to the transformation just dis-\\ncussed.\\n We know from Leibniz’s rule in calculus that the derivative of a deﬁnite \\nintegral with respect to its upper limit is the integrand evaluated at the limit. That is,\\n \\nds\\ndr\\ndT r\\ndr\\nL\\nd\\ndr\\npd\\nLp r\\nr\\nr\\nr\\n=\\n=−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=−\\n()\\n() ( )\\n() ( )\\n1\\n1\\n0\\n2\\nww\\n  \\n(3-12)\\nDIP4E_GLOBAL_Print_Ready.indb   136\\n6/16/2017   2:03:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 137}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n137\\nSubstituting this result for \\ndr ds\\n in Eq. (3-10), and noting that all probability values \\nare positive\\n, gives the result\\n \\nps pr\\ndr\\nds\\npr\\nLp r\\nL\\nsL\\nsr\\nr\\nr\\n() ()\\n()\\n() ( )\\n=\\n=\\n−\\n=\\n−\\n−\\n1\\n1\\n1\\n1\\n01\\n≤≤\\n  \\n(3-13)\\nWe recognize the form of \\nps\\ns\\n()\\n in the last line of this equation as a \\nuniform \\nprob-\\nability density function.\\n Thus, performing the intensity transformation in Eq. (3-11) \\nyields a random variable, \\ns\\n, characterized by a uniform PDF. What is important is \\nthat \\nps\\ns\\n()\\n in Eq. (3-13) will \\nalwa\\nys\\n be uniform, \\nindependently\\n of the form of \\npr\\nr\\n() .\\n \\nF\\nigure 3.18 and the following example illustrate these concepts.\\nEXAMPLE 3.4 : Illustration of Eqs. (3-11) and (3-13).\\nSuppose that the (continuous) intensity values in an image have the PDF\\n \\npr\\nr\\nL\\nrL\\nr\\n()\\n()\\n=\\n−\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n2\\n1\\n01\\n0\\n2\\nfor \\notherwise\\n≤≤\\nFrom Eq. (3-11) \\n \\nsT r L p d\\nL\\nd\\nr\\nL\\nrr\\nr\\n== − =\\n−\\n=\\n−\\n() ( ) ( )\\n1\\n2\\n11\\n00\\n2\\n22\\nww w w\\nEq. (3-11)\\nr \\np\\nr\\n(\\nr\\n)\\n0\\nA\\nL \\n/H11002\\n 1\\ns \\np\\ns\\n(\\ns\\n)\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\n1\\nb a\\nFIGURE 3.18\\n  (a) An arbitrary PDF. (b) Result of applying Eq. (3-11) to the input PDF. The \\nresulting PDF is always uniform, independently of the shape of the input.\\nDIP4E_GLOBAL_Print_Ready.indb   137\\n6/16/2017   2:03:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 138}),\n",
       " Document(page_content='138\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nSuppose that we form a new image with intensities, \\ns\\n, obtained using this transformation; that is, the \\ns\\n \\nvalues are formed by squaring the corresponding intensity values of the input image, then dividing them \\nby \\nL\\n−\\n1.\\n We can verify that the PDF of the intensities in the new image, \\nps\\ns\\n() ,\\n is uniform by substituting \\npr\\nr\\n()\\n into Eq. (3-13), and using the fact that \\nsr L\\n=−\\n2\\n1\\n() ;\\n that is,\\n \\nps pr\\ndr\\nds\\nr\\nL\\nds\\ndr\\nr\\nL\\nd\\ndr\\nr\\nL\\nsr\\n() ()\\n()\\n()\\n==\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n−\\n2\\n1\\n2\\n1\\n1\\n2\\n1\\n2\\n2\\n⎥ ⎥\\n=\\n−\\n−\\n=\\n−\\n−\\n1\\n2\\n2\\n1\\n1\\n2\\n1\\n1\\nr\\nL\\nL\\nrL\\n()\\n()\\nThe last step follows because \\nr\\n is nonnegative and \\nL\\n>\\n1.\\n As expected, the result is a uniform PDF.\\nF\\nor discrete values, we work with probabilities and summations instead of prob-\\nability density functions and integrals (but the requirement of monotonicity stated \\nearlier still applies). Recall that the probability of occurrence of intensity level \\nr\\nk\\n in \\na digital image is approximated by\\n \\npr\\nn\\nMN\\nrk\\nk\\n()\\n=\\n \\n(3-14)\\nwhere \\nMN\\n is the total number of pixels in the image\\n, and \\nn\\nk\\n denotes the number of \\npixels that have intensity \\nr\\nk\\n.\\n As noted in the beginning of this section, \\npr\\nrk\\n() ,\\n with \\nrL\\nk\\n∈−\\n[, ] ,\\n01\\n is commonly referred to as a normalized image histogram.\\nT\\nhe discrete form of the transformation in Eq. (3-11) is\\n \\nsT r L p r k L\\nkk\\nr j\\nj\\nk\\n== − = −\\n=\\n∑\\n() ( ) () , , , ,\\n1 012 1\\n0\\n…\\n \\n(3-15)\\nwhere, as before, \\nL\\n is the number of possible intensity levels in the image (e\\n.g., 256 \\nfor an 8-bit image). Thu\\ns, a processed (output) image is obtained by using Eq. (3-15) \\nto map each pixel in the input image with intensity \\nr\\nk\\n into a corresponding pixel with \\nlevel \\ns\\nk\\n in the output image, This is called a \\nhistogram equalization\\n or \\nhistogram \\nlinearization\\n transformation. It is not difficult to show (see Problem 3.9) that this \\ntransformation satisfies conditions (a) and (b) stated previously in this section.\\nEXAMPLE 3.5 : Illustration of the mechanics of histogram equalization.\\nIt will be helpful to work through a simple example. Suppose that a 3-bit image \\n()\\nL\\n=\\n8\\n of size \\n64 64\\n×\\n \\npixels \\n()\\nMN\\n=\\n4096\\n has the intensity distribution in Table 3.1, where the intensity levels are integers in \\nthe range \\n[, ] [, ] .\\n01\\n0 7\\nL\\n−=\\n The histogram of this image is sketched in Fig. 3.19(a).Values of the histo-\\ngram equalization transformation function are obtained using Eq.\\n (3-15). For instance,\\n \\nsT r p r p r\\nrj r\\nj\\n00 0\\n0\\n0\\n77 1 3 3\\n== = =\\n=\\n∑\\n() () () .\\nDIP4E_GLOBAL_Print_Ready.indb   138\\n6/16/2017   2:03:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 139}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n139\\nSimilarly, \\nsT r\\n11\\n30 8\\n==\\n() . ,\\n \\ns\\n2\\n45 5\\n=\\n.,\\n \\ns\\n3\\n56 7\\n=\\n.,\\n \\ns\\n4\\n62 3\\n=\\n.,\\n \\ns\\n5\\n66 5\\n=\\n.,\\n \\ns\\n6\\n68 6\\n=\\n.,\\n and \\ns\\n7\\n70 0\\n=\\n..\\n This trans-\\nformation function has the staircase shape shown in F\\nig. 3.19(b).\\nAt this point, the \\ns\\n values are fractional because they were generated by summing probability values, \\nso we round them to their nearest integer values in the range \\n[, ] :\\n07\\n \\nssss\\nss\\ns\\n02 4 6\\n13 5\\n13 3 1 45 5 5 62 3 6 68 6 7\\n30 8 3 56 7 6 6\\n=→ =→ =→ =→\\n=→ =→ =\\n.. . .\\n... ..\\n65 7 7 00 7\\n7\\n→= →\\ns\\nThese are the values of the equalized histogram. Observe that the transformation yielded only ﬁve \\ndistinct intensity levels\\n. Because \\nr\\n0\\n0\\n=\\n was mapped to \\ns\\n0\\n1\\n=\\n,\\n there are 790 pixels in the histogram \\nequalized image with this value (see \\nTable 3.1). Also, there are 1023 pixels with a value of \\ns\\n1\\n3\\n=\\n and 850 \\npixels with a value of \\ns\\n2\\n5\\n=\\n.\\n However, both \\nr\\n3\\n and \\nr\\n4\\n were mapped to the same value, 6, so there are \\n()\\n656\\n329 985\\n+=\\n pixels in the equalized image with this value. Similarly, there are \\n()\\n245\\n122 81 448\\n++=\\n \\npixels with a value of 7 in the histogram equalized image\\n. Dividing these numbers by \\nMN\\n=\\n4096\\n yield-\\ned the equalized histogram in F\\nig. 3.19(c).\\nBecause a histogram is an approximation to a PDF, and no new allowed intensity levels are created \\nin the process, perfectly ﬂat histograms are rare in practical applications of histogram equalization using \\nthe method just discussed. Thus, unlike its continuous counterpart, it cannot be proved in general that \\ndiscrete histogram equalization using Eq. (3-15) results in a uniform histogram (we will introduce later in \\nr\\nk\\nn\\nk\\npr n M N\\nrk k\\n()\\n=\\nr\\n0\\n0\\n=\\n790\\n0.19\\nr\\n1\\n1\\n=\\n1023\\n0.25\\nr\\n2\\n2\\n=\\n850\\n0.21\\nr\\n3\\n3\\n=\\n656\\n0.16\\nr\\n4\\n4\\n=\\n329\\n0.08\\nr\\n5\\n5\\n=\\n245\\n0.06\\nr\\n6\\n6\\n=\\n122\\n0.03\\nr\\n7\\n7\\n=\\n81\\n0.02\\nTABLE \\n3.1\\nIntensity  \\ndistribution and \\nhistogram values \\nfor a 3-bit, \\n64 64\\n×\\n \\ndigital image\\n.\\nr\\nk\\n \\np\\nr\\n(\\nr\\nk\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n1.4\\n2.8\\n4.2\\n7.0\\n5.6\\n.05\\n.10\\n.15\\n.25\\n.20\\n01234567\\ns\\nk\\n \\np\\ns\\n(\\ns\\nk\\n)\\n01234567\\nr\\nk\\n \\ns\\nk\\n01234567\\nT\\n(\\nr\\n)\\nb a\\nc\\nFIGURE 3.19\\nHistogram  \\nequalization.  \\n(a) Original  \\nhistogram.  \\n(b) Transformation \\nfunction.  \\n(c) Equalized  \\nhistogram.\\nDIP4E_GLOBAL_Print_Ready.indb   139\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 140}),\n",
       " Document(page_content='140\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nthis section an approach for removing this limitation). However, as you will see shortly, using Eq. (3-15) \\nhas the general tendency to spread the histogram of the input image so that the intensity levels of the \\nequalized image span a wider range of the intensity scale. The net result is contrast enhancement.\\nWe discussed earlier the advantages of having intensity values that span the entire \\ngray scale. The method just derived produces intensities that have this tendency, and \\nalso has the advantage that it is fully automatic. In other words, the process of his-\\ntogram equalization consists entirely of implementing Eq. (3-15), which is based on \\ninformation that can be extracted directly from a given image, without the need for \\nany parameter speciﬁcations. This automatic, “hands-off” characteristic is important.\\nThe inverse transformation from \\ns\\n back to \\nr\\n is denoted by\\n \\nrT s\\nkk\\n=\\n−\\n1\\n()\\n  \\n(3-16)\\nIt can be shown (see Problem 3.9) that this inverse transformation satisfies conditions \\n(a\\n/H11032\\n) and (b) defined earlier \\nonly\\n if \\nall\\n intensity levels are present in the input image\\n. \\nThis implies that none of the bins of the image histogram are empty. Although the \\ninverse transformation is not used in histogram equalization, it plays a central role \\nin the histogram-matching scheme developed after the following example.\\nEXAMPLE 3.6 : Histogram equalization.\\nThe left column in Fig. 3.20 shows the four images from Fig. 3.16, and the center column shows the result \\nof performing histogram equalization on each of these images. The ﬁrst three results from top to bottom \\nshow signiﬁcant improvement. As expected, histogram equalization did not have much effect on the \\nfourth image because its intensities span almost the full scale already. Figure 3.21 shows the transforma-\\ntion functions used to generate the equalized images in Fig. 3.20. These functions were generated using \\nEq. (3-15). Observe that transformation (4) is nearly linear, indicating that the inputs were mapped to \\nnearly equal outputs. Shown is the mapping of an input value \\nr\\nk\\n to a corresponding output value \\ns\\nk\\n.\\n In \\nthis case\\n, the mapping was for image 1 (on the top left of Fig. 3.21), and indicates that a dark value was \\nmapped to a much lighter one, thus contributing to the brightness of the output image.\\nThe third column in Fig. 3.20 shows the histograms of the equalized images. While all the histograms \\nare different, the histogram-equalized images themselves are visually very similar. This is not totally \\nunexpected because the basic difference between the images on the left column is one of contrast, not \\ncontent. Because the images have the same content, the increase in contrast resulting from histogram \\nequalization was enough to render any intensity differences between the equalized images visually \\nindistinguishable. Given the signiﬁcant range of contrast differences in the original images, this example \\nillustrates the power of histogram equalization as an adaptive, autonomous contrast-enhancement tool.\\nHISTOGRAM MATCHING (SPECIFICATION)\\nAs explained in the last section, histogram equalization produces a transformation \\nfunction that seeks to generate an output image with a uniform histogram. When \\nautomatic enhancement is desired, this is a good approach to consider because the \\nDIP4E_GLOBAL_Print_Ready.indb   140\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 141}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n141\\nFIGURE 3.20\\n Left column: Images from Fig. 3.16. Center column: Corresponding histogram-equalized images. Right \\ncolumn: histograms of the images in the center column (compare with the histograms in Fig. 3.16). \\nDIP4E_GLOBAL_Print_Ready.indb   141\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 142}),\n",
       " Document(page_content='142\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192 255\\n(2)\\n(1)\\n(3)\\n(4)\\nIntensity values\\n \\nof original images\\nr\\nIntensity values of histogram-equalized images\\ns\\nr\\nk\\ns\\nk\\nFIGURE 3.21\\nTransformation \\nfunctions for histo-\\ngram equalization. \\nTransformations (1) \\nthrough (4) were \\nobtained using  \\nEq. (3-15) and the  \\nhistograms of the \\nimages on the left \\ncolumn of Fig. 3.20. \\nMapping of one \\nintensity value \\nr\\nk\\n in \\nimage 1 to its cor-\\nresponding value \\ns\\nk\\n \\nis shown.\\nresults from this technique are predictable and the method is simple to implement. \\nHowever, there are applications in which histogram equalization is not suitable. In \\nparticular, it is useful sometimes to be able to specify the shape of the histogram that \\nwe wish the processed image to have. The method used to generate images that have \\na specified histogram is called \\nhistogram matching\\n or \\nhistogram\\n \\nspecification\\n.\\nConsider for a moment continuous intensities \\nr\\n and \\nz \\nwhich, as before, we treat \\nas random variables with PDFs \\npr\\nr\\n()\\n and \\npz\\nz\\n() ,\\n respectively. Here, \\nr\\n and \\nz\\n denote \\nthe intensity levels of the input and output (processed) images\\n, respectively. We can \\nestimate \\npr\\nr\\n()\\n from the given input image, and \\npz\\nz\\n()\\n is the \\nspeciﬁed\\n PDF that we \\nwish the output image to have\\n.\\nLet \\ns\\n be a random variable with the property\\n \\nsT r L p d\\nr\\nr\\n== −\\n() ( ) ( )\\n1\\n0\\n2\\nww\\n \\n(3-17)\\nwhere \\nw\\n is dummy variable of integration. This is the same as Eq. (3-11), which we \\nrepeat here for convenience\\n.\\nDeﬁne a function \\nG\\n on variable \\nz\\n with the property\\n \\nGz L p d\\nz\\nz\\n() ( ) ()\\n=−\\n1\\n0\\n2\\nvv = s\\n \\n(3-18)\\nwhere \\nv\\n is a dummy variable of integration. It follows from the preceding two equa-\\ntions that \\nGz s Tr\\n()\\n()\\n==\\n and, therefore, that \\nz\\n must satisfy the condition\\n \\nzG s G T r\\n==\\n[]\\n−−\\n11\\n() ()\\n \\n(3-19)\\nDIP4E_GLOBAL_Print_Ready.indb   142\\n6/16/2017   2:03:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 143}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n143\\nThe transformation function \\nTr\\n()\\n can be obtained using Eq. (3-17) after \\npr\\nr\\n()\\n has  \\nbeen  estimated using the input image\\n. Similarly, function \\nGz\\n()\\n can be obtained from \\nEq.\\n (3-18) because \\npz\\nz\\n()\\n is given.\\nEquations (3-17) through (3-19) imply that an image whose intensity levels have \\na speciﬁed PDF can be obtained using the following procedure:\\n1. \\nObtain \\npr\\nr\\n()\\n \\nfrom the input image to use in Eq.\\n (3-17).\\n2. \\nUse the speciﬁed PDF, \\npz\\nz\\n() ,\\n in Eq. (3-18) to obtain the function \\nGz\\n()\\n.\\n3. \\nCompute the inverse transformation \\nzGs\\n=\\n−\\n1\\n() ;\\n this is a mapping from \\ns\\n to \\nz\\n, \\nthe latter being the values that have the speciﬁed PDF\\n.\\n4. \\nObtain the output image by ﬁrst equalizing the input image using Eq. (3-17); the \\npixel values in this image are the \\ns\\n values\\n. For each pixel with value \\ns\\n in the equal-\\nized image, perform the inverse mapping \\nzGs\\n=\\n−\\n1\\n()\\n to obtain the corresponding \\npixel in the output image\\n. When all pixels have been processed with this trans-\\nformation, the PDF of the output image, \\npz\\nz\\n() ,\\n will be equal to the speciﬁed PDF. \\nBecause \\ns\\n is related to \\nr\\n by \\nTr\\n()\\n, it is possible for the mapping that yields \\nz\\n from \\ns\\n \\nto be expressed directly in terms of \\nr\\n.\\n In general, however, finding analytical expres-\\nsions for \\nG\\n−\\n1\\n is not a trivial task. Fortunately, this is not a problem when working \\nwith discrete quantities, as you will see shortly.\\nAs before, we have to convert the continuous result just derived into a discrete \\nform. This means that we work with histograms instead of PDFs. As in histogram \\nequalization, we lose in the conversion the ability to be able to guarantee a result that \\nwill have the exact speciﬁed histogram. Despite this, some very useful results can be \\nobtained even with approximations.\\nThe discrete formulation of Eq. (3-17) is the histogram equalization transforma-\\ntion in Eq. (3-15), which we repeat here for convenience:\\n \\nsT r L p r k L\\nkk\\nr j\\nj\\nk\\n== − = −\\n=\\n∑\\n() ( ) () , , , ,\\n1 012 1\\n0\\n…\\n \\n(3-20)\\nwhere the components of this equation are as before. Similarly, given a specific value \\nof \\ns\\nk\\n,\\n the discrete formulation of Eq. (3-18) involves computing the transformation \\nfunction\\n \\nGz L p z\\nqz\\ni\\ni\\nq\\n() ( ) ( )\\n=−\\n=\\n∑\\n1\\n0\\n \\n(3-21)\\nfor a value of \\nq\\n so that\\n \\nGz s\\nqk\\n()\\n=\\n \\n(3-22)\\nwhere \\npz\\nzi\\n()\\n is the \\ni\\nth value of the specified histogram.\\n Finally, we obtain the desired \\nvalue \\nz\\nq\\n from the inverse transformation:\\nDIP4E_GLOBAL_Print_Ready.indb   143\\n6/16/2017   2:03:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 144}),\n",
       " Document(page_content='144\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n \\nzG s\\nqk\\n=\\n−\\n1\\n()\\n \\n(3-23)\\nWhen performed over all pixels, this is a mapping from the \\ns\\n values in the histogram-\\nequalized image to the corresponding \\nz\\n values in the output image\\n.\\nIn practice, there is no need to compute the inverse of \\nG\\n. Because we deal with \\nintensity levels that are integers, it is a simple matter to compute all the possible \\nvalues of \\nG\\n using Eq. (3-21) for \\nqL\\n=−\\n012\\n1\\n,,, , .\\n…\\n These values are rounded to their \\nnearest integer values spanning the range \\n[, ]\\n01\\nL\\n−\\n and stored in a lookup table. \\nT\\nhen, given a particular value of \\ns\\nk\\n,\\n we look for the closest match in the table. For \\nexample\\n, if the 27th entry in the table is the closest value to \\ns\\nk\\n,\\n then \\nq\\n=\\n26\\n (recall \\nthat we start counting intensities at 0) and \\nz\\n26\\n is the best solution to Eq. (3-23). \\nThus, the given value \\ns\\nk\\n would map to \\nz\\n26\\n.\\n Because the \\nz\\n’\\ns are integers in the range \\n[, ] ,\\n01\\nL\\n−\\n it follows that \\nz\\n0\\n0\\n=\\n, \\nzL\\nL\\n−\\n=−\\n1\\n1,\\n and, in general, \\nzq\\nq\\n=\\n.\\n Therefore, \\nz\\n26\\n \\nwould equal intensity value 26. We repeat this procedure to ﬁnd the mapping from \\neach value \\ns\\nk\\n to the value \\nz\\nq\\n that is its closest match in the table. These mappings are \\nthe solution to the histogram-speciﬁcation problem.\\nGiven an input image, a speciﬁed histogram, \\npz\\nzi\\n() ,\\n \\niL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n and recall-\\ning that the \\nss\\nk\\n’\\n are the values resulting from Eq. (3-20), we may summarize the \\nprocedure for discrete histogram speciﬁcation as follows:\\n1. \\nCompute the histogram, \\npr\\nr\\n() ,\\n of the input image, and use it in Eq. (3-20) to map \\nthe intensities in the input image to the intensities in the histogram-equalized \\nimage\\n. Round the \\nresulting values, \\ns\\nk\\n, to the integer range \\n[, ] .\\n01\\nL\\n−\\n2.\\n \\nCompute all values of function \\nGz\\nq\\n()\\n using the Eq. (3-21) for \\nqL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n \\nwhere \\npz\\nzi\\n()\\n are the values of the speciﬁed histogram. Round the values of \\nG\\n to \\nintegers in the range \\n[, ] .\\n01\\nL\\n−\\n Store the rounded values of \\nG\\n in a lookup table\\n.\\n3. \\nFor every value of \\nsk L\\nk\\n,, , , , ,\\n=−\\n012 1\\n…\\n use the stored values of \\nG\\n from Step 2 \\nto ﬁnd the corresponding value of \\nz\\nq\\n so that \\nGz\\nq\\n()\\n is closest to \\ns\\nk\\n.\\n Store these \\nmappings from \\ns\\n to \\nz\\n.\\n When more than one value of \\nz\\nq\\n gives the same match \\n(i.e., the mapping is not unique), choose the smallest value by convention.\\n4. \\nForm the histogram-speciﬁed image by mapping every equalized pixel with val-\\nue \\ns\\nk\\n to the corresponding pixel with value \\nz\\nq\\n in the histogram-speciﬁed image, \\nusing the mappings found in Step 3. \\nAs in the continuous case, the intermediate step of equalizing the input image is \\nconceptual. It can be skipped by combining the two transformation functions, \\nT\\n and \\nG\\n−\\n1\\n, as Example 3.7 below shows.\\nW\\ne mentioned at the beginning of the discussion on histogram equalization that, \\nin addition to condition (b), inverse functions (\\nG\\n−\\n1\\n in the present discussion) have to \\nbe strictly monotonic to satisfy condition (a\\n/H11032\\n). In terms of Eq. (3-21), this means that \\nnone of the values \\npz\\nzi\\n()\\n in the speciﬁed histogram can be zero (see Problem 3.9). \\nW\\nhen this condition is not satisﬁed, we use the “work-around” procedure in Step 3. \\nThe following example illustrates this numerically.\\nDIP4E_GLOBAL_Print_Ready.indb   144\\n6/16/2017   2:03:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 145}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n145\\nEXAMPLE 3.7 : Illustration of the mechanics of histogram speciﬁcation.\\nConsider the \\n64 64\\n×\\n hypothetical image from Example 3.5, whose histogram is repeated in Fig. 3.22(a). \\nIt is desired to transform this histogram so that it will have the values speciﬁed in the second column of \\nT\\nable 3.2. Figure 3.22(b) shows this histogram.\\nThe ﬁrst step is to obtain the histogram-equalized values, which we did in Example 3.5:\\n \\nssssssss\\n012 34 5 6 7\\n13566777\\n====== = =\\n;;;;;;;\\nIn the next step, we compute the values of \\nGz\\nq\\n()\\n using the values of \\npz\\nzq\\n()\\n from Table 3.2 in Eq. (3-21): \\n \\nGz Gz Gz Gz\\nGz\\nGz\\n( ). ( ). ( ). ( ).\\n() . () .\\n0246\\n13\\n00 0 00 0 24 5 59 5\\n00 0 1\\n====\\n==\\n0 05 4 55 7 00\\n57\\nGz Gz\\n() . () .\\n==\\nAs in Example 3.5, these fractional values are rounded to integers in the range \\n[, ] :\\n07\\n \\nGz\\nGz\\nGz\\nGz\\nGz\\n() .\\n() .\\n() .\\n() .\\n() .\\n04\\n15\\n2\\n00 0 0\\n24 5 2\\n00 0 0\\n45 5 5\\n00 0\\n=→ =→\\n=→ =→\\n=→\\n→= →\\n=→ =→\\n0\\n5 95 6\\n10 5 1\\n70 0 7\\n6\\n37\\nGz\\nGz\\nGz\\n() .\\n() .\\n() .\\nThese results are summarized in Table 3.3. The transformation function, \\nGz\\nq\\n() ,\\n is sketched in Fig. 3.23(c). \\nBecause its ﬁrst three values are equal,\\n \\nG\\n is not strictly monotonic, so condition (a\\n/H11032\\n) is violated. Therefore, \\nwe use the approach outlined in Step 3 of the algorithm to handle this situation. According to this step, \\nwe ﬁnd the smallest value of \\nz\\nq\\n so that the value \\nGz\\nq\\n()\\n is the closest to \\ns\\nk\\n.\\n We do this for every value of \\nr\\nk\\n \\np\\nr\\n(\\nr\\nk\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n.30\\n01234567\\nz\\nq\\n \\np\\nz\\n(\\nz\\nq\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n.30\\n01234567\\nz\\nq\\n \\np\\nz\\n(\\nz\\nq\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n01234567\\nz\\nq\\n \\nG\\n(\\nz\\nq\\n)\\n1\\n2\\n3\\n4\\n7\\n6\\n5\\n01234567\\nb a\\nd c\\nFIGURE 3.22\\n(a) Histogram of a \\n3-bit image.  \\n(b) Speciﬁed  \\nhistogram.  \\n(c) Transformation \\nfunction obtained \\nfrom the speciﬁed \\nhistogram.  \\n(d) Result of  \\nhistogram  \\nspeciﬁcation.  \\nCompare the \\nhistograms in (b) \\nand (d).\\nDIP4E_GLOBAL_Print_Ready.indb   145\\n6/16/2017   2:03:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 146}),\n",
       " Document(page_content='146\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nTABLE \\n3.3\\nRounded values \\nof the  \\ntransformation \\nfunction \\nGz\\nq\\n() .\\n \\nz\\nq\\nGz\\nq\\n()\\nz\\n0\\n0\\n=\\n0\\nz\\n1\\n1\\n=\\n0\\nz\\n2\\n2\\n=\\n0\\nz\\n3\\n3\\n=\\n1\\nz\\n4\\n4\\n=\\n2\\nz\\n5\\n5\\n=\\n5\\nz\\n6\\n6\\n=\\n6\\nz\\n7\\n7\\n=\\n7\\ns\\nk\\n to create the required mappings from \\ns\\n to \\nz\\n. For example, \\ns\\n0\\n1\\n=\\n,\\n and we see that \\nGz\\n()\\n,\\n3\\n1\\n=\\n which is \\na perfect match in this case\\n, so we have the correspondence \\nsz\\n03\\n→\\n.\\n Every pixel whose value is 1 in the \\nhistogram equalized image would map to a pixel valued 3 in the histogram-speciﬁed image\\n. Continuing \\nin this manner, we arrive at the mappings in Table 3.4.\\nIn the ﬁnal step of the procedure, we use the mappings in Table 3.4 to map every pixel in the his-\\ntogram equalized image into a corresponding pixel in the newly created histogram-speciﬁed image. \\nThe values of the resulting histogram are listed in the third column of Table 3.2, and the histogram is \\nshown in Fig. 3.22(d). The values of \\npz\\nzq\\n()\\n were obtained using the same procedure as in Example 3.5. \\nF\\nor instance, we see in Table 3.4 that \\ns\\nk\\n=\\n1\\n maps to \\nz\\nq\\n=\\n3,\\n and there are 790 pixels in the histogram-\\nequalized image with a value of 1.\\n Therefore, \\npz\\nz\\n()\\n..\\n3\\n790 4096 0 19\\n==\\nAlthough the ﬁnal result in Fig. 3.22(d) does not match the speciﬁed histogram exactly, the gen-\\neral trend of moving the intensities toward the high end of the intensity scale deﬁnitely was achieved.\\n \\nAs mentioned earlier, obtaining the histogram-equalized image as an intermediate step is useful for \\nz\\nq\\nSpeciﬁed\\npz\\nzq\\n()\\nActual\\npz\\nzq\\n()\\nz\\n0\\n0\\n=\\n0.00\\n0.00\\nz\\n1\\n1\\n=\\n0.00\\n0.00\\nz\\n2\\n2\\n=\\n0.00\\n0.00\\nz\\n3\\n3\\n=\\n0.15\\n0.19\\nz\\n4\\n4\\n=\\n0.20\\n0.25\\nz\\n5\\n5\\n=\\n0.30\\n0.21\\nz\\n6\\n6\\n=\\n0.20\\n0.24\\nz\\n7\\n7\\n=\\n0.15\\n0.11\\nTABLE \\n3.2\\nSpeciﬁed and \\nactual histograms \\n(the values in \\nthe third column \\nare computed in \\nExample 3.7).\\nDIP4E_GLOBAL_Print_Ready.indb   146\\n6/16/2017   2:03:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 147}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n147\\nexplaining the procedure, but this is not necessary. Instead, we could list the mappings from the \\nr\\n’s to \\nthe \\ns\\n’s and from the \\ns\\n’s to the \\nz\\n’s in a three-column table. Then, we would use those mappings to map \\nthe original pixels directly into the pixels of the histogram-speciﬁed image.\\nEXAMPLE 3.8 : Comparison between histogram equalization and histogram speciﬁcation.\\nFigure 3.23(a) shows an image of the Mars moon, Phobos, taken by NASA’s Mars Global Surveyor. \\nFigure 3.23(b) shows the histogram of Fig. 3.23(a). The image is dominated by large, dark areas, result-\\ning in a histogram characterized by a large concentration of pixels in the dark end of the gray scale. At \\nﬁrst glance, one might conclude that histogram equalization would be a good approach to enhance this \\nimage, so that details in the dark areas become more visible. It is demonstrated in the following discus-\\nsion that this is not so.\\nFigure 3.24(a) shows the histogram equalization transformation [Eq. (3-20)] obtained using the histo-\\ngram in Fig. 3.23(b). The most relevant characteristic of this transformation function is how fast it rises \\nfrom intensity level 0 to a level near 190. This is caused by the large concentration of pixels in the input \\nhistogram having levels near 0. When this transformation is applied to the levels of the input image to \\nobtain a histogram-equalized result, the net effect is to map a very narrow interval of dark pixels into the \\nTABLE \\n3.4\\nMapping of  \\nvalues \\ns\\nk\\n into  \\ncorresponding \\nvalues \\nz\\nq\\n.\\nsz\\nkq\\n→\\n13\\n→\\n34\\n→\\n55\\n→\\n66\\n→\\n77\\n→\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\nb a\\nFIGURE 3.23\\n(a) An image, and \\n(b) its histogram.\\nDIP4E_GLOBAL_Print_Ready.indb   147\\n6/16/2017   2:03:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 148}),\n",
       " Document(page_content='148\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192 255\\nInput intensity\\nOutput intensity\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\nb\\na\\nc\\nFIGURE 3.24\\n(a) Histogram \\nequalization \\ntransformation \\nobtained using \\nthe histogram \\nin Fig. 3.23(b). \\n(b) Histogram \\nequalized image. \\n(c) Histogram of \\nequalized image.\\nupper end of the gray scale of the output image. Because numerous pixels in the input image have levels \\nprecisely in this interval, we would expect the result to be an image with a light, washed-out appearance. \\nAs Fig. 3.24(b) shows, this is indeed the case. The histogram of this image is shown in Fig. 3.24(c). Note \\nhow all the intensity levels are biased toward the upper one-half of the gray scale.\\nBecause the problem with the transformation function in Fig. 3.24(a) was caused by a large con-\\ncentration of pixels in the original image with levels near 0, a reasonable approach is to modify the \\nhistogram of that image so that it does not have this property. Figure 3.25(a) shows a manually speci-\\nﬁed function that preserves the general shape of the original histogram, but has a smoother transition \\nof levels in the dark region of the gray scale. Sampling this function into 256 equally spaced discrete \\nvalues produced the desired speciﬁed histogram. The transformation function, \\nGz\\nq\\n() ,\\n obtained from this \\nhistogram using Eq.\\n (3-21) is labeled transformation (1) in Fig. 3.25(b). Similarly, the inverse transfor-\\nmation \\nGs\\nk\\n−\\n1\\n() ,\\n from Eq. (3-23) (obtained using the step-by-step procedure discussed earlier) is labeled \\ntransformation (2) in F\\nig. 3.25(b). The enhanced image in Fig. 3.25(c) was obtained by applying trans-\\nformation (2) to the pixels of the histogram-equalized image in Fig. 3.24(b). The improvement of the \\nhistogram-speciﬁed image over the result obtained by histogram equalization is evident by comparing \\nthese two images. It is of interest to note that a rather modest change in the original histogram was all \\nthat was required to obtain a signiﬁcant improvement in appearance. Figure 3.25(d) shows the histo-\\ngram of Fig. 3.25(c). The most distinguishing feature of this histogram is how its low end has shifted right \\ntoward the lighter region of the gray scale (but not excessively so), as desired.\\nDIP4E_GLOBAL_Print_Ready.indb   148\\n6/16/2017   2:03:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 149}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n149\\nLOCAL HISTOGRAM PROCESSING\\nThe histogram processing methods discussed thus far are \\nglobal\\n, in the sense that \\npixels are modified by a transformation function based on the intensity distribution \\nof an entire image. This global approach is suitable for overall enhancement, but \\ngenerally fails when the objective is to enhance details over small areas in an image. \\nThis is because the number of pixels in small areas have negligible influence on \\nthe computation of global transformations. The solution is to devise transformation \\nfunctions based on the intensity distribution of pixel neighborhoods.\\nThe histogram processing techniques previously described can be adapted to local \\nenhancement. The procedure is to deﬁne a neighborhood and move its center from \\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192\\n255\\nInput intensity\\nOutput intensity\\n(2)\\n(1)\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\na\\nb\\nc\\nd\\nFIGURE 3.25\\nHistogram  \\nspeciﬁcation.  \\n(a) Speciﬁed histo-\\ngram.  \\n(b) Transformation \\nGz\\nq\\n() ,\\n labeled (1), \\nand \\nGs\\nk\\n−\\n1\\n() ,\\n  \\nlabeled (2).\\n  \\n(c) Result of  \\nhistogram  \\nspeciﬁcation.  \\n(d) Histogram of \\nimage (c).\\nDIP4E_GLOBAL_Print_Ready.indb   149\\n6/16/2017   2:03:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 150}),\n",
       " Document(page_content='150\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\npixel to pixel in a horizontal or vertical direction. At each location, the histogram of \\nthe points in the neighborhood is computed, and either a histogram equalization or \\nhistogram speciﬁcation transformation function is obtained. This function is used to \\nmap the intensity of the pixel centered in the neighborhood. The center of the neigh-\\nborhood is then moved to an adjacent pixel location and the procedure is repeated. \\nBecause only one row or column of the neighborhood changes in a one-pixel trans-\\nlation of the neighborhood, updating the histogram obtained in the previous loca-\\ntion with the new data introduced at each motion step is possible (see Problem 3.14). \\nThis approach has obvious advantages over repeatedly computing the histogram of \\nall pixels in the neighborhood region each time the region is moved one pixel loca-\\ntion. Another approach used sometimes to reduce computation is to utilize nonover-\\nlapping regions, but this method usually produces an undesirable “blocky” effect.\\nEXAMPLE 3.9 : Local histogram equalization.\\nFigure 3.26(a) is an 8-bit, \\n512 512\\n×\\n image consisting of ﬁve black squares on a light gray background. \\nT\\nhe image is slightly noisy, but the noise is imperceptible. There are objects embedded in the dark \\nsquares, but they are invisible for all practical purposes. Figure 3.26(b) is the result of global histogram \\nequalization. As is often the case with histogram equalization of smooth, noisy regions, this image shows \\nsigniﬁcant enhancement of the noise. However, other than the noise, Fig. 3.26(b) does not reveal any \\nnew signiﬁcant details from the original. Figure 3.26(c) was obtained using local histogram equaliza-\\ntion of Fig. 3.26(a) with a neighborhood of size \\n33\\n×\\n. Here, we see signiﬁcant detail within all the dark \\nsquares\\n. The intensity values of these objects are too close to the intensity of the dark squares, and their \\nsizes are too small, to inﬂuence global histogram equalization signiﬁcantly enough to show this level of \\nintensity detail.\\nUSING HISTOGRAM STATISTICS FOR IMAGE ENHANCEMENT\\nStatistics obtained directly from an image histogram can be used for image enhance-\\nment. Let \\nr\\n denote a discrete random variable representing intensity values in the range \\n[, ]\\n01\\nL\\n−\\n, and let \\npr\\ni\\n()\\n denote the normalized histogram component corresponding to \\nintensity value \\nr\\ni\\n.\\n As indicated earlier, we may view \\npr\\ni\\n()\\n as an estimate of the prob-\\nability that intensity \\nr\\ni\\n occurs in the image from which the histogram was obtained.\\nb a\\nc\\nFIGURE 3.26\\n(a) Original  \\nimage. (b) Result \\nof global  \\nhistogram  \\nequalization.  \\n(c) Result of local \\nhistogram  \\nequalization.\\nDIP4E_GLOBAL_Print_Ready.indb   150\\n6/16/2017   2:03:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 151}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n151\\nFor an image with intensity levels in the range \\n[, ] ,\\n01\\nL\\n−\\n the \\nn\\nth moment of \\nr\\n \\nabout its mean,\\n \\nm\\n, is deﬁned as\\n \\nm\\nni\\nn\\ni\\ni\\nL\\nrm p r\\n=−\\n=\\n−\\n∑\\n() ( )\\n0\\n1\\n \\n(3-24)\\nwhere \\nm\\n is given by \\n \\nmr p r\\nii\\ni\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(3-25)\\nThe mean is a measure of average intensity and \\nthe variance (or stan\\ndard deviation,\\n \\ns\\n), given by\\n \\nsm\\n2\\n2\\n2\\n0\\n1\\n== −\\n=\\n−\\n∑\\n() ( )\\nrm p r\\nii\\ni\\nL\\n \\n(3-26)\\nis a measure of image contrast.\\nW\\ne consider two uses of the mean and variance for enhancement purposes. The \\nglobal\\n mean and variance [Eqs. (3-25) and (3-26)] are computed over an entire \\nimage and are useful for gross adjustments in overall intensity and contrast. A more \\npowerful use of these parameters is in local enhancement, where the \\nlocal\\n mean and \\nvariance are used as the basis for making changes that depend on image character-\\nistics in a neighborhood about each pixel in an image.\\nLet \\n(, )\\nxy\\n denote the coordinates of any pixel in a given image, and let \\nS\\nxy\\n denote \\na neighborhood of speciﬁed size, centered on \\n(, ) .\\nxy\\n The mean value of the pixels in \\nthis neighborhood is given by the expression\\n \\nmr p r\\nSi S i\\ni\\nL\\nxy\\nxy\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(3-27)\\nwhere \\np\\nS\\nxy\\n is the histogram of the pixels in region \\nS\\nxy\\n.\\n This histogram has \\nL\\n bins\\n, \\ncorresponding to the \\nL\\n possible intensity values in the input image. However, many \\nof the bins will have 0 counts, depending on the size of \\nS\\nxy\\n.\\n For example, if the neigh-\\nborhood is of size \\n33\\n×\\n and \\nL\\n=\\n256\\n,\\n only between 1 and 9 of the 256 bins of the \\nhistogram of the neighborhood will be nonzero (the maximum number of possible \\ndifferent\\n intensities in a \\n33\\n×\\n region is 9, and the minimum is 1). These non-zero \\nvalues will correspond to the number of different intensities in \\nS\\nxy\\n .\\nThe variance of the pixels in the neighborhood is similarly given by\\n \\ns\\nSi S S i\\ni\\nL\\nxy\\nxy xy\\nrm pr\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\n \\n(3-28)\\nAs before, the local mean is a measure of average intensity in neighborhood \\nS\\nxy\\n,\\n and \\nthe local variance (or standard deviation) is a measure of intensity contrast in that \\nneighborhood.\\nSee the tutorials section \\nin the book website for a \\nreview of probability.\\nWe follow convention \\nin using \\nm\\n for the mean \\nvalue. Do not confuse it \\nwith our use of the same \\nsymbol to denote the \\nnumber of rows in an  \\nm\\n \\n/H11003\\n \\nn\\n neighborhood.\\nDIP4E_GLOBAL_Print_Ready.indb   151\\n6/16/2017   2:03:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 152}),\n",
       " Document(page_content='152\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nAs the following example illustrates, an important aspect of image processing \\nusing the local mean and variance is the ﬂexibility these parameters afford in devel-\\noping simple, yet powerful enhancement rules based on statistical measures that \\nhave a close, predictable correspondence with image appearance.\\nEXAMPLE 3.10 :  Local enhancement using histogram statistics.\\nFigure 3.27(a) is the same image as Fig. 3.26(a), which we enhanced using local histogram equalization. \\nAs noted before, the dark squares contain embedded symbols that are almost invisible. As before, we \\nwant to enhance the image to bring out these hidden features. \\nWe can use the concepts presented in this section to formulate an approach for enhancing low-con-\\ntrast details embedded in a background of similar intensity. The problem at hand is to enhance the low-\\ncontrast detail in the dark areas of the image, while leaving the light background unchanged. \\nA method used to determine whether an area is relatively light or dark at a point \\n(, )\\nxy\\n is to com-\\npare the average local intensity\\n, \\nm\\nS\\nxy\\n,\\n to the average image intensity (the global mean), denoted by \\nm\\nG\\n.\\n We obtain \\nm\\nG\\n using Eq. (3-25) with the histogram of the entire image. Thus, we have the ﬁrst ele-\\nment of our enhancement scheme: We will consider the pixel at \\n(, )\\nxy\\n as a candidate for processing if \\nkm m km\\nGS G\\nxy\\n01\\n≤≤\\n,\\n where \\nk\\n0\\n and \\nk\\n1\\n are nonnegative constants and \\nkk\\n01\\n<\\n.\\n For example, if our focus is \\non areas that are darker than one-quarter of the mean intensity\\n, we would choose \\nk\\n0\\n0\\n=\\n and \\nk\\n1\\n02 5\\n=\\n..\\nBecause we are interested in enhancing areas that have low contrast, we also need a measure to \\ndetermine whether the contrast of an area makes it a candidate for enhancement.\\n We consider the \\npixel at \\n(, )\\nxy\\n as a candidate if \\nkk\\nGS G\\nxy\\n23\\nss s\\n≤≤\\n,\\n where \\ns\\nG\\n is the global standard deviation obtained \\nwith Eq. (3-26) using the histogram of the entire image, and \\nk\\n2\\n and \\nk\\n3\\n are nonnegative constants, with \\nkk\\n23\\n<\\n.\\n For example, to enhance a dark area of low contrast, we might choose \\nk\\n2\\n0\\n=\\n and \\nk\\n3\\n01\\n=\\n..\\n A \\npixel that meets all the preceding conditions for local enhancement is processed by multiplying it by a \\nspeciﬁed constant,\\n \\nC\\n, to increase (or decrease) the value of its intensity level relative to the rest of the \\nimage. Pixels that do not meet the enhancement conditions are not changed.\\nWe summarize the preceding approach as follows. Let \\nfx y\\n(,\\n)\\n denote the value of an image at any \\nimage coordinates \\n(, ) ,\\nxy\\n and let \\ngxy\\n(,\\n)\\n be the corresponding value in the enhanced image at those \\ncoordinates\\n. Then,\\n \\ngxy\\nCf\\nx y k m m km k\\nk\\nfx y\\nGS G GS G\\nxy\\nxy\\n(, )\\n(, )\\n(, )\\n=\\nif  \\nAND  \\not\\n2\\n01 3\\n≤≤\\n≤≤\\nss s\\nh\\nherwise\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(3-29)\\nb a\\nFIGURE 3.27\\n(a) Original  \\nimage. (b) Result \\nof local  \\nenhancement \\nbased on local  \\nhistogram  \\nstatistics.  \\nCompare (b) with \\nFig. 3.26(c).\\nDIP4E_GLOBAL_Print_Ready.indb   152\\n6/16/2017   2:03:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 153}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n153\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , ,\\n…\\n where, as indicated above, \\nC\\n, \\nk\\n0\\n, \\nk\\n1\\n, \\nk\\n2\\n,\\n and \\nk\\n3\\n are \\nspeciﬁed constants, \\nm\\nG\\n is the global mean of the input image, and \\ns\\nG\\n is its standard deviation. Param-\\neters \\nm\\nS\\nxy\\n and \\ns\\nS\\nxy\\n are the local mean and standard deviation, respectively, which change for every loca-\\ntion \\n(, ) .\\nxy\\n As usual, \\nM\\n and \\nN\\n are the number of rows and columns in the input image\\n.\\nFactors such as the values of the global mean and variance relative to values in the areas to be \\nenhanced play a key role in selecting the parameters in Eq. (3-29), as does the range of differences \\nbetween the intensities of the areas to be enhanced and their background. In the case of Fig. 3.27(a), \\nm\\nG\\n=\\n161,\\n \\ns\\nG\\n=\\n103,\\n the maximum intensity values of the image and areas to be enhanced are 228 and \\n10,\\n respectively, and the minimum values are 0 in both cases.\\nWe would like for the maximum value of the enhanced features to be the same as the maximum value \\nof the image, so we select \\nC\\n=\\n22\\n8\\n..\\n The areas to be enhanced are quite dark relative to the rest of the \\nimage\\n, and they occupy less than a third of the image area; thus, we expect the mean intensity in the \\ndark areas to be much less than the global mean. Based on this, we let \\nk\\n0\\n0\\n=\\n and \\nk\\n1\\n01\\n=\\n..\\n Because the \\nareas to be enhanced are of very low contrast,\\n we let \\nk\\n2\\n0\\n=\\n.\\n For the upper limit of acceptable values \\nof standard deviation we set \\nk\\n3\\n01\\n=\\n.,\\n which gives us one-tenth of the global standard deviation. Figure \\n3.27(b) is the result of using Eq.\\n (3-29) with these parameters. By comparing this ﬁgure with Fig. 3.26(c), \\nwe see that the method based on local statistics detected the same hidden features as local histogram \\nequalization. But the present approach extracted signiﬁcantly more detail. For example, we see that all \\nthe objects are solid, but only the boundaries were detected by local histogram equalization. In addition, \\nnote that the intensities of the objects are not the same, with the objects in the top-left and bottom-right \\nbeing brighter than the others. Also, the horizontal rectangles in the lower left square evidently are of \\ndifferent intensities. Finally, note that the background in both the image and dark squares in Fig. 3.27(b) \\nis nearly the same as in the original image; by comparison, the same regions in Fig. 3.26(c) exhibit more \\nvisible noise and have lost their gray-level content. Thus, the additional complexity required to use local \\nstatistics yielded results in this case that are superior to local histogram equalization.\\n3.4 FUNDAMENTALS OF SPATIAL FILTERING  \\nIn this section, we discuss the use of spatial filters for image processing. Spatial filter-\\ning is used in a broad spectrum of image processing applications, so a solid under-\\nstanding of filtering principles is important. As mentioned at the beginning of this \\nchapter, the filtering examples in this section deal mostly with image enhancement. \\nOther applications of spatial filtering are discussed in later chapters. \\nThe name \\nﬁlter\\n is borrowed from frequency domain processing (the topic of \\nChapter 4) where “ﬁltering” refers to passing, modifying, or rejecting speciﬁed fre-\\nquency components of an image. For example, a ﬁlter that passes low frequencies \\nis called a \\nlowpass ﬁlter\\n. The net effect produced by a lowpass ﬁlter is to smooth an \\nimage by blurring it. We can accomplish similar smoothing directly on the image \\nitself by using \\nspatial ﬁlters\\n. \\nSpatial ﬁltering modiﬁes an image by replacing the value of each pixel by a func-\\ntion of the values of the pixel and its neighbors. If the operation performed on the \\nimage pixels is linear, then the ﬁlter is called a \\nlinear spatial ﬁlter\\n. Otherwise, the \\nﬁlter is a \\nnonlinear spatial ﬁlter\\n.\\n \\nWe will focus attention ﬁrst on linear ﬁlters and then \\nintroduce some basic nonlinear ﬁlters\\n. Section 5.3 contains a more comprehensive \\nlist of nonlinear ﬁlters and their application.\\n3.4\\nSee Section 2.6 regarding \\nlinearity.\\nDIP4E_GLOBAL_Print_Ready.indb   153\\n6/16/2017   2:03:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='154\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nTHE MECHANICS OF LINEAR SPATIAL FILTERING\\nA linear spatial filter performs a sum-of-products operation between an image \\nf\\n and a \\nfilter kernel\\n, \\nw\\n.\\n The kernel is an array whose size defines the neighborhood \\nof opera-\\ntion,\\n and whose coefficients determine the nature of the filter. Other terms used to \\nrefer to a spatial filter kernel are \\nmask\\n, \\ntemplate\\n, and \\nwindow\\n. We use the term \\nfilter \\nkernel\\n or simply \\nkernel\\n.\\nFigure 3.28 illustrates the mechanics of linear spatial ﬁltering using a \\n33\\n×\\n ker-\\nnel.\\n At any point \\n(, )\\nxy\\n in the image, the response, \\ngxy\\n(,\\n) ,\\n of the ﬁlter is the sum of \\nproducts of the kernel coefﬁcients and the image pixels encompassed by the kernel:\\n \\ngxy\\nf x y\\nf x y\\nfx\\ny\\n( , )( , ) ( , )( , ) ( , )\\n(,)(, )\\n=− − − − +− − +\\n++\\nww\\nw\\n11 1 1 1 0 1\\n00\\n…\\n…\\n+\\n++ +\\nw\\n(, ) ( , )\\n11 1 1\\nfx y\\n \\n(3-30)\\nAs coordinates \\nx\\n and \\ny\\n are varied,\\n the center of the kernel moves from pixel to pixel, \\ngenerating the filtered image, \\ng\\n, in the process.\\n†\\nObserve that the center coefﬁcient of the kernel, \\nw\\n(,\\n)\\n00\\n, aligns with the pixel at \\nlocation \\n(, ) .\\nxy\\n For a kernel of size \\nmn\\n×\\n,\\n we assume that \\nma\\n=+\\n21\\n and \\nnb\\n=+\\n21\\n,\\nwhere \\na\\n and \\nb\\n are nonnegative integers\\n. This means that our focus is on kernels of \\nodd size in both coordinate directions. In general, linear spatial ﬁltering of an image \\nof size \\nMN\\n×\\n with a kernel of size \\nmn\\n×\\n is given by the expression\\n \\ngxy\\nstf x sy t\\ntb\\nb\\nsa\\na\\n(, ) ( ,)( , )\\n=+\\n+\\n=− =−\\n∑ ∑\\nw\\n \\n(3-31)\\nwhere \\nx\\n and \\ny\\n are varied so that the center (origin) of the kernel visits every pixel in \\nf\\n once\\n. For a fixed value of \\n(, ) ,\\nxy\\n Eq. (3-31) implements the \\nsum of products\\n of the \\nform shown in Eq.\\n (3-30), but for a kernel of arbitrary odd size. As you will learn in \\nthe following section, this equation is a central tool in linear filtering.\\nSPATIAL CORRELATION AND CONVOLUTION\\nSpatial correlation\\n is illustrated graphically in Fig. 3.28, and it is described mathemati-\\ncally by Eq. \\n(3-31). Correlation consists of moving the center of a kernel over an \\nimage, and computing the sum of products at each location. The mechanics of \\nspatial \\nconvolution\\n are the same, except that the correlation kernel is rotated by 180°. Thus, \\nwhen the values of a kernel are symmetric about its center, correlation and convolu-\\ntion yield the same result. The reason for rotating the kernel will become clear in \\nthe following discussion. The best way to explain the differences between the two \\nconcepts is by example. \\nWe begin with a 1-D illustration, in which case Eq. (3-31) becomes\\n \\ngx sf x s\\nsa\\na\\n() ( )( )\\n=+\\n=−\\n∑\\nw\\n \\n(3-32)\\n†\\n  A ﬁltered pixel value typically is assigned to a corresponding location in a new image created to hold the results \\nof ﬁltering. It is seldom the case that ﬁltered pixels replace the values of the corresponding location in the origi-\\nnal image, as this would change the content of the image while ﬁltering is being performed.\\nIt certainly is possible \\nto work with kernels of \\neven size, or mixed even \\nand odd sizes. However, \\nworking with odd sizes \\nsimpliﬁes indexing and \\nis also more intuitive \\nbecause the kernels have \\ncenters falling on integer \\nvalues, and they are \\nspatially symmetric.\\nDIP4E_GLOBAL_Print_Ready.indb   154\\n6/16/2017   2:03:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 155}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n155\\nFigure 3.29(a) shows a 1-D function, \\nf\\n, and a kernel, \\nw.\\n The kernel is of size \\n15\\n×\\n,\\n so \\na\\n=\\n2\\n and \\nb\\n=\\n0\\n in this case. Figure 3.29(b) shows the starting position used to per-\\nform correlation,\\n in which \\nw\\n is positioned so that its center coefficient is coincident \\nwith the origin of \\nf\\n. \\nT\\nhe first thing we notice is that part of \\nw\\n lies outside \\nf\\n,\\n so the summation is \\nundefined in that area. A solution to this problem is to \\npad\\n function \\nf\\n with enough \\n0’s on either side. In general, if the kernel is of size \\n1\\n×\\nm\\n,\\n we need \\n()\\nm\\n−\\n12\\n zeros \\non either side of \\nf\\n in order to handle the beginning and ending configurations of \\nw\\n \\nwith respect to \\nf\\n.\\n Figure 3.29(c) shows a properly padded function. In this starting \\nconﬁguration, all coefﬁcients of the kernel overlap valid values. \\nZero padding is not the \\nonly padding option, as \\nwe will discuss in detail \\nlater in this chapter.\\nPixel values under kernel \\nwhen it is centered on (\\nx\\n, \\ny\\n)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n)\\nf\\n(\\nx\\n, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n, \\ny\\n)\\nw\\n(\\n/H11002\\n1,\\n/H11002\\n1)\\nw\\n(0,\\n/H11002\\n1)\\nw\\n(\\n/H11002\\n1,0)\\nw\\n(\\n/H11002\\n1,1)\\nw\\n(0,1)\\nw\\n(1,1)\\nw\\n(0,0)\\nw\\n(1,0)\\nw\\n(1,\\n/H11002\\n1)\\nKernel coefficients\\nx\\nImage \\nf\\ny\\nImage origin\\nFilter kernel,\\nMagnified view showing filter kernel\\ncoefficients and corresponding pixels\\nin the image\\nFilter kernel\\nKernel origin\\nImage pixels\\nw\\n(\\ns\\n,\\nt\\n)\\nFIGURE 3.28\\nThe mechanics \\nof linear spatial \\nﬁltering  \\nusing a \\n33\\n×\\n  \\nkernel.\\n The pixels \\nare shown as \\nsquares to sim-\\nplify the graph-\\nics. Note that \\nthe origin of the \\nimage is at the top \\nleft, but the origin \\nof the kernel is at \\nits center. Placing \\nthe origin at the \\ncenter of spatially \\nsymmetric kernels \\nsimpliﬁes writing \\nexpressions for \\nlinear ﬁltering.\\nDIP4E_GLOBAL_Print_Ready.indb   155\\n6/16/2017   2:03:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 156}),\n",
       " Document(page_content='156\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nThe ﬁrst correlation value is the sum of products in this initial position, computed \\nusing Eq. (3-32) with \\nx\\n=\\n0:\\n  \\ngs f s\\ns\\n() ()( )\\n00\\n0\\n2\\n2\\n=+ =\\n=−\\n∑\\nw\\n \\nT\\nhis value is in the leftmost location of the correlation result in\\n \\nFig. 3.29(g). \\nT\\no obtain the second value of correlation, we shift the relative positions of \\nw\\n and \\nf\\n one pixel location to the right [i.e\\n., we let \\nx\\n=\\n1\\n in Eq. (3-32)] and compute the sum \\nof products again.\\n The result is \\ng\\n()\\n,\\n18\\n=\\n as shown in the leftmost, nonzero location \\nin F\\nig. 3.29(g). When \\nx\\n=\\n2,\\n we obtain \\ng\\n()\\n.\\n22\\n=\\n When \\nx\\n=\\n3,\\n we get \\ng\\n()\\n34\\n=\\n [see Fig. \\n3.29(e)].\\n Proceeding in this manner by varying \\nx \\none shift at a time, we “build” the \\ncorrelation result in Fig. 3.29(g). Note that it took 8 values of \\nx\\n (i.e., \\nx\\n=\\n012\\n7\\n,,, ,\\n…\\n) \\nto fully shift \\nw\\n past \\nf\\n so the\\n center\\n coefﬁcient in \\nw\\n visited \\nevery\\n pixel in \\nf\\n.\\n Sometimes, \\nit is useful to have every element of \\nw\\n visit every pixel in \\nf\\n.\\n For this, we have to start \\n(i)\\n(j)\\n(k)\\n(l)\\n(m)\\n(n)\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n000\\n1\\n00 0\\n000\\n1\\n0000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\nCorrelation\\nConvolution\\nStarting position alignment\\nPosition after 1 shift\\nFinal position\\n( h ) 00082421 000\\n0\\nExtended (full) correlation result\\n(p)\\n000124280000\\nExtended (full) convolution result\\n(g)\\n0 8 2 4 2 1 0 0\\n        Correlation result\\n(o)\\n01242800\\n        Convolution result\\n000\\n1\\n0000\\nOrigin\\nf\\n82421\\nw\\n rotated 180\\n/H11034\\n000\\n1\\n0000\\nOrigin\\nf\\n2428\\n1\\nw\\nPosition after 3 shifts\\nZero padding\\nStarting position\\nZero padding\\nStarting position\\nStarting position alignment\\nPosition after 1 shift\\nPosition after 3 shifts\\nFinal position\\nFIGURE 3.29\\nIllustration of 1-D \\ncorrelation and \\nconvolution of a \\nkernel, \\nw\\n,  with a \\nfunction \\nf  \\nconsisting of a  \\ndiscrete unit \\nimpulse\\n. Note that \\ncorrelation and \\nconvolution are \\nfunctions of the \\nvariable \\nx\\n, which \\nacts to \\ndisplace\\n \\none function with \\nrespect to the \\nother. For the \\nextended  \\ncorrelation and \\nconvolution \\nresults, the  \\nstarting  \\nconﬁguration \\nplaces the right-\\nmost element of \\nthe kernel to be \\ncoincident with \\nthe origin of \\nf\\n. \\nAdditional  \\npadding must be \\nused.\\nDIP4E_GLOBAL_Print_Ready.indb   156\\n6/16/2017   2:03:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 157}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n157\\nwith the rightmost element of \\nw\\n coincident with the origin of \\nf\\n,\\n and end with the \\nleftmost element of \\nw\\n being coincident the last element of \\nf\\n (additional padding \\nwould be required).\\n Figure Fig. 3.29(h) shows the result of this \\nextended\\n, or \\nfull\\n, cor-\\nrelation. As Fig. 3.29(g) shows, we can obtain the “standard” correlation by cropping \\nthe full correlation in Fig. 3.29(h).\\nThere are two important points to note from the preceding discussion. First, cor-\\nrelation is a function of \\ndisplacement\\n of the ﬁlter kernel relative to the image. In \\nother words, the ﬁrst value of correlation corresponds to zero displacement of the \\nkernel, the second corresponds to one unit displacement, and so on.\\n†\\n The second \\nthing to notice is that correlating a kernel \\nw\\n with a function that contains all 0’s and \\na single 1 yields a \\ncop\\ny\\n of \\nw\\n, but \\nrotated\\n \\nby 180°. A function that contains a single 1 \\nwith the rest being 0’s is called a \\ndiscrete unit impulse\\n. Correlating a kernel with a dis-\\ncrete unit impulse yields a \\nrotated\\n version of the kernel at the location of the impulse. \\nThe right side of Fig. 3.29 shows the sequence of steps for performing convolution \\n(we will give the equation for convolution shortly). The only difference here is that \\nthe kernel is \\npre-rotated\\n by 180° prior to performing the shifting/sum of products \\noperations. As the\\n \\nconvolution in Fig. 3.29(o) shows, the result of pre-rotating the \\nkernel is that now we have an \\nexact\\n copy of the kernel at the location of the unit \\nimpulse\\n. In fact, a foundation of linear system theory is that convolving a function \\nwith an impulse yields a copy of the function at the location of the impulse. We will \\nuse this property extensively in Chapter 4.\\nThe 1-D concepts just discussed extend easily to images, as Fig. 3.30 shows. For a \\nkernel of size \\nmn\\n×\\n,\\n we pad the image with a minimum of \\n()\\nm\\n−\\n12\\n rows of 0’s at \\nthe top and bottom and \\n()\\nn\\n−\\n12\\n columns of 0’s on the left and right. In this case, \\nm\\n and \\nn\\n are equal to 3,\\n so we pad \\nf\\n with one row of 0’s above and below and one \\ncolumn of 0’s to the left and right, as Fig. 3.30(b) shows. Figure 3.30(c) shows the \\ninitial position of the kernel for performing correlation, and Fig. 3.30(d) shows the \\nﬁnal result after  the center of \\nw\\n visits every pixel in \\nf\\n,\\n computing a sum of products \\nat each location. As before, the result is a copy of the kernel, rotated by 180°. We will \\ndiscuss the extended correlation result shortly.\\nFor convolution, we pre-rotate the kernel as before and repeat the sliding sum of \\nproducts just explained. Figures 3.30(f) through (h) show the result. You see again \\nthat convolution of a function with an impulse copies the function to the location \\nof the impulse. As noted earlier, correlation and convolution yield the same result if \\nthe kernel values are symmetric about the center. \\nThe concept of an impulse is fundamental in linear system theory, and is used in \\nnumerous places throughout the book. A \\ndiscrete impulse of strength\\n (\\namplitude\\n) \\nA\\n \\nlocated at coordinates \\n(,)\\nxy\\n00\\n is deﬁned as\\n \\nd\\n(,\\n)\\nxx yy\\nAx\\nxy y\\n−− =\\n==\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n00\\n00\\n0\\nif  and \\notherwise\\n \\n(3-33)\\n†\\n  In reality, we are shifting \\nf\\n to the left of \\nw\\n every time we increment \\nx\\n in Eq.\\n (3-32). However, it is more intuitive \\nto think of the smaller kernel moving right over the larger array \\nf\\n. The motion of the two is relative, so either \\nway of looking at the motion is acceptable. The reason we increment \\nf\\n and not \\nw\\n is that indexing the equations \\nfor correlation and convolution is much easier (and clearer) this way\\n, especially when working with 2-D arrays.\\nRotating a 1-D kernel \\nby 180° is equivalent to \\nﬂipping the kernel about \\nits axis. \\nIn 2-D, rotation by 180° \\nis equivalent to ﬂipping \\nthe kernel about one axis \\nand then the other.\\nDIP4E_GLOBAL_Print_Ready.indb   157\\n6/16/2017   2:03:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='158\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFor example, the unit impulse in Fig. 3.29(a) is given by \\nd\\n()\\nx\\n−\\n3\\n in the 1-D version of \\nthe preceding equation.\\n  Similarly, the impulse in Fig. 3.30(a) is given by \\nd\\n(,\\n)\\nxy\\n−−\\n22\\n \\n[remember\\n, the origin is at \\n(,)\\n00\\n].\\n Summarizing the preceding discussion in equation form,\\n the correlation of a \\nkernel \\nw\\n of size \\nmn\\n×\\n with an image \\nfx y\\n(,\\n)\\n, denoted as \\n(w\\n/H22845\\nfx y\\n)( , ),\\n is given by \\nEq.\\n (3-31), which we repeat here for convenience:\\n \\n(w\\nw\\n/H22845\\nfx y s t f xs yt\\ntb\\nb\\nsa\\na\\n)( , )\\n( , ) ( , )\\n=+\\n+\\n=− =−\\n∑ ∑\\n \\n(3-34)\\nBecause our kernels do not depend on \\n(,) ,\\nxy\\n we will sometimes make this fact explic-\\nit by writing the left side of the preceding equation as \\nw\\n/H22845\\nfx y\\n(, ) .\\n Equation (3-34) is \\nevaluated for all values of the displacement variables \\nx\\n and \\ny\\n so that the center point \\nof \\nw\\n visits every pixel in \\nf\\n,\\n†\\n where we assume that \\nf\\n has been padded appropriately. \\n†\\n  As we mentioned earlier, the \\nminimum\\n number of required padding elements for a 2-D correlation is \\n()\\nm\\n−\\n12\\n \\nrows above and below \\nf,\\n and \\n()\\nn\\n−\\n12\\n columns on the left and right. With this padding, and assuming that \\nf\\n \\nis of size \\nMN\\n×\\n,\\n the values of \\nx\\n and \\ny\\n required to obtain a complete correlation are \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n This assumes that the starting conﬁguration is such that the \\ncenter\\n of the kernel coincides \\nwith the \\norigin\\n of the image\\n, which we have deﬁned to be at the top, left (see Fig. 2.19). \\nRecall that \\nA\\n = 1 for a \\nunit impulse.\\n00000\\n00\\n1\\n00\\n00000\\n00000\\n00000\\n0000\\n0000\\n0000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\n0000\\n0000\\n0000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\nOrigin\\nRotated \\nw\\n0\\n123\\n0\\n0\\n456\\n0\\n0\\n789\\n0\\n00000\\n00000\\nConvolution result\\n0000000\\n0000000\\n00\\n987\\n00\\n00\\n654\\n00\\n00\\n321\\n00\\n0000000\\n0000000\\nFull correlation result\\n0\\n987\\n0\\n0\\n654\\n0\\n0\\n321\\n0\\n00000\\n00000\\nCorrelation result\\n456\\n789\\n123\\nw\\n(a)\\nPadded \\nf\\n0000000\\n0000000\\n0000000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\n(b)\\n0000000\\n0000000\\n00\\n123\\n00\\n00\\n456\\n00\\n00\\n789\\n00\\n0000000\\n0000000\\nFull convolution result\\n(d)\\n(g)\\n(h)\\n(f)\\n(e)\\n(c)\\n456\\n789\\n123\\n654\\n321\\n987\\nInitial position for \\nw\\nf\\nFIGURE 3.30\\nCorrelation \\n(middle row) and \\nconvolution (last \\nrow) of a 2-D \\nkernel with an \\nimage consisting \\nof a discrete unit \\nimpulse. The 0’s \\nare shown in gray \\nto simplify visual \\nanalysis. Note that \\ncorrelation and \\nconvolution are \\nfunctions of \\nx\\n and \\ny\\n. As these  \\nvariable change,  \\nthey  \\ndisplace\\n one  \\nfunction with  \\nrespect to the \\nother. See the \\ndiscussion of Eqs. \\n(3-36) and (3-37) \\nregarding full \\ncorrelation and \\nconvolution.\\nDIP4E_GLOBAL_Print_Ready.indb   158\\n6/16/2017   2:03:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 159}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n159\\nAs explained earlier, \\nam\\n=−\\n() ,\\n12\\n \\nbn\\n=−\\n() ,\\n12\\n and we assume that \\nm\\n and \\nn\\n are \\nodd integers\\n. \\nIn a similar manner, the \\nconvolution\\n of a kernel \\nw\\n of size \\nmn\\n×\\n with an image \\nfx y\\n(,\\n) ,\\n denoted by \\n(w\\n/H22841\\nfx y\\n)(\\n,) ,\\n is deﬁned as\\n \\n(w\\nw\\n/H22841\\nfx y s t f xs yt\\ntb\\nb\\nsa\\na\\n)( , )\\n( , ) ( , )\\n=−\\n−\\n=− =−\\n∑ ∑\\n \\n(3-35)\\nwhere the minus signs align the coordinates of \\nf\\n and \\nw\\n when one of the functions is \\nrotated by \\n180\\n°\\n (see Problem 3.17). This equation implements the sum of products \\nprocess to which we refer throughout the book as \\nlinear spatial filtering\\n.\\n That is, lin-\\near spatial filtering and spatial convolution are synonymous.\\nBecause convolution is commutative (see Table 3.5), it is immaterial whether \\nw\\n \\nor \\nf \\nis rotated,\\n but rotation of the kernel is used by convention. Our kernels do not \\ndepend on \\n(,) ,\\nxy\\n a fact that we sometimes make explicit by writing the left side \\nof Eq.\\n (3-35) as \\nw\\n/H22841\\nfx y\\n(, ) .\\n When the meaning is clear, we let the dependence of \\nthe previous two equations on \\nx\\n and \\ny\\n be implied, and use the simpliﬁed notation \\nw\\n/H22845\\nf\\n and \\nw\\n/H22841\\nf\\n.\\n As with correlation, Eq. (3-35) is evaluated for all values of the \\ndisplacement variables \\nx\\n and \\ny\\n so that the center of \\nw\\n visits every pixel in \\nf\\n,\\n which \\nwe assume has been padded. The values of \\nx\\n and \\ny\\n needed to obtain a full convolu-\\ntion are \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n The size of the result is \\nMN\\n×\\n.\\nWe can deﬁne correlation and convolution so that \\nevery\\n element of \\nw\\n(instead of \\njust its center) visits \\nevery\\n pixel in \\nf\\n.\\n This requires that the starting conﬁguration be \\nsuch that the right, lower corner of the kernel coincides with the origin of the image. \\nSimilarly, the ending conﬁguration will be with the top left corner of the kernel coin-\\nciding with the lower right corner of the image. If the kernel and image are of sizes \\nmn\\n×\\n and \\nMN\\n×\\n,\\n respectively, the padding would have to increase to \\n()\\nm\\n−\\n1\\n pad-\\nding elements above and below the image\\n, and \\n()\\nn\\n−\\n1\\n elements to the left and right. \\nUnder these conditions\\n, the size of the resulting full correlation or convolution array \\nwill be of size \\nSS\\nvh\\n×\\n, where (see Figs. 3.30(e) and (h), and Problem 3.19),\\n \\nSm M\\nv\\n=+−\\n1  \\n(3-36)\\nand\\n \\nSn N\\nh\\n=+ −\\n1  \\n(3-37)\\nOften, spatial ﬁltering algorithms are based on correlation and thus implement \\nEq.\\n (3-34) instead. To use the algorithm for correlation, we input \\nw\\n into it; for con-\\nvolution,\\n we input \\nw\\n rotated by \\n180\\n°\\n.\\n The opposite is true for an algorithm that \\nimplements Eq.\\n (3-35). Thus, either Eq. (3-34) or Eq. (3-35) can be made to perform \\nthe function of the other by rotating the ﬁlter kernel. Keep in mind, however, that \\nthe \\norder\\n of the functions input into a correlation algorithm \\ndoes\\n make a difference, \\nbecause correlation is neither commutative nor associative (see Table 3.5). \\nDIP4E_GLOBAL_Print_Ready.indb   159\\n6/16/2017   2:03:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 160}),\n",
       " Document(page_content='160\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFigure 3.31 shows two kernels used for smoothing the intensities of an image. To \\nﬁlter an image using one of these kernels, we perform a convolution of the kernel \\nwith the image in the manner just described. When talking about ﬁltering and ker-\\nnels, you are likely to encounter the terms \\nconvolution ﬁlter\\n, \\nconvolution mask,\\n or \\nconvolution kernel\\n to denote ﬁlter kernels of the type we have been discussing. Typi-\\ncally, these terms are used in the literature to denote a spatial ﬁlter kernel, and not \\nto imply necessarily that the kernel is used for convolution. Similarly, “convolving a \\nkernel with an image” often is used to denote the sliding, sum-of-products process \\nwe just explained, and does not necessarily differentiate between correlation and \\nconvolution. Rather, it is used generically to denote either of the two operations. \\nThis imprecise terminology is a frequent source of confusion. In this book, when we \\nuse the term \\nlinear spatial ﬁltering\\n, we mean \\nconvolving a kernel with an image\\n.\\nSometimes an image is ﬁltered (i.e., convolved) sequentially, in stages, using a dif-\\nferent kernel in each stage. For example, suppose than an image \\nf\\n is ﬁltered with a \\nkernel \\nw\\n1\\n,\\n the result ﬁltered with kernel \\nw\\n2\\n,\\n that result ﬁltered with a third kernel, \\nand so on,\\n for \\nQ\\n stages. Because of the commutative property of convolution, this \\nmultistage ﬁltering can be done in a single ﬁltering operation, \\nw\\n/H22841\\nf\\n,\\n where\\n \\nww w w w\\n=\\n123\\n/H22841/H22841/H22841 /H22841\\n/midhorizellipsis\\nQ\\n \\n(3-38)\\nThe size of \\nw\\n is obtained from the sizes of the individual kernels by successive \\napplications of Eqs\\n. (3-36) and (3-37). If all the individual kernels are of size \\nmn\\n×\\n, \\nit follows from these equations that \\nw\\n will be of size \\nWW\\nvh\\n×\\n, where\\n \\nWQm m\\nv\\n=− +\\n×\\n()\\n1\\n \\n(3-39)\\nand\\n \\nWQn n\\nh\\n=− +\\n×\\n()\\n1\\n \\n(3-40)\\nThese equations assume that every value of a kernel visits every value of the array \\nresulting from the convolution in the previous step\\n. That is, the initial and ending \\nconfigurations, are as described in connection with Eqs. (3-36) and (3-37).\\nBecause the values of \\nthese kernels are sym-\\nmetric about the center, \\nno rotation is required \\nbefore convolution.\\nWe could not write a \\nsimilar equation for  \\ncorrelation because it is \\nnot commutative.\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n9\\n/H11003\\n1.0000\\n0.3679 0.6065\\n0.3679\\n0.3679\\n0.3679\\n0.6065 0.6065\\n0.6065\\n/H11003\\n4.8976\\n1\\nb a\\nFIGURE 3.31\\nExamples of \\nsmoothing kernels: \\n(a) is a \\nbox\\n kernel; \\n(b) is a \\nGaussian\\n \\nkernel.\\nProperty\\nConvolution\\nCorrelation\\nCommutative\\nfggf\\n/H22841/H22841\\n=\\n—\\nAssociative\\nfg f g h\\nh\\n/H22841/H22841 /H22841/H22841\\n(\\n)\\n=\\n(\\n)\\n—\\nDistributive\\nfg h f g f h\\n/H22841/H22841 /H22841\\n+\\n(\\n)\\n=\\n(\\n)\\n+\\n(\\n)\\nfg h f g f h\\n/H22845/H22845 /H22845\\n+\\n(\\n)\\n=\\n(\\n)\\n+\\n(\\n)\\nTABLE \\n3.5\\nSome fundamen-\\ntal properties of \\nconvolution and \\ncorrelation. A \\ndash means that \\nthe property does \\nnot hold.\\nDIP4E_GLOBAL_Print_Ready.indb   160\\n6/16/2017   2:03:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 161}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n161\\nSEPARABLE FILTER KERNELS\\nAs noted in Section 2.6, a 2-D function \\nGxy\\n(,\\n)\\n is said to be \\nseparable\\n if it can be written \\nas the product of two 1-D functions\\n, \\nGx\\n1\\n()\\n and \\nGx\\n2\\n() ;\\n that is, \\nGxy G xG y\\n(,\\n) () () .\\n=\\n12\\n \\nA spatial filter kernel is a matrix, and a separable kernel is a matrix that can be \\nexpressed as the outer product of two vectors. For example, the \\n23\\n*\\n kernel\\n \\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n111\\n111\\nis separable because it can be expressed as the outer product of the vectors\\n \\ncr\\n==\\n1\\n1\\n1\\n1\\n1\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nan\\nd\\nThat is,\\n \\ncr\\nT\\n== =\\n1\\n1\\n111\\n111\\n111\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nw\\n \\nA separable kernel of size \\nmn\\n×\\n can be expressed as the outer product of two vec-\\ntors\\n, \\nv\\n and \\nw\\n:\\n \\nw\\n=\\nvw\\nT\\n \\n(3-41)\\nwhere \\nv\\n and \\nw\\n are vectors of size \\nm\\n×\\n1\\n and \\nn\\n×\\n1,\\n respectively. For a square kernel \\nof size \\nmm\\n×\\n, we write\\n \\nw\\n=\\nvv\\nT\\n \\n(3-42)\\nIt turns out that the product of a column vector and a row vector is the same as the \\n2-D convolution of the vectors (see Problem 3.24).\\n \\nThe importance of separable kernels lies in the computational advantages that \\nresult from the associative property of convolution. If we have a kernel \\nw\\n that can \\nbe decomposed into two simpler kernels\\n, such that \\nww w\\n12\\n=\\n/H22841\\n,\\n then it follows \\nfrom the commutative and associative properties in \\nTable 3.5 that \\n     \\nwf w w w w w w w w\\n/H22841/H22841 /H22841/H22841 /H22841 /H22841 /H22841 /H22841 /H22841\\n====\\n() () ) )\\n((\\n12 21 2 1 1 2\\nff f f\\n \\n(3-43)\\nThis equation says that convolving a separable kernel with an image is the same as \\nconvolving \\nw\\n1\\n with \\nf\\n  first, and then convolving the result with \\nw\\n2\\n. \\nF\\nor an image of size \\nMN\\n×\\n and a kernel of size \\nmn\\n×\\n,\\n implementation of Eq. \\n(3-35) requires on the order of \\nMNmn\\n multiplications and additions. This is because \\nit follows directly from that equation that \\neac\\nh\\n pixel in the output (ﬁltered) image \\ndepends on \\nall\\n the coefﬁcients in the ﬁlter kernel. But, if the kernel is separable and \\nwe use Eq. (3-43), then the ﬁrst convolution, \\nw\\n1\\n/H22841\\nf\\n,\\n requires on the order of \\nMNm\\n \\nTo be strictly consistent \\nin notation, we should \\nuse uppercase, bold  \\nsymbols for kernels when \\nwe refer to them as  \\nmatrices. However,  \\nkernels are mostly \\ntreated in the book as \\n2-D functions, which we \\ndenote in italics. To avoid \\nconfusion, we continue \\nto use italics for kernels \\nin this short section, with \\nthe understanding that \\nthe two notations are \\nintended to be equivalent \\nin this case.\\nWe assume that the \\nvalues of \\nM\\n and \\nN\\n \\ninclude any padding of \\nf\\n prior to performing \\nconvolution.\\nDIP4E_GLOBAL_Print_Ready.indb   161\\n6/16/2017   2:03:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 162}),\n",
       " Document(page_content='162\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nmultiplications and additions because \\nw\\n1\\n is of size \\nm\\n×\\n1.\\n The result is of size \\nMN\\n×\\n, \\nso the convolution of \\nw\\n2\\n with the result requires \\nMNn\\n such operations, for a total of \\nMN m n\\n()\\n+\\n multiplication and addition operations. Thus, the \\ncomputational advan-\\ntage\\n of performing convolution with a separable\\n, as opposed to a nonseparable, ker-\\nnel is deﬁned as\\n \\nC\\nMNmn\\nMN m n\\nmn\\nmn\\n=\\n+\\n=\\n+\\n()\\n \\n(3-44)\\nFor a kernel of modest size, say \\n11 11\\n×\\n,\\n the computational advantage (and thus exe-\\ncution-time advantage) is a respectable 5.2.\\n For kernels with hundreds of elements, \\nexecution times can be reduced by a factor of a hundred or more, which is significant. \\nWe will illustrate the use of such large kernels in Example 3.16.\\nWe know from matrix theory that a matrix resulting from the product of a column \\nvector and a row vector \\nalways\\n has a rank of 1. By deﬁnition, a separable kernel is \\nformed by such a product. Therefore, to determine if a kernel is separable, all we \\nhave to do is determine if its rank is 1. Typically, we ﬁnd the rank of a matrix using a \\npre-programmed function in the computer language being used. For example, if you \\nuse MATLAB, function \\nrank\\n will do the job.\\nOnce you have determined that the rank of a kernel matrix is 1, it is not difﬁcult \\nto ﬁnd two vectors \\nv\\n and \\nw\\n such that their outer product, \\nvw\\nT\\n,\\n is equal to the kernel. \\nT\\nhe approach consists of only three\\n \\nsteps:\\n1. \\nFind any nonzero element in the kernel and let \\nE\\n denote its value\\n.\\n2. \\nForm vectors \\nc\\n and \\nr\\n equal,\\n respectively, to the column and row in the kernel \\ncontaining the element found in Step 1.\\n3. \\nWith reference to Eq. (3-41), let \\nvc\\n=\\n and \\nwr\\nT\\nE\\n=\\n.\\nThe reason why this simple three-step method works is that the rows and columns \\nof a matrix whose rank is 1 are linearly dependent.\\n That is, the rows differ only by a \\nconstant multiplier, and similarly for the columns. It is instructive to work through \\nthe mechanics of this procedure using a small kernel (see Problems 3.20 and 3.22).\\nAs we explained above, the objective is to ﬁnd two 1-D kernels, \\nw\\n1\\n and \\nw\\n2\\n,\\n in \\norder to implement 1-D convolution.\\n In terms of the preceding notation, \\nw\\n1\\n==\\ncv\\n \\nand \\nw\\n2\\n==\\nrw\\nE\\nT\\n.\\n For circularly symmetric kernels, the column through the center \\nof the kernel describes the entire kernel;\\n that is, \\nw\\n=\\nvv\\nT\\nc\\n,\\n where \\nc\\n is the value of \\nthe center coefﬁcient.\\n Then, the 1-D components are \\nw\\n1\\n=\\nv\\n and \\nw\\n2\\n=\\nv\\nT\\nc\\n. \\nSOME IMPORTANT COMPARISONS BETWEEN FILTERING IN THE  \\nSPATIAL AND FREQUENCY DOMAINS\\nAlthough filtering in the frequency domain is the topic of Chapter 4, we introduce \\nat this junction some important concepts from the frequency domain that will help \\nyou master the material that follows. \\nThe tie between spatial- and frequency-domain processing is the \\nFourier trans-\\nform\\n. We use the Fourier transform to go from the spatial to the frequency domain; \\nAs we will discuss later \\nin this chapter, the only \\nkernels that are sepa-\\nrable \\nand\\n whose values \\nare circularly symmetric \\nabout the center are \\nGaussian kernels, which \\nhave a nonzero center \\ncoefﬁcient (i.e., \\nc \\n>\\n \\n0 for \\nthese kernels).\\nDIP4E_GLOBAL_Print_Ready.indb   162\\n6/16/2017   2:03:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 163}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n163\\nto return to the spatial domain we use the \\ninverse Fourier transform\\n. This will be \\ncovered in detail in Chapter 4. The focus here is on two fundamental properties \\nrelating the spatial and frequency domains:\\n1. \\nConvolution, which is the basis for ﬁltering in the spatial domain, is equivalent \\nto multiplication in the frequenc\\ny domain, and vice versa.\\n2. \\nAn impulse of strength \\nA\\n in the spatial domain is a constant of value \\nA\\n in the \\nfrequenc\\ny domain, and vice versa. \\nAs explained in Chapter 4, a function (e.g., an image) satisfying some mild condi-\\ntions can be expressed as the sum of sinusoids of different frequencies and ampli-\\ntudes. Thus, the \\nappearance\\n of an image depends on the frequencies of its sinusoidal \\ncomponents—change the frequencies of those components, and you will change the \\nappearance of the image. What makes this a powerful concept is that it is possible to \\nassociate certain frequency bands with image characteristics. For example, regions \\nof an image with intensities that vary slowly (e.g., the walls in an image of a room) \\nare characterized by sinusoids of low frequencies. Similarly, edges and other sharp \\nintensity transitions are characterized by high frequencies. Thus, reducing the high-\\nfrequency components of an image will tend to blur it.\\nLinear ﬁltering is concerned with ﬁnding suitable ways to modify the frequency \\ncontent of an image. In the spatial domain we do this via convolution ﬁltering. In \\nthe frequency domain we do it with multiplicative ﬁlters. The latter is a much more \\nintuitive approach, which is one of the reasons why it is virtually impossible to truly \\nunderstand spatial ﬁltering without having at least some rudimentary knowledge of \\nthe frequency domain. \\nAn example will help clarify these ideas. For simplicity, consider a 1-D func-\\ntion (such as an intensity scan line through an image) and suppose that we want to \\neliminate all its frequencies above a cutoff value, \\nu\\n0\\n,\\n while “passing” all frequen-\\ncies below that value\\n. Figure 3.32(a)\\n \\nshows a frequency-domain ﬁlter function for \\ndoing this\\n. (The term \\nﬁlter transfer function\\n is used to denote ﬁlter functions in the \\nfrequency domain—this is analogous to our use of the term “ﬁlter kernel” in the \\nspatial domain.) Appropriately, the function in Fig. 3.32(a) is called a \\nlowpass\\n ﬁlter \\ntransfer function. In fact, this is an \\nideal\\n lowpass ﬁlter function because it eliminates \\nall\\n frequencies above \\nu\\n0\\n,\\n while passing all frequencies below this value.\\n†\\n That is, the \\n†\\n All the frequency domain ﬁlters in which we are interested are symmetrical about the origin and encompass \\nboth positive and negative frequencies, as we will explain in Section 4.3 (see Fig. 4.8). For the moment, we show \\nonly the right side (positive frequencies) of 1-D ﬁlters for simplicity in this short explanation. \\nSee the explanation of \\nEq. (3-33) regarding \\nimpulses. \\nAs we did earlier with \\nspatial ﬁlters, when the \\nmeaning is clear we use \\nthe term \\nﬁlter\\n inter-\\nchangeably with \\nﬁlter \\ntransfer function\\n when \\nworking in the frequency \\ndomain.\\n0\\nu\\nu\\nPassband\\nfrequency\\nStopband\\nFrequency domain\\n1\\nx\\nSpatial domain\\n0\\nu\\nb a\\nFIGURE 3.32\\n(a) Ideal 1-D low-\\npass ﬁlter transfer \\nfunction in the  \\nfrequency domain. \\n(b) Corresponding \\nﬁlter kernel in the \\nspatial domain.\\nDIP4E_GLOBAL_Print_Ready.indb   163\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 164}),\n",
       " Document(page_content='164\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\ntransition of the ﬁlter between low and high frequencies is instantaneous. Such ﬁlter \\nfunctions are not realizable with physical components, and have issues with “ringing” \\nwhen implemented digitally. However, ideal ﬁlters are very useful for illustrating \\nnumerous ﬁltering phenomena, as you will learn in Chapter 4.\\nTo lowpass-ﬁlter a spatial signal in the frequency domain, we ﬁrst convert it to the \\nfrequency domain by computing its Fourier transform, and then \\nmultiply\\n the result \\nby the ﬁlter transfer function in Fig. 3.32(a)\\n \\nto eliminate frequency components with \\nvalues higher than \\nu\\n0\\n.\\n To return to the spatial domain, we take the inverse Fourier \\ntransform of the ﬁltered signal.\\n The result will be a blurred spatial domain function.\\nBecause of the duality between the spatial and frequency domains, we can obtain \\nthe same result in the spatial domain by \\nconvolving\\n the equivalent spatial domain \\nﬁlter kernel with the input spatial function. The equivalent spatial ﬁlter kernel \\nis the inverse Fourier transform of the frequency-domain ﬁlter transfer function. \\nFigure 3.32(b) shows the spatial ﬁlter kernel corresponding to the frequency domain \\nﬁlter transfer function in Fig. 3.32(a). The ringing characteristics of the kernel are \\nevident in the ﬁgure. A central theme of digital ﬁlter design theory is obtaining faith-\\nful (and practical) approximations to the sharp cut off of ideal frequency domain \\nﬁlters while reducing their ringing characteristics.\\nA WORD ABOUT HOW SPATIAL FILTER KERNELS ARE CONSTRUCTED\\nWe consider three basic approaches for constructing spatial filters in the following \\nsections of this chapter. One approach is based on formulating filters based on \\nmathematical properties. For example, a filter that computes the average of pixels \\nin a neighborhood blurs an image. Computing an average is analogous to integra-\\ntion. Conversely, a filter that computes the local derivative of an image sharpens the \\nimage. We give numerous examples of this approach in the following sections.\\nA second approach is based on sampling a 2-D spatial function whose shape has \\na desired property. For example, we will show in the next section that samples from \\na Gaussian function can be used to construct a weighted-average (lowpass) ﬁlter. \\nThese 2-D spatial functions sometimes are generated as the inverse Fourier trans-\\nform of 2-D ﬁlters speciﬁed in the frequency domain. We will give several examples \\nof this approach in this and the next chapter. \\nA third approach is to design a spatial ﬁlter with a speciﬁed frequency response. \\nThis approach is based on the concepts discussed in the previous section, and falls \\nin the area of digital ﬁlter design. A 1-D spatial ﬁlter with the desired response is \\nobtained (typically using ﬁlter design software). The 1-D ﬁlter values can be expressed \\nas a vector \\nv\\n, and a 2-D separable kernel can then be obtained using Eq. (3-42). Or the \\n1-D ﬁlter can be rotated about its center to generate a 2-D kernel that approximates a \\ncircularly symmetric function. We will illustrate these techniques in Section 3.7.\\n3.5 SMOOTHING (LOWPASS) SPATIAL FILTERS  \\nSmoothing\\n (also called \\naveraging\\n) spatial filters are used to reduce sharp transi-\\ntions in intensity. Because random noise typically consists of sharp transitions in \\n3.5\\nDIP4E_GLOBAL_Print_Ready.indb   164\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 165}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n165\\nintensity, an obvious application of smoothing is noise reduction. Smoothing prior \\nto image resampling to reduce aliasing, as will be discussed in Section 4.5, is also \\na common application. Smoothing is used to reduce irrelevant detail in an image, \\nwhere “irrelevant” refers to pixel regions that are small with respect to the size of \\nthe filter kernel. Another application is for smoothing the false contours that result \\nfrom using an insufficient number of intensity levels in an image, as discussed in Sec-\\ntion 2.4. Smoothing filters are used in combination with other techniques for image \\nenhancement, such as the histogram processing techniques discussed in Section 3.3, \\nand unsharp masking, as discussed later in this chapter. We begin the discussion \\nof smoothing filters by considering linear smoothing filters in some detail. We will \\nintroduce nonlinear smoothing filters later in this section.\\nAs we discussed in Section 3.4, linear spatial ﬁltering consists of convolving an \\nimage with a ﬁlter kernel. Convolving a smoothing kernel with an image blurs the \\nimage, with the degree of blurring being determined by the size of the kernel and \\nthe values of its coefﬁcients. In addition to being useful in countless applications of \\nimage processing, lowpass ﬁlters are fundamental, in the sense that other impor-\\ntant ﬁlters, including sharpening (highpass), bandpass, and bandreject ﬁlters, can be \\nderived from lowpass ﬁlters, as we will show in Section 3.7.\\nWe discuss in this section lowpass ﬁlters based on \\nbox\\n and \\nGaussian\\n kernels, \\nboth of which are separable. Most of the discussion will center on Gaussian kernels \\nbecause of their numerous useful properties and breadth of applicability. We will \\nintroduce other smoothing ﬁlters in Chapters 4 and 5.\\nBOX FILTER KERNELS\\nThe simplest, separable lowpass filter kernel is the \\nbox kernel\\n, whose coefficients \\nhave the same value (typically 1). The name “box kernel” comes from a constant \\nkernel resembling a box when viewed in 3-D. We showed a \\n33\\n×\\n box filter in Fig. \\n3.31(a).\\n An \\nmn\\n×\\n box filter is an \\nmn\\n×\\n array of 1’s, with a normalizing constant in \\nfront,\\n whose value is 1 divided by the sum of the values of the coefficients (i.e.,  \\n1\\nmn\\n \\nwhen all the coefficients are 1’\\ns). This normalization, which we apply to all lowpass \\nkernels, has two purposes. First, the average value of an area of constant intensity \\nwould equal that intensity in the filtered image, as it should. Second, normalizing \\nthe kernel in this way prevents introducing a \\nbias\\n during filtering; that is, the sum \\nof the pixels in the original and filtered images will be the same (see Problem 3.31). \\nBecause in a box kernel all rows and columns are identical, the rank of these kernels \\nis 1, which, as we discussed earlier, means that they are separable.\\nEXAMPLE 3.11 :  Lowpass ﬁltering with a box kernel.\\nFigure 3.33(a) shows a test pattern image of size \\n1024 1024\\n×\\n pixels. Figures 3.33(b)-(d) are the results \\nobtained using box ﬁlters of size\\n \\nmm\\n×\\n with \\nm\\n=\\n31\\n1\\n,,\\n and 21, respectively. For \\nm\\n=\\n3,\\n we note a slight \\noverall blurring of the image\\n, with the image features whose sizes are comparable to the size of the \\nkernel being affected signiﬁcantly more. Such features include the thinner lines in the image and the \\nnoise pixels contained in the boxes on the right side of the image. The ﬁltered image also has a thin gray \\nborder, the result of zero-padding the image prior to ﬁltering. As indicated earlier, padding extends the \\nboundaries of an image to avoid undeﬁned operations when parts of a kernel lie outside the border of \\nDIP4E_GLOBAL_Print_Ready.indb   165\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 166}),\n",
       " Document(page_content='166\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nthe image during ﬁltering. When zero (black) padding is used, the net result of smoothing at or near the \\nborder is a dark gray border that arises from including black pixels in the averaging process. Using the \\n11 11\\n×\\n kernel resulted in more pronounced blurring throughout the image, including a more prominent \\ndark border\\n. The result with the \\n21 21\\n×\\n kernel shows signiﬁcant blurring of all components of the image, \\nincluding the loss of the characteristic shape of some components\\n, including, for example, the small \\nsquare on the top left and the small character on the bottom left. The dark border resulting from zero \\npadding is proportionally thicker than before. We used zero padding here, and will use it a few more \\ntimes, so that you can become familiar with its effects. In Example 3.14 we discuss two other approaches \\nto padding that eliminate the dark-border artifact that usually results from zero padding. \\nLOWPASS GAUSSIAN FILTER KERNELS\\nBecause of their simplicity, box filters are suitable for quick experimentation and \\nthey often yield smoothing results that are visually acceptable. They are useful also \\nwhen it is desired to reduce the effect of smoothing on edges (see Example 3.13). \\nHowever, box filters have limitations that make them poor choices in many appli-\\ncations. For example, a defocused lens is often modeled as a lowpass filter, but \\nbox filters are poor approximations to the blurring characteristics of lenses (see \\nProblem 3.33). Another limitation is the fact that box filters favor blurring along \\nperpendicular directions. In applications involving images with a high level of detail, \\nb a\\nd c\\nFIGURE 3.33\\n(a) Test pattern of \\nsize \\n1024 1024\\n×\\n \\npixels\\n.  \\n(b)-(d) Results of \\nlowpass ﬁltering \\nwith box kernels \\nof sizes \\n33\\n×\\n, \\n11 11\\n×\\n,  \\nand \\n21 21\\n×\\n,  \\nrespectively\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   166\\n6/16/2017   2:03:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 167}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n167\\nor with strong geometrical components, the directionality of box filters often pro-\\nduces undesirable results. (Example 3.13 illustrates this issue.) These are but two \\napplications in which box filters are not suitable.\\nThe kernels of choice in applications such as those just mentioned are \\ncircularly \\nsymmetric\\n (also called \\nisotropic\\n, meaning their response is independent of orienta-\\ntion). As it turns out, Gaussian kernels of the form\\n \\nw\\n(,) (,)\\nst Gst K e\\nst\\n==\\n−\\n+\\n22\\n2\\n2\\ns\\n \\n(3-45)\\nare the \\nonly\\n circularly symmetric kernels that are also separable (Sahoo [1990]).\\n \\nThus, because Gaussian kernels of this form are separable, Gaussian filters enjoy the \\nsame computational advantages as box filters, but have a host of additional proper-\\nties that make them ideal for image processing, as you will learn in the following \\ndiscussion. Variables \\ns\\n and \\nt\\n in Eq. (3-45), are real (typically discrete) numbers. \\nBy letting \\nrs t\\n=+\\n[]\\n22\\n12\\n we can write Eq. (3-45) as\\n \\nGr K e\\nr\\n()\\n=\\n−\\n2\\n2\\n2\\ns\\n(3-46)\\nThis equivalent form simplifies derivation of expressions later in this section. This \\nform also reminds us that the function is circularly symmetric\\n. Variable \\nr\\n is the dis-\\ntance from the center to any point on function \\nG\\n. Figure 3.34 shows values of \\nr\\n for \\nseveral kernel sizes using integer values for \\ns\\n and \\nt\\n. Because we work generally with \\nodd kernel sizes, the centers of such kernels fall on integer values, and it follows that \\nall values of \\nr\\n2\\n are integers also. You can see this by squaring the values in Fig. 3.34 \\nOur interest here is \\nstrictly on the bell shape \\nof the Gaussian function; \\nthus, we dispense with \\nthe traditional multiplier \\nof the Gaussian PDF and \\nuse a general constant, \\nK\\n, instead. Recall that \\ns \\ncontrols the “spread” of a \\nGaussian function about \\nits mean.\\nFIGURE 3.34\\nDistances from \\nthe center for  \\nvarious sizes of \\nsquare kernels.\\n0\\n1\\n2\\n22\\n32\\n42\\n5\\n10\\n17\\n13\\n2\\n3\\n4\\n1234\\n25\\n5\\n5\\n10\\n13\\n17\\n25 5\\n1 2 3 4\\n1\\n2\\n3\\n4\\n42\\n17\\n25 5\\n32 5\\n10\\n13\\n22\\n13\\n25\\n5\\n2\\n5\\n10\\n17\\n2\\n5\\n10\\n17\\n22\\n13\\n25\\n5\\n32\\n5\\n10\\n13\\n42\\n17\\n25\\n5\\n2\\n5\\n10\\n17\\n22\\n13\\n25\\n5\\n32\\n5\\n10\\n13\\n42\\n17\\n25\\n5\\n. . . . .\\n. . . . .\\n. . . . .\\n. . . . .\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n*\\nmm\\n9\\n*\\n9\\n33\\n*\\n55\\n*\\n77\\n*\\nDIP4E_GLOBAL_Print_Ready.indb   167\\n6/16/2017   2:03:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 168}),\n",
       " Document(page_content='168\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n(for a formal proof, see Padfield [2011]). Note in particular that the distance squared \\nto the corner points for a kernel of size \\nmm\\n×\\n is\\n \\nr\\nmm\\nmax\\n() ()\\n2\\n2\\n2\\n1\\n2\\n2\\n1\\n2\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n--\\n \\n(3-47)\\nThe kernel in Fig. 3.31(b) was obtained by sampling Eq. (3-45) (with \\nK\\n=\\n1\\n and \\ns\\n=\\n1\\n). Figure 3.35(a) shows a perspective plot of a Gaussian function, and illustrates \\nthat the samples used to generate that kernel were obtained by specifying values of \\ns\\n and \\nt,\\n then \\n“reading” the values of the function at those coordinates. These values \\nare the coefﬁcients of the kernel. Normalizing the kernel by dividing its coefﬁcients \\nby the sum of the coefﬁcients completes the speciﬁcation of the kernel. The reasons \\nfor normalizing the kernel are as discussed in connection with box kernels. Because \\nGaussian kernels are separable, we could simply take samples along a cross section \\nthrough the center and use the samples to form vector \\nv\\n in Eq. (3-42), from which \\nwe obtain the 2-D kernel.\\nSeparability is one of many fundamental properties of circularly symmetric \\nGaussian kernels. For example, we know that the values of a Gaussian function at a \\ndistance larger than \\n3\\ns\\n from the mean are small enough that they can be ignored. \\nT\\nhis means that if we select the size of a Gaussian kernel to be \\nLMLM\\n66\\nss\\n×\\n (the nota-\\ntion \\nLM\\nc\\n is used to denote the \\nceiling\\n of \\nc\\n;\\n that is, the smallest integer not less than \\nc\\n), we are assured of getting essentially the same result as if we had used an arbi-\\ntrarily large Gaussian kernel. Viewed another way, this property tells us that there \\nis nothing to be gained by using a Gaussian kernel larger than \\nLMLM\\n66\\nss\\n×\\n for image \\nprocessing\\n. Because typically we work with kernels of odd dimensions, we would use \\nthe smallest \\nodd\\n integer that satisﬁes this condition (e.g., a \\n43 43\\n×\\n kernel if \\ns\\n=\\n7).\\n \\nT\\nwo other fundamental properties of Gaussian functions are that the product \\nand convolution of two Gaussians are Gaussian functions also. Table 3.6 shows the \\nmean and standard deviation of the product and convolution of two 1-D Gaussian \\nfunctions, \\nf\\n and \\ng \\n(remember, because of separability, we only need a 1-D Gauss-\\nian to form a circularly symmetric 2-D function). The mean and standard deviation \\nSmall Gaussian kernels \\ncannot capture the char-\\nacteristic Gaussian bell \\nshape, and thus behave \\nmore like box kernels. As \\nwe discuss below, a prac-\\ntical size for Gaussian \\nkernels is on the order of \\n6\\ns\\n/H11003\\n6\\ns\\n.\\nAs we explained in \\nSection 2.6, the symbols \\n<\\n⋅\\n= \\nand \\n:\\n⋅\\n;\\n denote the \\nceiling\\n \\nand \\nfloor\\n func-\\ntions\\n. That is, the ceiling \\nand floor functions map \\na real number to the \\nsmallest following, or the \\nlargest previous, integer, \\nrespectively.\\nProofs of the results in \\nTable 3.6 are simpliﬁed \\nby working with the  \\nFourier transform and \\nthe frequency domain, \\nboth of which are topics \\nin Chapter 4.\\n0.3679 0.6065 0.3679\\n1.0000\\n0.6065 0.6065\\n0.3679\\n0.3679 0.6065\\n/H11003\\n4.8976\\n1\\ns\\nt\\n1\\n1\\n1\\n/H11002\\n1\\nG\\n(\\ns\\n, \\nt\\n)\\nb a\\nFIGURE 3.35\\n(a) Sampling a  \\nGaussian function \\nto obtain a discrete  \\nGaussian kernel. \\nThe values shown \\nare for \\nK\\n=\\n1 and \\ns\\n=\\n1. (b) Resulting \\n33\\n×\\n kernel [this \\nis the same as Fig. \\n3.31(b)]. \\nDIP4E_GLOBAL_Print_Ready.indb   168\\n6/16/2017   2:03:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 169}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n169\\ncompletely deﬁne a Gaussian, so the parameters in Table 3.6 tell us all there is to \\nknow about the functions resulting from multiplication and convolution of Gauss-\\nians. As indicated by Eqs. (3-45) and (3-46), Gaussian kernels have zero mean, so our \\ninterest here is in the standard deviations. \\nThe convolution result is of particular importance in ﬁltering. For example, we \\nmentioned in connection with Eq. (3-43) that ﬁltering sometimes is done in succes-\\nsive stages, and that the same result can be obtained by one stage of ﬁltering with a \\ncomposite kernel formed as the convolution of the individual kernels. If the kernels \\nare Gaussian, we can use the result in Table 3.6 (which, as noted, generalizes directly \\nto more than two functions) to compute the standard deviation of the composite \\nkernel (and thus completely deﬁne it) without actually having to perform the con-\\nvolution of all the individual kernels.\\nEXAMPLE 3.12 :  Lowpass ﬁltering with a Gaussian kernel.\\nTo compare Gaussian and box kernel ﬁltering, we repeat Example 3.11 using a Gaussian kernel.  Gauss-\\nian kernels have to be larger than box ﬁlters to achieve the same degree of blurring. This is because, \\nwhereas a box kernel assigns the same weight to all pixels, the values of Gaussian kernel coefﬁcients \\n(and hence their effect) decreases as a function of distance from the kernel center. As explained earlier, \\nwe use a size equal to the closest odd integer to \\nLMLM\\n66\\nss\\n×\\n.\\n Thus, for a Gaussian kernel of size \\n21 21\\n×\\n, \\nwhich is the size of the kernel we used to generate F\\nig. 3.33(d), we need \\ns\\n=\\n35\\n..\\n Figure 3.36(b) shows the \\nresult of lowpass ﬁltering the test pattern with this kernel.\\n Comparing this result with Fig. 3.33(d), we see \\nthat the Gaussian kernel resulted in signiﬁcantly less blurring. A little experimentation would show that \\nwe need \\ns\\n=\\n7\\n to obtain comparable results. This implies a Gaussian kernel of size \\n43 43\\n×\\n.\\n Figure 3.36(c) \\nshows the results of ﬁltering the test pattern with this kernel.\\n Comparing it with Fig. 3.33(d), we see that \\nthe results indeed are very close. \\nWe mentioned earlier that there is little to be gained by using a Gaussian kernel larger than \\nLMLM\\n66\\nss\\n×\\n. \\nT\\no demonstrate this, we ﬁltered the test pattern in Fig. 3.36(a) using a Gaussian kernel with \\ns\\n=\\n7\\n again, \\nbut of size \\n85 85\\n×\\n.\\n Figure 3.37(a) is the same as Fig. 3.36(c), which we generated using the smallest \\nodd kernel satisfying the \\nLM LM\\n66\\n×\\n condition (\\n43 43\\n×\\n,\\n for \\ns\\n=\\n7).\\n Figure 3.37(b) is the result of using the \\n85 85\\n×\\n kernel, which is double the size of the other kernel. As you can see, not discernible additional \\nfg\\nfg\\n×\\nf\\ng\\n/H22841\\nMean\\nStandard deviation\\ns\\nf\\ns\\ng\\nm\\nf\\nm\\ng\\nm\\nmm\\nfg\\nfg\\ngf\\nfg\\n×\\n=\\n+\\n+\\nss\\nss\\n22\\n22\\nmm m\\nfg f g\\n/H22841\\n=+\\ns\\nss\\nss\\nfg\\nfg\\nfg\\n×\\n=\\n+\\n22\\n22\\nss s\\nfg f g\\n/H22841\\n=+\\n22\\nTABLE \\n3.6\\n \\nMean and standard deviation of the product \\n()\\n×\\n and convolution \\n(\\n)\\n/H22841\\n of two 1-D Gaussian functions, \\nf\\n \\nand \\ng\\n. These results generalize directly to the product and convolution of more than two 1-D Gaussian functions \\n(see Problem 3.25).\\nDIP4E_GLOBAL_Print_Ready.indb   169\\n6/16/2017   2:03:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 170}),\n",
       " Document(page_content='170\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nblurring occurred. In fact, the difference image in Fig 3.37(c) indicates that the two images are nearly \\nidentical, their maximum difference being 0.75, which is less than one level out of 256 (these are 8-bit \\nimages). \\n EXAMPLE 3.13 :  Comparison of Gaussian and box ﬁlter smoothing characteristics.\\nThe results in Examples 3.11 and 3.12 showed little visual difference in blurring. Despite this, there are \\nsome subtle differences that are not apparent at ﬁrst glance. For example, compare the large letter “a” \\nin Figs. 3.33(d) and 3.36(c); the latter is much smoother around the edges. Figure 3.38 shows this type \\nof different behavior between box and Gaussian kernels more clearly. The image of the rectangle was \\nb a\\nc\\n \\nFIGURE 3.36\\n  (a)A test pattern of size \\n1024 1024\\n×\\n. (b) Result of lowpass ﬁltering the pattern with a Gaussian kernel \\nof size \\n21 21\\n×\\n,\\n with standard deviations \\ns\\n=\\n35\\n..\\n (c) Result of using a kernel of size \\n43 43\\n×\\n,\\n with \\ns\\n=\\n7.\\n This result \\nis comparable to F\\nig. 3.33(d). We used \\nK\\n=\\n1 in all cases.\\nb a\\nc\\nFIGURE 3.37\\n (a) Result of ﬁltering Fig. 3.36(a) using a Gaussian kernels of size \\n43 43\\n×\\n,\\n with \\ns\\n=\\n7.\\n (b) Result of using \\na kernel of \\n85 85\\n×\\n, with the same value of \\ns\\n. (c) Difference image.\\nDIP4E_GLOBAL_Print_Ready.indb   170\\n6/16/2017   2:03:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 171}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n171\\nsmoothed using a box and a Gaussian kernel with the sizes and parameters listed in the ﬁgure. These \\nparameters were selected to give blurred rectangles of approximately the same width and height, in \\norder to show the effects of the ﬁlters on a comparable basis. As the intensity proﬁles show, the box ﬁlter \\nproduced linear smoothing, with the transition from black to white (i.e., at an edge) having the shape \\nof a ramp. The important features here are hard transitions at the onset and end of the ramp. We would \\nuse this type of ﬁlter when less smoothing of edges is desired. Conversely, the Gaussian ﬁlter yielded \\nsigniﬁcantly smoother results around the edge transitions. We would use this type of ﬁlter when gener-\\nally uniform smoothing is desired.\\nAs the results in Examples 3.11, 3.12, and 3.13 show, zero padding an image intro-\\nduces dark borders in the ﬁltered result, with the thickness of the borders depending \\non the size and type of the ﬁlter kernel used. Earlier, when discussing correlation \\nand convolution, we mentioned two other methods of image padding: \\nmirror \\n(also \\ncalled\\n symmetric\\n) \\npadding\\n, in which values outside the boundary of the image are \\nobtained by mirror-reﬂecting the image across its border; and \\nreplicate padding\\n, in \\nwhich values outside the boundary are set equal to the nearest image border value. \\nThe latter padding is useful when the areas near the border of the image are con-\\nstant. Conversely, mirror padding is more applicable when the areas near the border \\ncontain image details. In other words, these two types of padding attempt to “extend” \\nthe characteristics of an image past its borders. \\nFigure 3.39 illustrates these padding methods, and also shows the effects of more \\naggressive smoothing. Figures 3.39(a) through 3.39(c) show the results of ﬁltering \\nFig. 3.36(a) with a Gaussian kernel of size \\n187 187\\n×\\n elements with \\nK\\n=\\n1\\n and \\ns\\n=\\n31\\n,\\n \\nusing zero\\n, mirror, and replicate padding, respectively. The differences between the \\nborders of the results with the zero-padded image and the other two are obvious, \\nb a\\nc\\n \\nFIGURE 3.38\\n (a) Image of a white rectangle on a black background, and a horizontal intensity proﬁle along the scan \\nline shown dotted. (b) Result of smoothing this image with a box kernel of size \\n71 71\\n×\\n,\\n and corresponding intensity \\nproﬁle\\n. (c) Result of smoothing the image using a Gaussian kernel of size \\n151 151\\n×\\n,\\n with \\nK\\n=\\n1\\n and \\ns\\n=\\n25.\\n Note \\nthe smoothness of the proﬁle in (c) compared to (b).\\n The image and rectangle are of sizes \\n1024 1024\\n×\\n and \\n768 128\\n×\\n \\npixels\\n, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   171\\n6/16/2017   2:03:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 172}),\n",
       " Document(page_content='172\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nand indicate that mirror and replicate padding yield more visually appealing results \\nby eliminating the dark borders resulting from zero padding.\\nEXAMPLE 3.14 :  Smoothing performance as a function of kernel and image size.\\nThe amount of relative blurring produced by a smoothing kernel of a given size depends directly on \\nimage size. To illustrate, Fig. 3.40(a) shows the same test pattern used earlier, but of size \\n4096 4096\\n×\\n \\npixels\\n, four times larger in each dimension than before. Figure 3.40(b) shows the result of ﬁltering this \\nimage with the same Gaussian kernel and padding used in Fig. 3.39(b). By comparison, the former \\nimage shows considerably less blurring for the same size ﬁlter. In fact, Fig. 3.40(b) looks more like the \\nb a\\nc\\nFIGURE 3.39\\n Result of ﬁltering the test pattern in Fig. 3.36(a) using (a) zero padding, (b) mirror padding, and (c) rep-\\nlicate padding. A Gaussian kernel of size \\n187 187\\n×\\n, with \\nK\\n=\\n1 and \\ns\\n=\\n31\\n was used in all three cases.\\nb a\\nc\\nFIGURE 3.40\\n (a) Test pattern of size \\n4096 4096\\n×\\n pixels. (b) Result of ﬁltering the test pattern with the same Gaussian \\nkernel used in F\\nig. 3.39. (c) Result of ﬁltering the pattern using a Gaussian kernel of size \\n745 745\\n×\\n elements, with \\nK\\n=\\n1 and \\ns\\n=\\n124.\\n Mirror padding was used throughout. \\nDIP4E_GLOBAL_Print_Ready.indb   172\\n6/16/2017   2:03:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 173}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n173\\nimage in Fig. 3.36(d), which was ﬁltered using a \\n43 43\\n×\\n Gaussian kernel. In order to obtain results that \\nare comparable to F\\nig. 3.39(b) we have to increase the size and standard deviation of the Gaussian \\nkernel by four, the same factor as the increase in image dimensions. This gives a kernel of (odd) size \\n745 745\\n×\\n (with \\nK\\n=\\n1\\n and \\ns\\n=\\n124).\\n Figure 3.40(c) shows the result of using this kernel with mirror pad-\\nding\\n. This result is quite similar to Fig. 3.39(b). After the fact, this may seem like a trivial observation, but \\nyou would be surprised at how frequently not understanding the relationship between kernel size and \\nthe size of objects in an image can lead to ineffective performance of spatial ﬁltering algorithms. \\nEXAMPLE 3.15 :  Using lowpass ﬁltering and thresholding for region extraction.\\nFigure 3.41(a) is a \\n2566 2758\\n×\\n Hubble Telescope image of the \\nHic\\nkson Compact Group\\n (see ﬁgure \\ncaption), whose intensities were scaled to the range \\n[,] .\\n01\\n Our objective is to illustrate lowpass ﬁltering \\ncombined with intensity thresholding for eliminating irrelevant detail in this image\\n. In the present con-\\ntext, “irrelevant” refers to pixel regions that are small compared to kernel size.\\nFigure 3.41(b) is the result of ﬁltering the original image with a Gaussian kernel of size \\n151 151\\n×\\n \\n(approximately 6% of the image width) and standard deviation \\ns\\n=\\n25\\n. We chose these parameter val-\\nues in order generate a sharper\\n, more selective Gaussian kernel shape than we used in earlier examples. \\nThe ﬁltered image shows four predominantly bright regions. We wish to extract only those regions from \\nthe image. Figure 3.41(c) is the result of thresholding the ﬁltered image with a threshold \\nT\\n=\\n04\\n.\\n (we will \\ndiscuss threshold selection in\\n \\nChapter 10). As the ﬁgure shows, this approach effectively extracted the \\nfour regions of interest,\\n and eliminated details deemed irrelevant in this application.\\nEXAMPLE 3.16 :  Shading correction using lowpass ﬁltering.\\nOne of the principal causes of image shading is nonuniform illumination. \\nShading correction\\n (also \\ncalled \\nﬂat-ﬁeld correction\\n) is important because shading is a common cause of erroneous measurements, \\ndegraded performance of automated image analysis algorithms, and difﬁculty of image interpretation \\nb a\\nc\\nFIGURE 3.41\\n (a) A \\n2566 2758\\n×\\n Hubble Telescope image of the \\nHic\\nkson Compact Group\\n. (b) Result of lowpass ﬁlter-\\ning with a Gaussian kernel. (c) Result of thresholding the ﬁltered image (intensities were scaled to the range [0, 1]). \\nThe Hickson Compact Group contains dwarf galaxies that have come together, setting off thousands of new star \\nclusters. (Original image courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   173\\n6/16/2017   2:03:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 174}),\n",
       " Document(page_content='174\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nby humans. We introduced shading correction in Example 2.7, where we corrected a shaded image by \\ndividing it by the shading pattern. In that example, the shading pattern was given. Often, that is not the \\ncase in practice, and we are faced with having to estimate the pattern directly from available samples of \\nshaded images. Lowpass ﬁltering is a rugged, simple method for estimating shading patterns.\\nConsider the \\n2048 2048\\n×\\n checkerboard image in Fig. 3.42(a), whose inner squares are of size \\n128 128\\n×\\n \\npixels\\n. Figure 3.42(b) is the result of lowpass ﬁltering the image with a \\n512 512\\n×\\n Gaussian kernel (four \\ntimes the size of the squares),\\n \\nK\\n=\\n1,\\n and \\ns\\n=\\n128\\n (equal to the size of the squares). This kernel is just \\nlarge enough to blur\\n-out the squares (a kernel three times the size of the squares is too small to blur \\nthem out sufﬁciently). This result is a good approximation to the shading pattern visible in Fig. 3.42(a). \\nFinally, Fig. 3.42(c) is the result of dividing (a) by (b). Although the result is not perfectly ﬂat, it deﬁnitely \\nis an improvement over the shaded image. \\nIn the discussion of separable kernels in Section 3.4, we pointed out that the computational advan-\\ntage of separable kernels can be signiﬁcant for large kernels. It follows from Eq. (3-44) that the compu-\\ntational advantage of the kernel used in this example (which of course is separable) is 262 to 1. Thinking \\nof computation time, if it took 30 sec to process a set of images similar to Fig. 3.42(b) using the two 1-D \\nseparable components of the Gaussian kernel, it would have taken 2.2 hrs to achieve the same result \\nusing a nonseparable lowpass kernel, or if we had used the 2-D Gaussian kernel directly, without decom-\\nposing it into its separable parts.\\nORDER-STATISTIC (NONLINEAR) FILTERS\\nOrder-statistic filters are nonlinear spatial filters whose response is based on ordering \\n(ranking) the pixels contained in the region encompassed by the filter. Smoothing is \\nachieved by replacing the value of the center pixel with the value determined by the \\nranking result. The best-known filter in this category is the \\nmedian filter\\n, which, as \\nits name implies, replaces the value of the center pixel by the median of the intensity \\nvalues in the neighborhood of that pixel (the value of the center pixel is included \\nb a\\nc\\nFIGURE 3.42\\n (a) Image shaded by a shading pattern oriented in the \\n−\\n45\\n°\\n direction. (b) Estimate of the shading \\npatterns obtained using lowpass ﬁltering\\n. (c) Result of dividing (a) by (b). (See Section 9.8 for a morphological \\napproach to shading correction).\\nDIP4E_GLOBAL_Print_Ready.indb   174\\n6/16/2017   2:03:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 175}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n175\\nin computing the median). Median filters provide excellent noise reduction capa-\\nbilities for certain types of random noise, with considerably less blurring than lin-\\near smoothing filters of similar size. Median filters are particularly effective in the \\npresence of \\nimpulse noise\\n (sometimes called \\nsalt-and-pepper noise,\\n when it manis-\\nfests itself as white and black dots superimposed on an image).\\nThe \\nmedian\\n, \\nj\\n,\\n of a set of values is such that half the values in the set are less than \\nor equal to \\nj\\n and half are greater than or equal to \\nj\\n.\\n In order to perform median \\nﬁltering at a point in an image\\n, we ﬁrst sort the values of the pixels in the neighbor-\\nhood, determine their median, and assign that value to the pixel in the ﬁltered image \\ncorresponding to the center of the neighborhood. For example, in a \\n33\\n×\\n neighbor-\\nhood the median is the 5th largest value\\n, in a \\n55\\n×\\n neighborhood it is the 13th largest \\nvalue\\n, and so on. When several values in a neighborhood are the same, all equal val-\\nues are grouped. For example, suppose that a \\n33\\n×\\n neighborhood has values (10, 20, \\n20,\\n 20, 15, 20, 20, 25, 100). These values are sorted as (10, 15, 20, 20, 20, 20, 20, 25, 100), \\nwhich results in a median of 20. Thus, the principal function of median ﬁlters is to \\nforce points to be more like their neighbors. Isolated clusters of pixels that are light \\nor dark with respect to their neighbors, and whose area is less than \\nm\\n2\\n2\\n (one-half \\nthe ﬁlter area),\\n are forced by an \\nmm\\n×\\n median ﬁlter to have the value of the median \\nintensity of the pixels in the neighborhood (see Problem 3.36).\\nT\\nhe median ﬁlter is by far the most useful order-statistic ﬁlter in image processing, \\nbut is not the only one. The median represents the 50th percentile of a ranked set \\nof numbers, but ranking lends itself to many other possibilities. For example, using \\nthe 100th percentile results in the so-called \\nmax ﬁlter\\n, which is useful for ﬁnding the \\nbrightest points in an image or for eroding dark areas adjacent to light regions. The \\nresponse of a \\n33\\n×\\n max ﬁlter is given by \\nRz k\\nk\\n==\\n{}\\nmax , , , , .\\n123 9\\n…\\n The 0th per-\\ncentile ﬁlter is the \\nmin ﬁlter\\n,\\n used for the opposite purpose. Median, max, min, and \\nseveral other nonlinear ﬁlters will be considered in more detail in Section 5.3.\\nEXAMPLE 3.17 :  Median ﬁltering.\\nFigure 3.43(a) shows an X-ray image of a circuit board heavily corrupted by salt-and-pepper noise. To \\nillustrate the superiority of median ﬁltering over lowpass ﬁltering in situations such as this, we show in \\nFig. 3.43(b) the result of ﬁltering the noisy image with a Gaussian lowpass ﬁlter, and in Fig. 3.43(c) the \\nresult of using a median ﬁlter. The lowpass ﬁlter blurred the image and its noise reduction performance \\nwas poor. The superiority in all respects of median over lowpass ﬁltering in this case is evident.\\n3.6 SHARPENING (HIGHPASS) SPATIAL FILTERS  \\nSharpening highlights transitions in intensity. Uses of image sharpening range from \\nelectronic printing and medical imaging to industrial inspection and autonomous \\nguidance in military systems. In Section 3.5, we saw that image blurring could be \\naccomplished in the spatial domain by pixel averaging (smoothing) in a neighbor-\\nhood. Because averaging is analogous to integration, it is logical to conclude that \\nsharpening can be accomplished by spatial differentiation. In fact, this is the case, \\nand the following discussion deals with various ways of defining and implementing \\noperators for sharpening by digital differentiation. The strength of the response of \\n3.6\\nDIP4E_GLOBAL_Print_Ready.indb   175\\n6/16/2017   2:03:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 176}),\n",
       " Document(page_content='176\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\na derivative operator is proportional to the magnitude of the intensity discontinuity \\nat the point at which the operator is applied. Thus, image differentiation enhances \\nedges and other discontinuities (such as noise) and de-emphasizes areas with slowly \\nvarying intensities. As noted in Section 3.5, smoothing is often referred to as lowpass \\nfiltering, a term borrowed from frequency domain processing. In a similar manner, \\nsharpening is often referred to as \\nhighpass \\nfiltering. In this case, high frequencies \\n(which are responsible for fine details) are passed, while low frequencies are attenu-\\nated or rejected.\\nFOUNDATION\\nIn the two sections that follow, we will consider in some detail sharpening filters that \\nare based on first- and second-order derivatives, respectively. Before proceeding \\nwith that discussion, however, we stop to look at some of the fundamental properties \\nof these derivatives in a digital context. To simplify the explanation, we focus atten-\\ntion initially on one-dimensional derivatives. In particular, we are interested in the \\nbehavior of these derivatives in areas of constant intensity, at the onset and end of \\ndiscontinuities (\\nstep\\n and \\nramp\\n \\ndiscontinuities\\n), and along \\nintensity ramps\\n. As you will \\nsee in Chapter 10, these types of discontinuities can be used to model noise points, \\nlines, and edges in an image.\\nDerivatives of a digital function are deﬁned in terms of differences. There are \\nvarious ways to deﬁne these differences. However, we require that any deﬁnition we \\nuse for a \\nﬁrst derivative:\\n \\n1. \\nMust be zero in areas of constant intensity.\\n2. \\nMust be nonzero at the onset of an intensity step or ramp. \\n3. \\nMust be nonzero along intensity ramps. \\nSimilarly\\n, any definition of a \\nsecond derivative\\n \\nb a\\nc\\nFIGURE 3.43\\n (a) X-ray image of a circuit board, corrupted by salt-and-pepper noise. (b) Noise reduction using a \\n19 19\\n×\\n Gaussian lowpass ﬁlter kernel with \\ns\\n=\\n3.\\n (c) Noise reduction using a \\n77\\n×\\n median ﬁlter. (Original image \\ncourtesy of Mr. Joseph E. Pascente, Lixi, Inc.)\\nDIP4E_GLOBAL_Print_Ready.indb   176\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 177}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n177\\n1. \\nMust be zero in areas of constant intensity. \\n2. \\nMust be nonzero at the onset \\nand\\n end of an intensity step or ramp\\n.\\n3. \\nMust be zero along intensity ramps. \\nW\\ne are dealing with digital quantities whose values are finite. Therefore, the maxi-\\nmum possible intensity change also is finite, and the shortest distance over which \\nthat change can occur is between adjacent pixels. \\nA basic deﬁnition of the \\nﬁrst-order\\n derivative of a one-dimensional function \\nfx\\n()\\n \\nis the difference\\n \\n∂\\n∂\\n=+ −\\nf\\nx\\nfx fx\\n() ( )\\n1\\n \\n(3-48)\\nWe used a partial derivative here in order to keep the notation consistent when we \\nconsider an image function of two variables\\n, \\nfx y\\n(,\\n)\\n, at which time we will be deal-\\ning with partial derivatives along the two spatial axes\\n. Clearly, \\n∂∂ =\\nf x df dx\\n when \\nthere is only one variable in the function;\\n the same is true for the second derivative.\\nWe deﬁne the \\nsecond-order\\n derivative of \\nfx\\n()\\n as the difference\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\nx\\nfx fx fx\\n() ()( )\\n \\n(3-49)\\nThese two definitions satisfy the conditions stated above, as we illustrate in Fig. 3.44, \\nwhere we also examine the similarities and differences between first- and second-\\norder derivatives of a digital function.\\nT\\nhe values denoted by the small squares in Fig. 3.44(a) are the intensity values \\nalong a horizontal intensity proﬁle (the dashed line connecting the squares is includ-\\ned to aid visualization). The actual numerical values of the scan line are shown inside \\nthe small boxes in 3.44(b). As Fig. 3.44(a) shows, the scan line contains three sections \\nof constant intensity, an intensity ramp, and an intensity step. The circles indicate the \\nonset or end of intensity transitions. The ﬁrst- and second-order derivatives, com-\\nputed using the two preceding deﬁnitions, are shown below the scan line values in \\nFig. 3.44(b), and are plotted in Fig. 3.44(c).When computing the ﬁrst derivative at a \\nlocation \\nx\\n, we subtract the value of the function at that location from the next point, \\nas indicated in Eq. (3-48), so this is a “look-ahead” operation. Similarly, to compute \\nthe second derivative at \\nx\\n, we use the previous and the next points in the computa-\\ntion, as indicated in Eq. (3-49). To avoid a situation in which the previous or next \\npoints are outside the range of the scan line, we show derivative computations in Fig. \\n3.44 from the second through the penultimate points in the sequence.\\nAs we traverse the proﬁle from left to right we encounter ﬁrst an area of constant \\nintensity and, as Figs. 3.44(b) and (c) show, both derivatives are zero there, so condi-\\ntion (1) is satisﬁed by both. Next, we encounter an intensity ramp followed by a step, \\nand we note that the ﬁrst-order derivative is nonzero at the onset of the ramp and \\nthe step; similarly, the second derivative is nonzero at the onset and end of both the \\nramp and the step; therefore, property (2) is satisﬁed by both derivatives. Finally, we \\nWe will return to Eq. \\n(3-48) in Section 10.2 and \\nshow how it follows from \\na Taylor series expansion. \\nFor now, we accept it as a \\ndeﬁnition.\\nDIP4E_GLOBAL_Print_Ready.indb   177\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 178}),\n",
       " Document(page_content='178\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nsee that property (3) is satisﬁed also by both derivatives because the ﬁrst derivative \\nis nonzero and the second is zero along the ramp. Note that the sign of the second \\nderivative changes at the onset and end of a step or ramp. In fact, we see in Fig. \\n3.44(c) that in a step transition a line joining these two values crosses the horizontal \\naxis midway between the two extremes. This \\nzero crossing\\n property is quite useful \\nfor locating edges, as you will see in Chapter 10. \\nEdges in digital images often are ramp-like transitions in intensity, in which case \\nthe ﬁrst derivative of the image would result in thick edges because the derivative \\nis nonzero along a ramp. On the other hand, the second derivative would produce a \\ndouble edge one pixel thick, separated by zeros. From this, we conclude that the sec-\\nond derivative enhances ﬁne detail much better than the ﬁrst derivative, a property \\nideally suited for sharpening images. Also, second derivatives require fewer opera-\\ntions to implement than ﬁrst derivatives, so our initial attention is on the former. \\nUSING THE SECOND DERIVATIVE FOR IMAGE SHARPENING—THE \\nLAPLACIAN\\nIn this section we discuss the implementation of 2-D, second-order derivatives and \\ntheir use for image sharpening. The approach consists of defining a discrete formu-\\nlation of the second-order derivative and then constructing a filter kernel based on \\nWe will return to the \\nsecond derivative in \\nChapter 10, where we use \\nit extensively for image \\nsegmentation.\\nb\\na\\nc\\nFIGURE 3.44\\n(a) A section of a  \\nhorizontal scan \\nline from an \\nimage, showing \\nramp and step \\nedges, as well as \\nconstant  \\nsegments. \\n(b)Values of the \\nscan line and its \\nderivatives. \\n(c) Plot of the \\nderivatives, show-\\ning a zero cross-\\ning. In (a) and (c) \\npoints were joined \\nby dashed lines as \\na visual aid.\\n4\\n5\\n6\\n3\\n2\\n1\\n0\\nConstant\\nintensity\\nValues of\\nscan line\\n1st derivative\\n2nd derivative\\nIntensity transition\\nIntensity\\nRamp\\nStep\\nx\\nx\\n66\\n00\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n10\\n0\\n000500\\n00\\n/H11002\\n1\\n0\\nZero crossing\\nFirst derivative\\nSecond derivative\\nIntensity\\nx\\n5\\n4\\n3\\n2\\n1\\n0\\n/H11002\\n5\\n/H11002\\n4\\n/H11002\\n3\\n/H11002\\n2\\n/H11002\\n1\\n00\\n0\\n00 1\\n0\\n0005\\n/H11002\\n50\\n00\\n/H11002\\n1\\n6543211111166666\\n6\\nData points\\nDIP4E_GLOBAL_Print_Ready.indb   178\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 179}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n179\\nthat formulation. As in the case of Gaussian lowpass kernels in Section 3.5, we are \\ninterested here in isotropic kernels, whose response is independent of the direction \\nof intensity discontinuities in the image to which the filter is applied. \\nIt can be shown (Rosenfeld and Kak [1982]) that the simplest isotropic deriva-\\ntive operator (kernel) is the \\nLaplacian\\n, which, for a function (image) \\nfx y\\n(,\\n)\\n of two \\nvariables\\n, is deﬁned as\\n \\n/H11612\\n2\\n2\\n2\\n2\\n2\\nf\\nf\\nx\\nf\\ny\\n=\\n∂\\n∂\\n+\\n∂\\n∂\\n \\n(3-50)\\nBecause derivatives of any order are linear operations, the Laplacian is a linear oper-\\nator\\n. To express this equation in discrete form, we use the definition in Eq. (3-49), \\nkeeping in mind that we now have a second variable. In the \\nx\\n-direction, we have\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\nx\\nfx y fx y fx y\\n(, ) (, ) ( , )\\n \\n(3-51)\\nand, similarly, in the \\ny\\n-direction,\\n we have\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\ny\\nfx y fx y fx y\\n(, ) (, ) (, )\\n  \\n(3-52)\\nIt follows from the preceding three equations that the discrete Laplacian of two \\nvariables is\\n    \\n/H11612\\n2\\n11 11 4\\nfx y fx y fx y fx y fx y fx y\\n( , )( , )( , )( , )( , ) ( , )\\n=+ +− + + + − −\\n \\n(3-53)\\nThis equation can be implemented using convolution with the kernel in Fig. 3.45(a); \\nthus\\n, the filtering mechanics for image sharpening are as described in Section 3.5 for \\nlowpass filtering; we are simply using different coefficients here.\\nThe kernel in Fig. 3.45(a) is isotropic for rotations in increments of 90° with respect \\nto the \\nx\\n- and \\ny\\n-axes. The diagonal directions can be incorporated in the deﬁnition of \\nthe digital Laplacian by adding four more terms to Eq. (3-53). Because each diagonal \\nterm would contains a \\n−\\n2\\nfx\\ny\\n(, )\\n term, the total subtracted from the difference terms \\nb\\na\\nc\\nd\\nFIGURE 3.45\\n  (a) Laplacian kernel used to implement Eq. (3-53). (b) Kernel used to implement \\nan extension of this equation that includes the diagonal terms. (c) and (d) Two other Lapla-\\ncian kernels.\\n/H11002\\n4 1\\n00\\n0\\n0\\n1\\n1\\n1\\n/H11002\\n8 1\\n11\\n1\\n1\\n1\\n1\\n1\\n4\\n/H11002\\n1\\n00\\n0\\n0\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n8\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   179\\n6/16/2017   2:04:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 180}),\n",
       " Document(page_content='180\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nnow would be \\n−\\n8\\nfx\\ny\\n(, ) .\\n Figure 3.45(b) shows the kernel used to implement this \\nnew deﬁnition.\\n This kernel yields isotropic results in increments of 45°. The kernels \\nin Figs. 3.45(c) and (d) also are used to compute the Laplacian. They are obtained \\nfrom deﬁnitions of the second derivatives that are the negatives of the ones we used \\nhere. They yield equivalent results, but the difference in sign must be kept in mind \\nwhen combining a Laplacian-ﬁltered image with another image.\\nBecause the Laplacian is a derivative operator, it highlights sharp intensity tran-\\nsitions in an image and de-emphasizes regions of slowly varying intensities. This \\nwill tend to produce images that have grayish edge lines and other discontinuities, \\nall superimposed on a dark, featureless background. Background features can be \\n“recovered” while still preserving the sharpening effect of the Laplacian by adding \\nthe Laplacian image to the original. As noted in the previous paragraph, it is impor-\\ntant to keep in mind which deﬁnition of the Laplacian is used. If the deﬁnition used \\nhas a negative center coefﬁcient, then we \\nsubtract\\n the Laplacian image from the \\noriginal to obtain a sharpened result. Thus, the basic way in which we use the Lapla-\\ncian for image sharpening is\\n \\ngxy f xy c f xy\\n(,\\n) (, ) (, )\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11612\\n2\\n \\n(3-54)\\nwhere \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n are the input and sharpened images, respectively. We let \\nc\\n=−\\n1\\n if the Laplacian kernels in Fig. 3.45(a) or (b) is used, and \\nc\\n=\\n1\\n if either of the \\nother two kernels is used.\\nEXAMPLE 3.18 :  Image sharpening using the Laplacian.\\nFigure 3.46(a) shows a slightly blurred image of the North Pole of the moon, and Fig. 3.46(b) is the result \\nof ﬁltering this image with the Laplacian kernel in Fig. 3.45(a) directly. Large sections of this image are \\nblack because the Laplacian image contains both positive and negative values, and all negative values \\nare clipped at 0 by the display. \\nFigure 3.46(c) shows the result obtained using Eq. (3-54), with \\nc\\n=−\\n1,\\n because we used the kernel in \\nF\\nig. 3.45(a) to compute the Laplacian. The detail in this image is unmistakably clearer and sharper than \\nin the original image. Adding the Laplacian to the original image restored the overall intensity varia-\\ntions in the image. Adding the Laplacian increased the contrast at the locations of intensity discontinui-\\nties. The net result is an image in which small details were enhanced and the background tonality was \\nreasonably preserved. Finally, Fig. 3.46(d) shows the result of repeating the same procedure but using \\nthe kernel in Fig. 3.45(b). Here, we note a signiﬁcant improvement in sharpness over Fig. 3.46(c). This is \\nnot unexpected because using the kernel in Fig. 3.45(b) provides additional differentiation (sharpening) \\nin the diagonal directions. Results such as those in Figs. 3.46(c) and (d) have made the Laplacian a tool \\nof choice for sharpening digital images.\\nBecause Laplacian images tend to be dark and featureless, a typical way to scale these images for dis-\\nplay is to use Eqs. (2-31) and (2-32). This brings the most negative value to 0 and displays the full range \\nof intensities. Figure 3.47 is the result of processing Fig. 3.46(b) in this manner. The dominant features of \\nthe image are edges and sharp intensity discontinuities. The background, previously black, is now gray as \\na result of scaling. This grayish appearance is typical of Laplacian images that have been scaled properly.\\nDIP4E_GLOBAL_Print_Ready.indb   180\\n6/16/2017   2:04:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 181}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n181\\nObserve in Fig. 3.45 that the coefﬁcients of each kernel sum to zero. Convolution-\\nbased ﬁltering implements a sum of products, so when a derivative kernel encom-\\npasses a constant region in a image, the result of convolution in that location must be \\nzero. Using kernels whose coefﬁcients sum to zero accomplishes this. \\nIn Section 3.5, we normalized smoothing kernels so that the sum of their coef-\\nﬁcients would be one. Constant areas in images ﬁltered with these kernels would \\nbe constant also in the ﬁltered image. We also found that the sum of the pixels in \\nthe original and ﬁltered images were the same, thus preventing a bias from being \\nintroduced by ﬁltering (see Problem 3.31). When convolving an image with a kernel \\nb a\\nd c\\nFIGURE 3.46\\n(a) Blurred  \\nimage of the \\nNorth Pole of the \\nmoon.  \\n(b) Laplacian  \\nimage obtained \\nusing the kernel \\nin Fig. 3.45(a).  \\n(c) Image  \\nsharpened  \\nusing Eq. (3-54) \\nwith \\nc\\n=−\\n1. \\n(d) Image  \\nsharpened using \\nthe same  \\nprocedure\\n, but \\nwith the kernel \\nin Fig. 3.45(b). \\n(Original  \\nimage courtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   181\\n6/16/2017   2:04:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 182}),\n",
       " Document(page_content='182\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nwhose coefﬁcients sum to zero, it turns out that the pixels of the ﬁltered image \\nwill \\nsum to zero\\n \\nalso\\n (see Problem 3.32). This implies that images ﬁltered with such ker-\\nnels will have negative values, and sometimes will require additional processing to \\nobtain suitable visual results. Adding the ﬁltered image to the original, as we did in \\nEq. (3-54), is an example of such additional processing. \\nUNSHARP MASKING AND HIGHBOOST FILTERING\\nSubtracting an unsharp (smoothed) version of an image from the original image is \\nprocess that has been used since the 1930s by the printing and publishing industry to \\nsharpen images. This process, called \\nunsharp masking\\n, consists of the following steps:\\n1. \\nBlur the original image.\\n2. \\nSubtract the blurred image from the original (the resulting difference is called \\nthe \\nmask\\n.)\\n3. \\nAdd the mask to the original.\\nLetting \\nfx y\\n(, )\\n denote the blurred image, the mask in equation form is given by:\\n \\ngx y f x y f x y\\nmask\\n(, ) (, ) (, )\\n=−\\n \\n(3-55)\\nThen we add a weighted portion of the mask back to the original image:\\n \\ngxy f xy k g xy\\n(,\\n) (, ) (, )\\n=+\\nmask\\n \\n(3-56)\\nThe photographic pro-\\ncess of unsharp masking \\nis based on creating a \\nblurred positive and \\nusing it along with the \\noriginal negative to \\ncreate a sharper image. \\nOur interest is in the \\ndigital\\n equivalent of this \\nprocess.\\nFIGURE 3.47\\nThe Laplacian  \\nimage from  \\nFig. 3.46(b), scaled \\nto the full [0, 255] \\nrange of intensity \\nvalues. Black pixels \\ncorrespond to the \\nmost negative  \\nvalue in the  \\nunscaled  \\nLaplacian image, \\ngrays are inter-\\nmediate values, \\nand white pixels \\ncorresponds to the \\nhighest positive \\nvalue.\\nDIP4E_GLOBAL_Print_Ready.indb   182\\n6/16/2017   2:04:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 183}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n183\\nwhere we included a weight, \\nkk\\n()\\n,\\n≥\\n0\\n for generality. When \\nk\\n=\\n1\\n we have unsharp \\nmasking\\n, as defined above. When \\nk\\n>\\n1,\\n the process is referred to as \\nhighboost filter\\n-\\ning\\n. Choosing \\nk\\n<\\n1 reduces the contribution of the unsharp mask.\\nF\\nigure 3.48 illustrates the mechanics of unsharp masking. Part (a) is a horizontal \\nintensity proﬁle across a vertical ramp edge that transitions from dark to light. Fig-\\nure 3.48(b) shows the blurred scan line superimposed on the original signal (shown \\ndashed). Figure 3.48(c) is the mask, obtained by subtracting the blurred signal from \\nthe original. By comparing this result with the section of Fig. 3.44(c) corresponding \\nto the ramp in Fig. 3.44(a), we note that the unsharp mask in Fig. 3.48(c) is similar \\nto what we would obtain using a second-order derivative. Figure 3.48(d) is the ﬁnal \\nsharpened result, obtained by adding the mask to the original signal. The points \\nat which a change of slope occurs in the signal are now emphasized (sharpened). \\nObserve that negative values were added to the original. Thus, it is possible for the \\nﬁnal result to have negative intensities if the original image has any zero values, or if \\nthe value of \\nk\\n is chosen large enough to emphasize the peaks of the mask to a level \\nlarger than the minimum value in the original signal. Negative values cause dark \\nhalos around edges that can become objectionable if \\nk\\n is too large. \\nEXAMPLE 3.19 :  Unsharp masking and highboost ﬁltering.\\nFigure 3.49(a) shows a slightly blurred image of white text on a dark gray background. Figure 3.49(b) \\nwas obtained using a Gaussian smoothing ﬁlter of size \\n31 31\\n×\\n with \\ns\\n=\\n5.\\n As explained in our earlier \\ndiscussion of Gaussian lowpass kernels\\n, the size of the kernel we used here is the smallest odd integer \\nno less than \\n66\\nss\\n×\\n.\\n Figure 3.49(c) is the unsharp mask, obtained using Eq. (3-55). To obtain the im-\\nOriginal signal\\nBlurred signal\\nUnsharp mask\\nSharpened signal\\nb\\na\\nc\\nd\\nFIGURE 3.48\\n1-D illustration of \\nthe mechanics of \\nunsharp masking.  \\n(a) Original \\nsignal. (b) Blurred \\nsignal with original \\nshown dashed for \\nreference.  \\n(c) Unsharp mask. \\n(d) Sharpened  \\nsignal, obtained by \\nadding (c) to (a).\\nDIP4E_GLOBAL_Print_Ready.indb   183\\n6/16/2017   2:04:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 184}),\n",
       " Document(page_content='184\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nage in Fig. 3.49(d) was used the unsharp masking expression room Eq. (3-56) with \\nk\\n=\\n1.\\n This image is \\nsigniﬁcantly sharper than the original image in F\\nig. 3.49(a), but we can do better, as we show in the fol-\\nlowing paragraph. \\nFigure 3.49(e) shows the result of using Eq. (3-56) with \\nk\\n=\\n45\\n..\\n This value is almost at the extreme of \\nwhat we can use without introducing some serious artifacts in the image\\n. The artifacts are dark, almost \\nblack, halos around the border of the characters. This is caused by the lower “blip” in Fig. 3.48(d) be-\\ncoming negative, as we explained earlier. When scaling the image so that it only has positive values for \\ndisplay, the negative values are either clipped at 0, or scaled so that the most negative values become 0, \\ndepending on the scaling method used. In either case, the blips will be the darkest values in the image.\\nThe results in Figs. 3.49(d) and 3.49(e) would be difﬁcult to generate using the traditional ﬁlm pho-\\ntography explained earlier, and it illustrates the power and versatility of image processing in the context \\nof digital photography.\\nUSING FIRST-ORDER DERIVATIVES FOR IMAGE SHARPENING—THE \\nGRADIENT\\nFirst derivatives in image processing are implemented using the magnitude of the \\ngradient. The \\ngradient\\n of an image \\nf\\n at coordinates \\n(, )\\nxy\\n is defined as the two-\\ndimensional column vector\\n \\n/H11612\\nff\\ng\\ng\\nf\\nx\\nf\\ny\\nx\\ny\\n≡=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∂\\n∂\\n∂\\n∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\ngrad( )\\n \\n(3-57)\\nWe will discuss the  \\ngradient in more detail \\nin Section 10.2. Here, \\nwe are interested only \\nin using it for image \\nsharpening.\\nb a\\nc\\ne d\\nFIGURE 3.49\\n (a) Original image of size \\n600 259\\n×\\n pixels. (b) Image blurred using a \\n31 31\\n×\\n Gaussian lowpass ﬁlter with \\ns\\n=\\n5.\\n (c) Mask. (d) Result of unsharp masking using Eq. (3-56) with \\nk\\n=\\n1.\\n (e) Result of highboost ﬁltering with \\nk\\n=\\n45\\n..\\n \\nDIP4E_GLOBAL_Print_Ready.indb   184\\n6/16/2017   2:04:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 185}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n185\\nThis vector has the important geometrical property that it points in the direction of \\nthe greatest rate of change of \\nf\\n at location \\n(, ) .\\nxy\\n \\nT\\nhe \\nmagnitude\\n (\\nlength\\n) of vector \\n/H11612\\nf\\n,\\n denoted as \\nMxy\\n(,\\n)\\n (the vector norm nota-\\ntion \\n/H11612\\nf\\n is also used frequently), where\\n \\nMxy f f g g\\nxy\\n(, )\\n=\\n()\\n=+\\n/H11612/H11612\\n=m a g\\n22\\n \\n(3-58)\\nis the \\nvalue\\n at \\n(, )\\nxy\\n of the \\nrate of c\\nhange\\n in the direction of the gradient vector. Note \\nthat \\nMxy\\n(,\\n)\\n is an image of the same size as the original, created when \\nx\\n and \\ny\\n are \\nallowed to vary over all pixel locations in \\nf\\n.\\n It is common practice to refer to this \\nimage as the\\n gradient image\\n (or simply as the \\ngradient\\n when the meaning is clear).\\nBecause the components of the gradient vector are derivatives, they are linear \\noperators. However, the magnitude of this vector is not, because of the squaring and \\nsquare root operations. On the other hand, the partial derivatives in Eq. (3-57) are \\nnot rotation invariant, but the magnitude of the gradient vector is. \\nIn some implementations, it is more suitable computationally to approximate the \\nsquares and square root operations by absolute values:\\n \\nMxy g g\\nxy\\n(, )\\n≈+\\n@@@@\\n \\n(3-59)\\nThis expression still preserves the relative changes in intensity, but the isotropic \\nproperty is lost in general.\\n However, as in the case of the Laplacian, the isotropic \\nproperties of the discrete gradient defined in the following paragraph are preserved \\nonly for a limited number of rotational increments that depend on the kernels used \\nto approximate the derivatives. As it turns out, the most popular kernels used to \\napproximate the gradient are isotropic at multiples of 90°. These results are inde-\\npendent of whether we use Eq. (3-58) or (3-59), so nothing of significance is lost in \\nusing the latter equation if we choose to do so.\\nAs in the case of the Laplacian, we now deﬁne discrete approximations to the \\npreceding equations, and from these formulate the appropriate kernels. In order \\nto simplify the discussion that follows, we will use the notation in Fig. 3.50(a) to \\ndenote the intensities of pixels in a \\n33\\n×\\n region. For example, the value of the center \\npoint,\\n \\nz\\n5\\n,\\n denotes the value of \\nfx y\\n(,\\n)\\n at an arbitrary location, \\n(, ) ;\\nxy\\n \\nz\\n1\\n denotes the \\nvalue of \\nfx y\\n(,\\n) ;\\n−−\\n11\\n and so on. As indicated in Eq. (3-48), the simplest approxi-\\nmations to a ﬁrst-order derivative that satisfy the conditions stated at the beginning \\nof this section are \\ngz z\\nx\\n=−\\n()\\n85\\n and \\ngz z\\ny\\n=−\\n() .\\n65\\n Two other deﬁnitions, proposed \\nby Roberts [1965] in the early development of digital image processing, use cross \\ndifferences:\\n \\ngz z gz z\\nxy\\n=−\\n=−\\n() ()\\n95\\n86\\nand\\n \\n(3-60)\\nIf we use Eqs. (3-58) and (3-60), we compute the gradient image as\\n \\nMxy z z z z\\n(, ) ( ) ( )\\n=−+−\\n⎡\\n⎣\\n⎤\\n⎦\\n95\\n2\\n86\\n2\\n12\\n \\n(3-61)\\nThe vertical bars denote \\nabsolute values. \\nDIP4E_GLOBAL_Print_Ready.indb   185\\n6/16/2017   2:04:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 186}),\n",
       " Document(page_content='186\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nIf we use Eqs. (3-59) and (3-60), then\\n \\nMxy z z z z\\n(,\\n)\\n≈− + −\\n@@ @@\\n95 86\\n \\n(3-62)\\nwhere it is understood that \\nx\\n and \\ny\\n vary over the dimensions of the image in the \\nmanner described earlier\\n. The difference terms needed in Eq. (3-60) can be imple-\\nmented using the two kernels in Figs. 3.50(b) and (c). These kernels are referred to \\nas the \\nRoberts cross-gradient operators\\n. \\nAs noted earlier, we prefer to use kernels of odd sizes because they have a unique, \\n(integer) center of spatial symmetry. The smallest kernels in which we are interested \\nare of size \\n33\\n×\\n.\\n Approximations to \\ng\\nx\\n and \\ng\\ny\\n using a \\n33\\n×\\n neighborhood centered \\non \\nz\\n5\\n are as follows:\\n \\ng\\nf\\nx\\nzz z zz z\\nx\\n=\\n∂\\n∂\\n=+ + − + +\\n() ( )\\n78 9 12 3\\n22\\n \\n(3-63)\\nand\\n \\ng\\nf\\ny\\nzz z zz z\\ny\\n=\\n∂\\n∂\\n=+ + − + +\\n() ( )\\n36 9 14 7\\n22\\n \\n(3-64)\\nThese equations can be implemented using the kernels in Figs. 3.50(d) and (e). The \\ndifference between the third and first rows of the \\n33\\n×\\n image region approximates the \\npartial derivative in the \\nx\\n-direction,\\n and is implemented using the kernel in Fig. 3.50(d). \\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n000\\n121\\n/H11002\\n1\\n01\\n/H11002\\n2\\n0\\n2\\n/H11002\\n1\\n01\\n0\\n/H11002\\n1\\n10\\n/H11002\\n1\\n0\\n01\\nz\\n1\\nz\\n2\\nz\\n3\\nz\\n4\\nz\\n5\\nz\\n6\\nz\\n7\\nz\\n8\\nz\\n9\\na\\nb\\nc\\nd\\ne\\nFIGURE 3.50\\n(a) A \\n33\\n×\\n region \\nof an image\\n, \\nwhere the \\nz\\ns are \\nintensity values.  \\n(b)–(c) Roberts \\ncross-gradient \\noperators.  \\n(d)–(e) Sobel \\noperators. All the \\nkernel  \\ncoefﬁcients sum \\nto zero, as expect-\\ned of a derivative \\noperator. \\nDIP4E_GLOBAL_Print_Ready.indb   186\\n6/16/2017   2:04:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 187}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n187\\nThe difference between the third and first columns approximates the partial deriva-\\ntive in the \\ny\\n-direction and is implemented using the kernel in Fig. 3.50(e). The partial \\nderivatives at all points in an image are obtained by convolving the image with these \\nkernels. We then obtain the magnitude of the gradient as before. For example, substitut-\\ning \\ng\\nx\\n and \\ng\\ny\\n into Eq. (3-59) yields\\n \\nMxy g g z z z z z z\\nzz\\nxy\\n(, )\\n( ) ( )\\n(\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n=+ + − + +\\n[]\\n⎡\\n⎣\\n++ +\\n22\\n1\\n2\\n78 9 12 3\\n2\\n36\\n22\\n2\\nz\\nzz z z\\n91 4 7\\n2\\n1\\n2\\n2\\n)( )\\n−++\\n[]\\n⎤\\n⎦\\n \\n(3-65)\\nThis equation indicates that the value of \\nM \\nat any image coordinates \\n(,)\\nxy\\n is given \\nby squaring values of the convolution of the two kernels with image \\nf \\nat those coor\\n-\\ndinates, summing the two results, and taking the square root. \\nThe kernels in Figs. 3.50(d) and (e) are called the \\nSobel operators\\n. The idea behind \\nusing a weight value of 2 in the center coefﬁcient is to achieve some smoothing by \\ngiving more importance to the center point (we will discuss this in more detail in \\nChapter 10). The coefﬁcients in all the kernels in Fig. 3.50 sum to zero, so they would \\ngive a response of zero in areas of constant intensity, as expected of a derivative \\noperator. As noted earlier, when an image is convolved with a kernel whose coef-\\nﬁcients sum to zero, the elements of the resulting ﬁltered image sum to zero also, so \\nimages convolved with the kernels in Fig. 3.50 will have negative values in general.\\nThe computations of \\ng\\nx\\n and \\ng\\ny\\n are linear operations and are implemented using \\nconvolution, as noted above. The nonlinear aspect of sharpening with the gradient is \\nthe computation of \\nMxy\\n(,\\n)\\n involving squaring and square roots, or the use of abso-\\nlute values\\n, all of which are nonlinear operations. These operations are performed \\nafter the linear process (convolution) that yields \\ng\\nx\\n and \\ng\\ny\\n.\\nEXAMPLE 3.20 :  Using the gradient for edge enhancement.\\nThe gradient is used frequently in industrial inspection, either to aid humans in the detection of defects \\nor, what is more common, as a preprocessing step in automated inspection. We will have more to say \\nabout this in Chapter 10. However, it will be instructive now to consider a simple example to illustrate \\nhow the gradient can be used to enhance defects and eliminate slowly changing background features. \\nFigure 3.51(a) is an optical image of a contact lens, illuminated by a lighting arrangement designed \\nto highlight imperfections, such as the two edge defects in the lens boundary seen at 4 and 5 o’clock. \\nFigure 3.51(b) shows the gradient obtained using Eq. (3-65) with the two Sobel kernels in Figs. 3.50(d) \\nand (e). The edge defects are also quite visible in this image, but with the added advantage that constant \\nor slowly varying shades of gray have been eliminated, thus simplifying considerably the computational \\ntask required for automated inspection. The gradient can be used also to highlight small specs that may \\nnot be readily visible in a gray-scale image (specs like these can be foreign matter, air pockets in a sup-\\nporting solution, or miniscule imperfections in the lens). The ability to enhance small discontinuities in \\nan otherwise ﬂat gray ﬁeld is another important feature of the gradient.\\nDIP4E_GLOBAL_Print_Ready.indb   187\\n6/16/2017   2:04:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 188}),\n",
       " Document(page_content='188\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n3.7 HIGHPASS, BANDREJECT, AND BANDPASS FILTERS FROM LOW-\\nPASS FILTERS  \\nSpatial and frequency-domain linear ﬁlters are classiﬁed into four broad categories: \\nlowpass and highpass ﬁlters, which we introduced in Sections 3.5 and 3.6, and \\nband-\\npass \\nand \\nbandreject\\n ﬁlters, which we introduce in this section. We mentioned at the \\nbeginning of Section 3.5 that the other three types of filters can be constructed from \\nlowpass filters. In this section we explore methods for doing this. Also, we illustrate \\nthe third approach discussed at the end of Section 3.4 for obtaining spatial filter ker-\\nnels. That is, we use a filter design software package to generate 1-D filter functions. \\nThen, we use these to generate 2-D separable filters functions either via Eq.(3-42), \\nor by rotating the 1-D functions about their centers to generate 2-D kernels. The \\nrotated versions are approximations of circularly symmetric (isotropic) functions.\\nFigure 3.52(a) shows the transfer function of a 1-D ideal lowpass ﬁlter in the \\nfrequency domain [this is the same as Fig. 3.32(a)]. We know from earlier discus-\\nsions in this chapter that lowpass ﬁlters attenuate or delete high frequencies, while \\npassing low \\nfrequencies. A \\nhighpass ﬁlter\\n behaves in exactly the opposite manner. \\nAs Fig. 3.52(b) shows, a highpass ﬁlter deletes or attenuates all frequencies below a \\ncut-off value, \\nu\\n0\\n,\\n and passes all frequencies above this value. Comparing Figs. 3.52(a) \\nand (b),\\n we see that a highpass ﬁlter transfer function is obtained by subtracting a \\nlowpass function from 1. This operation is in the frequency domain. As you know \\nfrom Section 3.4, a constant in the frequency domain is an impulse in the spatial \\ndomain. Thus, we obtain a highpass ﬁlter kernel in the spatial domain by subtracting \\na lowpass ﬁlter kernel from a unit impulse with the same center as the kernel. An \\nimage ﬁltered with this kernel is the same as an image obtained by subtracting a low-\\npass-ﬁltered image from the original image. The unsharp mask deﬁned by Eq. (3-55) \\nis precisely this operation. Therefore, Eqs. (3-54) and (3-56) implement equivalent \\noperations (see Problem 3.42).\\nFigure 3.52(c) shows the transfer function of a bandreject ﬁlter. This transfer \\nfunction can be constructed from the sum of a lowpass and a highpass function with \\n3.7\\nRecall from the discus-\\nsion of Eq. (3-33) that a \\nunit impulse is an array \\nof 0’s with a single 1.\\nb a\\nFIGURE 3.51\\n(a) Image of a \\ncontact lens (note \\ndefects on the \\nboundary at 4 and \\n5 o’clock).  \\n(b) Sobel gradient. \\n(Original image \\ncourtesy of  \\nPerceptics  \\nCorporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   188\\n6/16/2017   2:04:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 189}),\n",
       " Document(page_content='3.7\\n  \\nHighpass, Bandreject, and Bandpass Filters from Lowpass Filters\\n    \\n189\\ndifferent cut-off frequencies (the highpass function can be constructed from a \\ndif-\\nferent\\n lowpass function). The bandpass ﬁlter transfer function in Fig. 3.52(d) can be \\nobtained by subtracting the bandreject function from 1 (a unit impulse in the spatial \\ndomain). Bandreject ﬁlters are also referred to as \\nnotch\\n ﬁlters, but the latter tend \\nto be more locally oriented, as we will show in Chapter 4. Table 3.7 summarizes the \\npreceding discussion. \\nThe key point in Fig. 3.52 and Table 3.7 is that all transfer functions shown can \\nbe obtained starting with a lowpass ﬁlter transfer function. This is important. It is \\nimportant also to realize that we arrived at this conclusion via simple graphical \\ninterpretations in the frequency domain. To arrive at the same conclusion based on \\nconvolution in the spatial domain would be a much harder task.\\nEXAMPLE 3.21 :  Lowpass, highpass, bandreject, and bandpass ﬁltering.\\nIn this example we illustrate how we can start with a 1-D lowpass ﬁlter transfer function generated \\nusing a software package, and then use that transfer function to generate spatial ﬁlter kernels based on \\nthe concepts introduced in this section. We also examine the spatial ﬁltering properties of these kernels.\\n1\\n1\\n0\\nu\\nu\\nPassband\\nStopband\\nHighpass filter\\n1\\nu\\nPassband\\nStopband\\n1\\nu\\n2\\nu\\nStopband\\nBandpass filter\\n0\\nu\\nu\\nPassband Stopband\\nLowpass filter\\n1\\nu\\nPassband\\nStopband\\n1\\nu\\n2\\nu\\nPassband\\nBandreject filter\\nb a\\nd c\\nFIGURE 3.52\\nTransfer functions \\nof ideal 1-D ﬁlters \\nin the frequency \\ndomain (\\nu\\n denotes \\nfrequency). \\n(a) Lowpass ﬁlter. \\n(b) Highpass ﬁlter.  \\n(c) Bandreject ﬁlter.  \\n(d) Bandpass ﬁlter. \\n(As before, we \\nshow only positive \\nfrequencies for \\nsimplicity.)\\nFilter type\\nSpatial kernel in terms of lowpass kernel, \\nlp\\nLowpass\\nlp x y\\n(,)\\nHighpass\\nh pxy xy l pxy\\n(,) (,) (,)\\n=−\\nd\\nBandreject\\nb rxy l p xy h p xy\\nl\\np xy xy l p xy\\n(,) (,) (,)\\n(,) (,) (,)\\n=+\\n=+ −\\n[]\\n12\\n12\\nd\\nBandpass\\nb pxy xy b rxy\\nxy\\nl p xy xy l p xy\\n(,) (,) (,)\\n(,) (,) (,) (,)\\n=−\\n=− + −\\n[]\\n⎡\\nd\\ndd\\n12\\n⎣ ⎣\\n⎤\\n⎦\\nTABLE \\n3.7\\nSummary of the \\nfour principal  \\nspatial ﬁlter types \\nexpressed in \\nterms of low-\\npass ﬁlters. The \\ncenters of the \\nunit impulse and \\nthe ﬁlter kernels \\ncoincide.\\nDIP4E_GLOBAL_Print_Ready.indb   189\\n6/16/2017   2:04:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 190}),\n",
       " Document(page_content='190\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFigure 3.53 shows a so-called \\nzone plate\\n image that is used frequently for testing the characteristics of \\nﬁltering approaches. There are various versions of zone plates; the one in Fig. 3.53 was generated using \\nthe equation\\n \\nzxy\\nx y\\n(,) c o s\\n=+ +\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n1\\n2\\n1\\n22\\n \\n(3-66)\\nwith \\nx\\n and \\ny\\n varying in the range \\n[. , . ] ,\\n−\\n82\\n82\\n in increments of 0.0275. This resulted in an image of size \\n597 597\\n×\\n pixels. The bordering black region was generated by setting to 0 all pixels with distance great-\\ner than 8.2 from the image center\\n. The key characteristic of a zone plate is that its spatial frequency \\nincreases as a function of distance from the center, as you can see by noting that the rings get narrower \\nthe further they are from the center. This property makes a zone plate an ideal image for illustrating the \\nbehavior of the four ﬁlter types just discussed. \\nFigure 3.54(a) shows a 1-D, 128-element spatial lowpass ﬁlter function designed using MATLAB \\n[compare with Fig. 3.32(b)]. As discussed earlier, we can use this 1-D function to construct a 2-D, separa-\\nble lowpass ﬁlter kernel based on Eq. (3-42), or we can rotate it about its center to generate a 2-D, isotro-\\npic kernel. The kernel in Fig. 3.54(b) was obtained using the latter approach. Figures 3.55(a) and (b) are \\nthe results of ﬁltering the image in Fig. 3.53 with the separable and isotropic kernels, respectively. Both \\nﬁlters passed the low frequencies of the zone plate while attenuating the high frequencies signiﬁcantly. \\nObserve, however, that the separable ﬁlter kernel produced a “squarish” (non-radially symmetric) result \\nin the passed frequencies. This is a consequence of ﬁltering the image in perpendicular directions with \\na separable kernel that is not isotropic. Using the isotropic kernel yielded a result that is uniform in all \\nradial directions. This is as expected, because both the ﬁlter and the image are isotropic. \\nFIGURE 3.53\\nA zone plate \\nimage of size \\n597 597\\n×\\n pixels.\\n0\\n0.04\\n0.06\\n0.12\\n-\\n0.02\\n03 26 4 9 6\\n128\\nb a\\nFIGURE 3.54\\n(a) A 1-D spatial \\nlowpass ﬁlter \\nfunction. (b) 2-D \\nkernel obtained \\nby rotating the \\n1-D proﬁle about \\nits center.\\nDIP4E_GLOBAL_Print_Ready.indb   190\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 191}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n191\\nFigure 3.56 shows the results of ﬁltering the zone plate with the four ﬁlters described in Table 3.7. We \\nused the 2-D lowpass kernel in Fig. 3.54(b) as the basis for the highpass ﬁlter, and similar lowpass ker-\\nnels for the bandreject ﬁlter. Figure 3.56(a) is the same as Fig. 3.55(b), which we repeat for convenience. \\nFigure 3.56(b) is the highpass-ﬁltered result. Note how effectively the low frequencies were ﬁltered out. \\nAs is true of highpass-ﬁltered images, the black areas were caused by negative values being clipped at 0 \\nby the display. Figure 3.56(c) shows the same image scaled using Eqs. (2-31) and (2-32). Here we see \\nclearly that only high frequencies were passed by the ﬁlter. Because the highpass kernel was constructed \\nusing the same lowpass kernel that we used to generate Fig. 3.56(a), it is evident by comparing the two \\nresults that the highpass ﬁlter passed the frequencies that were attenuated by the lowpass ﬁlter.\\nFigure 3.56(d) shows the bandreject-ﬁltered image, in which the attenuation of the mid-band of \\nfrequencies is evident. Finally, Fig. 33.56(e) shows the result of bandpass ﬁltering. This image also has \\nnegative values, so it is shown scaled in Fig. 3.56(f). Because the bandpass kernel was constructed by \\nsubtracting the bandreject kernel from a unit impulse, we see that the bandpass ﬁlter passed the fre-\\nquencies that were attenuated by the bandreject ﬁlter. We will give additional examples of bandpass and \\nbandreject ﬁltering in Chapter 4.\\n3.8 COMBINING SPATIAL ENHANCEMENT METHODS  \\nWith a few exceptions, such as combining blurring with thresholding (Fig. 3.41), we \\nhave focused attention thus far on individual spatial-domain processing approaches. \\nFrequently, a given task will require application of several complementary tech-\\nniques in order to achieve an acceptable result. In this section, we illustrate how to \\ncombine several of the approaches developed thus far in this chapter to address a \\ndifficult image enhancement task.\\nThe image in Fig. 3.57(a) is a nuclear whole body bone scan, used to detect dis-\\neases such as bone infections and tumors. Our objective is to enhance this image by \\nsharpening it and by bringing out more of the skeletal detail. The narrow dynamic \\nrange of the intensity levels and high noise content make this image difﬁcult to \\nenhance. The strategy we will follow is to utilize the Laplacian to highlight ﬁne detail, \\nand the gradient to enhance prominent edges. For reasons that will be explained \\nshortly, a smoothed version of the gradient image will be used to mask the Laplacian \\n3.8\\nIn this context, masking \\nrefers to multiplying two \\nimages, as in \\nFig. 2.34\\n. \\nThis is not be confused \\nwith the mask used in \\nunsharp masking.\\nb a\\nFIGURE 3.55\\n(a) Zone plate  \\nimage ﬁltered \\nwith a separable \\nlowpass kernel. \\n(b) Image ﬁltered \\nwith the isotropic \\nlowpass kernel in \\nFig. 3.54(b).\\nDIP4E_GLOBAL_Print_Ready.indb   191\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 192}),\n",
       " Document(page_content='192\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nimage. Finally, we will attempt to increase the dynamic range of the intensity levels \\nby using an intensity transformation.\\nFigure 3.57(b) shows the Laplacian of the original image, obtained using the \\nkernel in Fig. 3.45(d). This image was scaled (for display only) using the same \\ntechnique as in Fig. 3.47. We can obtain a sharpened image at this point simply by \\nadding Figs. 3.57(a) and (b), according to Eq. (3-54). Just by looking at the noise \\nlevel in Fig. 3.57(b), we would expect a rather noisy sharpened image if we added \\nFigs. 3.57(a) and (b). This is conﬁrmed by the result in Fig. 3.57(c). One way that \\ncomes immediately to mind to reduce the noise is to use a median ﬁlter. However, \\nmedian ﬁltering is an aggressive nonlinear process capable of removing image fea-\\ntures. This is unacceptable in medical image processing.\\nAn alternate approach is to use a mask formed from a smoothed version of the \\ngradient of the original image. The approach is based on the properties of ﬁrst- and \\nb a\\nc\\ne d\\nf\\nFIGURE 3.56\\nSpatial ﬁltering of the zone plate image. (a) Lowpass result; this is the same as Fig. 3.55(b). (b) Highpass result. \\n(c) Image (b) with intensities scaled. (d) Bandreject result. (e) Bandpass result. (f) Image (e) with intensities scaled. \\nDIP4E_GLOBAL_Print_Ready.indb   192\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 193}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n193\\nb a\\nd c\\nFIGURE 3.57\\n(a) Image of whole \\nbody bone scan.  \\n(b) Laplacian of (a). \\n(c) Sharpened image \\nobtained by adding \\n(a) and (b).  \\n(d) Sobel gradient of \\nimage  (a). (Original \\nimage courtesy of \\nG.E. Medical Sys-\\ntems.)\\nDIP4E_GLOBAL_Print_Ready.indb   193\\n6/16/2017   2:04:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 194}),\n",
       " Document(page_content='194\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nsecond-order derivatives we discussed when explaining Fig. 3.44. The Laplacian, is \\na second-order derivative operator and has the deﬁnite advantage that it is superior \\nfor enhancing ﬁne detail. However, this causes it to produce noisier results than \\nthe gradient. This noise is most objectionable in smooth areas, where it tends to be \\nmore visible. The gradient has a stronger response in areas of signiﬁcant intensity \\ntransitions (ramps and steps) than does the Laplacian. The response of the gradient \\nto noise and ﬁne detail is lower than the Laplacian’s and can be lowered further by \\nsmoothing the gradient with a lowpass ﬁlter. The idea, then, is to smooth the gradient \\nand multiply it by the Laplacian image. In this context, we may view the smoothed \\ngradient as a mask image. The product will preserve details in the strong areas, while \\nreducing noise in the relatively ﬂat areas. This process can be interpreted roughly as \\ncombining the best features of the Laplacian and the gradient. The result is added to \\nthe original to obtain a ﬁnal sharpened image.\\nFigure 3.57(d) shows the Sobel gradient of the original image, computed using \\nEq. (3-59). Components \\ng\\nx\\n and \\ng\\ny\\n were obtained using the kernels in Figs. 3.50(d) \\nand (e), respectively. As expected, the edges are much more dominant in this image \\nthan in the Laplacian image. The smoothed gradient image in Fig. 3.57(e) was \\nobtained by using a box ﬁlter of size \\n55\\n×\\n.\\n The fact that Figs. 3.57(d) and (e) are \\nmuch brighter than F\\nig. 3.57(b) is further evidence that the gradient of an image \\nwith signiﬁcant edge content has values that are higher in general than in a Lapla-\\ncian image.\\nFigure 3.57(f) shows the product of the Laplacian and smoothed gradient image. \\nNote the dominance of the strong edges and the relative lack of visible noise, which \\nis the reason for masking the Laplacian with a smoothed gradient image. Adding the \\nproduct image to the original resulted in the sharpened image in Fig. 3.57(g). The \\nincrease in sharpness of detail in this image over the original is evident in most parts \\nof the image, including the ribs, spinal cord, pelvis, and skull. This type of improve-\\nment would not have been possible by using the Laplacian or the gradient alone.\\nThe sharpening procedure just discussed did not affect in an appreciable way the \\ndynamic range of the intensity levels in an image. Thus, the ﬁnal step in our enhance-\\nment task is to increase the dynamic range of the sharpened image. As we discussed \\nin some detail in Sections 3.2 and 3.3, there are several intensity transformation \\nfunctions that can accomplish this objective. Histogram processing is not a good \\napproach on images whose histograms are characterized by dark and light compo-\\nnents, which is the case here. The dark characteristics of the images with which we \\nare dealing lend themselves much better to a power-law transformation. Because \\nwe wish to spread the intensity levels, the value of \\ng\\n in Eq. (3-5) has to be less than 1. \\nAfter a few trials with this equation, we arrived at the result in Fig. 3.57(h), obtained \\nwith \\ng\\n=\\n0\\n5\\n.\\n and \\nc\\n=\\n1.\\n Comparing this image with Fig. 3.57(g), we note that signiﬁ-\\ncant new detail is visible in F\\nig. 3.57(h). The areas around the wrists, hands, ankles, \\nand feet are good examples of this. The skeletal bone structure also is much more \\npronounced, including the arm and leg bones. Note the faint deﬁnition of the outline \\nof the body, and of body tissue. Bringing out detail of this nature by expanding the \\ndynamic range of the intensity levels also enhanced noise, but Fig. 3.57(h) is a signiﬁ-\\ncant visual improvement over the original image.\\nDIP4E_GLOBAL_Print_Ready.indb   194\\n6/16/2017   2:04:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n195\\nf e\\nh\\ng\\nFIGURE 3.57\\n(Continued)\\n \\n(e) Sobel image \\nsmoothed with a \\n55\\n×\\n box ﬁlter.  \\n(f) Mask image \\nformed by the \\nproduct of (b)  \\nand (e).\\n  \\n(g) Sharpened \\nimage obtained \\nby the adding \\nimages (a) and (f). \\n(h) Final result \\nobtained by  \\napplying a power-\\nlaw transformation \\nto (g). Compare \\nimages (g) and (h) \\nwith (a). (Original \\nimage courtesy \\nof G.E. Medical \\nSystems.) \\nDIP4E_GLOBAL_Print_Ready.indb   195\\n6/16/2017   2:04:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 196}),\n",
       " Document(page_content='196\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nSummary, References, and Further Reading\\n  \\nThe material in this chapter is representative of current techniques used for intensity transformations and spatial \\nﬁltering. The topics were selected for their value as fundamental material that would serve as a foundation in an \\nevolving ﬁeld. Although most of the examples used in this chapter deal with image enhancement, the techniques \\npresented are perfectly general, and you will encounter many of them again throughout the remaining chapters in \\ncontexts unrelated to enhancement.\\nThe material in Section 3.1 is from Gonzalez [1986]. For additional reading on the material in Section 3.2, see \\nSchowengerdt [2006] and Poyton [1996]. Early references on histogram processing (Section 3.3) are Gonzalez and \\nFittes [1977], and Woods and Gonzalez [1981]. Stark [2000] gives some interesting generalizations of histogram \\nequalization for adaptive contrast enhancement. \\nFor complementary reading on linear spatial ﬁltering (Sections 3.4-3.7), see Jain [1989], Rosenfeld and Kak \\n[1982], Schowengerdt [2006], Castleman [1996], and Umbaugh [2010]. For an interesting approach for generating \\nGaussian kernels with integer coefﬁcients see Padﬁeld [2011]. The book by Pitas and Venetsanopoulos [1990] is a \\ngood source for additional reading on median and other nonlinear spatial ﬁlters.\\nFor details on the software aspects of many of the examples in this chapter, see Gonzalez, Woods, and Eddins \\n[2009].\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com). \\n3.1 \\nGive a single intensity transformation function \\nfor spreading the intensities of an image so the \\nlowest intensity is 0 and the highest is \\nL\\n−\\n1.\\n3.2 \\nDo the following:\\n(a) * \\nGive a continuous function for implement-\\ning the contrast stretching transformation in \\nF\\nig. 3.2(a). In addition to \\nm\\n, your function \\nmust include a parameter, \\nE\\n, for control-\\nling the slope of the function as it transi-\\ntions from low to high intensity values. Your \\nfunction should be normalized so that its \\nminimum and maximum values are 0 and 1, \\nrespectively.\\n(b) \\nSketch a family of transformations as a \\nfunction of parameter \\nE\\n,\\n for a ﬁxed value \\nmL\\n=\\n2,\\n where \\nL\\n is the number of intensity \\nlevels in the image\\n..\\n3.3 \\nDo the following:\\n(a) * \\nPropose a \\nset\\n of intensity-slicing transforma-\\ntion functions capable of producing all the \\nindividual bit planes of an 8-bit monochrome \\nimage\\n. For example, applying to an image a \\ntransformation function with the property \\nTr\\n()\\n=\\n0\\n if \\nr\\n is 0 or even,\\n and \\nTr\\n()\\n=\\n1\\n if \\nr\\n is \\nodd,\\n produces an image of the least signiﬁ-\\ncant bit plane (see Fig. 3.13). (\\nHint:\\n Use an \\n8-bit truth table to determine the form of \\neach transformation function.)\\n(b) \\nHow many intensity transformation functions \\nwould there be for 16-bit images?\\n(c) \\nIs the basic approach in (a) limited to images \\nin which the number of intensity levels is an \\ninteger power of 2,\\n or is the method general \\nfor any number of \\nintege\\nr intensity levels?\\n(d) \\nIf the method is general, how would it be dif-\\nferent from your solution in (a)?\\n3.4 \\nDo the following:\\n(a) \\nPropose a method for extracting the bit planes \\nof an image based on converting the value of \\nits pixels to binary\\n. \\n(b) \\nFind all the bit planes of the following 4-bit \\nimage:\\n \\n01 8 6\\n22 1 1\\n11 51 41 2\\n36 91 0\\n3.5 \\nIn general:\\n(a) * \\nWhat effect would setting to zero the lower-\\nDIP4E_GLOBAL_Print_Ready.indb   196\\n6/16/2017   2:04:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 197}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n197\\norder bit planes have on the histogram of an \\nimage?\\n(b) \\nWhat would be the effect on the histogram \\nif we set to zero the higher\\n-order bit planes \\ninstead?\\n3.6 \\nExplain why the discrete histogram equalization \\ntechnique does not yield a ﬂat histogram in gen-\\neral.\\n3.7 \\nSuppose that a digital image is subjected to histo-\\ngram equalization.\\n Show that a second pass of his-\\ntogram equalization (on the histogram-equalized \\nimage) will produce exactly the same result as the \\nﬁrst pass.\\n3.8 \\nAssuming continuous values, show by an exam-\\nple that it is possible to have a case in which the \\ntransformation function given in Eq.\\n (3-11) satis-\\nﬁes conditions (a) and (b) discussed in Section 3.3, \\nbut its inverse may fail condition (a\\n/H11032\\n).\\n3.9 \\nDo the following:\\n(a) \\nShow that the discrete transformation func-\\ntion given in Eq.\\n (3-15) for histogram equal-\\nization satisﬁes conditions (a) and (b) stated \\nat the beginning of Section 3.3.\\n(b) * \\nShow that the inverse discrete transforma-\\ntion in Eq.\\n (3-16) satisﬁes conditions (a\\n/H11032\\n) \\nand (b) in Section 3.3 \\nonly if \\nnone of the \\nintensity levels \\nr\\nk\\n, \\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n are \\nmissing in the original image\\n.\\n3.10 \\nTwo images, \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n have unnormalized \\nhistograms \\nh\\nf\\n and \\nh\\ng\\n.\\n Give the conditions (on the \\nvalues of the pixels in \\nf\\n and \\ng\\n) under which you \\ncan determine the histograms of images formed \\nas follows:\\n(a) * \\nfx\\ny g x y\\n(,) (,)\\n+\\n(b) \\nfx y g x y\\n(,) (,)\\n−\\n(c) \\nfx y g x y\\n(,) (,)\\n×\\n(d) \\nfx y g x y\\n(,) (,)\\n÷\\nShow how the histograms would be formed in \\neach case\\n. The arithmetic operations are element-\\nwise operations, as deﬁned in Section 2.6.\\n3.11 \\nAssume continuous intensity values, and sup-\\npose that the intensity values of an image have \\nthe PDF \\np\\nrr L\\nr\\n() ( )\\n=−\\n21\\n2\\n for \\n01\\n≤≤\\nrL\\n−\\n,\\n and \\npr\\nr\\n()\\n=\\n0 for other values of \\nr\\n. \\n(a) * \\nFind the transformation function that will \\nmap the input intensity values\\n, \\nr\\n, into values, \\ns\\n, of a histogram-equalized image.\\n(b) * \\nFind the transformation function that (when \\napplied to the histogram-equalized intensi-\\nties\\n, \\ns\\n) will produce an image whose intensity \\nPDF is \\npz z L\\nz\\n() ( )\\n=−\\n31\\n23\\n for \\n01\\n≤≤\\nzL\\n−\\n \\nand \\npz\\nz\\n()\\n=\\n0 for other values of \\nz\\n.\\n(c) \\nExpress the transformation function from (b) \\ndirectly in terms of \\nr\\n,\\n the intensities of the \\ninput image.\\n3.12 \\nAn image with intensities in the range \\n[,]\\n01\\n has \\nthe PDF\\n, \\npr\\nr\\n() ,\\n shown in the following ﬁgure. It \\nis desired to transform the intensity levels of this \\nimage so that they will have the speciﬁed \\npz\\nz\\n()\\n \\nshown in the ﬁgure\\n. Assume continuous quantities, \\nand ﬁnd the transformation (expressed in terms \\nof \\nr\\n and \\nz\\n) that will accomplish this.\\n2\\n1\\n2\\n1\\np\\nr\\n(\\nr\\n)\\np\\nz\\n(\\nz\\n)\\nr\\nz\\n3.13 * \\nIn Fig. 3.25(b), the transformation function \\nlabeled (2) \\n[\\nGs\\nk\\n−\\n1\\n()\\n from Eq. (3-23)] is the mirror image of \\n(1) [\\nGz\\nq\\n()\\n in Eq. (3-21)] about a line joining the \\ntwo end points\\n. Does this property always hold \\nfor these two transformation functions? Explain.\\n3.14 * \\nThe local histogram processing method discussed \\nin Section 3.3 requires that a histogram be com-\\nputed at each neighborhood location.\\n Propose \\na method for updating the histogram from one \\nneighborhood to the next, rather than computing \\na new histogram each time.\\n3.15 \\nWhat is the behavior of Eq. (3-35) when \\nab\\n==\\n0?\\n \\nExplain.\\n3.16 \\nYou are given a computer chip that is capable of \\nperforming linear ﬁltering in real time\\n, but you \\nare not told whether the chip performs correla-\\ntion or convolution. Give the details of a test you \\nwould perform to determine which of the two \\noperations the chip performs.\\n3.17 * \\nWe mentioned in Section 3.4 that to perform con-\\nDIP4E_GLOBAL_Print_Ready.indb   197\\n6/16/2017   2:04:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 198}),\n",
       " Document(page_content='198\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nvolution we rotate the kernel by \\n180\\n°\\n.\\n The rota-\\ntion is \\n“built” into Eq. (3-35). Figure 3.28 corre-\\nsponds to correlation. Draw the part of the ﬁgure \\nenclosed by the large ellipse, but with \\nw\\n rotated   \\n180\\n°\\n.\\n Expand Eq. (3-35) for a general \\n33\\n×\\n kernel \\nand show that the result of your expansion corre-\\nsponds to your ﬁgure\\n. This shows graphically that \\nconvolution and correlation differ by the rotation \\nof the kernel.\\n3.18 \\nYou are given the following kernel and image:\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n121\\n242\\n121\\n00000\\n00100\\n00100\\n00100\\n00000\\nf\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n(a) * \\nGive a sketch of the area encircled by the \\nlarge ellipse in F\\nig. 3.28 when the kernel is \\ncentered at point \\n(,)\\n23\\n (2nd row, 3rd col) of \\nthe image shown above\\n. Show speciﬁc values \\nof \\nw\\n and \\nf\\n.\\n(b) * \\nCompute the convolution \\nw\\n/H22841\\nf\\n using the \\nminimum\\n zero padding needed.\\n Show the \\ndetails of your computations when the ker-\\nnel is centered on point \\n(,)\\n23\\n of \\nf\\n;\\n and then \\nshow the ﬁnal full convolution result.\\n(c) \\nRepeat (b), but for correlation, \\nw\\n/H22845\\nf\\n.\\n3.19 * \\nProve the validity of Eqs. (3-36) and (3-37).\\n3.20 \\nThe kernel, \\nw\\n,  in Problem 3.18 is separable.\\n(a) * \\nBy inspection, ﬁnd two kernels, \\nw\\n1\\n and \\nw\\n2\\n so \\nthat \\nww w\\n=\\n12\\n/H22841\\n.\\n(b) \\nUsing the image in Problem 3.18, compute \\nw\\n1\\n/H22841\\nf\\n using the \\nminimum\\n zero padding (see \\nF\\nig. 3.30). Show the details of your compu-\\ntation when the kernel is centered at point \\n(,)\\n23\\n (2nd row, 3rd col) of \\nf\\n and then show \\nthe full convolution.\\n(c) \\nCompute the convolution of \\nw\\n2\\n with the \\nresult from (b). Show the details of your \\ncomputation when the kernel is centered at \\npoint \\n(,)\\n33\\n of the result from (b), and then \\nshow the full convolution.\\n Compare with the \\nresult in Problem 3.18(b).\\n3.21 \\nGiven the following kernel and image:\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n121\\n242\\n121\\n11111\\n11111\\n11111\\n11111\\n11111\\nf\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n(a) \\nGive the convolution of the two.\\n(b) \\nDoes your result have a bias?\\n3.22 \\nAnswer the following:\\n(a) * \\nIf \\nv\\n=\\n[]\\n121\\nT\\n and \\nw\\nT\\n=\\n[]\\n2113 ,\\n is the \\nkernel formed by \\nvw\\nT\\n separable?\\n(b) \\nThe following kernel is separable. Find \\nw\\n1\\n \\nand \\nw\\n2\\n such that \\nww w\\n=\\n12\\n/H22841\\n.\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n131\\n262\\n3.23 \\nDo the following:\\n(a) * \\nShow that the Gaussian kernel, \\nGst\\n(,\\n) ,\\n in \\nEq.\\n (3-45) is separable. (\\nHint:\\n Read the ﬁrst \\nparagraph in the discussion of separable ﬁl-\\nter kernels in Section 3.4.)\\n(b) \\nBecause \\nG\\n is separable and circularly sym-\\nmetric\\n, it can be expressed in the form \\nG\\nT\\n=\\nvv\\n.\\n Assume that the kernel form in \\nEq.\\n (3-46) is used, and that the function is \\nsampled to yield an \\nmm\\n×\\n kernel. What is \\nv\\n \\nin this case?\\n3.24 * \\nShow that the product of a column vector with a \\nrow vector is equivalent to the 2-D convolution \\nof the two vectors\\n. The vectors do not have to \\nbe of the same length. You may use a graphical \\napproach (as in Fig. 3.30) to support the explana-\\ntion of your proof.\\n3.25 \\nGiven \\nK\\n,\\n 1-D Gaussian kernels, \\ngg g\\nK\\n12\\n,,, ,\\n…\\n with \\narbitrary means and standard deviations:\\n(a) * \\nDetermine what the entries in the third col-\\numn of \\nTable 3.6 would be for the product \\ngg g\\nK\\n12\\n×× ×\\n/midhorizellipsis\\n. \\n(b) \\nWhat would the fourth column look like for \\nthe convolution \\ngg g\\nK\\n12\\n/H22841/H22841 /H22841\\n/midhorizellipsis\\n?\\n(\\nHint:\\n It is easier to work with the variance;\\n the \\nstandard deviation is just the square root of your \\nresult.)\\nDIP4E_GLOBAL_Print_Ready.indb   198\\n6/16/2017   2:04:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 199}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n199\\n3.26 \\nThe two images shown in the following ﬁgure are \\nquite different,\\n but their histograms are the same. \\nSuppose that each image is blurred using a \\n33\\n×\\n \\nbox kernel.\\n(a) * \\nWould the histograms of the blurred images \\nstill be equal? Explain.\\n(b) \\nIf your answer is no, either sketch the two \\nhistograms or give two tables detailing the \\nhistogram components\\n.\\n3.27 \\nAn image is ﬁltered four times using a Gaussian \\nkernel of size \\n33\\n×\\n with a standard deviation of \\n1.0.\\n Because of the associative property of con-\\nvolution, we know that equivalent results can be \\nobtained using a single Gaussian kernel formed \\nby convolving the individual kernels.\\n(a) * \\nWhat is the size of the single Gaussian ker-\\nnel?\\n(b) \\nWhat is its standard deviation?\\n3.28 \\nAn image is ﬁltered with three Gaussian lowpass \\nkernels of sizes \\n33\\n×\\n, \\n55\\n×\\n,\\n and \\n77\\n×\\n,\\n and stan-\\ndard deviations 1.5,\\n 2, and 4, respectively. A com-\\nposite ﬁlter, \\nw\\n,\\n is formed as the convolution of \\nthese three ﬁlters\\n.\\n(a) * \\nIs the resulting ﬁlter Gaussian? Explain.\\n(b) \\nWhat is its standard deviation?\\n(c) \\nWhat is its size?\\n3.29 * \\nDiscuss the limiting effect of repeatedly ﬁltering \\nan image with a \\n33\\n×\\n lowpass ﬁlter kernel. You \\nmay ignore border effects\\n.\\n3.30 \\nIn Fig. 3.42(b) the corners of the estimated shad-\\ning pattern appear darker or lighter than their \\nsurrounding areas\\n. Explain the reason for this.\\n3.31 * \\nAn image is ﬁltered with a kernel whose coefﬁ-\\ncients sum to 1.\\n Show that the sum of the pixel \\nvalues in the original and ﬁltered images is the \\nsame.\\n3.32 \\nAn image is ﬁltered with a kernel whose coefﬁ-\\ncients sum to 0.\\n Show that the sum of the pixel \\nvalues in the ﬁltered image also is 0.\\n3.33 \\nA single point of light can be modeled by a digital \\nimage consisting of all 0’\\ns, with a 1 in the location \\nof the point of light. If you view a single point of \\nlight through a defocused lens, it will appear as a \\nfuzzy blob whose size depends on the amount by \\nwhich the lens is defocused. We mentioned in Sec-\\ntion 3.5 that ﬁltering an image with a box kernel \\nis a poor model for a defocused lens, and that a \\nbetter approximation is obtained with a Gauss-\\nian kernel. Using the single-point-of-light analogy, \\nexplain why this is so.\\n3.34 \\nIn the original image used to generate the three \\nblurred images shown,\\n the vertical bars are 5 pix-\\nels wide, 100 pixels high, and their separation is \\n20 pixels. The image was blurred using square box \\nkernels of sizes 23, 25, and 45 elements on the side, \\nrespectively. The vertical bars on the left, lower \\npart of (a) and (c) are blurred, but a clear separa-\\ntion exists between them. \\n(a)\\n(b)\\n(c)\\nHowever, the bars have merged in image (b), de-\\nspite the fact that the kernel used to generate this \\nimage is much smaller than the kernel that pro-\\nduced image (c). Explain the reason for this.\\n3.35 \\nConsider an application such as in Fig. 3.41, in \\nwhich it is desired to eliminate objects smaller \\nthan those enclosed by a square of size \\nqq\\n×\\n pix-\\nels\\n. Suppose that we want to reduce the average \\nDIP4E_GLOBAL_Print_Ready.indb   199\\n6/16/2017   2:04:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 200}),\n",
       " Document(page_content='200\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nintensity of those objects to one-tenth of their \\noriginal average value\\n. In this way, their intensity \\nwill be closer to the intensity of the background \\nand they can be eliminated by thresholding. Give \\nthe (odd) size of the smallest box kernel that will \\nyield the desired reduction in average intensity in \\nonly one pass of the kernel over the image. \\n3.36 \\nWith reference to order-statistic ﬁlters (see Sec-\\ntion 3.5):\\n(a) * \\nWe mentioned that isolated clusters of dark \\nor light (with respect to the background) pix-\\nels whose area is less than one-half the area \\nof a median ﬁlter are forced to the median \\nvalue of the neighbors by the ﬁlter\\n. Assume \\na ﬁlter of size \\nnn\\n×\\n (\\nn\\n odd) and explain why \\nthis is so\\n.\\n(b) \\nConsider an image having various sets of \\npixel clusters\\n. Assume that all points in a \\ncluster are lighter or darker than the back-\\nground (but not both simultaneously in the \\nsame cluster), and that the area of each clus-\\nter is less than or equal to \\nn\\n2\\n2.\\n In terms of \\nn\\n,\\n under what condition would one or more \\nof these clusters cease to be isolated in the \\nsense described in part (a)?\\n3.37 \\nDo the following:\\n(a) * \\nDevelop a procedure for computing the median \\nof an \\nnn\\n×\\n neighborhood.\\n(b) \\nPropose a technique for updating the median \\nas the center of the neighborhood is moved \\nfrom pixel to pixel.\\n \\n3.38 \\nIn a given application, a smoothing kernel is \\napplied to input images to reduce noise\\n, then a \\nLaplacian kernel is applied to enhance ﬁne details. \\nWould the result be the same if the order of these \\noperations is reversed?\\n3.39 * \\nShow that the Laplacian deﬁned in Eq. (3-50) is \\nisotropic (invariant to rotation).\\n Assume continu-\\nous quantities. From Table 2.3, coordinate rota-\\ntion by an angle \\nu\\n is given by \\n   \\nxx y yx y\\n/H11032/H11032\\n=−\\n= +\\ncos sin\\nsin cos\\nuu uu\\n  and  \\nwhere \\n(,)\\nxy\\n and \\n(,)\\nxy\\n/H11032/H11032\\n are the unrotated and \\nrotated coordinates\\n, respectively.\\n3.40 * \\nYou saw in Fig. 3.46 that the Laplacian with a \\n−\\n8 \\nin the center yields sharper results than the one \\nwith a \\n−\\n4 in the center. Explain the reason why.\\n3.41 * \\nGive a \\n33\\n×\\n kernel for performing unsharp mask-\\ning in a single pass through an image\\n. Assume that \\nthe average image is obtained using a box ﬁlter of \\nsize \\n33\\n×\\n.\\n3.42 \\nShow that subtracting the Laplacian from an im-\\nage gives a result that is proportional to the un-\\nsharp mask in Eq.\\n (3-55). Use the deﬁnition for \\nthe Laplacian given in Eq. (3-53).\\n3.43 \\nDo the following:\\n(a) * \\nShow that the magnitude of the gradient giv-\\nen in Eq.\\n (3-58) is an isotropic operation (see \\nthe statement of Problem 3.39).\\n(b) \\nShow that the isotropic property is lost in \\ngeneral if the gradient is computed using \\nEq.\\n (3-59).\\n3.44 \\nAre any of the following highpass (sharpening)\\nkernels separable? F\\nor those that are, ﬁnd vectors \\nv\\n and \\nw\\n such that \\nvw\\nT\\n equals the kernel(s).\\n(a) \\nThe Laplacian kernels in Figs. 3.45(a) and (b).\\n(b) \\nThe Roberts cross-gradient kernels shown in \\nF\\nigs. 3.50(b) and (c).\\n(c) * \\nThe Sobel kernels in Figs. 3.50(d) and (e).\\n3.45 \\nIn a character recognition application, text pages \\nare reduced to binary using a thresholding trans-\\nformation function of the form in F\\nig. 3.2(b). This \\nis followed by a procedure that thins the charac-\\nters until they become strings of binary 1’s on a \\nbackground of 0’s. Due to noise, binarization and \\nthinning result in broken strings of characters \\nwith gaps ranging from 1 to 3 pixels. One way \\nto “repair” the gaps is to run a smoothing kernel \\nover the binary image to blur it, and thus create \\nbridges of nonzero pixels between gaps.\\n(a) * \\nGive the (odd) size of the smallest box ker-\\nnel capable of performing this task.\\n(b) \\nAfter bridging the gaps, the image is thresh-\\nolded to convert it back to binary form.\\n For \\nyour answer in (a), what is the minimum val-\\nue of the threshold required to accomplish \\nthis, without causing the segments to break \\nup again?\\nDIP4E_GLOBAL_Print_Ready.indb   200\\n6/16/2017   2:04:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n201\\n3.46 \\nA manufacturing company purchased an imag-\\ning system whose function is to either smooth \\nor sharpen images\\n. The results of using the sys-\\ntem on the manufacturing ﬂoor have been poor, \\nand the plant manager suspects that the system \\nis not smoothing and sharpening images the way \\nit should. You are hired as a consultant to deter-\\nmine if the system is performing these functions \\nproperly. How would you determine if the system \\nis working correctly? (\\nHint: \\nStudy the statements \\nof Problems 3.31 and 3.32).\\n3.47 \\nA CCD TV camera is used to perform a long-term \\nstudy b\\ny \\nobserving the same area 24 hours a day, \\nfor \\n30 days. Digital images are captured and transmit-\\nted to a central location every 5 minutes. The illu-\\nmination of the scene changes from natural day-\\nlight to artiﬁcial lighting. At no time is the scene \\nwithout illumination, so it is always possible to \\nobtain an acceptable image. Because the range of \\nillumination is such that it is always in the linear \\noperating range of the camera, it is decided not \\nto employ any compensating mechanisms on the \\ncamera itself. Rather, it is decided to use image \\nprocessing techniques to post-process, and thus \\nnormalize, the images to the equivalent of con-\\nstant illumination. Propose a method to do this. \\nYou are at liberty to use any method you wish, \\nbut state clearly all the assumptions you made in \\narriving at your design. \\nDIP4E_GLOBAL_Print_Ready.indb   201\\n6/16/2017   2:04:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 202}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 203}),\n",
       " Document(page_content='2034\\nFiltering in the Frequency \\nDomain\\nPreview\\nAfter a brief historical introduction to the Fourier transform and its importance in image processing, we \\nstart from basic principles of function sampling, and proceed step-by-step to derive the one- and two-\\ndimensional discrete Fourier transforms. Together with convolution, the Fourier transform is a staple of \\nfrequency-domain processing. During this development, we also touch upon several important aspects \\nof sampling, such as aliasing, whose treatment requires an understanding of the frequency domain and \\nthus are best covered in this chapter. This material is followed by a formulation of ﬁltering in the fre-\\nquency domain, paralleling the spatial ﬁltering techniques discussed in Chapter 3. We conclude the \\nchapter with a derivation of the equations underlying the fast Fourier transform (FFT), and discuss its \\ncomputational advantages. These advantages make frequency-domain ﬁltering practical and, in many \\ninstances, superior to ﬁltering in the spatial domain.\\nUpon completion of this chapter, readers should:\\n Understand the meaning of frequency domain \\nﬁltering, and how it differs from ﬁltering in the \\nspatial domain.\\n Be familiar with the concepts of sampling,  func- \\ntion reconstruction, and aliasing.\\n Understand convolution in the frequency \\ndomain, and how it is related to ﬁltering.\\n Know how to obtain frequency domain ﬁlter \\nfunctions from spatial kernels, and vice versa.\\n Be able to construct ﬁlter transfer functions \\ndirectly in the frequency domain.\\n Understand why image padding is important.\\n Know the steps required to perform ﬁltering \\nin the frequency domain.\\n Understand when frequency domain ﬁltering \\nis superior to ﬁltering in the spatial domain.\\n Be familiar with other ﬁltering techniques in \\nthe frequency domain, such as unsharp mask-\\ning and homomorphic ﬁltering.\\n Understand the origin and mechanics of the \\nfast Fourier transform, and how to use it effec- \\ntively in image processing. \\nFilter:\\n A device or material for suppressing or minimizing waves or \\noscillations of certain frequencies.\\nFrequency:\\n The number of times that a periodic function repeats \\nthe same sequence of values during a unit variation of the  \\nindependent variable.\\nWebster’s New Collegiate Dictionary\\nDIP4E_GLOBAL_Print_Ready.indb   203\\n6/16/2017   2:04:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 204}),\n",
       " Document(page_content='204\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.1 BACKGROUND  \\nWe begin the discussion with a brief outline of the origins of the Fourier transform \\nand its impact on countless branches of mathematics, science, and engineering.\\nA BRIEF HISTORY OF THE FOURIER SERIES AND TRANSFORM\\nThe French mathematician Jean Baptiste Joseph Fourier was born in 1768 in the \\ntown of Auxerre, about midway between Paris and Dijon. The contribution for \\nwhich he is most remembered was outlined in a memoir in 1807, and later pub-\\nlished in 1822 in his book, \\nLa Théorie Analitique de la Chaleur\\n (\\nThe Analytic Theory \\nof Heat\\n). This book was translated into English 55 years later by Freeman (see \\nFreeman [1878]). Basically, Fourier’s contribution in this field states that any peri-\\nodic function can be expressed as the sum of sines and/or cosines of different fre-\\nquencies, each multiplied by a different coefficient (we now call this sum a \\nFourier \\nseries\\n). It does not matter how complicated the function is; if it is periodic and satis-\\nfies some mild mathematical conditions, it can be represented by such a sum. This \\nis taken for granted now but, at the time it first appeared, the concept that compli-\\ncated functions could be represented as a sum of simple sines and cosines was not \\nat all intuitive (see Fig. 4.1). Thus, it is not surprising that Fourier’s ideas were met \\ninitially with skepticism.\\nFunctions that are not periodic (but whose area under the curve is ﬁnite) can be \\nexpressed as the integral of sines and/or cosines multiplied by a weighting function. \\nThe formulation in this case is the \\nFourier transform\\n, and its utility is even greater \\nthan the Fourier series in many theoretical and applied disciplines. Both representa-\\ntions share the important characteristic that a function, expressed in either a Fourier \\nseries or transform, can be reconstructed (recovered) completely via an inverse pro-\\ncess, with no loss of information. This is one of the most important characteristics of \\nthese representations because it allows us to work in the \\nFourier domain\\n (generally \\ncalled the \\nfrequency domain\\n) and then return to the original domain of the function \\nwithout losing any information. Ultimately, it is the utility of the Fourier series and \\ntransform in solving practical problems that makes them widely studied and used as \\nfundamental tools. \\nThe initial application of Fourier’s ideas was in the ﬁeld of heat diffusion, where \\nthey allowed formulation of differential equations representing heat ﬂow in such \\na way that solutions could be obtained for the ﬁrst time. During the past century, \\nand especially in the past 60 years, entire industries and academic disciplines have \\nﬂourished as a result of Fourier’s initial ideas. The advent of digital computers and \\nthe “discovery” of a fast Fourier transform (FFT) algorithm in the early 1960s revo-\\nlutionized the ﬁeld of signal processing. These two core technologies allowed for the \\nﬁrst time practical processing of a host of signals of exceptional importance, ranging \\nfrom medical monitors and scanners to modern electronic communications.\\nAs you learned in Section 3.4, it takes on the order of \\nMNmn \\noperations (multi-\\nplications and additions) to ﬁlter an \\nM\\nN\\n×\\n image with a kernel of size \\nmn\\n×\\n ele-\\nments\\n. If the kernel is separable, the number of operations is reduced to \\nMN m n\\n()\\n.\\n+\\n \\nIn Section 4.11,\\n you will learn that it takes on the order of \\n2\\n2\\nMN MN\\nlog\\n operations \\nto perform the equivalent ﬁltering process in the frequenc\\ny domain, where the 2 in \\nfront arises from the fact that we have to compute a forward and an inverse FFT. \\n4.1\\nDIP4E_GLOBAL_Print_Ready.indb   204\\n6/16/2017   2:04:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 205}),\n",
       " Document(page_content='4.1\\n  \\nBackground\\n    \\n205\\nTo get an idea of the relative computational advantages of ﬁltering in the frequency  \\nversus the spatial domain, consider square images and kernels, of sizes \\nMM\\n×\\n and \\nmm\\n×\\n,\\n respectively. The\\n computational advantage\\n (as a function of kernel size) of \\nﬁltering one such image with the FFT as opposed to using a nonseparable kernel is \\ndeﬁned as\\n \\nCm\\nMm\\nMM\\nm\\nM\\nn\\n()\\nlog\\nlog\\n=\\n=\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n4\\n \\n(4-1)\\nIf the kernel is separable, the advantage becomes\\n \\nCm\\nMm\\nMM\\nm\\nM\\ns\\n()\\nlog\\nlog\\n=\\n=\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n \\n(4-2)\\nIn either case, when \\nCm\\n()\\n>\\n1\\n the advantage (in terms of fewer computations) \\nbelongs to the FFT approach;\\n otherwise the advantage favors spatial filtering.\\nFIGURE 4.1\\nThe function at \\nthe bottom is the \\nsum of the four \\nfunctions above it. \\nFourier’s idea in \\n1807 that periodic \\nfunctions could be \\nrepresented as a \\nweighted sum of \\nsines and cosines \\nwas met with \\nskepticism. \\nDIP4E_GLOBAL_Print_Ready.indb   205\\n6/16/2017   2:04:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 206}),\n",
       " Document(page_content='206\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nFigure 4.2(a) shows a plot of \\nCm\\nn\\n()\\n as a function of \\nm\\n for an image of intermedi-\\nate size \\n() .\\nM\\n=\\n2048\\n The inset table shows a more detailed look for smaller kernel \\nsizes\\n. As you can see, the FFT has the advantage for kernels of sizes \\n77\\n×\\n and larger. \\nT\\nhe advantage grows rapidly as a function of \\nm\\n, being over 200 for \\nm\\n=\\n101\\n,\\n and \\nclose to 1000 for \\nm\\n=\\n201.\\n To give you a feel for the meaning of this advantage, if \\nﬁltering a bank of images of size \\n2048 2048\\n×\\n takes 1 minute with the FFT, it would \\ntake on the order of 17 hours to ﬁlter the same set of images with a nonseparable \\nkernel of size \\n201 201\\n×\\n elements. This is a signiﬁcant difference, and is a clear indica-\\ntor of the importance of frequenc\\ny-domain processing using the FFT.\\nIn the case of separable kernels, the computational advantage is not as dramatic, \\nbut it is still meaningful. The “cross over” point now is around \\nm\\n=\\n27\\n,\\n and when \\nm\\n=\\n101\\n the difference between frequency- and spatial-domain ﬁltering is still man-\\nageable\\n. However, you can see that with \\nm\\n=\\n201\\n the advantage of using the FFT \\napproaches a factor of 10,\\n which begins to be signiﬁcant. Note in both graphs that \\nthe FFT is an overwhelming favorite for large spatial kernels. \\nOur focus in the sections that follow is on the Fourier transform and its properties. \\nAs we progress through this chapter, it will become evident that Fourier techniques \\nare useful in a broad range of image processing applications. We conclude the chap-\\nter with a discussion of the FFT.\\nABOUT THE EXAMPLES IN THIS CHAPTER\\nAs in Chapter 3, most of the image filtering examples in this chapter deal with image \\nenhancement. For example, smoothing and sharpening are traditionally associated \\nwith image enhancement, as are techniques for contrast manipulation. By its very \\nnature, beginners in digital image processing find enhancement to be interesting \\nand relatively simple to understand. Therefore, using examples from image enhance-\\nment in this chapter not only saves having an extra chapter in the book but, more \\nimportantly, is an effective tool for introducing newcomers to filtering techniques in \\nthe frequency domain. We will use frequency domain processing methods for other \\napplications in Chapters 5, 7, 8, 10, and 11.\\nThe computational \\nadvantages given by Eqs. \\n(4-1) and (4-2) do not \\ntake into account the fact \\nthat the FFT performs \\noperations between \\ncomplex numbers, and \\nother secondary (but \\nsmall in comparison) \\ncomputations discussed \\nlater in the chapter. Thus, \\ncomparisons should be \\ninterpreted only as  \\nguidelines,\\nC\\ns\\n(\\nm\\n) \\n/H11003 \\n10\\nC\\nn\\n(\\nm\\n) \\n/H11003 \\n10\\n3\\n5\\n10\\n15\\n20\\n25\\n3\\n511\\n1023\\n767\\n255\\nm\\nC\\nn\\n(\\nm\\n)\\nm\\n1\\n2\\n3\\n4\\n5\\n3\\n7\\n11\\n15\\n21\\n27\\nm\\nC\\ns\\n(\\nm\\n)\\n3\\n511\\n1023\\n767\\n255\\nm\\nM = \\n2048\\nM = \\n2048\\n3\\n7\\n11\\n15\\n21\\n27\\n101\\n201\\n0.2\\n1.1\\n2.8\\n5.1\\n10.0\\n16.6\\n232\\n918\\n101\\n201\\n0.1\\n0.3\\n0.5\\n0.7\\n0.9\\n1.2\\n4.6\\n9.1\\nb a\\nFIGURE 4.2\\n(a) Computational \\nadvantage of the \\nFFT over non-\\nseparable spatial \\nkernels.  \\n(b) Advantage over \\nseparable kernels. \\nThe numbers for \\nCm\\n()\\n in the inset \\ntables are not to be \\nmultiplied by the \\nfactors of 10 shown \\nfor the curves\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   206\\n6/16/2017   2:04:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 207}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n207\\n4.2 PRELIMINARY CONCEPTS  \\nWe pause briefly to introduce several of the basic concepts that underlie the mate-\\nrial in later sections. \\nCOMPLEX NUMBERS\\nA complex number, \\nC\\n, is defined as \\n \\nCRj I\\n=+\\n \\n(4-3)\\nwhere \\nR\\n and \\nI\\n are real numbers and \\nj\\n=−\\n1.\\n Here, \\nR\\n denotes the \\nreal part\\n of the \\ncomplex number and \\nI\\n its\\n imaginary part\\n.\\n Real numbers are a subset of complex \\nnumbers in which \\nI\\n=\\n0.\\n The \\nconjugate\\n of a complex number \\nC\\n,\\n denoted \\nC\\n*\\n,\\n is \\ndefined as\\n \\nCR j I\\n*\\n=−\\n \\n(4-4)\\nComplex numbers can be viewed geometrically as points on a plane (called the \\ncom-\\nplex plane\\n) whose abscissa is the \\nreal axis\\n (values of \\nR\\n) and whose ordinate is the \\nimaginary axis\\n (values of \\nI\\n).\\n That is, the complex number \\nRj I\\n+\\n is point \\n(,)\\nRI\\n in the \\ncoordinate system of the complex plane\\n.\\nSometimes it is useful to represent complex numbers in polar coordinates,\\n \\nCC j\\n=+\\n(cos sin )\\nuu\\n \\n(4-5)\\nwhere \\nCR I\\n=+\\n22\\n is the length of the vector extending from the origin of the \\ncomplex plane to point \\n(,) ,\\nRI\\n and \\nu\\n is the angle between the vector and the real axis. \\nDrawing a diagram of the real and complex axes with the vector in the first quadrant \\nwill show that \\ntan ( )\\nu\\n=\\nIR\\n or \\nu\\n=\\narctan( ).\\nIR\\n The arctan function returns angles \\nin the range \\n[, ] .\\n−\\npp\\n22\\n \\nBut,\\n because \\nI\\n and \\nR\\n can be positive and negative inde-\\npendently, we need to be able to obtain angles in the full range \\n[, ] .\\n−\\npp\\n We do this \\nby keeping track of the sign of \\nI\\n and \\nR\\n when computing \\nu\\n.\\n Many programming \\nlanguages do this automatically via so called \\nfour\\n-quadrant arctangent functions\\n. For \\nexample, MATLAB provides the function \\natan2(Imag, Real)\\n for this purpose.\\nUsing \\nEuler’s formula\\n,\\n \\nej\\nj\\nu\\nuu\\n=+\\ncos sin\\n \\n(4-6)\\nwhere \\ne\\n=\\n2\\n71828\\n. ...,\\n gives the following familiar representation of complex num-\\nbers in polar coordinates\\n,\\n \\nCC e\\nj\\n=\\nu\\n \\n(4-7)\\nwhere \\nC\\n and \\nu\\n are as defined above. For example, the polar representation of the \\ncomplex number \\n12\\n+\\nj\\n is \\n5\\ne\\nj\\nu\\n,\\n where \\nu\\n=\\n63\\n4 .\\n°\\n or 1.1 radians. The preceding equa-\\ntions are applicable also to complex functions\\n. A complex function, \\nF\\n()\\n,\\nu\\n of a real \\nvariable \\nu\\n,\\n can be expressed as the sum \\nFRj I\\n()\\n() (\\nuuu ) ,\\n=+\\n where \\nRu\\n()\\n and \\nIu\\n()\\n are \\nthe real and imaginary component functions of \\nFu\\n()\\n.\\n As previously noted, the com-\\nplex conjugate is \\nFu R u j I u\\n*\\n() () () ,\\n=−\\n the magnitude is \\nFu Ru Iu\\n() [ () () ] ,\\n=+\\n22\\n12\\n \\n4.2\\nDIP4E_GLOBAL_Print_Ready.indb   207\\n6/16/2017   2:04:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 208}),\n",
       " Document(page_content='208\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nand the angle is \\nu\\n( ) arctan[ ( ) ( )].\\nuI u R u\\n=\\n We will return to complex functions sev-\\neral times in the course of this and the next chapter\\n.\\nFOURIER SERIES\\nAs indicated in the previous section, a function \\nft\\n()\\n of a continuous variable, \\nt\\n, \\nthat is periodic with a period,\\n \\nT\\n, can be expressed as the sum of sines and cosines \\nmultiplied by appropriate coefﬁcients. This sum, known as a \\nFourier\\n \\nseries\\n, has the \\nform\\n \\nft ce\\nn\\nj\\nn\\nT\\nt\\nn\\n()\\n=\\n=−\\n∑\\n2\\np\\n/H11009\\n/H11009\\n \\n(4-8)\\nwhere\\n \\nc\\nT\\nfte d t n\\nn\\nj\\nn\\nT\\nt\\nT\\nT\\n==\\n−\\n−\\n1\\n012\\n2\\n2\\n2\\n2\\n( )\\n, , ,...\\np\\nfor \\n±±\\n \\n(4-9)\\nare the coefficients. The fact that Eq. (4-8) is an expansion of sines and cosines fol-\\nlows from Euler’\\ns formula, Eq. (4-6).\\nIMPULSES AND THEIR SIFTING PROPERTIES\\nCentral to the study of linear systems and the Fourier transform is the concept of an \\nimpulse and its sifting property. A \\nunit impulse\\n of a continuous variable \\nt\\n, located at \\nt\\n=\\n0,\\n and denoted \\nd\\n()\\n,\\nt\\n is defined as\\n \\nd\\n()\\nt\\nt\\nt\\n=\\n=\\n⎧\\n⎨\\n⎩\\n/H11009\\nif\\n \\nif \\n0\\n00\\n≠\\n \\n(4-10)\\nand is constrained to satisfy the identity\\n \\n-\\n/H11009\\n/H11009\\n2\\nd\\n()\\ntd t\\n=\\n1  \\n(4-11)\\nPhysically, if we interpret \\nt\\n as time\\n, an impulse may be viewed as a spike of infinity \\namplitude and zero duration, having unit area. An impulse has the so-called \\nsifting \\nproperty\\n with respect to integration,\\n \\n-\\n/H11009\\n/H11009\\n2\\nft td t f\\n() () ( )\\nd\\n=\\n0  \\n(4-12)\\nprovided that \\nft\\n()\\n is continuous at \\nt\\n=\\n0,\\n a condition typically satisfied in practice. \\nSifting simply yields the \\nvalue\\n of the function \\nft\\n()\\n at the \\nlocation\\n of the impulse (i.e\\n., \\nat \\nt\\n=\\n0\\n in the previous equation). A more general statement of the sifting property \\ninvolves an impulse located at an arbitrary point,\\n \\nt\\n0\\n,\\n denoted as, \\nd\\n()\\n.\\ntt\\n−\\n0\\n In this case, \\n \\n-\\n/H11009\\n/H11009\\n2\\nft t t d t ft\\n() ( ) ( )\\nd\\n−=\\n00\\n \\n(4-13)\\nAn impulse is not a \\nfunction in the usual \\nsense. A more accurate \\nname is a \\ndistribution\\n \\nor \\ngeneralized\\n \\nfunction\\n. \\nHowever, one often \\nﬁnds in the literature the \\nnames \\nimpulse function\\n, \\ndelta function\\n, and \\nDirac \\ndelta function\\n, despite the \\nmisnomer.\\nTo \\nsift\\n means literally to \\nseparate, or to separate \\nout, by putting something \\nthrough a sieve.\\nDIP4E_GLOBAL_Print_Ready.indb   208\\n6/16/2017   2:04:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 209}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n209\\nwhich simply gives the value of the function at the location of the impulse. For \\nexample, if \\nft t\\n(\\n) cos( ),\\n=\\n using the impulse \\ndp\\n()\\nt\\n−\\n in Eq. (4-13) yields the result \\nf\\n()\\nc o s () .\\npp\\n== −\\n1  The power of the sifting concept will become evident shortly.\\nOf particular interest later in this section is an \\nimpulse train\\n, \\nst\\nT\\n/H9004\\n() ,\\n deﬁned as the \\nsum of inﬁnitely many impulses \\n/H9004\\nT\\n units apart:\\n \\nst t k T\\nT\\nk\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n() ( )\\n=−\\n=−\\n∑\\nd\\n \\n(4-14)\\nFigure 4.3(a) shows a single impulse located at \\ntt\\n=\\n0\\n,\\n and Fig. 4.3(b) shows an \\nimpulse train.\\n Impulses for continuous variables are denoted by up-pointing arrows \\nto simulate infinite height and zero width. For discrete variables the height is finite, \\nas we will show next.\\nLet \\nx\\n represent a \\ndiscrete\\n variable. As you learned in Chapter 3, the unit discrete \\nimpulse, \\nd\\n()\\n,\\nx\\n serves the same purposes in the context of discrete systems as the \\nimpulse \\nd\\n()\\nt\\n does when working with continuous variables. It is deﬁned as\\n \\nd\\n()\\nx\\nx\\nx\\n=\\n=\\n⎧\\n⎨\\n⎩\\n10\\n00\\nif\\n \\nif \\n≠\\n \\n(4-15)\\nClearly, this definition satisfies the discrete equivalent of Eq. (4-11):\\n \\nd\\n()\\nx\\nx\\n=\\n=−\\n∑\\n1\\n/H11009\\n/H11009\\n \\n(4-16)\\nThe sifting property for discrete variables has the form\\n \\nfx x f\\nx\\n()() ()\\nd\\n=\\n=−\\n∑\\n0\\n/H11009\\n/H11009\\n \\n(4-17)\\nb a\\nd c\\nFIGURE 4.3\\n(a) Continuous \\nimpulse located \\nat \\ntt\\n=\\n0\\n. (b) An \\nimpulse train \\nconsisting of  \\ncontinuous  \\nimpulses\\n. (c) Unit \\ndiscrete impulse \\nlocated at \\nxx\\n=\\n0\\n. \\n(d) An \\nimpulse \\ntrain consisting \\nof discrete unit \\nimpulses.\\nt \\n0\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\n3\\n/H9004\\nT\\nx\\n1\\nx\\n0\\n0\\nd\\n(\\nx\\n \\n/H11002\\n \\nx\\n0\\n)\\nd\\n(\\nx\\n)\\n. . .\\n. . .\\n/H11002\\n3\\n/H9004\\nT\\nt\\nt\\n0\\n0\\nd\\n(\\nt\\n \\n/H11002\\n \\nt\\n0\\n)\\nd\\n(\\nt\\n)\\nx \\n0\\ns\\n/H9004\\nX\\n(\\nx\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\n/H9004\\nX\\n/H11002/H9004\\nX\\n/H11002\\n2\\n/H9004\\nX\\n/H11002\\n3\\n/H9004\\nX\\n2\\n/H9004\\nX\\n3\\n/H9004\\nX\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   209\\n6/16/2017   2:04:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 210}),\n",
       " Document(page_content='210\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor, more generally using a discrete impulse located at \\nxx\\n=\\n0\\n (see Eq. 3-33),\\n \\nfx x x fx\\nx\\n()( ) ( )\\nd\\n−=\\n=−\\n∑\\n00\\n/H11009\\n/H11009\\n \\n(4-18)\\nAs before, we see that the sifting property yields the value of the function at the \\nlocation of the impulse\\n. Figure 4.3(c) shows the unit discrete impulse diagrammati-\\ncally, and Fig. 4.3(d) shows a train of discrete unit impulses, Unlike its continuous \\ncounterpart, the discrete impulse is an ordinary function.\\nTHE FOURIER TRANSFORM OF FUNCTIONS OF ONE CONTINUOUS \\nVARIABLE\\nThe \\nFourier transform\\n of a continuous function \\nft\\n()\\n of a continuous variable, \\nt\\n, \\ndenoted \\n/H5219\\nft\\n()\\n,\\n{}\\n is defined by the equation\\n \\n/H5219\\nft\\nft\\ned\\nt\\njt\\n()\\n()\\n{}\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\npm\\n \\n(4-19)\\nwhere \\nm\\n is a continuous variable also.\\n†\\n Because \\nt\\n is integrated out, \\n/H5219\\nft\\n()\\n{}\\nis a func-\\ntion only of \\nm\\n.\\n That is \\n/H5219\\nft\\nF\\n() ( ) ;\\n{}\\n=\\nm\\n therefore, we write the Fourier transform of \\nft\\n()\\n as \\n \\nF\\nft\\ned\\nt\\njt\\n()\\n()\\nm\\npm\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\n \\n(4-20)\\nConversely, given \\nF\\n()\\n,\\nm\\n we can obtain \\nft\\n()\\n back using the \\ninverse F\\nourier transform\\n, \\nwritten as\\n \\nft F e d\\njt\\n() ( )\\n=\\n-\\n/H11009\\n/H11009\\n2\\nmm\\npm\\n2\\n \\n(4-21)\\nwhere we made use of the fact that variable \\nm\\n is integrated out in the inverse \\ntransform and wrote simply \\nft\\n()\\n,\\n rather than the more cumbersome notation \\nft F\\n()\\n( ) .\\n=\\n{}\\n−\\n/H5219\\n1\\nm\\n Equations (4-20) and (4-21) comprise the so-called \\nF\\nourier \\ntransform pair\\n, often denoted as \\nft F\\n()\\n( ) .\\n⇔\\nm\\n The double arrow indicates that the \\nexpression on the right is obtained by taking the \\nforwar\\nd\\n Fourier transform of the \\nexpression on the left, while the expression on the left is obtained by taking the \\ninverse\\n Fourier transform of the expression on the right.\\nUsing Euler’s formula, we can write Eq. (4-20) as\\n \\nFf tt j t d t\\n()\\n( ) c o s ( ) s i n ( )\\nmp\\nm p\\nm\\n=−\\n[]\\n-\\n/H11009\\n/H11009\\n2\\n22\\n \\n(4-22)\\n†\\n Conditions for the existence of the Fourier transform are complicated to state in general (Champeney [1987]), \\nbut a sufﬁcient condition for its existence is that the integral of the absolute value of \\nft\\n()\\n,\\n or the integral of the \\nsquare of \\nft\\n()\\n,\\n be ﬁnite. Existence is seldom an issue in practice, except for idealized signals, such as sinusoids \\nthat extend forever\\n. These are handled using generalized impulses. Our primary interest is in the discrete Fourier \\ntransform pair which, as you will see shortly, is guaranteed to exist for all ﬁnite functions. \\nEquation (4-21) indicates \\nthe important fact men-\\ntioned in Section 4.1 that \\na function can be recov-\\nered from its transform.\\nBecause \\nt\\n is integrated \\nout in this equation, the \\nonly variable left is \\nm\\n, \\nwhich is the frequency of \\nthe sine and cosine terms.\\nDIP4E_GLOBAL_Print_Ready.indb   210\\n6/16/2017   2:04:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 211}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n211\\nIf \\nft\\n()\\n is real, we see that its transform in general is complex. Note that the Fourier \\ntransform is an expansion of \\nft\\n()\\n multiplied by sinusoidal terms whose frequencies \\nare determined by the values of \\nm\\n.\\n Thus, because the only variable left after integra-\\ntion is frequenc\\ny, we say that the domain of the Fourier transform is the \\nfrequency \\ndomain\\n. We will discuss the frequency domain and its properties in more detail later \\nin this chapter. In our discussion, \\nt\\n can represent any continuous variable, and the \\nunits of the frequency variable \\nm\\n depend on the units of \\nt\\n.\\n For example, if \\nt\\n repre-\\nsents time in seconds, the units of \\nm\\n are cycles/sec or Hertz (Hz). If \\nt\\n represents \\ndistance in meters\\n, then the units of \\nm\\n are cycles/meter, and so on. In other words, \\nthe units of the frequenc\\ny domain are cycles per unit of the independent variable of \\nthe input function.\\nEXAMPLE 4.1 :  Obtaining the Fourier transform of a simple continuous function.\\nThe Fourier transform of the function in Fig. 4.4(a) follows from Eq. (4-20):\\n \\nF f t e dt Ae dt\\nA\\nj\\ne\\njt\\njt\\njt\\nW\\nW\\n() ( )\\nm\\npm\\npm\\npm\\npm\\n==\\n=\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n−−\\n−\\n−\\n−\\n-\\n/H11009\\n/H11009\\n22\\n22\\n2\\n2\\n2\\n2\\nW W\\nW\\njW\\njW\\njW jW\\nA\\nj\\nee\\nA\\nj\\nee\\nAW\\n2\\n2\\n2\\n2\\n=\\n−\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n−\\npm\\npm\\np\\npm pm\\npm pm\\nsin(\\nm m\\npm\\nW\\nW\\n)\\n()\\nt\\n0\\nf\\n(\\nt\\n)\\nA\\n0\\n/H11002\\nW\\n/\\n2\\nW\\n/\\n2\\n0\\n \\n/H11002\\n1\\n/\\nW\\n 1\\n/\\nW\\n \\n/H11002\\n1\\n/\\nW\\n 1\\n/\\nW\\nF\\n(\\nm\\n)\\nAW\\n/H11341\\nF\\n(\\nm\\n)\\n/H11341\\nAW\\n \\n/H11002\\n2\\n/\\nW\\n . . .\\n 2\\n/\\nW\\n . . .\\n \\n/H11002\\n2\\n/\\nW\\n . . .\\n 2\\n/\\nW\\n . . .\\nm\\nm\\nb a\\nc\\nFIGURE 4.4\\n (a) A box function, (b) its Fourier transform, and (c) its spectrum. All functions extend to inﬁnity in both \\ndirections. Note the inverse relationship between the width, \\nW\\n, of the function and the zeros of the transform.\\nDIP4E_GLOBAL_Print_Ready.indb   211\\n6/16/2017   2:04:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 212}),\n",
       " Document(page_content='212\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere we used the trigonometric identity \\nsin ( ) .\\nu\\nuu\\n=−\\n−\\nee j\\njj\\n2\\n In this case, the complex terms of the \\nF\\nourier transform combined nicely into a real sine function. The result in the last step of the preceding \\nexpression is known as the \\nsinc\\n function, which has the general form\\n \\nsinc( )\\nsin( )\\n()\\nm\\nm\\nm\\n=\\np\\np\\n \\n(4-23)\\nwhere \\nsinc( )\\n01\\n=\\n and \\nsinc( )\\nm\\n=\\n0 for all other \\ninteger\\n values of \\nm\\n.\\n Figure 4.4(b) shows a plot of \\nF\\n()\\n.\\nm\\nIn general, the Fourier transform contains complex terms, and it is customary for display purposes to \\nwork with the magnitude of the transform (a real quantity),\\n which is called the \\nFourier spectrum\\n or the \\nfrequency spectrum\\n:\\n \\nFA W\\nW\\nW\\n()\\nsin( )\\n()\\nm\\npm\\npm\\n=\\nFigure 4.4(c) shows a plot of \\nF\\n()\\nm\\n as a function of frequency. The key properties to note are (1) that \\nthe locations of the zeros of both \\nF\\n()\\nm\\n and \\nF\\n()\\nm\\n are inversely proportional to the width,\\nW\\n,\\n of the “box” \\nfunction; (2) that the height of the lobes decreases as a function of distance from the origin; and (3) that \\nthe function extends to inﬁnity for both positive and negative values of \\nm\\n.\\n As you will see later, these \\nproperties are quite helpful in interpreting the spectra of two dimensional F\\nourier transforms of images.\\nEXAMPLE 4.2 :  Fourier transform of an impulse and an impulse train.\\nThe Fourier transform of a unit impulse located at the origin follows from Eq. (4-20):\\n \\n/H5219\\ndm\\nd\\nd\\npm\\npm\\np\\nm\\n() ( ) ()\\n()\\nt F\\nt e dt e t dt e\\njt\\njt\\nj\\n{}\\n==\\n=\\n=\\n−− −\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n22 2\\nwhere we used the sifting property from Eq. (4-12). Thus, we see that the Fourier transform of an \\nimpulse located at the origin of the spatial domain is a constant in the frequency domain (we discussed \\nthis brieﬂy in Section 3.4 in connection with Fig. 3.30). \\nSimilarly, the Fourier transform of an impulse located at \\ntt\\n=\\n0\\n is\\n \\n/H5219\\ndm\\nd\\nd\\npm\\npm\\n() ( ) ()\\n()\\nt t F t te d t e t td t\\njt\\njt\\n−\\n{}\\n== − = −=\\n−−\\n00\\n22\\n0\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\ne e\\njt\\n−\\n2\\n0\\npm\\n \\nwhere we used the sifting property from Eq. (4-13). The term \\ne\\njt\\n−\\n2\\n0\\npm\\n represents a unit circle centered on \\nthe origin of the complex plane, as you can easily see by using Euler’s formula to expand the exponential \\ninto its sine and cosine components.\\nIn Section 4.3, we will use the Fourier transform of a periodic impulse train. Obtaining this transform \\nis not as straightforward as we just showed for individual impulses. However, understanding how to \\nderive the transform of an impulse train is important, so we take the time to derive it here. We start by \\nnoting that the only basic difference in the form of Eqs. (4-20) and (4-21) is the sign of the exponential. \\nThus, if a function \\nft\\n()\\n has the Fourier transform \\nF\\n()\\n,\\nm\\n then evaluating this function at \\nt\\n, \\nFt\\n()\\n,\\n must \\nhave the transform \\nf\\n()\\n.\\n−\\nm\\n Using this \\nsymmetry\\n property and given,\\n as we showed above, that the Fou-\\nrier transform of an impulse \\nd\\n()\\ntt\\n−\\n0\\n is \\ne\\njt\\n−\\n2\\n0\\npm\\n,\\n it follows that the function \\ne\\njt\\n−\\n2\\n0\\npm\\n has the transform \\nDIP4E_GLOBAL_Print_Ready.indb   212\\n6/16/2017   2:04:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 213}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n213\\ndm\\n() .\\n−−\\nt\\n0\\n By letting \\n−=\\nta\\n0\\n,\\n it follows that the transform of \\ne\\nja t\\n2\\np\\n is \\ndm d m\\n()\\n( ) ,\\n−+ = −\\naa\\n where the last \\nstep is true because \\nd\\n is zero unless \\nm\\n=\\na\\n, which is the same condition for either \\ndm\\n()\\n−+\\na\\n or \\ndm\\n()\\n.\\n−\\na\\n \\nT\\nhe impulse train \\nst\\nT\\n/H9004\\n()\\n in Eq. (4-14) is periodic with period \\n/H9004\\nT\\n,\\n so it can be expressed as a Fourier \\nseries:\\n \\nst c e\\nTn\\nj\\nn\\nT\\nt\\nn\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n()\\n=\\n=−\\n∑\\n2\\np\\nwhere\\n \\nc\\nT\\nst e d t\\nnT\\nj\\nn\\nT\\nt\\nT\\nT\\n=\\n−\\n1\\n2\\n2\\n2\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n2\\n()\\n−\\np\\nWith reference to Fig. 4.3(b), we see that the integral in the interval \\n[, ]\\n−\\n/H9004/H9004\\nTT\\n22\\n encompasses only \\nthe impulse located at the origin.\\n Therefore, the preceding equation becomes\\n \\nc\\nT\\nte d t\\nT\\ne\\nT\\nn\\nj\\nn\\nT\\nt\\nT\\nT\\n==\\n=\\n−\\n−\\n11\\n1\\n2\\n2\\n2\\n0\\n/H9004/H9004\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n2\\nd\\np\\n()\\nwhere we used the sifting property of \\nd\\n()\\n.\\nt\\n The Fourier series then becomes\\n \\nst\\nT\\ne\\nT\\nj\\nn\\nT\\nt\\nn\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n()\\n=\\n=−\\n∑\\n1\\n2\\np\\nOur objective is to obtain the Fourier transform of this expression. Because summation is a linear pro-\\ncess, obtaining the Fourier transform of a sum is the same as obtaining the sum of the transforms of the \\nindividual components of the sum. These components are exponentials, and we established earlier in \\nthis example that\\n \\n/H5219\\nUV\\ne\\nn\\nT\\nj\\nn\\nT\\nt\\n2\\np\\ndm\\n/H9004\\n/H9004\\n=−\\nQR\\nSo, \\nS\\n()\\n,\\nm\\n the Fourier transform of the periodic impulse train, is\\n \\nSs t\\nT\\ne\\nT\\ne\\nT\\nj\\nn\\nT\\nt\\nn\\nj\\nn\\nT\\nt\\nn\\n() ( )\\nm\\npp\\n=\\n{}\\n===\\n=−\\n=−\\n∑∑\\n/H5219/H5219\\n/H5219\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n/H9004/H9004\\nUV U V\\n11 1\\n22\\n/H9004\\n/H9004/H9004\\n/H11009\\n/H11009\\nT\\nn\\nT\\nn\\ndm\\nQR\\n−\\n=−\\n∑\\nThis fundamental result tells us that the Fourier transform of an impulse train with period \\n/H9004\\nT\\n is also \\nan impulse train,\\n whose period is \\n1\\n/H9004\\nT\\n.\\n This \\ninverse proportionality\\n between the periods of \\nst\\nT\\n/H9004\\n()\\n and \\nS\\n()\\nm\\n is analogous to what we found in Fig. 4.4 in connection with a box function and its transform. This \\ninverse relationship plays a fundamental role in the remainder of this chapter\\n.\\nCONVOLUTION\\nWe showed in Section 3.4 that convolution of two functions involves flipping (rotat-\\ning by 180°) one function about its origin and sliding it past the other. At each dis-\\nplacement in the sliding process, we perform a computation, which, for discrete \\nvariables, is a sum of products [see Eq. (3-35)]. In the present discussion, we are \\nAs in Section 3.4, the \\nfact that convolution of a \\nfunction with an impulse \\nshifts the origin of the \\nfunction to the location of \\nthe impulse is also true for \\ncontinuous convolution. \\n(See Figs. 3.29 and 3.30.)\\nDIP4E_GLOBAL_Print_Ready.indb   213\\n6/16/2017   2:04:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 214}),\n",
       " Document(page_content='214\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ninterested in the convolution of two \\ncontinuous\\n functions, \\nft\\n()\\n and \\nht\\n()\\n, of one con-\\ntinuous variable\\n, \\nt\\n, so we have to use integration instead of a summation. The con-\\nvolution of these two functions, denoted as before by the operator \\n/H22841\\n,\\n is defined as\\n \\n() ( ) ( ) ( )\\nfh\\nt f h t d\\n/H22841\\n=−\\n-\\n/H11009\\n/H11009\\n2\\ntt t\\n \\n(4-24)\\nwhere the minus sign accounts for the flipping just mentioned, \\nt\\n is the \\ndisplacement\\n \\nneeded to slide one function past the other\\n, and \\nt\\n is a dummy variable that is inte-\\ngrated out.\\n We assume for now that the functions extend from \\n−\\n/H11009\\n to \\n/H11009\\n.\\nW\\ne illustrated the basic mechanics of convolution in Section 3.4, and we will do \\nso again later in this chapter and in Chapter 5. At the moment, we are interested in \\nﬁnding the Fourier transform of Eq. (4-24). We start with Eq. (4-19):\\n \\n/H5219\\n()\\n( ) ( ) ( )\\n()\\nfh t\\nf h t d e d t\\nf\\njt\\n/H22841\\n{}\\n=−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n−\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\ntt t\\nt\\npm\\n2\\n- -\\n/H11009\\n/H11009\\n2\\nht e d t d\\njt\\n()\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−\\ntt\\npm\\n2\\nThe term inside the brackets is the Fourier transform of \\nht\\n()\\n.\\n−\\nt\\n We will show later \\nin this chapter that \\n/H5219\\nht\\nH e\\nj\\n() ( ) ,\\n−\\n{}\\n=\\n−\\ntm\\npmt\\n2\\n where \\nH\\n()\\nm\\n is the Fourier transform \\nof \\nht\\n()\\n.\\n Using this in the preceding equation gives us\\n \\n/H5219\\n()\\n( ) ( ) ( )\\n() ( )\\nfh t f H e d\\nHf\\ned\\nj\\nj\\n/H22841\\n{}\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n−\\n-\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n2\\n2\\ntm t\\nmt\\npmt\\npmt\\n2\\n2\\nt t\\nmm\\nm\\n=\\n=\\nHF\\nHF\\n()()\\n() ( )\\ni\\nwhere “ \\ni\\n ” indicates multiplication. As noted earlier, if we refer to the domain of \\nt\\n \\nas the spatial domain, and the domain of \\nm\\n as the frequency domain, the preceding \\nequation tells us that the F\\nourier transform of the convolution of two functions in \\nthe spatial domain is equal to the product in the frequency domain of the Fourier \\ntransforms of the two functions. Conversely, if we have the product of the two trans-\\nforms, we can obtain the convolution in the spatial domain by computing the inverse \\nFourier transform. In other words, \\nfh\\n/H22841\\n and \\nHF\\ni\\n are a Fourier transform \\npair\\n.\\n This \\nresult is one-half of the \\nconvolution theorem\\n and is written as\\n \\n() ( ) () ( )\\nfh t H F\\n/H22841\\n⇔\\ni\\nm\\n \\n(4-25)\\nAs noted earlier, the double arrow indicates that the expression on the right is \\nobtained by taking the \\nforwar\\nd\\n Fourier transform of the expression on the left, while \\nRemember, convolution \\nis commutative, so the \\norder of the functions in \\nconvolution expressions \\ndoes not matter. \\nDIP4E_GLOBAL_Print_Ready.indb   214\\n6/16/2017   2:04:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 215}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled  Functions\\n    \\n215\\nthe expression on the left is obtained by taking the \\ninverse\\n Fourier transform of the \\nexpression on the right.\\nFollowing a similar development would result in the other half of the convolution \\ntheorem:\\n \\n() ( ) ( ) ( )\\nfh\\nt H F\\ni\\n⇔\\n/H22841\\nm\\n \\n(4-26)\\nwhich states that convolution in the frequency domain is analogous to multiplica-\\ntion in the spatial domain,\\n the two being related by the forward and inverse Fourier \\ntransforms, respectively. As you will see later in this chapter, the convolution theo-\\nrem is the foundation for filtering in the frequency domain.\\n4.3 SAMPLING AND THE FOURIER TRANSFORM OF SAMPLED  \\nFUNCTIONS  \\nIn this section, we use the concepts from Section 4.2 to formulate a basis for express-\\ning sampling mathematically. Starting from basic principles, this will lead us to the \\nFourier transform of sampled functions. That is, the discrete Fourier transform.\\nSAMPLING\\nContinuous functions have to be converted into a sequence of discrete values before \\nthey can be processed in a computer. This requires sampling and quantization, as \\nintroduced in Section 2.4. In the following discussion, we examine sampling in more \\ndetail.\\nConsider a continuous function, \\nft\\n()\\n,\\n that we wish to sample at uniform intervals, \\n/H9004\\nT\\n,\\n of the independent variable \\nt \\n(see F\\nig. 4.5). We assume initially that the function \\nextends from \\n−\\n/H11009\\n to \\n/H11009\\n with respect to \\nt\\n.\\n One way to model sampling is to multiply \\nft\\n()\\n by a sampling function equal to a train of impulses \\n/H9004\\nT\\n units apart. That is,\\n \\n/tildenosp\\nft\\nfts t ft t nT\\nT\\nn\\n() () () () ( )\\n== −\\n=−\\n∑\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\nd\\n \\n(4-27)\\nwhere \\n/tildenosp\\nft\\n()\\n denotes the sampled function. Each component of this summation is an \\nimpulse weighted by the value of \\nft\\n()\\n at the location of the impulse, as Fig. 4.5(c) \\nshows\\n. The \\nvalue\\n of each sample is given by the “strength” of the weighted impulse, \\nwhich we obtain by integration. That is, the value, \\nf\\nk\\n,\\n of an arbitrary sample in the \\nsampled sequence is given by\\n \\nff t t k T d t\\nfk\\nT\\nk\\n=−\\n=\\n-\\n/H11009\\n/H11009\\n/H9004\\n/H9004\\n2\\n() ( )\\n()\\nd\\n \\n(4-28)\\nwhere we used the sifting property of \\nd\\n in Eq. (4-13). Equation (4-28) holds for any \\ninteger value \\nk\\n=−\\n−\\n..., , , , , ,....\\n2 1012\\n Figure 4.5(d) shows the result, which con-\\nsists of equally spaced samples of the original function.\\nThese two expressions \\nalso hold for discrete \\nvariables, with the \\nexception that the right \\nside of Eq. (4-26) is \\nmultiplied by (1/\\nM\\n), \\nwhere \\nM\\n is the number \\nof discrete samples (see \\nProblem 4.18).\\n4.3\\nTaking samples \\nΔ\\nΤ\\n units \\napart implies a \\nsampling \\nrate\\n equal to 1/\\nΔ\\nΤ\\n. If the \\nunits of \\nΔ\\nΤ\\n are seconds, \\nthen the sampling rate is \\nin samples/s. If the units \\nof \\nΔ\\nΤ\\n are meters, then \\nthe sampling rate is in \\nsamples/m, and so on.\\nDIP4E_GLOBAL_Print_Ready.indb   215\\n6/16/2017   2:04:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 216}),\n",
       " Document(page_content='216\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nTHE FOURIER TRANSFORM OF SAMPLED FUNCTIONS\\nLet \\nF\\n()\\nm\\n denote the Fourier transform of a continuous function \\nft\\n()\\n.\\n As discussed \\nin the previous section,\\n the corresponding sampled function, \\n/tildenosp\\nft\\n()\\n,\\n is the product of \\nft\\n()\\n and an impulse train. We know from the convolution theorem that the Fourier \\ntransform of the product of two functions in the spatial domain is the convolution \\nof the transforms of the two functions in the frequenc\\ny domain. Thus, the Fourier \\ntransform of the sampled function is:\\n \\n/tildenosp\\n/tildenosp\\nFf\\nt f t s t\\nFS\\nT\\n( ) () () ()\\n() ( )\\nm\\nm\\n=\\n{}\\n=\\n{}\\n=\\n/H5219/H5219\\n/H9004\\n/H22841\\n \\n(4-29)\\nwhere, from Example 4.2,\\n \\nS\\nT\\nn\\nT\\nn\\n()\\nmd m\\n=−\\n=−\\n∑\\n1\\n/H9004/H9004\\n/H11009\\n/H11009\\nQR\\n \\n(4-30)\\nt\\n0\\nf\\n(\\nt\\n)\\nt\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\nt\\nf\\n(\\nt\\n)\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\nk\\nf\\nk\\n \\n/H11005\\n \\nf\\n(\\nk\\n/H9004\\nT\\n)\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n1\\n/H11002\\n1\\n/H11002\\n22\\n. .\\n. . \\nb\\na\\nc\\nd\\nFIGURE 4.5\\n(a) A continuous \\nfunction. (b) Train \\nof impulses used to \\nmodel sampling.  \\n(c) Sampled  \\nfunction formed as \\nthe product of (a) \\nand (b). (d) Sample \\nvalues obtained by \\nintegration and  \\nusing the sifting \\nproperty of  \\nimpulses. (The \\ndashed line in (c) is \\nshown for refer-\\nence. It is not part \\nof the data.)\\nDIP4E_GLOBAL_Print_Ready.indb   216\\n6/16/2017   2:04:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 217}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n217\\nis the Fourier transform of the impulse train \\nst\\nT\\n/H9004\\n() .\\n We obtain the convolution of \\nF\\n()\\nm\\n and \\nS\\n()\\nm\\n directly from the 1-D definition of convolution in Eq. (4-24):\\n \\n/tildenosp\\nFF S F\\nSd\\nT\\nF\\nn\\nT\\nn\\n() ( ) () ( )( )\\n()\\nmmt m t t\\ntd m t\\n== −\\n=−\\n−\\n=−\\n/H22841\\n-\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004/H9004\\n/H11009\\n/H11009\\n2\\n2\\n1\\nQR\\n∑ ∑\\n∑\\n∑\\n=−\\n−\\n=−\\n=−\\n=−\\nd\\nT\\nF\\nn\\nT\\nd\\nT\\nF\\nn\\nT\\nn\\nn\\nt\\ntdm t t\\nm\\n1\\n1\\n/H9004/H9004\\n/H9004/H9004\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n-\\n2\\n()\\nQR\\nQR\\n \\n(4-31)\\nwhere the final step follows from the sifting property of the impulse, Eq. (4-13).\\nT\\nhe summation in the last line of Eq. (4-31) shows that the Fourier transform \\n/tildenosp\\nF\\n()\\nm\\n of the sampled function \\n/tildenosp\\nft\\n()\\n is an \\ninﬁnite\\n, \\nperiodic\\n sequence of \\ncopies\\n of the \\ntransform of the original,\\n continuous function. The separation between copies is \\ndetermined by the value of \\n1\\n/H9004\\nT\\n.\\n Observe that although \\n/tildenosp\\nft\\n()\\n is a sampled function, \\nits transform,\\n \\n/tildenosp\\nF\\n()\\n,\\nm\\n is \\ncontinuous\\n because it consists of copies of \\nF\\n()\\n,\\nm\\n which is a \\ncontinuous function.\\nF\\nigure 4.6 is a graphical summary of the preceding results.\\n†\\n Figure 4.6(a) is a \\nsketch of the Fourier transform, \\nF\\n()\\n,\\nm\\n of a function \\nft\\n()\\n, and Fig. 4.6(b) shows the \\ntransform,\\n \\n/tildenosp\\nF\\n()\\n,\\nm\\n of the sampled function, \\n/tildenosp\\nft\\n()\\n.\\n As mentioned in the previous sec-\\ntion,\\n the quantity \\n1\\n/H9004\\nT\\n is the \\nsampling rate\\n used to generate the sampled function.\\n \\nSo, in Fig. 4.6(b) the sampling rate was high enough to provide sufﬁcient separation \\nbetween the periods, and thus preserve the integrity (i.e., perfect copies) of \\nF\\n()\\n.\\nm\\n In \\nF\\nig. 4.6(c), the sampling rate was just enough to preserve \\nF\\n()\\n,\\nm\\n but in Fig. 4.6(d), the \\nsampling rate was below the minimum required to maintain distinct copies of \\nF\\n()\\n,\\nm\\n \\nand thus failed to preserve the original transform.\\n Figure 4.6(b) is the result of an \\nover-sampled\\n signal, while Figs. 4.6(c) and (d) are the results of \\ncritically sampling\\n \\nand \\nunder-sampling \\nthe signal, respectively. These concepts are the basis that will \\nhelp you grasp the fundamentals of the sampling theorem, which we discuss next.\\nTHE SAMPLING THEOREM\\nWe introduced the idea of sampling intuitively in Section 2.4. Now we consider sam-\\npling formally, and establish the conditions under which a continuous function can \\nbe recovered uniquely from a set of its samples.\\nA function \\nft\\n()\\n whose Fourier transform is zero for values of frequencies outside \\na ﬁnite interval (band) \\n[, ]\\nmax max\\n−\\nmm\\n about the origin is called a \\nband-limited \\nfunc-\\ntion.\\n Figure 4.7(a), which is a magniﬁed section of Fig. 4.6(a), is such a function. Simi-\\nlarly, Fig. 4.7(b) is a more detailed view of the transform of the critically sampled \\n†\\n  For the sake of clarity in sketches of Fourier transforms in Fig. 4.6, and other similar ﬁgures in this chapter, we \\nignore the fact that Fourier transforms typically are complex functions. Our interest here is on concepts.\\nDIP4E_GLOBAL_Print_Ready.indb   217\\n6/16/2017   2:04:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 218}),\n",
       " Document(page_content='218\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nfunction [see Fig. 4.6(c)]. A higher value of \\n/H9004\\nT\\n would cause the periods in \\n/tildenosp\\nF\\n()\\nm\\n to \\nmerge;\\n a lower value would provide a clean separation between the periods.\\nWe can recover \\nft\\n()\\n from its samples if we can isolate a single copy of \\nF\\n()\\nm\\n from \\nthe periodic sequence of copies of this function contained in \\n/tildenosp\\nF\\n()\\n,\\nm\\n the transform of \\nthe sampled function \\n/tildenosp\\nft\\n()\\n.\\n Recall from the discussion in the previous section that \\n/tildenosp\\nF\\n()\\nm\\n is a \\ncontinuous\\n, \\nperiodic\\n function with period \\n1\\n/H9004\\nT\\n.\\n Therefore, all we need is \\none complete period to characterize the entire transform.\\n In other words, we can \\nrecover \\nft\\n()\\n from that single period by taking its inverse Fourier transform.\\nExtracting from \\n/tildenosp\\nF\\n()\\nm\\n a single period that is equal to \\nF\\n()\\nm\\n is possible if the sepa-\\nration between copies is sufﬁcient (see F\\nig. 4.6). In terms of Fig. 4.7(b), sufﬁcient \\nseparation is guaranteed if \\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n or\\n \\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n \\n(4-32)\\nThis equation indicates that a continuous, band-limited function can be recovered \\ncompletely from a set of its samples if the samples are acquired at a rate exceeding \\nRemember, the sampling \\nrate is the number of \\nsamples taken per unit of \\nthe independent variable. \\n. . .\\n. . .\\n. . .\\n. . .\\n0\\nF\\n(\\nm\\n)\\nm\\nF\\n(\\nm\\n)\\n~\\nF\\n(\\nm\\n)\\n~\\nF\\n(\\nm\\n)\\n~\\nm\\nm\\nm\\n. . .\\n. . .\\n0\\n0\\n0\\n1\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n/H11002\\n3\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n3\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n. . .\\n. . .\\nb\\na\\nc\\nd\\nFIGURE 4.6\\n(a) Illustrative \\nsketch of the \\nFourier transform \\nof a band-limited \\nfunction.  \\n(b)–(d) Trans-\\nforms of the \\ncorresponding \\nsampled functions \\nunder the  \\nconditions of \\nover-sampling, \\ncritically  \\nsampling, and \\nunder-sampling, \\nrespectively. \\nDIP4E_GLOBAL_Print_Ready.indb   218\\n6/16/2017   2:04:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 219}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n219\\ntwice the highest frequency content of the function. This exceptionally important \\nresult is known as the \\nsampling theorem\\n.\\n†\\n We can say based on this result that no \\ninformation is lost if a continuous, band-limited function is represented by samples \\nacquired at a rate greater than twice the highest frequency content of the function. \\nConversely, we can say that the \\nmaximum\\n frequency that can be “captured” by sam-\\npling a signal at a rate \\n1\\n/H9004\\nT\\n is \\nm\\nmax\\n.\\n=\\n12\\n/H9004\\nT\\n A sampling rate \\nexactly\\n equal to twice \\nthe highest frequenc\\ny is called the \\nNyquist rate\\n. Sampling at exactly the Nyquist rate \\nsometimes is sufficient for perfect function recovery, but there are cases in which \\nthis leads to difficulties, as we will illustrate later in Example 4.3. This is the reason \\nwhy the sampling theorem specifies that sampling must exceed the Nyquist rate. \\nFigure 4.8 illustrates the procedure for recovering \\nF\\n()\\nm\\n from \\n/tildenosp\\nF\\n()\\nm\\n when a function \\nis sampled at a rate higher than the Nyquist rate\\n. The function in Fig. 4.8(b) is deﬁned \\nby the equation\\n \\nH\\nT\\n()\\nmax\\nmax\\nm\\nmm m\\n=\\n−\\n⎧\\n⎨\\n⎩\\n/H9004\\n≤≤\\n0 otherwise\\n \\n(4-33)\\nWhen multiplied by the periodic sequence in Fig. 4.8(a), this function isolates the \\nperiod centered on the origin.\\n Then, as Fig. 4.8(c) shows, we obtain \\nF\\n()\\nm\\n by multiply-\\ning \\n/tildenosp\\nF\\n()\\nm\\n by \\nH\\n()\\n:\\nm\\n†\\n  The sampling theorem is a cornerstone of digital signal processing theory. It was ﬁrst formulated in 1928 by \\nHarry Nyquist, a Bell Laboratories scientist and engineer. Claude E. Shannon, also from Bell Labs, proved the \\ntheorem formally in 1949. The renewed interest in the sampling theorem in the late 1940s was motivated by the \\nemergence of early digital computing systems and modern communications, which created a need for methods \\ndealing with digital (sampled) data.\\nThe \\nΔ\\nΤ \\nin Eq. (4-33) \\ncancels out the 1/\\nΔ\\nΤ\\n in \\nEq. (4-31).\\n0\\nF\\n(\\nm\\n)\\nm\\n0\\nF\\n(\\nm\\n)\\nm\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n1\\n2\\n/H9004\\nT\\n–––\\n/H11002\\n1\\n/H9004\\nT\\n––\\n1\\n2\\n/H9004\\nT\\n–––\\n~\\n. . .\\n. . .\\nb\\na\\nFIGURE 4.7\\n(a) Illustrative \\nsketch of the \\nFourier  \\ntransform of a \\nband-limited \\nfunction.  \\n(b) Transform \\nresulting from \\ncritically sampling \\nthat band-limited  \\nfunction. \\nDIP4E_GLOBAL_Print_Ready.indb   219\\n6/16/2017   2:04:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 220}),\n",
       " Document(page_content='220\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nFH F\\n()\\n()()\\nmm m\\n=\\n/tildenosp\\n \\n(4-34)\\nOnce we have \\nF\\n()\\n,\\nm\\n we can recover \\nft\\n()\\n using the inverse Fourier transform:\\n \\nft F e d\\njt\\n() ( )\\n=\\n-\\n/H11009\\n/H11009\\n2\\nmm\\npm\\n2\\n \\n(4-35)\\nEquations (4-33) through (4-35) prove that, theoretically, it is possible to recover a \\nband-limited function from samples obtained at a rate exceeding twice the highest \\nfrequenc\\ny content of the function. As we will discuss in the following section, the \\nrequirement that \\nft\\n()\\n must be band-limited implies in general that \\nft\\n()\\n must extend \\nfrom \\n−\\n/H11009\\n to \\n/H11009\\n, a condition that cannot be met in practice. As you will see shortly, \\nhaving to limit the duration of a function prevents perfect recovery of the function \\nfrom its samples\\n, except in some special cases.\\nFunction \\nH\\n()\\nm\\n is called a \\nlo\\nwpass ﬁlter\\n because it passes frequencies in the low \\nend of the frequency range, but it eliminates (ﬁlters out) higher frequencies. It is \\ncalled also an \\nideal lowpass ﬁlter\\n because of its instantaneous transitions in ampli-\\ntude (between 0 and \\n/H9004\\nT\\n at location \\n−\\nm\\nmax\\n and the reverse at \\nm\\nmax\\n), a characteristic \\nthat cannot be implemented physically in hardware. We can simulate ideal ﬁlters \\nin software, but even then there are limitations (see Section 4.8). Because they are \\ninstrumental in recovering (reconstructing) the original function from its samples, \\nﬁlters used for the purpose just discussed are also called \\nreconstruction ﬁlters\\n.\\nIn Fig. 3.32 we sketched \\nthe radial cross sections \\nof ﬁlter transfer functions \\nusing only positive fre-\\nquencies, for simplicity. \\nNow you can see that \\nfrequency domain ﬁlter \\nfunctions encompass \\nboth positive and nega-\\ntive frequencies. \\nF\\n(\\nm\\n)\\n~\\nm\\n. . .\\n. . .\\n0\\n1\\n/\\n/H9004\\nT\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\nH\\n(\\nm\\n)\\nm\\n0\\n~\\nF\\n(\\nm\\n) \\n/H11005\\n \\nH\\n(\\nm\\n)\\nF\\n(\\nm\\n)\\nm\\n0\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n. . .\\n. . .\\nb\\na\\nc\\nFIGURE 4.8\\n(a) Fourier \\ntransform of a \\nsampled,  \\nband-limited  \\nfunction.  \\n(b) Ideal lowpass \\nﬁlter transfer \\nfunction.  \\n(c) The product \\nof (b) and (a), \\nused to extract \\none period of the \\ninﬁnitely periodic \\nsequence in (a). \\nDIP4E_GLOBAL_Print_Ready.indb   220\\n6/16/2017   2:04:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 221}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n221\\nALIASING\\nLiterally, the word \\nalias\\n means “a false identity.” In the field of signal processing, \\naliasing refers to sampling phenomena that cause different signals to become indis-\\ntinguishable from one another after sampling; or, viewed another way, for one signal \\nto “masquerade” as another. \\nConceptually, the relationship between sampling and aliasing is not difﬁcult to \\ngrasp. The foundation of aliasing phenomena as it relates to sampling is that we \\ncan describe a digitized function \\nonly\\n by the values of its samples. This means that \\nit is possible for two (or more) totally \\ndifferent\\n continuous functions to coincide at \\nthe values of their respective samples, but we would have no way of knowing the \\ncharacteristics of the functions between those samples. To illustrate, Fig. 4.9 shows \\ntwo completely different sine functions sampled at the same rate. As you can see \\nin Figs. 4.9(a) and (c), there are numerous places where the sampled values are the \\nsame in the two functions, resulting in identical sampled functions, as Figs. 4.9(b) \\nand (d) show. \\nTwo continuous functions having the characteristics just described are called an \\naliased pair\\n, and such pairs are indistinguishable after sampling. Note that the reason \\nthese functions are aliased is because we used a sampling rate that is too coarse. That \\nis, the functions were \\nunder-sampled\\n. It is intuitively obvious that if sampling were \\nreﬁned, more and more of the differences between the two continuous functions \\nwould be revealed in the sampled signals. The principal objective of the following \\ndiscussion is to answer the question: What is the minimum sampling rate required \\nto avoid (or reduce) aliasing? This question has both a theoretical and a practical \\nanswer and, in the process of arriving at the answers, we will establish the conditions \\nunder which aliasing occurs.\\nWe can use the tools developed earlier in this section to formally answer the \\nquestion we just posed. All we have to do is ask it in a different form: What happens \\nAlthough we show \\nsinusoidal functions for \\nsimplicity, aliasing occurs \\nbetween any arbitrary \\nsignals whose values are \\nthe same at the sample \\npoints.\\nb a\\nd c\\nFIGURE 4.9\\nThe functions in \\n(a) and (c) are \\ntotally different, \\nbut their digi-\\ntized versions in \\n(b) and (d) are \\nidentical. Aliasing \\noccurs when the \\nsamples of two or \\nmore functions \\ncoincide, but the \\nfunctions are dif-\\nferent elsewhere. \\nDIP4E_GLOBAL_Print_Ready.indb   221\\n6/16/2017   2:04:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 222}),\n",
       " Document(page_content='222\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nif a band-limited function is sampled at less than the Nyquist rate (i.e., at less than \\ntwice its highest frequency)? This is precisely the under-sampled situation discussed \\nearlier in this section and mentioned in the previous paragraph. \\nFigure 4.10(a) is the same as Fig. 4.6(d); it shows schematically the Fourier trans-\\nform of an under-sampled, band-limited function. This ﬁgure illustrates that the net \\neffect of lowering the sampling rate below the Nyquist rate is that the periods of the \\nFourier transform now overlap, and it becomes impossible to isolate a single period \\nof the transform, regardless of the ﬁlter used. For instance, using the ideal lowpass \\nﬁlter in Fig. 4.10(b) would result in a transform that is corrupted by frequencies from \\nadjacent periods, as Fig. 4.10(c) shows. The inverse transform would then yield a \\nfunction, \\nft\\na\\n() ,\\n different from the original. That is, \\nft\\na\\n()\\n would be an aliased function \\nbecause it would contain frequenc\\ny components not present in the original. Using \\nour earlier terminology, \\nft\\na\\n()\\n would masquerade as a different function. It is pos-\\nsible for aliased functions to bear no resemblance whatsoever to the functions from \\nwhich they originated.\\nUnfortunately\\n, except in some special cases mentioned below, aliasing is always \\npresent in sampled signals. This is because, even if the original sampled function is \\nband-limited, inﬁnite frequency components are introduced the moment we limit \\nthe duration of the function, which we always have to do in practice. As an illustra-\\ntion, suppose that we want to limit the duration of a band-limited function, \\nft\\n()\\n,\\n to a \\nﬁnite interval,\\n say \\n[, ] .\\n0\\nT\\n We can do this by multiplying \\nft\\n()\\n by the function\\n \\nht\\ntT\\n()\\n=\\n⎧\\n⎨\\n⎩\\n10\\n0\\n≤≤\\not\\nherwise\\n \\n(4-36)\\nThis function has the same basic shape as Fig. 4.4(a), whose Fourier transform, \\nH\\n()\\n,\\nm\\n \\nhas frequenc\\ny components extending to infinity in both directions, as Fig. 4.4(b) shows. \\nFrom the convolution theorem, we know that the transform of the product \\nht f t\\n()\\n()\\n \\nis the convolution in the frequenc\\ny domain of the transforms \\nF\\n()\\nm\\n and \\nH\\n()\\n.\\nm\\n Even \\nif \\nF\\n()\\nm\\n is band-limited, convolving it with \\nH\\n()\\nm\\n , which involves sliding one function \\nacross the other\\n, will yield a result with frequency components extending to infinity \\nin both directions (see Problem 4.12). From this we conclude that no function of \\nfinite\\n duration can be band-limited. Conversely, a function that is band-limited must \\nextend from \\n−\\n/H11009\\n to \\n/H11009\\n.\\n†\\n \\nAlthough aliasing is an inevitable fact of working with sampled records of ﬁnite \\nlength,\\n the effects of aliasing can be reduced by smoothing (lowpass ﬁltering) the \\ninput function to attenuate its higher frequencies. This process, called \\nanti-aliasing\\n, \\nhas to be done \\nbefore\\n the function is sampled because aliasing is a sampling issue \\nthat cannot be “undone after the fact” using computational techniques.\\n† An important special case is when a function that extends from \\n−\\n/H11009\\n to \\n/H11009\\n is band-limited \\nand\\n periodic\\n. In this \\ncase, the function can be truncated and still be band-limited, \\nprovided\\n that the truncation encompasses \\nexactly\\n \\nan integral number of periods. A single truncated period (and thus the function) can be represented by a set of \\ndiscrete samples satisfying the sampling theorem, taken over the truncated interval.\\nIf we cannot isolate one \\nperiod of the transform, \\nwe cannot recover the \\nsignal without aliasing,\\nDIP4E_GLOBAL_Print_Ready.indb   222\\n6/16/2017   2:04:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 223}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n223\\nEXAMPLE 4.3 : Aliasing.\\nFigure 4.11 shows a classic illustration of aliasing. A pure sine wave extending inﬁnitely in both direc-\\ntions has a single frequency so, obviously, it is band-limited. Suppose that the sine wave in the ﬁgure \\n(ignore the large dots for now) has the equation \\nft t\\n(\\n) sin( ),\\n=\\np\\n and that the horizontal axis corresponds \\nto time\\n, \\nt\\n, in seconds. The function crosses the axis at \\nt\\n=\\n01\\n2\\n,, , .\\n±±\\n…\\nRecall that a function \\nft\\n()\\n is \\nperiodic\\n with \\nperiod\\n \\nP\\n if \\nft P ft\\n()\\n( )\\n+=\\n for all values of \\nt\\n.\\n The period \\nis the number (including fractions) of units of the independent variable that it takes for the function \\nto complete one cycle. The \\nfrequency\\n of a \\nperiodic\\n function is the number of periods (cycles) that the \\nfunction completes in one unit of the independent variable. Thus, the frequency of a periodic function \\nis the \\nreciprocal\\n of the period. As before, the sampling rate is the number of samples taken per unit of \\nthe independent variable.\\n In the present example, the independent variable is time, and its units are seconds. The period, \\nP\\n, \\nof \\nsin( )\\np\\nt\\n is 2 s, and its frequency is \\n1\\nP\\n,\\n or \\n12\\n cycles/s. According to the sampling theorem, we can \\nrecover this signal from a set of its samples if the sampling rate exceeds twice the highest frequenc\\ny \\nof the signal. This means that a sampling rate greater than 1 sample/s \\n()\\n21 21\\n×=\\n is required to \\n. . .\\n. . .\\nm\\n \\n0\\n/H11002\\n3\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n3\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\nF\\n(\\nm\\n)\\n~\\n0\\nH\\n(\\nm\\n)\\nm\\n \\n0\\n~\\nF\\n(\\nm\\n) \\n/H11005\\n \\nH\\n(\\nm\\n)\\nF\\n(\\nm\\n)\\nm\\n \\n0\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n. . .\\n. . .\\n/H9004\\nT\\nb\\na\\nc\\nFIGURE 4.10\\n (a) Fourier transform of an under-sampled, band-limited function. (Interference between adjacent peri-\\nods is shown dashed). (b) The same ideal lowpass ﬁlter used in Fig. 4.8. (c) The product of (a) and (b).The interfer-\\nence from adjacent periods results in aliasing that prevents perfect recovery of \\nF\\n()\\nm\\n and, consequently, of \\nft\\n()\\n.\\n \\nDIP4E_GLOBAL_Print_Ready.indb   223\\n6/16/2017   2:04:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 224}),\n",
       " Document(page_content='224\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nrecover the signal. Viewed another way, the separation, \\n/H9004\\nT\\n,\\n between samples has to be less than 1 s. \\nObserve that sampling this signal at \\nexactly\\n twice the frequenc\\ny (1 sample/s), with samples taken at \\nt\\n=\\n01\\n2\\n,, ,,\\n±±\\n…\\n results in \\n…\\n…\\nsin( ), sin( ), sin( ) ,\\n−\\npp\\n0\\n all of which are 0. This illustrates the reason \\nwhy the sampling theorem requires a sampling rate that exceeds twice the highest frequenc\\ny of the \\nfunction, as mentioned earlier.\\nThe large dots in Fig. 4.11 are samples taken uniformly at a rate \\nbelow\\n the required 1 sample/s (i.e., \\nthe samples are taken \\nmore\\n than 1 s apart; in fact, the separation between samples exceeds 2 s). The \\nsampled signal \\nlooks\\n like a sine wave, but its frequency is about one-tenth the frequency of the original \\nfunction. This sampled signal, having a frequency well below anything present in the original continu-\\nous function, is an example of aliasing. If the signal had been sampled at a rate slightly exceeding the \\nNyquist rate, the samples would not look like a sine wave at all (see Problem 4.6).\\nFigure 4.11 also illustrates how aliasing can be extremely problematic in musical recordings by intro-\\nducing frequencies not present in the original sound. In order to mitigate this, signals with frequencies \\nabove half the sampling rate \\nmust\\n be ﬁltered out to reduce the effect of aliased signals introduced into \\ndigital recordings. This is the reason why digital recording equipment contains lowpass ﬁlters speciﬁcally \\ndesigned to remove frequency components above half the sampling rate used by the equipment. \\nIf we were given just the samples in Fig. 4.11, another issue illustrating the seriousness of aliasing is \\nthat we would have no way of knowing that these samples are not a true representation of the original \\nfunction. As you will see later in this chapter, aliasing in images can produce similarly misleading results.\\nFUNCTION RECONSTRUCTION (RECOVERY) FROM SAMPLED DATA\\nIn this section, we show that reconstructing a function from a set of its samples \\nreduces in practice to interpolating between the samples. Even the simple act of \\ndisplaying an image requires reconstruction of the image from its samples by the dis-\\nplay medium. Therefore, it is important to understand the fundamentals of sampled \\ndata reconstruction. Convolution is central to developing this understanding, dem-\\nonstrating again the importance of this concept.\\nThe discussion of Fig. 4.8 and Eq. (4-34) outlines the procedure for perfect recov-\\nery of a band-limited function from its samples using frequency domain methods. \\n. . .\\n. . .\\nt \\n/H9004\\nT\\n04\\n. . .\\n123 5\\n. . .\\nFIGURE\\n \\n4.11\\n Illustration of aliasing. The under-sampled function (dots) looks like a sine wave having a frequency \\nmuch lower than the frequency of the continuous signal. The period of the sine wave is 2 s, so the zero crossings of \\nthe horizontal axis occur every second. \\n/H9004\\nT\\n is the separation between samples. \\nDIP4E_GLOBAL_Print_Ready.indb   224\\n6/16/2017   2:04:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 225}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n225\\nUsing the convolution theorem, we can obtain the equivalent result in the spatial \\ndomain. From Eq. (4-34), \\nFH F\\n()\\n()() ,\\nmm m\\n=\\n/tildenosp\\n so it follows that\\n \\nft F\\nHF\\nht\\nft\\n() ( )\\n()()\\n()\\n()\\n=\\n{}\\n=\\n{}\\n=\\n−\\n−\\n/H5219\\n/H5219\\n1\\n1\\nm\\nmm\\n/tildenosp\\n/tildenosp\\n/H22841\\n \\n(4-37)\\nwhere, as before, \\n/tildenosp\\nft\\n()\\n denotes the sampled function, and the last step follows from \\nthe convolution theorem,\\n Eq. (4-25). It can be shown (see Problem 4.13), that sub-\\nstituting Eq. (4-27) for \\n/tildenosp\\nft\\n()\\n into Eq. (4-37), and then using Eq. (4-24), leads to the \\nfollowing spatial domain expression for \\nft\\n()\\n:\\n \\nft fnT t nT T\\nn\\n() ( ) ( )\\n=−\\n[]\\n=−\\n∑\\n/H9004/H9004 /H9004\\n/H11009\\n/H11009\\nsinc\\n \\n(4-38)\\nwhere the sinc function is defined in Eq. (4-23). This result is not unexpected because \\nthe inverse F\\nourier transform of the ideal (box) filter, \\nH\\n()\\n,\\nm\\n is a sinc function (see \\nExample 4.1).\\n Equation (4-38) shows that the perfectly reconstructed function, \\nft\\n()\\n,\\n \\nis an infinite sum of sinc functions weighted by the sample values\\n. It has the impor-\\ntant property that the reconstructed function is identically equal to the sample val-\\nues at multiple integer increments of \\n/H9004\\nT\\n.\\n That is, for any \\ntk T\\n=\\n/H9004\\n,\\n where \\nk\\n is an inte-\\nger\\n, \\nft\\n()\\n is equal to the \\nk\\nth sample\\n, \\nfk T\\n()\\n.\\n/H9004\\n This follows from Eq. (4-38) because \\nsinc( )\\n01\\n=\\n and \\nsinc( )\\nm\\n=\\n0\\n for any other integer value of \\nm\\n.\\n Between sample points, \\nvalues of \\nft\\n()\\n are \\ninterpolations\\n formed by the sum of the sinc functions\\n. \\nEquation (4-38) requires an inﬁnite number of terms for the interpolations \\nbetween samples. In practice, this implies that we have to look for approximations \\nthat are \\nﬁnite\\n interpolations between the samples. As we discussed in Section 2.6, the \\nprincipal interpolation approaches used in image processing are nearest-neighbor, \\nbilinear, and bicubic interpolation. We will discuss the effects of interpolation on \\nimages in Section 4.5.\\n4.4 THE DISCRETE FOURIER TRANSFORM OF ONE VARIABLE  \\nOne of the principal goals of this chapter is the derivation of the \\ndiscrete Fourier \\ntransform\\n (DFT) starting from basic principles. The material up to this point may \\nbe viewed as the foundation of those basic principles, so now we have in place the \\nnecessary tools to derive the DFT.\\nOBTAINING THE DFT FROM THE CONTINUOUS TRANSFORM OF A \\nSAMPLED FUNCTION\\nAs we discussed in Section 4.3, the Fourier transform of a sampled, band-limited func-\\ntion extending from \\n−\\n/H11009\\n to \\n/H11009\\n is a \\ncontinuous\\n, \\nperiodic\\n function that also extends from \\n−\\n/H11009\\n to \\n/H11009\\n.\\n In practice, we work with a finite number of samples, and the objective of \\nthis section is to derive the DFT of such finite sample sets\\n.\\nEquation (4-31) gives the transform, \\n/tildenosp\\nF\\n()\\n,\\nm\\n of sampled data in terms of the trans-\\nform of the original function,\\n but it does not give us an expression for \\n/tildenosp\\nF\\n()\\nm\\n in terms \\nSee Section 2.4 regard-\\ning interpolation.\\n4.4\\nDIP4E_GLOBAL_Print_Ready.indb   225\\n6/16/2017   2:04:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 226}),\n",
       " Document(page_content='226\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nof the sampled function \\n/tildenosp\\nft\\n()\\n itself. We ﬁnd that expression directly from the deﬁni-\\ntion of the F\\nourier transform in Eq. (4-19):\\n \\n/tildenosp\\n/tildenosp\\nFf\\nt e d t\\njt\\n() ( )\\nm\\npm\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\n \\n(4-39)\\nBy substituting Eq. (4-27) for \\n/tildenosp\\nft\\n()\\n,\\n we obtain\\n \\n/tildenosp\\n/tildenosp\\nF\\nf t e dt\\nf t t n T e dt\\njt\\njt\\nn\\n( ) ()\\n() ( )\\nmd\\npm\\npm\\n== −\\n=\\n−−\\n=−\\n∑\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n22\\n22\\n/H11009 /H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n2\\nn\\njt\\nn\\njn T\\nn\\nft t nTe d t\\nfe\\n=−\\n−\\n−\\n=−\\n∑\\n∑\\n−\\n=\\n() ( )\\nd\\npm\\npm\\n2\\n2\\n \\n(4-40)\\nThe last step follows from Eq. (4-28) and the sifting property of the impulse. \\nAlthough \\nf\\nn\\n is a discrete function, its Fourier transform, \\n/tildenosp\\nF\\n()\\n,\\nm\\n is continuous and \\ninfinitely periodic with period \\n1\\n/H9004\\nT\\n,\\n as we know from Eq. (4-31). Therefore, all we \\nneed to characterize \\n/tildenosp\\nF\\n()\\nm\\n is one period, and sampling one period of this function is \\nthe basis for the DFT\\n.\\nSuppose that we want to obtain \\nM\\n equally spaced samples of \\n/tildenosp\\nF\\n()\\nm\\n taken over the \\none period interval from \\nm\\n=\\n0\\n to \\nm\\n=\\n1\\n/H9004\\nT\\n (see Fig. 4.8). This is accomplished by \\ntaking the samples at the following frequencies:\\n \\nm\\n== −\\nm\\nMT\\nmM\\n/H9004\\n012 1\\n,,, ,\\n…\\n \\n(4-41)\\nSubstituting this result for \\nm\\n into Eq. (4-40) and letting \\nF\\nm\\n denote the result yields\\n \\nFf e m M\\nmn\\nn\\nM\\njm n M\\n== −\\n=\\n−\\n−\\n∑\\n0\\n1\\n2\\n012 1\\np\\n,,, ,\\n…\\n \\n(4-42)\\nThis expression is the \\ndiscrete F\\nourier transform\\n we are seeking.\\n†\\n Given a set \\n{}\\nf\\nm\\n \\nconsisting of \\nM\\n samples of \\nft\\n()\\n,\\n Eq. (4-42) yields a set \\n{}\\nF\\nm\\n of \\nM\\n complex values \\ncorresponding to the discrete Fourier transform of the input sample set. Conversely, \\n†\\n Referring back to Fig. 4.6(b), note that the interval \\n[, ]\\n01\\n/H9004\\nT\\n over which we sampled one period of \\n/tildenosp\\nF\\n()\\nm\\n covers \\ntwo adjacent half periods of the transform (but with the lowest half of period appearing at higher frequencies).\\n \\nThis means that the data in \\nF\\nm\\n requires re-ordering to obtain samples that are ordered from the lowest to the \\nhighest frequency of the period. This is the price paid for the notational convenience of taking the samples at \\nmM\\n=−\\n01\\n2 1\\n,, , , ,\\n…\\n instead of using samples on either side of the origin, which would require the use of nega-\\ntive notation.\\n The procedure used to order the transform data will be discussed in Section 4.6.  \\nDIP4E_GLOBAL_Print_Ready.indb   226\\n6/16/2017   2:04:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 227}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n227\\ngiven \\n{} ,\\nF\\nm\\n we can recover the sample set \\n{}\\nf\\nm\\n by using the \\ninverse discrete Fourier \\ntransform\\n (IDFT)\\n \\nf\\nM\\nFe n M\\nnm\\nm\\nM\\njm n M\\n== −\\n=\\n−\\n∑\\n1\\n012 1\\n0\\n1\\n2\\np\\n,,, ,\\n…\\n \\n(4-43)\\nIt is not difficult to show (see Problem 4.15) that substituting Eq. (4-43) for \\nf\\nn\\n into \\nEq. (4-42) gives the identity \\nFF\\nmm\\n≡\\n.\\n Similarly, substituting Eq. (4-42) into Eq. (4-43) \\nfor \\nF\\nm\\n yields \\nff\\nnn\\n≡\\n.\\n This implies that Eqs. (4-42) and (4-43) constitute a \\ndiscrete \\nF\\nourier transform pair\\n. Furthermore, these identities indicate that the forward and \\ninverse Fourier transforms exist for any set of samples whose values are finite. Note \\nthat neither expression depends explicitly on the sampling interval \\n/H9004\\nT\\n,\\n nor on the \\nfrequenc\\ny intervals of Eq. (4-41). Therefore, the DFT pair is applicable to \\nany\\n finite \\nset of discrete samples taken uniformly.\\nWe used \\nm\\n and \\nn\\n in the preceding development to denote discrete variables \\nbecause it is typical to do so for derivations. However, it is more intuitive, especially \\nin two dimensions, to use the notation \\nx\\n and \\ny\\n for image coordinate variables and \\nu\\n and \\nv\\n for frequency variables, where these are understood to be integers.\\n†\\n Then, \\nEqs. (4-42) and (4-43) become \\n \\nFu fxe u M\\nju x M\\nx\\nM\\n() ()\\n,, , ,\\n== −\\n−\\n=\\n−\\n∑\\n2\\n0\\n1\\n012 1\\np\\n…\\n \\n(4-44)\\nand\\n \\nfx\\nM\\nFue x M\\nju x M\\nu\\nM\\n() ()\\n,, , ,\\n== −\\n=\\n−\\n∑\\n1\\n012 1\\n2\\n0\\n1\\np\\n…\\n \\n(4-45)\\nwhere we used functional notation instead of subscripts for simplicity. Comparing \\nEqs\\n. (4-42) through (4-45), you can see that \\nFu F\\nm\\n()\\n≡\\n and \\nfx f\\nn\\n() .\\n≡\\n From this point \\non,\\n we use Eqs. (4-44) and (4-45) to denote the 1-D DFT pair. As in the continuous \\ncase, we often refer to Eq. (4-44) as the \\nforward\\n DFT of \\nfx\\n()\\n,\\n and to Eq. (4-45) as \\nthe \\ninverse\\n DFT of \\nFu\\n()\\n.\\n As before, we use the notation \\nfx Fu\\n()\\n()\\n⇔\\n to denote a \\nF\\nourier transform pair. Sometimes you will encounter in the literature the \\n1\\nM\\n term \\nin front of Eq.\\n (4-44) instead. That does not affect the proof that the two equations \\nform a Fourier transform pair (see Problem 4.15). \\nKnowledge that \\nfx\\n()\\n and \\nFu\\n()\\n are a transform pair is useful in proving relation-\\nships between functions and their transforms\\n. For example, you are asked in Prob-\\nlem 4.17 to show that \\nfx x Fue\\nju x M\\n() ( )\\n−⇔\\n−\\n0\\n2\\n0\\np\\n is a Fourier transform pair. That is, \\nyou have to show that the DFT of \\nfx x\\n()\\n−\\n0\\n is \\nFue\\nju x M\\n()\\n−\\n2\\n0\\np\\n and, conversely, that \\nthe \\ninverse\\n DFT of \\nFue\\nju x M\\n()\\n−\\n2\\n0\\np\\n is \\nfx x\\n()\\n.\\n−\\n0\\n Because this is done by substituting \\n†\\n  We have been careful in using \\nt\\n for \\ncontinuous\\n spatial variables and \\nm\\n for the corresponding \\ncontinuous\\n fre-\\nquenc\\ny variables. From this point on, we will use \\nx\\n and \\nu\\n to denote 1-D \\ndiscrete\\n spatial and frequency variables, \\nrespectively. When working in 2-D, we will use \\n(, )\\ntz\\n, and \\n(,) ,\\nmn\\n to denote continuous spatial and frequency \\ndomain variables\\n, respectively. Similarly, we will use \\n(, )\\nxy\\n and \\n(,)\\nuv\\n to denote their discrete counterparts. \\nDIP4E_GLOBAL_Print_Ready.indb   227\\n6/16/2017   2:04:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 228}),\n",
       " Document(page_content='228\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ndirectly into Eqs. (4-44) and (4-45), and you will have proved already that these two \\nequations constitute a Fourier transform pair (Problem 4.15), if you prove that one \\nside of “\\n⇔\\n” is the DFT (IDFT) of the other, then it must be true the other side is the \\nIDFT (DFT) of the side you just proved.\\n It turns out that having the option to prove \\none side or the other often simpliﬁes proofs signiﬁcantly.\\n \\nThis is true also of the 1-D \\ncontinuous and 2-D continuous and discrete F\\nourier transform pairs.\\nIt can be shown (see Problem 4.16) that both the forward and inverse discrete \\ntransforms are inﬁnitely periodic, with period \\nM\\n. That is,\\n \\nFu Fu k M\\n()\\n( )\\n=+\\n \\n(4-46)\\nand\\n \\nfx fx k M\\n()\\n( )\\n=+\\n \\n(4-47)\\nwhere \\nk\\n is an integer\\n.\\nThe discrete equivalent of the 1-D convolution in Eq. (4-24) is\\n \\nfx\\nfm h x m x M\\nhx\\nm\\nM\\n()\\n( )( ) ,, , ,\\n()\\n/H22841\\n=\\n−= −\\n=\\n−\\n∑\\n0\\n1\\n012 1\\n…\\n \\n(4-48)\\nBecause in the preceding formulations the functions are periodic, their convolu-\\ntion also is periodic\\n. Equation (4-48) gives one period of the periodic convolution. \\nFor this reason, this equation often is referred to as \\ncircular convolution\\n. This is a \\ndirect result of the periodicity of the DFT and its inverse. This is in contrast with the \\nconvolution you studied in Section 3.4, in which values of the displacement, \\nx\\n, were \\ndetermined by the requirement of sliding one function completely past the other, \\nand were not fixed to the range \\n[, ]\\n01\\nM\\n−\\n as in circular convolution. We will discuss \\nthis difference and its significance in Section 4.6 and in F\\nig. 4.27.\\nFinally, we point out that the convolution theorem given in Eqs. (4-25) and (4-26) \\nis applicable also to discrete variables, with the exception that the right side of \\nEq. (4-26) is multiplied by \\n1\\nM\\n (Problem 4.18).\\nRELATIONSHIP BETWEEN THE SAMPLING AND FREQUENCY  \\nINTERVALS\\nIf \\nfx\\n()\\n consists of \\nM\\n samples of a function \\nft\\n()\\n taken \\n/H9004\\nT\\n units apart, the length of \\nthe record comprising the set \\nfx x M\\n()\\n, ,, , , ,\\n{}\\n=−\\n012 1\\n…\\n is\\n \\nTM T\\n=\\n/H9004\\n \\n(4-49)\\nThe corresponding spacing, \\n/H9004\\nu\\n, in the frequency domain follows from Eq. (4-41):\\n \\n/H9004\\n/H9004\\nu\\n==\\n11\\nMT T\\n \\n(4-50)\\nDIP4E_GLOBAL_Print_Ready.indb   228\\n6/16/2017   2:04:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 229}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n229\\nThe entire frequency range spanned by the \\nM\\n components of the DFT is then\\n \\nRM u\\nT\\n==\\n/H9004\\n/H9004\\n1\\n \\n(4-51)\\nThus, we see from Eqs. (4-50) and (4-51) that the resolution in frequency, \\n/H9004\\nu\\n,\\n of \\nthe DFT depends inversely on the length (duration,\\n if \\nt\\n is time) of the record, \\nT\\n, \\nover which the continuous function, \\nft\\n()\\n, is sampled; and the range of frequencies \\nspanned by the DFT depends on the sampling interval \\n/H9004\\nT\\n.\\n Keep in mind these \\ninverse\\n relationships between \\n/H9004\\nu\\n and \\n/H9004\\nT\\n.\\nEXAMPLE 4.4 :  The mechanics of computing the DFT.\\nFigure 4.12(a) shows four samples of a continuous function, \\nft\\n()\\n,\\n taken \\n/H9004\\nT\\n units apart. Figure 4.12(b) \\nshows the samples in the \\nx\\n-domain.\\n The values of \\nx\\n are 0, 1, 2, and 3, which refer to the number of the \\nsamples in sequence, counting up from 0. For example, \\nff tT\\n()\\n( ) ,\\n22\\n0\\n=+\\n/H9004\\n the third sample of \\nft\\n()\\n.\\nFrom Eq. (4-44), the ﬁrst value of \\nFu\\n()\\n [i.e., \\nF\\n()\\n]\\n0  is\\n \\nFf x f f f f\\nx\\n() () () () () ()\\n0 0123 1 2 4 4 1 1\\n0\\n3\\n== + + +\\n[]\\n=+++=\\n=\\n∑\\nThe next value of \\nFu\\n()\\n is\\n \\nFf x e e e e e j\\njx\\nx\\njj j\\n() ( )\\n()\\n11\\n2\\n4\\n4\\n3\\n2\\n21 4\\n0\\n3\\n02 3 2\\n==\\n+ +\\n+ =\\n−\\n+\\n−\\n=\\n−− −\\n∑\\npp\\np\\np\\nSimilarly, \\nFj\\n()\\n( )\\n21 0\\n=− +\\n and \\nFj\\n()\\n( ) .\\n33 2\\n=− +\\n Observe that \\nall\\n values of \\nfx\\n()\\n are used in computing \\neac\\nh\\n value of \\nFu\\n()\\n.\\nIf we were given \\nFu\\n()\\n instead, and were asked to compute its inverse, we would proceed in the same \\nmanner\\n, but using the inverse Fourier transform. For instance, \\n \\nfF u e F u j j\\nju\\nuu\\n() ()\\n()\\n()\\n0\\n1\\n4\\n1\\n4\\n1\\n4\\n11 3 2 1 3 2\\n1\\n4\\n20\\n0\\n3\\n0\\n3\\n== =\\n−\\n+\\n−\\n−\\n−\\n[]\\n=\\n==\\n∑∑\\np\\n[\\n[]\\n41\\n=\\nwhich agrees with Fig. 4.12(b). The other values of \\nfx\\n()\\n are obtained in a similar manner.\\n1\\n02\\n3\\nt\\nf\\n(\\nt\\n)\\n0\\n1\\n2\\n3\\n4\\n5\\nf\\n(\\nx\\n)\\n0\\n1\\n2\\n3\\n4\\n5\\nx \\nt\\n0\\nt\\n0\\n \\n/H11001\\n \\n/H9004\\nTt\\n0\\n \\n/H11001\\n 2\\n/H9004\\nTt\\n0\\n \\n/H11001\\n 3\\n/H9004\\nT\\n0\\nb a\\nFIGURE 4.12\\n(a) A continuous \\nfunction sampled \\n/H9004\\nT\\n units apart. \\n(b) Samples in the \\nx\\n-domain.\\n  \\nVariable \\nt\\n is  \\ncontinuous, while \\nx\\n is discrete.\\nDIP4E_GLOBAL_Print_Ready.indb   229\\n6/16/2017   2:04:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 230}),\n",
       " Document(page_content='230\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.5 EXTENSIONS TO FUNCTIONS OF TWO VARIABLES  \\nIn the following discussion we extend to two variables the concepts introduced in \\nthe previous sections of this chapter.\\nTHE 2-D IMPULSE AND ITS SIFTING PROPERTY\\nThe impulse, \\nd\\n(,\\n) ,\\ntz\\n of two continuous variables, \\nt\\n and \\nz\\n,\\n is defined as before:\\n \\nd\\n(,\\n)\\ntz\\ntz\\n=\\n==\\n⎧\\n⎨\\n⎩\\n10\\n0\\nif \\notherwise\\n \\n(4-52)\\nand\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nd\\n(, )\\nt z dtdz\\n=\\n1  \\n(4-53)\\nAs in the 1-D case, the 2-D impulse exhibits the \\nsifting property\\n under integration,\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nftz tzd t d z f\\n(, ) (, ) ( , )\\nd\\n=\\n00\\n \\n(4-54)\\nor. more generally for an impulse located at \\n(, ) ,\\ntz\\n00\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nf t z t tz zd t d z f tz\\n(, ) ( , ) ( , )\\nd\\n−− =\\n00 0 0\\n \\n(4-55)\\nAs before, we see that the sifting property yields the value of the function at the \\nlocation of the impulse\\n.\\nFor discrete variables \\nx\\n and \\ny\\n, the 2-D discrete unit impulse is deﬁned as\\n \\nd\\n(,\\n)\\nxy\\nxy\\n=\\n==\\n⎧\\n⎨\\n⎩\\n10\\n0\\nif \\notherwise\\n \\n(4-56)\\nand its sifting property is\\n \\nfx y x y f\\ny x\\n(,)(,) (,)\\nd\\n=\\n=− =−\\n∑ ∑\\n00\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n \\n(4-57)\\nwhere \\nfx y\\n(,\\n)\\n is a function of discrete variables \\nx\\n and \\ny\\n.\\n For an impulse located at \\ncoordinates \\n(,)\\nxy\\n00\\n (see Fig. 4.13) the sifting property is\\n \\nf x y x xy y f xy\\ny x\\n(,)( , ) ( , )\\nd\\n−− =\\n=− =−\\n∑ ∑\\n00 0 0\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n \\n(4-58)\\nWhen working with an image of finite dimensions, the limits in the two preceding \\nequations are replaced by the dimensions of the image\\n.\\n4.5\\nDIP4E_GLOBAL_Print_Ready.indb   230\\n6/16/2017   2:04:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 231}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n231\\nTHE 2-D CONTINUOUS FOURIER TRANSFORM PAIR\\nLet \\nftz\\n(,\\n)\\n be a continuous function of two continuous variables, \\nt\\n and \\nz\\n.\\n The two-\\ndimensional, continuous Fourier transform pair is given by the expressions\\n \\nFf\\nt\\nz\\ne d\\nt\\nd\\nz\\njt z\\n(,) ( ,)\\n()\\nmn\\npm n\\n=\\n−+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\n \\n(4-59)\\nand\\n \\nftz F e d d\\njt z\\n(, )\\n( , )\\n()\\n=\\n+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nmn\\nm n\\npm n\\n2\\n \\n(4-60)\\nwhere \\nm\\n and \\nn\\n are the frequency variables. When referring to images, \\nt\\n and \\nz\\n are \\ninterpreted to be continuous \\nspatial\\n variables\\n. As in the 1-D case, the domain of the \\nvariables \\nm\\n and \\nn\\n defines the \\ncontinuous frequency domain\\n.\\nEXAMPLE 4.5 :  Obtaining the Fourier transform of a 2-D box function.\\nFigure 4.14(a) shows the 2-D equivalent of the 1-D box function in Example 4.1. Following a procedure \\nsimilar to the one used in that example gives the result \\n \\nFf\\nt\\nz\\ne d\\nt\\nd\\nz A\\ne\\njt z\\nj\\nT\\nT\\nZ\\nZ\\n(,) ( ,)\\n()\\n(\\nmn\\npm n\\npm\\n==\\n−+\\n−\\n−−\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2 2\\n22\\n2\\n2\\n2\\n2\\nt\\ntz\\ndt dz\\nATZ\\nT\\nT\\nZ\\nZ\\n+\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nn\\npm\\npm\\npn\\npn\\n)\\nsin( )\\n()\\nsin( )\\n()\\nFigure 4.14(b) shows a portion of the spectrum about the origin. As in the 1-D case, the locations of the \\nzeros in the spectrum are inversely proportional to the values of \\nT\\n and \\nZ\\n.\\n In this example, \\nT\\n is larger \\nthan \\nZ\\n, so the spectrum is the more “contracted” along the \\nm\\n-axis.\\n \\n2-D SAMPLING AND THE 2-D SAMPLING THEOREM\\nIn a manner similar to the 1-D case, sampling in two dimensions can be modeled \\nusing a sampling function (i.e., a 2-D impulse train):\\nx\\n0\\ny\\n0\\n x\\nd\\n(\\nx \\n/H11002\\n x\\n0\\n, \\ny \\n/H11002\\n y\\n0\\n)\\ny \\n1\\nd\\nFIGURE 4.13\\n2-D unit discrete  \\nimpulse. Variables \\nx\\n and \\ny\\n are  \\ndiscrete, and \\nd\\n is \\nzero everywhere \\nexcept at  \\ncoordinates \\n(,) ,\\nxy\\n00\\n where its \\nvalue is 1.\\nDIP4E_GLOBAL_Print_Ready.indb   231\\n6/16/2017   2:04:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 232}),\n",
       " Document(page_content='232\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nst z t m T z n Z\\nTZ\\nn m\\n/H9004/H9004\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004/H9004\\n(, )\\n( , )\\n=− −\\n=− =−\\n∑ ∑\\nd\\n \\n(4-61)\\nwhere \\n/H9004\\nT\\n and \\n/H9004\\nZ\\n are the separations between samples along the \\nt\\n- and \\nz\\n-axis of \\nthe continuous function \\nftz\\n(,\\n) .\\n Equation (4-61) describes a set of periodic impulses \\nextending infinitely along the two axes (see F\\nig. 4.15). As in the 1-D case illustrated \\nin Fig. 4.5, multiplying \\nftz\\n(,\\n)\\n by \\nst z\\nTZ\\n/H9004/H9004\\n(, )\\n yields the sampled function. \\nFunction \\nftz\\n(,\\n)\\n is said to be \\nband limited\\n if its F\\nourier transform is 0 outside a \\nrectangle established in the frequency domain by the intervals \\nmm\\n−\\n[]\\nmax max\\n,\\n and \\nnn\\n−\\n[]\\nmax max\\n,;\\n that is,\\n \\nF\\n(,)\\nmax\\nmax\\nmn\\nm m n n\\n=\\n0 for    and  \\n≥≥\\n \\n(4-62)\\nThe \\ntwo-dimensional sampling theorem\\n states that a continuous\\n, band-limited func-\\ntion \\nftz\\n(,\\n)\\n can be recovered with no error from a set of its samples if the sampling \\nintervals are\\n \\n/H9004\\nT\\n<\\n1\\n2\\nm\\nmax\\n \\n(4-63)\\nand\\n \\n/H9004\\nZ\\n<\\n1\\n2\\nn\\nmax\\n \\n(4-64)\\nor, expressed in terms of the sampling rate, if\\nZ\\nT\\nT\\n/\\n2\\nZ\\n/\\n2\\n t\\nf\\n(\\nt\\n, \\nz\\n)\\nz\\nm\\nn\\nATZ\\n/H11341\\nF\\n(\\nm\\n, \\nn\\n)\\n/H11341\\nA\\nb a\\nFIGURE 4.14\\n(a) A 2-D function \\nand (b) a section \\nof its spectrum. \\nThe box is longer \\nalong the \\nt\\n-axis, \\nso the spectrum is \\nmore contracted \\nalong the \\nm\\n-axis.\\n  t\\n/H9004\\nZ\\n/H9004\\nT\\ns\\n/H9004\\nT\\n/H9004\\nZ\\n(\\nt\\n, \\nz\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\nz \\nFIGURE 4.15\\n2-D impulse train.\\nDIP4E_GLOBAL_Print_Ready.indb   232\\n6/16/2017   2:04:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 233}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n233\\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n(4-65)\\nand\\n \\n1\\n2\\n/H9004\\nZ\\n>\\nn\\nmax\\n(4-66)\\nStated another way, we say that no information is lost if a 2-D, band-limited, con-\\ntinuous function is represented by samples acquired at rates greater than twice the \\nhighest frequenc\\ny content of the function in both the \\nm\\n- and \\nn\\n-\\ndirections.\\nFigure 4.16 shows the 2-D equivalents of Figs. 4.6(b) and (d). A 2-D ideal ﬁl-\\nter transfer function has the form illustrated in F\\nig. 4.14(a) (but in the frequency \\ndomain). The dashed portion of Fig. 4.16(a) shows the location of the ﬁlter function \\nto achieve the necessary isolation of a single period of the transform for recon-\\nstruction of a band-limited function from its samples, as in Fig. 4.8. From Fig 4.10, \\nwe know that if the function is under-sampled, the periods overlap, and it becomes \\nimpossible to isolate a single period, as Fig. 4.16(b) shows. Aliasing would result \\nunder such conditions.\\nALIASING IN IMAGES\\nIn this section, we extend the concept of aliasing to images, and discuss in detail sev-\\neral aspects of aliasing related to image sampling and resampling.\\nExtensions from 1-D Aliasing\\nAs in the 1-D case, a continuous function \\nftz\\n(,\\n)\\n of two continuous variables, \\nt\\n and \\nz\\n, \\ncan be band-limited in general only if it extends infinitely in both coordinate direc-\\ntions\\n. The very act of limiting the spatial duration of the function (e.g., by multiply-\\ning it by a box function) introduces corrupting frequency components extending to \\ninfinity in the frequency domain, as explained in Section 4.3 (see also Problem 4.12). \\nBecause we cannot sample a function infinitely, aliasing is always present in digital \\nimages, just as it is present in sampled 1-D functions. There are two principal mani-\\nfestations of aliasing in images: spatial aliasing and temporal aliasing. \\nSpatial aliasing\\n \\nis caused by under-sampling, as discussed in Section 4.3, and tends to be more visible \\nm\\nm\\nv\\nv\\nmax\\nv\\nm\\nmax\\nFootprint of a\\n2-D ideal lowpass\\n(box) filter\\nb a\\nFIGURE 4.16\\nTwo-dimensional \\nFourier  \\ntransforms of (a) an \\nover-sampled, and \\n(b) an under-sam-\\npled, band-limited \\nfunction. \\nDIP4E_GLOBAL_Print_Ready.indb   233\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 234}),\n",
       " Document(page_content='234\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n(and objectionable) in images with repetitive patterns. \\nTemporal aliasing\\n is related \\nto time intervals between images of a sequence of dynamic images. One of the most \\ncommon examples of temporal aliasing is the “wagon wheel” effect, in which wheels \\nwith spokes in a sequence of images (for example, in a movie) appear to be rotating \\nbackwards. This is caused by the frame rate being too low with respect to the speed \\nof wheel rotation in the sequence, and is similar to the phenomenon described in \\nFig. 4.11, in which under sampling produced a signal that appeared to be of much \\nlower frequency than the original. \\nOur focus in this chapter is on spatial aliasing. The key concerns with spatial alias-\\ning in images are the introduction of artifacts such as jaggedness in line features, spu-\\nrious highlights, and the appearance of frequency patterns not present in the original \\nimage. Just as we used Fig. 4.9 to explain aliasing in 1-D functions, we can develop \\nan intuitive grasp of the nature of aliasing in images using some simple graphics. The \\nsampling grid in the center section of Fig. 4.17 is a 2-D representation of the impulse \\ntrain in Fig. 4.15. In the grid, the little white squares correspond to the location of the \\nimpulses (where the image is sampled) and black represents the separation between \\nsamples. Superimposing the sampling grid on an image is analogous to multiplying \\nthe image by an impulse train, so the same sampling concepts we discussed in con-\\nnection with the impulse train in Fig. 4.15 are applicable here. The focus now is to \\nanalyze graphically the interaction between sampling rate (the separation of the \\nsampling points in the grid) and the frequency of the 2-D signals being sampled.\\nFigure 4.17 shows a sampling grid partially overlapping three 2-D signals (regions \\nof an image) of low, mid, and high spatial frequencies (relative to the separation \\nbetween sampling cells in the grid). Note that the level of spatial “detail” in the \\nregions is proportional to frequency (i.e., higher-frequency signals contain more \\nbars). The sections of the regions inside the sampling grip are rough manifestations \\nof how they would appear after sampling. As expected, all three digitized regions \\nSampling grid\\nLow frequency\\nMid frequency\\nHigh frequency\\nFIGURE 4.17\\nVarious aliasing \\neffects resulting \\nfrom the  \\ninteraction  \\nbetween the \\nfrequency of 2-D \\nsignals and the \\nsampling rate \\nused to digitize \\nthem. The regions \\noutside the \\nsampling grid are \\ncontinuous and \\nfree of aliasing.\\nDIP4E_GLOBAL_Print_Ready.indb   234\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 235}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n235\\nexhibit aliasing to some degree, but the effects are dramatically different, worsening \\nas the discrepancy between detail (frequency) and sampling rate increases. The low-\\nfrequency region is rendered reasonably well, with some mild jaggedness around \\nthe edges. The jaggedness increases as the frequency of the region increases to the \\nmid-range because the sampling rate is the same. This edge distortion (appropriately \\ncalled \\njaggies\\n) is common in images with strong line and/or edge content.\\nThe digitized high-frequency region in the top right of Fig. 4.17 exhibits totally \\ndifferent and somewhat surprising behavior. Additional stripes (of lower frequen-\\ncy) appear in the digitized section, and these stripes are rotated signiﬁcantly with \\nrespect to the direction of the stripes in the continuous region. These stripes are an \\nalias of a totally different signal. As the following example shows, this type of behav-\\nior can result in images that appear “normal” and yet bear no relation to the original.\\nEXAMPLE 4.6 :  Aliasing in images.\\nConsider an imaging system that is perfect, in the sense that it is noiseless and produces an exact digi-\\ntal image of what it sees, but the number of samples it can take is ﬁxed at \\n96 96\\n×\\n pixels. For simplicity, \\nassume that pixels are little squares of unit width and length.\\n We want to use this system to digitize \\ncheckerboard images of alternating black and white squares. Checkerboard images can be interpreted \\nas periodic, extending inﬁnitely in both dimensions, where one period is equal to adjacent black/white \\npairs. If we specify “valid” digitized images as being those extracted from an inﬁnite sequence in such \\na way that the image contains an integer multiple of periods, then, based on our earlier comments, we \\nknow that properly sampled periodic images will be free of aliasing. In the present example, this means \\nthat the sizes of the squares must be such that dividing 96 by the size yields an even number. This will \\ngive an integer number of periods (pairs of black/white squares). The smallest size of squares under the \\nstated conditions is 1 pixel.\\nThe principal objective of this example is to examine what happens when checkerboard images with \\nsquares of sizes less than 1 pixel on the side are presented to the system. This will correspond to the \\nundersampled case discussed earlier, which will result in aliasing. A horizontal or vertical scan line of the \\ncheckerboard images results in a 1-D square wave, so we can focus the analysis on 1-D signals.\\nTo understand the capabilities of our imaging system in terms of sampling, recall from the discussion \\nof the 1-D sampling theorem that, given the sampling rate, the maximum frequency allowed before \\naliasing occurs in the sampled signal has to be less than one-half the sampling rate. Our sampling rate is \\nﬁxed, at one sample per unit of the independent variable (the units are pixels). Therefore, the maximum \\nfrequency our signal can have in order to avoid aliasing is 1/2 cycle/pixel. \\nWe can arrive at the same conclusion by noting that the most demanding image our system can \\nhandle is when the squares are 1 unit (pixel) wide, in which case the period (cycle) is two pixels. The \\nfrequency is the reciprocal of the period, or 1/2 cycle/pixel, as in the previous paragraph. \\nFigures 4.18(a) and (b) show the result of sampling checkerboard images whose squares are of sizes \\n16 16\\n×\\n and \\n66\\n×\\n pixels, respectively. The frequencies of scan lines in either direction of these two images \\nare 1/32 and 1/6 c\\nycles/pixel. These are well below the 1/2 cycles/pixel allowed for our system. Because, as \\nmentioned earlier, the images are perfectly registered in the ﬁeld of view of the system, the results are free \\nof aliasing, as expected.\\nWhen the size of the squares is reduced to slightly less than one pixel, a severely aliased image results, \\nas Fig. 4.18(c) shows (the squares used were approximately of size \\n09 5 09 5\\n..\\n×\\n pixels). Finally, reducing \\nDIP4E_GLOBAL_Print_Ready.indb   235\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='236\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthe size of the squares to slightly less than 0.5 pixels on the side yielded the image in Fig. 4.18(d). In \\nthis case, the aliased result looks like a normal checkerboard pattern. In fact, this image would result \\nfrom sampling a checkerboard image whose squares are 12 pixels on the side. This last image is a good \\nreminder that aliasing can create results that may be visually quite misleading. \\nThe effects of aliasing can be reduced by slightly defocusing the image to be digi-\\ntized so that high frequencies are attenuated. As explained in Section 4.3, anti-alias-\\ning ﬁltering has to be done at the “front-end,” \\nbefore\\n the image is sampled. There \\nare no such things as after-the-fact software anti-aliasing ﬁlters that can be used to \\nreduce the effects of aliasing caused by violations of the sampling theorem. Most \\ncommercial digital image manipulation packages do have a feature called “anti-\\naliasing.” However, as illustrated in Example 4.8 below, this term is related to blur-\\nring a digital image to reduce additional aliasing artifacts caused by resampling. The \\nterm does not apply to reducing aliasing in the original sampled image. A signiﬁcant \\nnumber of commercial digital cameras have true anti-aliasing ﬁltering built in, either \\nin the lens or on the surface of the sensor itself. Even nature uses this approach to \\nreduce the effects of aliasing in the human eye, as the following example shows.\\nEXAMPLE 4.7 :  Nature obeys the limits of the sampling theorem.\\nWhen discussing Figs. 2.1 and 2.2, we mentioned that cones are the sensors responsible for sharp vision. \\nCones are concentrated in the fovea, in line with the visual axis of the lens, and their concentration is \\nmeasured in degrees off that axis. A standard test of visual acuity (the ability to resolve ﬁne detail) in \\nhumans is to place a pattern of alternating black and white stripes in one degree of the visual ﬁeld. If the \\ntotal number of stripes exceeds 120 (i.e., a frequency of 60 cycles/degree), experimental evidence shows \\nthat the observer will perceive the image as a single gray mass. That is, the lens in the eye automatically \\nlowpass ﬁlters spatial frequencies higher than 60 cycles/degree. Sampling in the eye is done by the cones, \\nso, based on the sampling theorem, we would expect the eye to have on the order of 120 cones/degree \\nin order to avoid the effects of aliasing. As it turns out, that is exactly what we have! \\nb a\\nd c\\nFIGURE 4.18\\nAliasing. In (a) and \\n(b) the squares are \\nof sizes 16 and 6 \\npixels on the side. \\nIn (c) and (d) the \\nsquares are of sizes \\n0.95 and 0.48 pixels, \\nrespectively. Each \\nsmall square in (c) \\nis one pixel. Both \\n(c) and (d) are \\naliased. Note how \\n(d) masquerades as \\na “normal” image.\\nDIP4E_GLOBAL_Print_Ready.indb   236\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 237}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n237\\nImage Resampling and Interpolation\\nAs in the 1-D case, perfect reconstruction of a band-limited image function from a set \\nof its samples requires 2-D convolution in the spatial domain with a sinc function.  As \\nexplained in Section 4.3, this theoretically perfect reconstruction requires interpola-\\ntion using infinite summations which, in practice, forces us to look for approximate \\ninterpolation methods. One of the most common applications of 2-D interpolation \\nin image processing is in image resizing (zooming and shrinking). Zooming may \\nbe viewed as over-sampling, while shrinking may be viewed as under-sampling. The \\nkey difference between these two operations and the sampling concepts discussed \\nin previous sections is that we are applying zooming and shrinking to digital images.\\nWe introduced interpolation in Section 2.4. Our interest there was to illustrate the \\nperformance of nearest neighbor, bilinear, and bicubic interpolation. In this section, \\nthe focus is on sampling and anti-aliasing issues. Aliasing generally is introduced \\nwhen an image is scaled, either by zooming or by shrinking. For example, a special \\ncase of nearest neighbor interpolation is zooming by \\npixel replication\\n, which we use \\nto increase the size of an image an integer number of times. To double the size of \\nan image, we duplicate each column. This doubles the image size in the horizontal \\ndirection. Then, we duplicate each row of the enlarged image to double the size in \\nthe vertical direction. The same procedure is used to enlarge the image any integer \\nnumber of times. The intensity level assignment of each pixel is predetermined by \\nthe fact that new locations are exact duplicates of old locations. In this crude method \\nof enlargement, one of the principal aliasing effects is the introduction of jaggies \\non straight lines that are not horizontal or vertical. The effects of aliasing in image \\nenlargement often are reduced signiﬁcantly by using more sophisticated interpola-\\ntion, as we discussed in Section 2.4. We show in the following example that aliasing \\ncan also be a serious problem in image shrinking. \\nEXAMPLE 4.8 :  Illustration of aliasing in resampled natural images.\\nThe effects of aliasing generally are worsened when the size of a digital image is reduced. Figure 4.19(a) \\nis an image containing regions purposely selected to illustrate the effects of aliasing (note the thinly \\nspaced parallel lines in all garments worn by the subject). There are no objectionable aliasing artifacts \\nin Fig. 4.19(a), indicating that the sampling rate used initially was sufﬁcient to mitigate visible aliasing. \\nIn Fig. 4.19(b), the image was reduced to 33% of its original size using row/column deletion. The \\neffects of aliasing are quite visible in this image (see, for example, the areas around scarf and the sub-\\nject’s knees). Images (a) and (b) are shown in the same size because the reduced image was brought \\nback to its original size by pixel replication (the replication did not alter appreciably the effects of alias-\\ning just discussed. \\nThe digital “equivalent” of the defocusing of continuous images mentioned earlier for reducing alias-\\ning, is to attenuate the high frequencies of a \\ndigital\\n image by smoothing it with a lowpass ﬁlter before \\nresampling. Figure 4.19(c) was processed in the same manner as Fig. 4.19(b), but the original image was \\nsmoothed using a \\n55\\n×\\n spatial averaging ﬁlter (see Section 3.5) before reducing its size. The improve-\\nment over F\\nig. 4.19(b) is evident. The image is slightly more blurred than (a) and (b), but aliasing is no \\nlonger objectionable.\\nDIP4E_GLOBAL_Print_Ready.indb   237\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 238}),\n",
       " Document(page_content='238\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nAliasing and Moiré Patterns\\nIn optics, a \\nmoiré pattern\\n is a secondary, visual phenomenon produced, for example, \\nby superimposing two gratings of approximately equal spacing. These patterns are \\ncommon, everyday occurrences. For instance, we see them in overlapping insect win-\\ndow screens and on the interference between TV raster lines and striped or high-\\nly textured materials in the background, or worn by individuals. In digital image \\nprocessing, moiré-like patterns arise routinely when sampling media print, such as \\nnewspapers and magazines, or in images with periodic components whose spacing \\nis comparable to the spacing between samples. It is important to note that moiré \\npatterns are more general than sampling artifacts. For instance, Fig. 4.20 shows the \\nmoiré effect using vector drawings that have not been digitized. Separately, the pat-\\nterns are clean and void of interference. However, the simple acts of superimposing \\none pattern on the other creates a pattern with frequencies not present in either of \\nthe original patterns. Note in particular the moiré effect produced by two patterns \\nof dots, as this is the effect of interest in the following discussion.\\nEXAMPLE 4.9 :  Sampling printed media.\\nNewspapers and other printed materials use so called \\nhalftone dots\\n, which are black dots or ellipses \\nwhose sizes and various grouping schemes are used to simulate gray tones. As a rule, the following num-\\nbers are typical: newspapers are printed using 75 halftone dots per inch (dpi), magazines use 133 dpi, and \\nThe term \\nmoiré\\n is a \\nFrench word (not the \\nname of a person) that \\nappears to have  \\noriginated with weavers, \\nwho ﬁrst noticed what \\nappeared to be interfer-\\nence patterns visible on \\nsome fabrics. The root \\nof the word is from the \\nword \\nmohair\\n, a cloth \\nmade from Angora goat \\nhairs.\\nb a\\nc\\nFIGURE 4.19\\n Illustration of aliasing on resampled natural images. (a) A digital image of size \\n772 548\\n×\\n pixels with visu-\\nally negligible aliasing\\n. (b) Result of resizing the image to 33% of its original size by pixel deletion and then restor-\\ning it to its original size by pixel replication. Aliasing is clearly visible. (c) Result of blurring the image in (a) with an \\naveraging ﬁlter prior to resizing. The image is slightly more blurred than (b), but aliasing is not longer objectionable. \\n(Original image courtesy of the Signal Compression Laboratory, University of California, Santa Barbara.) \\nDIP4E_GLOBAL_Print_Ready.indb   238\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 239}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n239\\nb a\\nc\\ne d\\nf\\nFIGURE 4.20\\nExamples of the \\nmoiré effect. \\nThese are vector \\ndrawings, not \\ndigitized patterns. \\nSuperimposing \\none pattern on the \\nother is analogous \\nto multiplying the \\npatterns. \\nhigh-quality brochures use 175 dpi. Figure 4.21 shows what happens when a newspaper image is (under) \\nsampled at 75 dpi. The sampling lattice (which is oriented vertically and horizontally) and dot patterns \\non the newspaper image (oriented at \\n±°\\n45\\n) interact to create a uniform moiré-like pattern that makes \\nthe image look blotchy\\n. (We will discuss a technique in Section 4.10 for reducing the effects of moiré \\npatterns in under-sampled print media.)\\nFIGURE 4.21\\nA newspaper \\nimage digitized at \\n75 dpi. Note the \\nmoiré-like pattern \\nresulting from \\nthe interaction \\nbetween the \\n±°\\n45\\n \\norientation of the \\nhalf-tone dots and \\nthe north-south \\norientation of the \\nsampling elements \\nused to digitized \\nthe image.\\nDIP4E_GLOBAL_Print_Ready.indb   239\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 240}),\n",
       " Document(page_content='240\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nTHE 2-D DISCRETE FOURIER TRANSFORM AND ITS INVERSE\\nA development similar to the material in Sections 4.3 and 4.4 would yield the follow-\\ning 2-D discrete Fourier transform (DFT):\\n \\nFf\\nx\\ny\\ne\\njx M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nuv\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n \\n(4-67)\\nwhere \\nfx y\\n(,\\n)\\n is a digital image of size \\nMN\\n×\\n.\\n As in the 1-D case, Eq. (4-67) must be \\nevaluated for values of the discrete variables \\nu\\n and \\nv\\n in the ranges \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n \\nand \\nv\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n†\\n Given the transform \\nF\\n(,\\n) ,\\nuv\\n we can obtain \\nfx y\\n(,\\n)\\n by using the \\ninverse discrete \\nF\\nourier transform\\n (IDFT):\\n \\nfx y\\nMN\\nFu e\\nju x My N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nv\\nv\\nv u\\np\\n \\n(4-68)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As in the 1-D case, [Eqs. (4-44) \\nand (4-45)],\\n Eqs. (4-67) and (4-68) constitute a 2-D \\ndiscrete Fourier transform pair,\\n \\nfx y F\\n(,\\n) (,) .\\n⇔\\nuv\\n (The proof is a straightforward extension of the 1-D case in Prob-\\nlem 4.15.) \\nThe rest of this chapter is based on properties of these two equations and \\ntheir use for image filtering in the frequency domain. The comments made in con-\\nnection with Eqs. (4-44) and (4-45) are applicable to Eqs. (4-67) and (4-68); that is, \\nknowing that \\nfx y\\n(,\\n)\\n and \\nF\\n(,\\n)\\nuv\\n are a Fourier transform pair can be quite useful in \\nproving relationships between functions and their transforms\\n. \\n4.6 SOME PROPERTIES OF THE 2-D DFT AND IDFT \\nIn this section, we introduce several properties of the 2-D discrete Fourier transform \\nand its inverse.\\nRELATIONSHIPS BETWEEN SPATIAL AND FREQUENCY INTERVALS\\nThe relationships between spatial sampling and the corresponding frequency \\ndomain intervals are as explained in Section 4.4. Suppose that a continuous func-\\ntion \\nftz\\n(,\\n)\\n is sampled to form a digital image, \\nfx y\\n(,\\n) ,\\n consisting of \\nMN\\n×\\n samples \\ntaken in the \\nt\\n- and \\nz\\n-directions\\n, respectively. Let \\n/H9004\\nT\\n and \\n/H9004\\nZ\\n denote the separations \\nbetween samples (see F\\nig. 4.15). Then, the separations between the corresponding \\ndiscrete, frequency domain variables are given by \\n \\n/H9004\\n/H9004\\nu\\n=\\n1\\nMT\\n \\n(4-69)\\n†\\n As mentioned in Section 4.4, keep in mind that in this chapter we use \\n(, )\\ntz\\n and \\n(,)\\nmn\\n to denote 2-D \\ncontinuous\\n \\nspatial and frequenc\\ny-domain variables. In the 2-D \\ndiscrete\\n case, we use \\n(, )\\nxy\\n for spatial variables and \\n(,)\\nuv\\n for \\nfrequenc\\ny-domain variables, all of which are discrete.\\nSometimes you will ﬁnd \\nin the literature the  \\n1\\n/H20862\\nMN constant in front \\nof the DFT instead of \\nthe IDFT. At times, \\nthe square root of this \\nconstant is included in \\nfront of the forward \\nand inverse transforms, \\nthus creating a more \\nsymmetrical pair. Any \\nof these formulations is \\ncorrect, provided they \\nare used consistently.\\n4.6\\nDIP4E_GLOBAL_Print_Ready.indb   240\\n6/16/2017   2:05:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 241}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n241\\nand\\n \\n/H9004\\n/H9004\\nv\\n=\\n1\\nNZ\\n \\n(4-70)\\nrespectively. Note the important property that the separations between samples in \\nthe frequenc\\ny domain are inversely proportional both to the spacing between spa-\\ntial samples \\nand\\n to the number of samples.\\nTRANSLATION AND ROTATION\\nThe validity of the following Fourier transform pairs can be demonstrated by direct \\nsubstitution into Eqs. (4-67) and (4-68) (see Problem 4.27):\\n \\nfx ye\\nFu u\\njx M y N\\n(,)\\n( , )\\n()\\n2\\n00\\n00\\np\\nuv\\nvv\\n+\\n⇔− −\\n \\n(4-71)\\nand\\n \\nfx x y y F e\\njx M y N\\n(,) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nuv\\nuv\\np\\n \\n(4-72)\\nThat is, multiplying \\nfx y\\n(,\\n)\\n by the exponential shown shifts the origin of the DFT to \\n(,)\\nuv\\n00\\n and, conversely, multiplying \\nF\\n(,\\n)\\nuv\\n by the negative of that exponential shifts \\nthe origin of \\nfx y\\n(,\\n)\\n to \\n(,) .\\nxy\\n00\\n As we illustrate in Example 4.13, translation has no \\neffect on the magnitude (spectrum) of \\nF\\n(,\\n) .\\nuv\\nUsing the polar coordinates\\n \\nxr yr u\\n==\\n= =\\ncos\\nsin\\ncos\\nsin\\nuu v w v w\\nv\\n \\nresults in the following transform pair:\\n \\nfr F\\n(,\\n) ( , )\\nuu v wu\\n+⇔ +\\n00\\n \\n(4-73)\\nwhich indicates that rotating \\nfx y\\n(,\\n)\\n by an angle \\nu\\n0\\n rotates \\nF\\n(,\\n)\\nuv\\n by the same angle. \\nConversely\\n, rotating \\nF\\n(,\\n)\\nuv\\n rotates \\nfx y\\n(,\\n)\\n by the same angle.\\nPERIODICITY\\nAs in the 1-D case, the 2-D Fourier transform and its inverse are infinitely periodic\\nin the \\nu\\n and \\nv\\n directions; that is,\\n \\nF F kM F kN F kM kN\\n(,\\n) ( ,) (, ) ( , )\\nuv u v uv u v\\n=+ = + =+ +\\n12 1\\n2\\n \\n(4-74)\\nand\\n \\nfx y fx k M y fx y k N fx k M y k N\\n(,\\n) ( ,) (, ) ( , )\\n=+ = + =+ +\\n12 1\\n2\\n \\n(4-75)\\nwhere \\nk\\n1\\n and \\nk\\n2\\n are integers.\\nThe periodicities of the transform and its inverse are important issues in the \\nimplementation of DFT-based algorithms. Consider the 1-D spectrum in Fig. 4.22(a). \\nAs explained in Section 4.4\\n \\n[see the footnote to Eq.\\n \\n(4-42)], the transform data in the \\ninterval from 0 to \\nM\\n−\\n1\\n consists of two half periods meeting at point \\nM\\n2,\\n but with \\nRecall that we use the \\nsymbol “\\n⇔\\n” to denote \\nFourier transform pairs. \\nThat is, the term on the \\nright is the transform \\nof the term on the left, \\nand the term on the left \\nis the inverse Fourier \\ntransform of the term on \\nthe right.\\nDIP4E_GLOBAL_Print_Ready.indb   241\\n6/16/2017   2:05:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 242}),\n",
       " Document(page_content='242\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthe lower part of the period appearing at higher frequencies. For display and ﬁlter-\\ning purposes, it is more convenient to have in this interval a complete period of the \\ntransform in which the data are contiguous and ordered properly, as in Fig. 4.22(b). \\nIt follows from Eq. (4-71) that\\nfxe Fu u\\nju x M\\n()\\n( )\\n()\\n2\\n0\\np\\n⇔−\\n0\\nIn other words, multiplying \\nfx\\n()\\n by the exponential term shown shifts the transform \\ndata so that the origin,\\n \\nF\\n()\\n,\\n0\\n is moved to \\nu\\n0\\n.\\n If we let \\nuM\\n0\\n2\\n=\\n,\\n the exponential \\nterm becomes \\ne\\njx\\np\\n, which is equal to \\n()\\n−\\n1\\nx\\n because \\nx\\n is an integer. In this case,\\nb\\na\\nd c\\nFIGURE 4.22\\nCentering the \\nFourier transform. \\n(a) A 1-D DFT \\nshowing an inﬁnite \\nnumber of peri-\\nods. (b) Shifted \\nDFT obtained \\nby multiplying \\nfx\\n()\\n by \\n()\\n−\\n1\\nx\\n \\nbefore computing \\nFu\\n()\\n.\\n (c) A 2-D \\nDFT showing an \\ninﬁnite number of \\nperiods\\n. The area \\nwithin the dashed \\nrectangle is the \\ndata array, \\nF\\n(,\\n) ,\\nuv\\n \\nobtained with \\nEq.\\n (4-67) with \\nan image \\nfx y\\n(,\\n)\\n \\nas the input.\\n This \\narray consists of \\nfour quarter peri-\\nods. (d) Shifted \\narray obtained \\nby multiplying \\nfx y\\n(,\\n)\\n by \\n()\\n−\\n+\\n1\\nxy\\n \\nbefore computing \\nF\\n(,\\n) .\\nuv\\n The data \\nnow contains one \\ncomplete\\n, centered \\nperiod, as in (b). \\n/H11002\\nM\\n/\\n2\\nM\\n/\\n2 \\n/H11002\\n 1\\n0\\n0\\nM\\n/\\n2\\nM\\n/\\n2\\nM\\n \\n/H11002\\n 1\\nM\\n \\n/H11002\\n 1\\nM\\nTwo adjacent half \\nperiods meet here.\\nF\\n(\\nu\\n)\\nF\\n(\\nu\\n)\\nu\\nu\\nTwo adjacent half\\nperiods meet here.\\nOne period (\\nM\\n samples)\\nM\\n/\\n2\\nM\\n \\n/H11002\\n 1\\n(0, 0)\\nN\\n/\\n2\\nN\\n \\n/H11002\\n 1\\nu\\nv\\nv\\nu\\nN\\n/\\n2\\nN\\n \\n/H11002\\n 1\\nM\\n/\\n2\\n \\nM\\n/H11002\\n 1\\n(0, 0)\\n/H11005\\n \\nM\\n \\n/H11003\\n \\nN\\n data array computed by the DFT with             as input \\n(,)\\nfx y\\n/H11005\\n \\nM\\n \\n/H11003\\n \\nN\\n data array computed by the DFT with                         as input \\n(,) (1 )\\nxy\\nfx y\\n+\\n−\\n= Periods of the DFT\\nFour adjacent quarter\\nperiods meet here\\n(0,0)\\nF\\nDIP4E_GLOBAL_Print_Ready.indb   242\\n6/16/2017   2:05:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 243}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n243\\n \\nfx Fu M\\nx\\n() ( ) ( / )\\n−⇔ −\\n12\\nThat is, multiplying \\nfx\\n()\\n by \\n()\\n−\\n1\\nx\\n shifts the data so that \\nFu\\n()\\n is centered on the inter-\\nval \\n[, ] ,\\n01\\nM\\n−\\n which corresponds to Fig. 4.22(b), as desired. \\nIn 2-D the situation is more difﬁcult to graph,\\n but the principle is the same, as \\nFig. 4.22(c) shows. Instead of two half periods, there are now four quarter periods \\nmeeting at the point \\n(,) .\\nMN\\n22\\n As in the 1-D case, we want to shift the data so \\nthat \\nF\\n(,\\n)\\n00\\n is at \\n(,) .\\nMN\\n22\\n Letting \\n(,)( , )\\nuM N\\n00\\n22\\nv\\n=\\n in Eq. (4-71) results in \\nthe expression\\n \\nfx y Fu M N\\nxy\\n(,) ( ) ( , )\\n−⇔ − −\\n+\\n12 2\\nv\\n \\n(4-76)\\nUsing this equation shifts the data so that \\nF\\n(,\\n)\\n00\\n is moved to the center of \\nthe \\nfrequency rectangle\\n (i.e\\n., the rectangle deﬁned by the intervals \\n[, ]\\n01\\nM\\n−\\n and \\n[, ]\\n01\\nN\\n−\\n in the frequency domain). Figure 4.22(d) shows the result. \\nK\\neep in mind that in all our discussions, coordinate values in both the spatial and \\nfrequency domains are integers. As we explained in Section 2.4 (see Fig. 2.19) if, as \\nin our case), the origin of an \\nMN\\n×\\n image or transform is at \\n(,) ,\\n00\\n then the center of \\nthat image or transform is at \\nfloor floor\\n() , () .\\nMN\\n22\\n()\\n This expression is applicable \\nto both even and odd values of \\nM\\n and \\nN\\n. For example, the center of an array of size \\n20 15\\n×\\n is at point \\n(, ) .\\n10\\n7\\n Because we start counting from 0, these are the 11th and \\n8th points in the ﬁrst and second coordinate axes of the array\\n, respectively.\\nSYMMETRY PROPERTIES\\nAn important result from functional analysis is that any real \\nor\\n complex function, \\nw\\n(,\\n) ,\\nxy\\n can be expressed as the sum of an even and an odd part, each of which can \\nbe real or complex:\\n \\nww w\\n(,\\n) (,) (,)\\nxy xy xy\\neo\\n=+\\n \\n(4-77)\\nwhere the \\neven\\n and \\nodd\\n parts are defined as\\n \\nw\\nww\\ne\\nxy\\nxy\\nxy\\n(,)\\n(,) (\\n,)\\n≜\\n+− −\\n2\\n \\n(4-78)\\nand\\n \\nw\\nww\\no\\nxy\\nxy\\nxy\\n(,)\\n(,) (\\n,)\\n≜\\n−− −\\n2\\n \\n(4-79)\\nfor all valid values of \\nx\\n and \\ny\\n.\\n Substituting Eqs. (4-78) and (4-79) into Eq. (4-77) gives \\nthe identity \\nww\\n(,\\n) (,) ,\\nxy xy\\n≡\\n thus proving the validity of the latter equation. It fol-\\nlows from the preceding definitions that\\n \\nww\\nee\\nxy x y\\n(,) ( , )\\n=− −\\n \\n(4-80)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   243\\n6/16/2017   2:05:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 244}),\n",
       " Document(page_content='244\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nww\\noo\\nxy x y\\n(,) ( , )\\n=− − −\\n \\n(4-81)\\nEven functions are said to be \\nsymmetric\\n and odd functions \\nantisymmetric\\n.\\n Because \\nall indices in the DFT and IDFT are nonnegative integers, when we talk about sym-\\nmetry (antisymmetry) we are referring to symmetry (antisymmetry) about the \\ncen-\\nter point\\n of a sequence, in which case the definitions of even and odd become:\\n \\nww\\nee\\nxy M xN y\\n(,) ( , )\\n=− −\\n \\n(4-82)\\nand\\n \\nww\\noo\\nxy M xN y\\n(,) ( , )\\n=− − −\\n \\n(4-83)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As usual, \\nM\\n and \\nN\\n are the number \\nof rows and columns of a 2-D array\\n.\\nWe know from elementary mathematical analysis that the product of two even or \\ntwo odd functions is even, and that the product of an even and an odd function is \\nodd. In addition, the only way that a discrete function can be odd is if all its samples \\nsum to zero. These properties lead to the important result that\\n \\nww\\neo\\ny\\nN\\nx\\nM\\nxy xy\\n(,) (,)\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n0\\n1\\n0\\n1\\n \\n(4-84)\\nfor any two discrete even and odd functions \\nw\\ne\\n and \\nw\\no\\n.\\n In other words, because the \\nargument of Eq.\\n (4-84) is odd, the result of the summations is 0. The functions can \\nbe real or complex.\\nEXAMPLE 4.10 :  Even and odd functions.\\nAlthough evenness and oddness are visualized easily for continuous functions, these concepts are not as \\nintuitive when dealing with discrete sequences. The following illustrations will help clarify the preceding \\nideas. Consider the 1-D sequence \\n \\nff ff f\\n=\\n{}\\n=\\n{}\\n( ) ,( ) ,( ) ,( ) , , ,\\n0123 2 1 1 1\\nin which \\nM\\n=\\n4.\\n To test for evenness, the condition \\nfx f x\\n()\\n( )\\n=−\\n4\\n must be satisﬁed for \\nx\\n=\\n012\\n3\\n,,,.\\n \\nT\\nhat is, we require that\\n \\nff ff ff ff\\n(\\n) () , () () , () () , () ()\\n04 13 22 31\\n====\\nBecause \\nf\\n()\\n4\\n is outside the range being examined and can be any value, the value of \\nf\\n()\\n0\\n is immaterial \\nin the test for evenness\\n. We see that the next three conditions are satisﬁed by the values in the array, so \\nthe sequence is even. In fact, we conclude that \\nany\\n 4-point even sequence has to have the form\\n \\nabcb\\n,,\\n,\\n{}\\n \\nThat is, only the second and last points must be equal in a 4-point even sequence. In general, when \\nM\\n \\nis an even number, a 1-D even sequence has the property that the points at locations 0 and \\nM\\n2\\n have \\nIn the context of this dis-\\ncussion, the \\nlocations\\n of \\nelements in a sequence \\nare denoted by integers. \\nTherefore, the same \\nobservations made a few \\nparagraphs back about \\nthe centers of arrays of \\neven and odd \\nsizes\\n are \\napplicable to sequences. \\nBut, do not confuse the \\nconcepts of even/odd \\nnumbers\\n and even/odd \\nfunctions\\n.\\nTo convince yourself that \\nthe samples of an odd \\nfunction sum to zero, \\nsketch one period of \\na 1-D sine wave about \\nthe origin or any other \\ninterval spanning one \\nperiod.\\nDIP4E_GLOBAL_Print_Ready.indb   244\\n6/16/2017   2:05:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 245}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n245\\narbitrary values. When \\nM\\n is odd, the ﬁrst point of an even sequence is still arbitrary, but the others form \\npairs with equal values.\\nOdd sequences have the interesting property that their ﬁrst term, \\nw\\no\\n(,) ,\\n00\\n is always 0, a fact that fol-\\nlows directly from Eq.\\n (4-79). Consider the 1-D sequence\\n \\ngg gg g\\n=\\n{}\\n=−\\n{}\\n( ) ,( ) ,( ) ,( ) , ,,\\n0123 0 1 0 1\\nWe can conﬁrm that this is an odd sequence by noting that the terms in the sequence satisfy the condi-\\ntion \\ngx g x\\n()\\n( )\\n=− −\\n4\\n for \\nx\\n=\\n12\\n3\\n,,.\\n All we have to do for \\nx\\n=\\n0\\n is to check that \\ng\\n()\\n.\\n00\\n=\\n We check the \\nother terms using the deﬁnition.\\n For example, \\ngg\\n()\\n() .\\n13\\n=−\\n Any 4-point odd sequence has the form\\n \\n00\\n,,\\n,\\n−\\n{}\\nbb\\nIn general, when \\nM\\n is an even number\\n, a 1-D odd sequence has the property that the points at locations \\n0 and \\nM\\n2\\n are always zero. When \\nM\\n is odd,\\n the ﬁrst term still has to be 0, but the remaining terms form \\npairs with equal value but opposite signs.\\nThe preceding discussion indicates that evenness and oddness of sequences depend also on the length \\nof the sequences. For example, we showed already that the sequence \\n01 0 1\\n,,\\n,\\n−\\n{}\\n is odd. However, the \\nsequence \\n01 0 1 0\\n,,\\n, ,\\n−\\n{}\\n is neither odd nor even, although the “basic” structure appears to be odd. This \\nis an important issue in interpreting DFT results. We will show later in this section that the DFTs of even \\nand odd functions have some very important characteristics. Thus, it often is the case that understanding \\nwhen a function is odd or even plays a key role in our ability to interpret image results based on DFTs.\\nThe same basic considerations hold in 2-D. For example, the \\n66\\n×\\n 2-D array with center at location \\n(,) ,\\n33\\n shown bold in the ﬁgure [remember, we start counting at \\n(,) ] ,\\n00\\n \\n0 000 0 0\\n0\\n000 0 0\\n00 1010\\n00 2 20\\n00 1010\\n0 000 0 0\\n−\\n−\\n−\\n0\\nis odd, as you can prove using Eq. (4-83). However, adding another row or column of 0’s would give \\na result that is neither odd nor even.\\n In general, inserting a 2-D array of \\neven dimensions\\n into a larger \\narray of zeros, \\nalso\\n of even dimensions, preserves the symmetry of the smaller array, provided that the \\ncenters coincide. Similarly, a 2-D array of \\nodd dimensions\\n can be inserted into a larger array of zeros of \\nodd dimensions\\n without affecting the symmetry. Note that the inner structure of the preceding array is \\na Sobel kernel (see Fig. 3.50). We return to this kernel in Example 4.15, where we embed it in a larger \\narray of zeros for ﬁltering purposes.\\nArmed with the preceding concepts, we can establish a number of important sym-\\nmetry properties of the DFT and its inverse. A property used frequently is that the \\nFourier transform of a \\nreal\\n function, \\nfx y\\n(,\\n) ,\\n is \\nconjugate symmetric\\n:\\nConjugate symmetry \\nis also called \\nhermitian \\nsymmetry\\n. The term \\nantihermitian\\n is used \\nsometimes to refer to \\nconjugate antisymmetry.\\nDIP4E_GLOBAL_Print_Ready.indb   245\\n6/16/2017   2:05:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 246}),\n",
       " Document(page_content='246\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nFF\\n*\\n(,) ( , )\\nuv u v\\n=− −\\n \\n(4-85)\\nWe show the validity of this equation as follows:\\n \\nFu f x y e\\nfx y\\ny\\nN\\nx\\nM\\nju x My N\\n*(\\n)\\n*\\n*\\n(,) (,)\\n(,\\nv\\nv\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n=\\n−\\n=\\n−\\n−+\\n∑ ∑\\n0\\n1\\n0\\n1\\n2\\np\\n) )\\n(,)\\n()\\n([\\ny\\nN\\nx\\nM\\nju x My N\\ny\\nN\\nx\\nM\\nj\\ne\\nfx ye\\n=\\n−\\n=\\n−\\n+\\n=\\n−\\n=\\n−\\n−−\\n∑ ∑\\n∑ ∑\\n=\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\n2\\np\\np\\nv\\nu\\nuxM yN\\nFu\\n][ ] )\\n(,)\\n+−\\n=− −\\nv\\nv\\nwhere the third step follows from the fact that \\nfx y\\n(,\\n)\\n is real. A similar approach \\ncan be used to prove that,\\n if \\nfx y\\n(,\\n)\\n is \\nimaginary\\n,\\n its Fourier transform is conjugate \\nantisymmetric\\n; that is, \\nFF\\n*\\n(,) ( , ) .\\n−− = −\\nuv u v\\n \\nT\\nable 4.1 lists symmetries and related properties of the DFT that are useful in \\ndigital image processing. Recall that the double arrows indicate Fourier transform \\npairs; that is, for any row in the table, the properties on the right are satisﬁed by the \\nFourier transform of the function having the properties listed on the left, and vice \\nversa. For example, entry 5 reads: The DFT of a real function \\nfx y\\n(,\\n) ,\\n in which \\n(,)\\nxy\\n \\nSpatial Domain\\n†\\nFrequency Domain\\n†\\n1)\\nfx y\\n(,\\n)\\n real\\n⇔\\nFF\\n*\\n(,) ( , )\\nuv u v\\n=− −\\n2)\\nfx y\\n(,\\n)\\n imaginary\\n⇔\\nFF\\n*\\n(,) ( , )\\n−− = −\\nuv u v\\n3)\\nfx y\\n(,\\n)\\n real\\n⇔\\nR\\nIu\\n(,) (,)\\nuv v\\n even;  odd\\n4)\\nfx y\\n(,\\n)\\n imaginary\\n⇔\\nRI u\\n(,\\n) (,)\\nuv v\\n odd;  even\\n5)\\nfx y\\n(,\\n)\\n−−\\n real\\n⇔\\nF\\n*\\n(,)\\nuv\\n complex\\n6)\\nfx y\\n(,\\n)\\n−−\\n complex\\n⇔\\nF\\n(,\\n)\\n−−\\nuv\\n complex\\n7)\\nfx y\\n*\\n(,)\\n complex\\n⇔\\nF\\n*\\n(,)\\n−−\\nuv\\n complex\\n8)\\nfx y\\n(,\\n)\\n real and even\\n⇔\\nF\\n(,\\n)\\nuv\\n real and even\\n9)\\nfx y\\n(,\\n)\\n real and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n imaginary and odd\\n10)\\nfx y\\n(,\\n)\\n imaginary and even\\n⇔\\nF\\n(,\\n)\\nuv\\n imaginary and even\\n11)\\nfx y\\n(,\\n)\\n imaginary and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n real and odd\\n12)\\nfx y\\n(,\\n)\\n complex and even\\n⇔\\nF\\n(,\\n)\\nuv\\n complex and even\\n13)\\nfx y\\n(,\\n)\\n complex and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n complex and odd\\nTABLE \\n4.1\\nSome symmetry \\nproperties of the \\n2-D DFT and its \\ninverse. \\nR\\n(,\\n)\\nuv\\n \\nand \\nI\\n(,\\n)\\nuv\\n are \\nthe real and \\nimaginary parts of \\nF\\n(,\\n) ,\\nuv\\n  \\nrespectively\\n. \\nUse of the word \\ncomplex\\n indicates \\nthat a function \\nhas nonzero real \\nand imaginary \\nparts. \\n†\\nRecall that \\nx\\n, y, \\nu\\n, and \\nv\\n are \\ndiscrete\\n (integer) variables\\n, with \\nx\\n and \\nu\\n in the range \\n[, ] ,\\n01\\nM\\n−\\n and \\ny\\n and \\nv\\n in \\nthe range \\n[, ] .\\n01\\nN\\n−\\n To say that a complex function is \\neven\\n means that its real \\nand\\n imaginary parts are even,\\n and \\nsimilarly for an \\nodd\\n complex function. As before, “\\n⇔\\n” indicates a Fourier transform pair.\\nDIP4E_GLOBAL_Print_Ready.indb   246\\n6/16/2017   2:05:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 247}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n247\\nis replaced by \\n(,) ,\\n−−\\nxy\\n is \\nF\\n*\\n(,) ,\\nuv\\n the complex conjugate of the DFT of \\nfx y\\n(,\\n) .\\n \\nConversely\\n, the IDFT of \\nF\\n*\\n(,)\\nuv\\n is \\nfx y\\n(,\\n) .\\n−−\\nEXAMPLE 4.11 :  1-D illustrations of the properties in Table 4.1.\\nThe 1-D sequences (functions) and their transforms in Table 4.2 are short examples of the properties \\nlisted in Table 4.1. For example, in property 3 we see that a real function with elements \\n1234\\n,,\\n,\\n{}\\n has a \\nFourier transform whose real part, \\n1 0222\\n,,,\\n,\\n−−−\\n{}\\n is even and whose imaginary part, \\n020 2\\n,,\\n, ,\\n−\\n{}\\n is \\nodd. Property 8 tells us that a real even function has a transform that is real and even also. Property 12 \\nshows that an even complex function has a transform that is also complex and even. The other listings \\nin the table are analyzed in a similar manner.\\nEXAMPLE 4.12 :  Proving some of the DFT symmetry properties from Table 4.1.\\nIn this example, we prove several of the properties in Table 4.1 to help you develop familiarity with \\nmanipulating these important properties, and to establish a basis for solving some of the problems at \\nthe end of the chapter. We prove only the properties on the right given the properties on the left. The \\nconverse is proved in a manner similar to the proofs we give here. \\nConsider property 3, which reads: If \\nfx y\\n(,\\n)\\n is a real function, the real part of its DFT is even and the \\nimaginary part is odd.\\n We prove this property as follows: \\nF\\n(,\\n)\\nuv\\n is complex in general,  so it can be expressed \\nas the sum of a real and an imaginary part:\\n \\nFR j I\\n(,\\n) (,) (,) .\\nuv uv uv\\n=+\\n Then, \\nFR j I\\n*\\n(,) (,) (,) .\\nuv uv uv\\n=−\\n \\nAlso\\n, \\nFR j I\\n(,\\n) (,) (,) .\\n−− = −− + −−\\nuv uv uv\\n But, as we proved earlier for Eq. (4-85), if \\nfx y\\n(,\\n)\\n is real then \\nFF\\n*\\n(,) ( , ) ,\\nuv u v\\n=− −\\n which, based on the preceding two equations, means that \\nRR\\n(,\\n) ( , )\\nuv u v\\n=− −\\n and \\nII\\n(,\\n) ( , ) .\\nuv u v\\n=− − −\\n In view of the deﬁnitions in Eqs. (4-80) and (4-81), this proves that \\nR\\n is an even \\nfunction and that \\nI\\n is an odd function.\\nNext,\\n we prove property 8. If \\nfx y\\n(,\\n)\\n is real, we know from property 3 that the real part of \\nF\\n(,\\n)\\nuv\\n is \\neven,\\n so to prove property 8 all we have to do is show that if \\nfx y\\n(,\\n)\\n is real and even then the imaginary \\npart of \\nF\\n(,\\n)\\nuv\\n is 0 (i.e., \\nF\\n is real).\\n The steps are as follows:\\nProperty\\nf\\n(\\nx\\n)\\nF\\n(\\nu\\n)\\n3\\n1234\\n,,\\n,\\n{}\\n⇔\\n1 0 02 22 02 2\\n+\\n(\\n)\\n−+\\n(\\n)\\n−+\\n(\\n)\\n−−\\n(\\n)\\n{}\\njjjj\\n,,,\\n4\\n1234\\njj\\njj\\n,,,\\n{}\\n⇔\\n02 5 5 5 0 5 5 5\\n+\\n(\\n)\\n−\\n(\\n)\\n−\\n(\\n)\\n−−\\n(\\n)\\n{}\\n., .., ., ..\\njj j j\\n8\\n2111\\n,,,\\n{}\\n⇔\\n5111\\n,,,\\n{}\\n9\\n01 0 1\\n,,\\n,\\n−\\n{}\\n⇔\\n00 02 00 02\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n−\\n(\\n)\\n{}\\njjjj\\n,,,\\n10\\n2111\\njjjj\\n,,,\\n{}\\n⇔\\n5\\njjjj\\n,,,\\n{}\\n11\\n01 0 1\\njj\\nj j\\n,, ,\\n−\\n{}\\n⇔\\n02 0 2\\n,, ,\\n−\\n{}\\n12\\n44 32 02 32\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n{}\\njjjj\\n,,,\\n⇔\\n1 01 0 42 22 42\\n+\\n(\\n)\\n+\\n(\\n)\\n−+\\n(\\n)\\n+\\n(\\n)\\n{}\\njj jj\\n,, ,\\n13\\n00 1 1 00 1\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n−−\\n(\\n)\\n{}\\njj j j\\n,, ,\\n⇔\\n00 22 00 22\\n+\\n(\\n)\\n−\\n(\\n)\\n+\\n(\\n)\\n−+\\n(\\n)\\n{}\\njjj j\\n,,,\\nTABLE \\n4.2\\n1-D examples of \\nsome of the prop-\\nerties in Table 4.1.\\nDIP4E_GLOBAL_Print_Ready.indb   247\\n6/16/2017   2:05:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 248}),\n",
       " Document(page_content='248\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\n/H5219\\nfx y F fx ye\\nfx y\\nju x My N\\ny\\nN\\nx\\nM\\nr\\n(,) (,) (,)\\n(,\\n()\\n{}\\n==\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\n2\\n0\\n1\\n0\\n1\\np\\n) )\\n(,)\\n()\\n[]\\n=\\n[]\\n=\\n−\\n=\\n−\\n−+\\n=\\n−\\n=\\n−\\n−\\n∑ ∑\\n∑ ∑\\ny\\nN\\nx\\nM\\nju x My N\\nr\\ny\\nN\\nx\\nM\\ne\\nfx ye\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\np\\nv\\nj\\nju x M j y N\\ne\\n22\\npp\\n() ()\\n−\\nv\\n \\nWe can expand the last line of this expression in terms of even and odd parts\\n \\nFj\\nj\\ny\\nN\\nx\\nM\\ny\\n(,)\\nuv\\n=\\n[]\\n−\\n[]\\n−\\n[]\\n=\\n[]\\n=\\n−\\n=\\n−\\n=\\n∑ ∑\\neven even odd even odd\\neven\\n0\\n1\\n0\\n1\\n0 0\\n1\\n0\\n1\\n0\\n2\\nN\\nx\\nM\\ny\\nj\\n−\\n=\\n−\\n=\\n∑ ∑\\n⋅− ⋅ − ⋅\\n[]\\n=⋅\\n[]\\neven even even odd odd odd\\neven even\\nN N\\nx\\nM\\ny\\nN\\nx\\nM\\ny\\nN\\nj\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n∑\\n∑∑\\n∑∑\\n−⋅\\n[]\\n−⋅\\n[]\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n2 even odd even even\\nx x\\nM\\n=\\n−\\n∑\\n=\\n0\\n1\\nreal.\\nThe ﬁrst step follows from Euler’s equation, and the fact that the cos and sin are even and odd functions, \\nrespectively\\n. We also know from property 8 that, in addition to being real, \\nfx y\\n(,\\n)\\n is an even function. \\nT\\nhe only term in the penultimate line containing imaginary components is the second term, which is 0 \\naccording to Eq. (4-84). Therefore, if \\nfx y\\n(,\\n)\\n is real and even, then \\nF\\n(,\\n)\\nuv\\n is real. As noted earlier, \\nF\\n(,\\n)\\nuv\\n \\nis even also because \\nfx y\\n(,\\n)\\n is real. This concludes the proof.\\nF\\ninally, we demonstrate the validity of property 6. From the deﬁnition of the DFT,\\n \\nℑ− −\\n{}\\n=− −\\n=\\n−\\n=\\n−\\n−+\\n∑ ∑\\nfx y fx y e\\ny\\nN\\nx\\nM\\nju x M y N\\n(,) (,)\\n()\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nWe are not making a change of variable here. We are evaluating the DFT of \\nfx y\\n(,\\n) ,\\n−−\\n so we sim-\\nply insert this function into the equation,\\n as we would any other function. Because of periodicity, \\nfx y f Mx Ny\\n(,\\n) ( , ) .\\n−− = − −\\n If we now deﬁne \\nmMx\\n=−\\n and \\nnNy\\n=−\\n, then\\n \\nℑ− −\\n{}\\n=\\n=\\n−\\n=\\n−\\n−− + −\\n∑ ∑\\nfx y f m n e\\nn\\nN\\nm\\nM\\nju M m M N n N\\n(,) ( , )\\n([ ] [ ] )\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nTo convince yourself that the summations are correct, try a 1-D transform and expand a few terms by \\nhand. Because \\nexp[ ( )] ,\\n−=\\nj\\n21\\np\\ninteger\\n it follows that\\n \\nℑ− −\\n{}\\n==\\n−\\n−\\n=\\n−\\n=\\n−\\n+\\n∑ ∑\\nfx y f m n e\\nFu\\nn\\nN\\nm\\nM\\njm u M n N\\n(,) ( , )\\n(,)\\n()\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nv\\nThis concludes the proof.\\nDIP4E_GLOBAL_Print_Ready.indb   248\\n6/16/2017   2:05:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 249}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n249\\nFOURIER SPECTRUM AND PHASE ANGLE\\nBecause the 2-D DFT is complex in general, it can be expressed in polar form:\\n \\nFu R j I\\nFu e\\nju\\n(,) (,) (,)\\n(,)\\n(,)\\nvu vu v\\nv\\nv\\n=+\\n=\\nf\\n \\n(4-86)\\nwhere the magnitude\\n \\nFu R u I u\\n(,) (,) (,)\\n/\\nvv v\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n12\\n \\n(4-87)\\nis called the \\nF\\nourier\\n (or \\nfrequency\\n) \\nspectrum\\n, and\\n \\nf\\n( , ) arctan\\n(,)\\n(,)\\nu\\nIu\\nRu\\nv\\nv\\nv\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n \\n(4-88)\\nis the \\nphase angle \\nor\\n phase spectrum\\n.\\n Recall from the discussion in Section 4.2 that \\nthe arctan must be computed using a four-quadrant arctangent function, such as \\nMATLAB’s \\natan2(Imag, Real)\\n function.\\nFinally, the \\npower spectrum\\n is deﬁned as\\n \\nPu Fu\\nRu Iu\\n(,) (,)\\n(,) (\\n,)\\nvv\\nvv\\n=\\n=+\\n2\\n22\\n \\n(4-89)\\nAs before, \\nR\\n and \\nI\\n are the real and imaginary parts of \\nF\\n(,\\n) ,\\nuv\\n and all computations  \\nare carried out for the discrete variables \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n and \\nv\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n \\nT\\nherefore, \\nF\\n(,) ,\\nuv\\n \\nf\\n(,\\n) ,\\nuv\\n and \\nP\\n(,\\n)\\nuv\\n are arrays of size \\nMN\\n×\\n.\\nThe Fourier transform of a real function is conjugate symmetric [see Eq. (4-85)], \\nwhich implies that the spectrum has \\neven\\n symmetry about the origin:\\n \\nFu F u\\n(,) ( , )\\nvv\\n=− −\\n \\n(4-90)\\nThe phase angle exhibits \\nodd\\n symmetry about the origin:\\n \\nff\\n(,\\n) ( , )\\nuu\\nvv\\n=− − −\\n \\n(4-91)\\nIt follows from Eq. (4-67) that\\n \\nFf\\nx\\ny\\ny\\nN\\nx\\nM\\n(,) (, )\\n00\\n0\\n1\\n0\\n1\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n \\nwhich indicates that the zero-frequency term of the DFT is proportional to the aver-\\nage of \\nfx y\\n(,\\n) .\\n That is, \\n \\nFM N\\nMN\\nfx y\\nMNf\\ny\\nN\\nx\\nM\\n(,)\\n(, )\\n00\\n1\\n0\\n1\\n0\\n1\\n=\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n \\n(4-92)\\nDIP4E_GLOBAL_Print_Ready.indb   249\\n6/16/2017   2:05:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 250}),\n",
       " Document(page_content='250\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere \\nf\\n (a scalar) denotes the average value of \\nfx y\\n(,\\n) .\\n Then, \\n \\nFM N f\\n(,)\\n00\\n=\\n \\n(4-93)\\nBecause the proportionality constant \\nMN\\n usually is large\\n, \\nF\\n(,)\\n00\\n typically is the \\nlargest component of the spectrum by a factor that can be several orders of magni-\\ntude larger than other terms\\n. Because frequency components \\nu\\n and \\nv\\n are zero at the \\norigin,\\n \\nF\\n(,\\n)\\n00\\n sometimes is called the \\ndc component\\n of the transform.\\n This terminol-\\nogy is from electrical engineering, where “dc” signifies direct current (i.e., current of \\nzero frequency).\\nEXAMPLE 4.13 :  The spectrum of a rectangle.\\nFigure 4.23(a) shows an image of a rectangle and Fig. 4.23(b) shows its spectrum, whose values were \\nscaled to the range \\n[, ]\\n0\\n255\\n and displayed in image form. The origins of both the spatial and frequency \\ndomains are at the top left.\\n This is the right-handed coordinate system convention we deﬁned in Fig. 2.19. \\nTwo things are apparent in Fig. 4.23(b). As expected, the area around the origin of the transform con-\\ntains the highest values (and thus appears brighter in the image). However, note that the four corners \\nx\\nu\\nuu\\ny\\nv\\nv\\nv\\nb a\\nd c\\nFIGURE 4.23\\n(a) Image.  \\n(b) Spectrum, \\nshowing small, \\nbright areas in the \\nfour corners (you \\nhave to look care-\\nfully to see them).  \\n(c) Centered  \\nspectrum.  \\n(d) Result after a \\nlog transformation. \\nThe zero crossings \\nof the spectrum \\nare closer in the \\nvertical direction \\nbecause the rectan-\\ngle in (a) is longer \\nin that direction. \\nThe right-handed  \\ncoordinate  \\nconvention used in \\nthe book places the \\norigin of the spatial \\nand frequency \\ndomains at the top \\nleft (see Fig. 2.19). \\nDIP4E_GLOBAL_Print_Ready.indb   250\\n6/16/2017   2:05:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 251}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n251\\nof the spectrum contain similarly high values. The reason is the periodicity property discussed in the \\nprevious section. To center the spectrum, we simply multiply the image in (a) by \\n()\\n−\\n+\\n1\\nxy\\n before comput-\\ning the DFT, as indicated in Eq. (4-76). Figure 4.23(c) shows the result, which clearly is much easier to \\nvisualize (note the symmetry about the center point). Because the dc term dominates the values of the \\nspectrum, the dynamic range of other intensities in the displayed image are compressed. To bring out \\nthose details, we used the log transformation deﬁned in Eq. (3-4) with \\nc\\n=\\n1.\\n Figure 4.23(d) shows the \\ndisplay of \\nlog( ( , ) ).\\n1\\n+\\nF\\nuv\\n The increased rendition of detail is evident. Most spectra shown in this and \\nsubsequent chapters are scaled in this manner\\n. \\nIt follows from Eqs. (4-72) and (4-73) that the spectrum is insensitive to image translation (the abso-\\nlute value of the exponential term is 1), but it rotates by the same angle of a rotated image. Figure \\n4.24 illustrates these properties. The spectrum in Fig. 4.24(b) is identical to the spectrum in Fig. 4.23(d). \\nb a\\nd c\\nFIGURE 4.24\\n(a) The rectangle \\nin Fig. 4.23(a) \\ntranslated.  \\n(b) Corresponding  \\nspectrum.  \\n(c) Rotated  \\nrectangle.  \\n(d) Corresponding \\n spectrum. \\nThe spectrum of \\nthe translated  \\nrectangle is \\nidentical to the \\nspectrum of the \\noriginal image in \\nFig. 4.23(a). \\nb a\\nc\\nFIGURE 4.25\\nPhase angle  \\nimages of  \\n(a) centered,  \\n(b) translated, \\nand (c) rotated \\nrectangles.\\nDIP4E_GLOBAL_Print_Ready.indb   251\\n6/16/2017   2:05:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 252}),\n",
       " Document(page_content='252\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nClearly, the images in Figs. 4.23(a) and 4.24(a) are different so, if their Fourier spectra are the same, \\nthen, based on Eq. (4-86), their phase angles must be different. Figure 4.25 conﬁrms this. Figures 4.25(a) \\nand (b) are the phase angle arrays (shown as images) of the DFTs of Figs. 4.23(a) and 4.24(a). Note the \\nlack of similarity between the phase images, in spite of the fact that the only differences between their \\ncorresponding images is simple translation. In general, visual analysis of phase angle images yields little \\nintuitive information. For instance, because of its 45° orientation, one would expect intuitively that the \\nphase angle in Fig. 4.25(a) should correspond to the rotated image in Fig. 4.24(c), rather than to the \\nimage in Fig. 4.23(a). In fact, as Fig. 4.25(c) shows, the phase angle of the rotated image has a strong \\norientation that is much less than 45°.\\nThe components of the spectrum of the DFT determine the amplitudes of the \\nsinusoids that combine to form an image. At any given frequency in the DFT of \\nan image, a large amplitude implies a greater prominence of a sinusoid of that fre-\\nquency in the image. Conversely, a small amplitude implies that less of that sinu-\\nsoid is present in the image. Although, as Fig. 4.25 shows, the contribution of the \\nphase components is less intuitive, it is just as important. The phase is a measure of \\ndisplacement of the various sinusoids with respect to their origin. Thus, while the \\nmagnitude of the 2-D DFT is an array whose components determine the intensities \\nin the image, the corresponding phase is an array of angles that carry much of the \\ninformation about where discernible objects are located in the image. The following \\nexample illustrates these ideas in more detail. \\nEXAMPLE 4.14 :  Contributions of the spectrum and phase angle to image formation.\\nFigure 4.26(b) shows as an image the phase-angle array, \\nf\\n(,\\n) ,\\nuv\\n of the DFT of Fig. 4.26(a), computed \\nusing Eq.\\n (4-88). Although there is no detail in this array that would lead us by visual analysis to associ-\\nate it with the structure of its corresponding image, the information in this array is crucial in determin-\\ning shape features of the image. To illustrate this, we reconstructed the boy’s image using only its phase \\nangle. The reconstruction consisted of computing the inverse DFT of Eq. (4-86) using \\nf\\n(,\\n) ,\\nuv\\n but setting \\nF\\n(,) .\\nuv\\n=\\n1\\n Figure Fig. 4.26(c) shows the result (the original result had much less contrast than is shown; \\nto bring out details important in this discussion,\\n we scaled the result using Eqs. (2-31) and (2-32), and \\nthen enhanced it using histogram equalization). However, even after enhancement, it is evident that \\nmuch of the intensity information has been lost (remember, that information is carried by the spectrum, \\nwhich we did not use in the reconstruction). However, the \\nshape\\n features in 4.26(c) are unmistakably \\nfrom Fig. 4.26(a). This illustrates vividly the importance of the phase angle in determining shape char-\\nacteristics in an image. \\nFigure 4.26(d) was obtained by computing the inverse DFT Eq. (4-86), but using only the spectrum. \\nThis means setting the exponential term to 1, which in turn implies setting the phase angle to 0. The \\nresult is not unexpected. It contains only intensity information, with the dc term being the most domi-\\nnant. There is no shape information in the image because the phase was set to zero.\\nFinally, Figs. 4.26(e) and (f) show yet again the dominance of the phase in determining the spatial \\nfeature content of an image. Figure 4.26(e) was obtained by computing the inverse DFT of Eq. (4-86) \\nusing the spectrum of the rectangle from Fig. 4.23(a) and the phase angle from the boy’s image. The \\nboy’s features clearly dominate this result. Conversely, the rectangle dominates Fig. 4.26(f), which was \\ncomputed using the spectrum of the boy’s image and the phase angle of the rectangle.\\nDIP4E_GLOBAL_Print_Ready.indb   252\\n6/16/2017   2:05:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n253\\nTHE 2-D DISCRETE CONVOLUTION THEOREM\\nExtending Eq. (4-48) to two variables results in the following expression for 2-D \\ncircular convolution\\n:\\n \\n() ( , ) ( , ) ( ,)\\nfh\\nx y f m n h xm y n\\nn\\nN\\nm\\nM\\n/H22841\\n=−\\n−\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n \\n(4-94)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As in Eq. (4-48), Eq. (4-94) gives \\none period of a 2-D periodic sequence\\n. The 2-D convolution theorem is give by\\n \\n() ( , ) () ( , )\\nfh\\nx y F H u\\n/H22841\\n⇔\\ni\\nv\\n \\n(4-95)\\nYou will ﬁnd it helpful \\nto review Eq. (4-48), \\nand the comments made \\nthere regarding circular \\nconvolution, as opposed \\nto the convolution we \\nstudied in Section 3.4.\\nb a\\nc\\ne d\\nf\\nFIGURE 4.26\\n (a) Boy image. (b) Phase angle. (c) Boy image reconstructed using only its phase angle (all shape features \\nare there, but the intensity information is missing because the spectrum was not used in the reconstruction). (d) Boy \\nimage reconstructed using only its spectrum. (e) Boy image reconstructed using its phase angle and the spectrum of \\nthe rectangle in Fig. 4.23(a). (f) Rectangle image reconstructed using its phase and the spectrum of the boy’s image. \\nDIP4E_GLOBAL_Print_Ready.indb   253\\n6/16/2017   2:05:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 254}),\n",
       " Document(page_content='254\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nand, conversely,\\n \\n() ( , ) ( ) ( , )\\nfhx y\\nMN\\nFH\\ni\\n⇔\\n1\\n/H22841\\nuv\\n \\n(4-96)\\nwhere \\nF\\n and \\nH\\n are the F\\nourier transforms of \\nf\\n and \\nh\\n, respectively, obtained using \\nEq. (4-67). As before, the double arrow is used to indicate that the left and right sides \\nof the expressions constitute a Fourier transform pair. Our interest in the remainder \\nof this chapter is in Eq. (4-95), which states that the Fourier transform of the spatial \\nconvolution of \\nf\\n and \\nh\\n, is the product of their transforms. Similarly, the inverse DFT \\nof the product \\n() ( , )\\nFH\\ni\\nuv\\n yields \\n() ( , ) .\\nfh\\nx y\\n/H22841\\n \\nEquation (4-95) is the foundation of linear ﬁltering in the frequency domain and, \\nas we will explain in Section 4.7, is the basis for all the ﬁltering techniques discussed \\nin this chapter. As you will recall from Chapter 3, spatial convolution is the foun-\\ndation for spatial ﬁltering, so Eq. (4-95) is the tie that establishes the equivalence \\nbetween spatial and frequency-domain ﬁltering, as we have mentioned several times \\nbefore. \\nUltimately, we are interested in the results of convolution in the spatial domain, \\nwhere we analyze images. However, the convolution theorem tell us that we have \\ntwo ways of computing the spatial convolution of two functions. We can do it directly \\nin the spatial domain with Eq. (3-35), using the approach described in Section 3.4 \\nor, according to Eq. (4-95), we can compute the Fourier transform of each function, \\nmultiply the transforms, and compute the inverse Fourier transform. Because we are \\ndealing with discrete quantities, computation of the Fourier transforms is carried \\nout using a DFT algorithm. This automatically implies periodicity, which means that \\nwhen we take the inverse Fourier transform of the product of the two transforms we \\nwould get a circular (i.e., periodic) convolution, one period of which is given by Eq. \\n(4-94). The question is: under what conditions will the direct spatial approach and \\nthe inverse Fourier transform method yield the same result? We arrive at the answer \\nby looking at a 1-D example ﬁrst, and then extending the results to two variables.\\nThe left column of Fig. 4.27 implements convolution of two functions, \\nf\\n and \\nh\\n, \\nusing the 1-D equivalent of Eq. (3-35), which, because the two functions are of same \\nsize, is written as\\n \\n() ( )\\n()( )\\nff x h x m\\nhx\\nm\\n/H22841\\n=\\n=\\n∑\\n−\\n0\\n399\\nRecall from our explanation of Figs. 3.29 and 3.30 that the procedure consists of (1) \\nrotating (flipping) \\nh\\n by \\n180\\n°\\n,\\n [see Fig. 4.27(c)], (2) translating the resulting function \\nby an amount \\nx\\n [F\\nig. 4.27(d)], and (3) for \\neach\\n value \\nx\\n of translation, computing the \\nentire sum of products in the right side of the equation. In terms of Fig. 4.27, this \\nmeans multiplying the function in Fig. 4.27(a) by the function in Fig. 4.27(d) for \\neach\\n \\nvalue of \\nx\\n. The displacement \\nx\\n ranges over all values required to completely slide \\nh\\n \\nacross \\nf\\n. Figure\\n \\n4.27(e) shows the convolution of these two functions. As you know, \\nconvolution is a function of the displacement variable\\n, \\nx\\n, and the range of \\nx\\n required \\nin this example to completely slide \\nh\\n past \\nf\\n is from 0 to 799.\\nThe function products \\nare elementwise products, \\nas deﬁned in Section 2.6. \\nWe will discuss efﬁcient \\nways for computing the \\nDFT in Section 4.11.\\nDIP4E_GLOBAL_Print_Ready.indb   254\\n6/16/2017   2:05:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 255}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n255\\nIf we use the DFT and the convolution theorem to try to obtain the same result \\nas in the left column of Fig. 4.27, we must take into account the periodicity inher-\\nent in the expression for the DFT. This is equivalent to convolving the two periodic \\nfunctions in Figs. 4.27(f) and (g) (i.e., as Eqs. (4-46) and (4-47) indicate, the func-\\ntions their transforms have implied periodicity). The convolution procedure is the \\nsame as we just discussed, but the two functions now are periodic. Proceeding with \\nthese two functions as in the previous paragraph would yield the result in Fig. 4.27(j), \\nwhich obviously is incorrect. Because we are convolving two periodic functions, the \\nconvolution itself is periodic. The closeness of the periods in Fig. 4.27 is such that \\nf\\n(\\nm\\n)\\nm\\nm\\n3\\n3\\n200 400\\n0\\n0 200\\n400\\nf\\n(\\nm\\n)\\n2\\nm\\nm\\n200\\n400\\n0\\n2\\n0 200 400\\nh\\n(\\nm\\n)\\nh\\n(\\nm\\n)\\nm\\nm\\n200 400\\n0\\n0 200 400\\nh\\n(\\n/H11002\\nm\\n)\\nh\\n(\\n/H11002\\nm\\n)\\nm\\nm\\n200 400\\n0\\n0 200 400\\nx\\nx\\nh\\n(\\nx\\n \\n/H11002\\n \\nm\\n)\\nh\\n(\\nx\\n \\n/H11002\\n \\nm\\n)\\nx\\nx\\nRange of\\nFourier transform\\ncomputation\\n200 400 600 800\\n0\\n600\\n1200\\n600\\n1200\\n0 200 400\\n() ( )\\ng\\nfx\\n/H22841\\n() ( )\\ng\\nfx\\n/H22841\\nb\\na\\nc\\ne\\nd\\nf\\nh\\ng\\ni\\nj\\nFIGURE 4.27\\nLeft column:  \\nSpatial  \\nconvolution \\ncomputed with \\nEq. (3-35), using \\nthe approach \\ndiscussed in  \\nSection 3.4.  \\nRight column: \\nCircular  \\nconvolution. The \\nsolid line in (j) \\nis the result we \\nwould obtain \\nusing the DFT, \\nor, equivalently, \\nEq. (4-48). This \\nerroneous result \\ncan be remedied \\nby using zero  \\npadding.\\nDIP4E_GLOBAL_Print_Ready.indb   255\\n6/16/2017   2:05:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 256}),\n",
       " Document(page_content='256\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthey interfere with each other to cause what is commonly referred to as \\nwraparound \\nerror\\n. According to the convolution theorem, if we had computed the DFT of the \\ntwo 400-point functions, \\nf\\n and \\nh\\n, multiplied the two transforms, and then computed \\nthe inverse DFT, we would have obtained the erroneous 400-point segment of the \\nperiodic convolution shown as a solid line in Fig. 4.27(j) (remember the limits of the \\n1-D DFT are \\nu\\n=−\\n012\\n1\\n,,, , ) .\\n…\\nM\\n This is also the result we would obtain if we used \\nEq.\\n (4-48) [the 1-D equivalent of Eq. (4-94)] to compute one period of the circular \\nconvolution.\\nFortunately, the solution to the wraparound error problem is simple. Consider \\ntwo functions, \\nfx\\n()\\n and \\nhx\\n()\\n composed of \\nA\\n and \\nB\\n samples\\n, respectively. It can be \\nshown (Brigham [1988]) that if we append zeros to both functions so that they have \\nthe same length, denoted by \\nP\\n, then wraparound is avoided by choosing\\n \\nPAB\\n≥+\\n−\\n1  \\n(4-97)\\nIn our example, each function has 400 points, so the minimum value we could use is \\nP\\n=\\n799,\\n which implies that we would append 399 zeros to the trailing edge of each \\nfunction.\\n This procedure is called \\nzero padding\\n, as we discussed in Section 3.4. As \\nan exercise, you should convince yourself that if the periods of the functions in Figs. \\n4.27(f) and (g) were lengthened by appending to each period at least 399 zeros, the \\nresult would be a periodic convolution in which each period is identical to the cor-\\nrect result in Fig. 4.27(e). Using the DFT via the convolution theorem would result \\nin a 799-point spatial function identical to Fig. 4.27(e). The conclusion, then, is that \\nto obtain the same convolution result between the “straight” representation of the \\nconvolution equation approach in Chapter 3, and the DFT approach, functions in \\nthe latter must be padded prior to computing their transforms.\\nVisualizing a similar example in 2-D is more difﬁcult, but we would arrive at the \\nsame conclusion regarding wraparound error and the need for appending zeros to \\nthe functions. Let \\nfx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n be two image arrays of sizes \\nAB\\n×\\n and \\nCD\\n×\\n \\npixels\\n, respectively. Wraparound error in their circular convolution can be avoided \\nby padding these functions with zeros, as follows:\\n \\nfx y\\nfx\\ny x A\\ny B\\nAxP ByQ\\np\\n(,)\\n(,)\\n=\\n≤≤ − ≤≤−\\n≤≤ ≤≤\\n⎧\\n⎨\\n⎩\\n01\\n01\\n0\\n  and  \\n  or  \\n \\n(4-98)\\nand\\n \\nhx y\\nhx\\ny x C\\ny D\\nCxP Dy Q\\np\\n(,)\\n(,)\\n=\\n≤≤− ≤≤ −\\n≤≤ ≤≤\\n⎧\\n⎨\\n⎩\\n01\\n01\\n0\\n  and  \\n  or  \\n \\n(4-99)\\nwith\\n \\nPA C\\n≥+\\n−\\n1  \\n(4-100)\\nand\\nThe padding zeros could \\nbe appended also at \\nthe beginning of the \\nfunctions, or they could \\nbe divided between the \\nbeginning and end of the \\nfunctions. It is simpler to \\nappend them at the end.\\nWe use zero-padding \\nhere for simplicity. Recall \\nfrom the discussion of \\nFig. 3.39 that replicate \\nand mirror padding \\ngenerally yield better \\nresults.\\nDIP4E_GLOBAL_Print_Ready.indb   256\\n6/16/2017   2:05:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 257}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n257\\n \\nQBD\\n≥+\\n−\\n1  \\n(4-101)\\nThe resulting padded images are of size \\nPQ\\n×\\n.\\n If both arrays are of the same size, \\nMN\\n×\\n,\\n then we require that \\nP\\nM\\n≥−\\n21\\n and \\nQN\\n≥−\\n21\\n.\\n As a rule, DFT algorithms \\ntend to execute faster with arrays of even size\\n, so it is good practice to select \\nP\\n and \\nQ\\n as the smallest even integers that satisfy the preceding equations. If the two arrays \\nare of the same size, this means that \\nP\\n and \\nQ\\n are selected as:\\n \\nPM\\n=\\n2  \\n(4-102)\\nand\\n \\nQN\\n=\\n2  \\n(4-103)\\nFigure 4.31 in the next section illustrates the effects of wraparound error on images. \\nT\\nhe two functions in Figs. 4.27(a) and (b) conveniently become zero before the \\nend of the sampling interval. If one or both of the functions were not zero at the end \\nof the interval, then a discontinuity would be created when zeros were appended \\nto the function to eliminate wraparound error. This is analogous to multiplying a \\nfunction by a box, which in the frequency domain would imply convolution of the \\noriginal transform with a sinc function (see Example 4.1). This, in turn, would create \\nso-called \\nfrequency leakage\\n, caused by the high-frequency components of the sinc \\nfunction. Leakage produces a blocky effect on images. Although leakage can never \\nbe totally eliminated, it can be reduced signiﬁcantly by multiplying the sampled \\nfunction by another function that tapers smoothly to near zero at both ends of the \\nsampled record. This idea is to dampen the sharp transitions (and thus the high fre-\\nquency components) of the box. This approach, called \\nwindowing\\n or \\napodizing\\n, is an \\nimportant consideration when ﬁdelity in image reconstruction (as in high-deﬁnition \\ngraphics) is desired. \\nSUMMARY OF 2-D DISCRETE FOURIER TRANSFORM PROPERTIES\\nTable 4.3 summarizes the principal DFT definitions introduced in this chapter. We \\nwill discuss the separability property in Section\\n \\n4.11, where we also show how to \\nobtain the inverse DFT using a forward transform algorithm.\\n Correlation will be \\ndiscussed in detail Chapter 12.\\nTable 4.4 summarizes some important DFT pairs. Although our focus is on dis-\\ncrete functions, the last two entries in the table are Fourier transform pairs that can \\nbe derived only for continuous variables (note the use of continuous variable nota-\\ntion).We include them here because, with proper interpretation, they are quite use-\\nful in digital image processing. The differentiation pair can be used to derive the fre-\\nquency-domain equivalent of the Laplacian deﬁned in Eq. (3-50) (see Problem 4.52). \\nThe Gaussian pair is discussed in Section 4.7. Tables\\n \\n4.1, 4.3 and 4.4 provide a sum-\\nmary of properties useful when working with the DFT\\n. Many of these properties \\nare key elements in the development of the material in the rest of this chapter, and \\nsome are used in subsequent chapters.\\nA simple apodizing \\nfunction is a triangle, cen-\\ntered on the data record, \\nwhich tapers to 0 at both \\nends of the record. This is \\ncalled a \\nBartlett window\\n. \\nOther common windows \\nare the Gaussian, the \\nHamming\\n and the \\nHann\\n \\nwindows. \\nDIP4E_GLOBAL_Print_Ready.indb   257\\n6/16/2017   2:05:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 258}),\n",
       " Document(page_content='258\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nName\\nExpression(s)\\n1) Discrete Fourier \\ntransform (DFT) of\\nfx y\\n(,\\n)\\nFf\\nx\\ny\\ne\\nju x M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nv\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n2) Inverse discrete  \\nFourier transform  \\n(IDFT) of \\nF\\n(,\\n)\\nuv\\nfx y\\nMN\\nFe\\nju x M y N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nuv\\nv\\nv u\\np\\n3) Spectrum\\nFR I R F I F\\n(,) (,) (,)\\n( ) ; ( )\\nuv uv uv\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n==\\n22\\n12\\nReal  Imag\\n4) Phase angle\\nf\\n(,) t a n\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n1\\nI\\nR\\n5) Polar representation\\nFF e\\nj\\n(,) (,)\\n(,)\\nuv uv\\nuv\\n=\\nf\\n6) Power spectrum\\nPF\\n(,) (,)\\nuv uv\\n=\\n2\\n7) Average value\\nf\\nMN\\nfx y\\nMN\\nF\\ny\\nN\\nx\\nM\\n==\\n=\\n−\\n=\\n−\\n∑ ∑\\n11\\n00\\n0\\n1\\n0\\n1\\n(,) (,)\\n8)\\nPeriodicity (\\nk\\n1\\n and  \\nk\\n2\\n are integers)\\nFF k M F k N\\nFk\\nk N\\nfx y fx k M y\\n(,) ( ,) (, )\\n(, )\\n(,) ( ,\\nuv u v uv\\nuv\\n=+ = +\\n=++\\n=+\\n12\\n12\\n1\\n)\\n)( , )\\n(,)\\n=+\\n=+ +\\nfx y k N\\nfx k M y k N\\n2\\n12\\n9) Convolution\\n((\\n,\\n)\\n(\\n,\\n)\\n)( , )\\nff\\nm\\nn\\nh\\nx\\nm\\ny\\nn\\nhx y\\nn\\nN\\nm\\nM\\n/H22841\\n=\\n−−\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n10) Correlation\\n((\\n,\\n)\\n(\\n,\\n)\\n)( , )\\n*\\nff\\nm\\nn\\nh\\nx\\nm\\ny\\nn\\nhx y\\nn\\nN\\nm\\nM\\n/H22845\\n=\\n++\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n11) Separability\\nThe 2-D DFT can be computed by computing 1-D DFT \\ntransforms along the rows (columns) of the image, followed \\nby 1-D transforms along the columns (rows) of the result. \\nSee Section 4.11.\\n12) Obtaining the IDFT \\nusing a DFT  \\nalgorithm\\nMNf x y F e\\nju x M y N\\nN M\\n** ( )\\n(,) (,)\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\nv u\\n2\\n0\\n1\\n0\\n1\\np\\n \\nThis equation indicates that inputting \\nF\\n*\\n(,)\\nuv\\n into an \\nalgorithm that computes the forward transform (right side \\nof above equation) yields \\nMNf x y\\n*\\n(,) .\\n Taking the complex \\nconjugate and dividing by \\nMN\\n gives the desired inverse\\n. See \\nSection 4.11.\\nTABLE \\n4.3\\nSummary of DFT \\ndeﬁnitions and \\ncorresponding \\nexpressions. \\nDIP4E_GLOBAL_Print_Ready.indb   258\\n6/16/2017   2:05:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 259}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n259\\nTABLE \\n4.4\\nSummmary of \\nDFT pairs. The  \\nclosed-form \\nexpressions in 12 \\nand 13 are valid \\nonly for  \\ncontinuous  \\nvariables. They \\ncan be used with \\ndiscrete variables \\nby sampling the  \\ncontinuous  \\nexpressions. \\nName\\nDFT Pairs\\n1) Symmetry \\nproperties\\nSee Table 4.1\\n2) Linearity\\na f xy b f xy a F b F\\n12 12\\n(,) (,) (,) (,)\\n+⇔+\\nuv uv\\n3) Translation \\n(general)\\nfx y e\\nFu u\\nfx x y y F e\\nj uxM yN\\nj\\n(,)\\n( , )\\n(,) ( , )\\n()\\n2\\n00\\n00\\n2\\n0\\np\\np\\n+\\n−\\n⇔−−\\n−− ⇔\\nv\\n0\\nvv\\nuv\\n(\\n()\\nux M y N\\n0\\n+\\nv\\n0\\n4) Translation \\nto center of \\nthe frequency \\nrectangle, \\n(,)\\nMN\\n22\\nfx y F M N\\nfx M N F\\nxy\\n(,) ( ) ( , )\\n(,) ( , ) ( )\\n−⇔\\n−−\\n−− ⇔ −\\n+\\n+\\n12 2\\n22 1\\nuv\\nyu\\nv\\nuv\\n5) Rotation\\nfr F\\nrx y y x\\n(, ) ( , )\\ntan (\\n)t\\na\\nn\\n(\\n)\\nuu v wu\\nuvw\\n+⇔ +\\n=+ = =+ =\\n−−\\n00\\n22 1 22 1\\nuv\\nv u\\n6) Convolution \\ntheorem\\n†\\nfF\\nH\\nfh x y M N F H\\nhx y\\n/H22841\\n/H22841\\n)( , )\\n() (\\n() ( , )\\n() ( , ) ) ( , )\\n⇔\\n⇔\\n[]\\ni\\ni\\nuv\\nuv\\n1\\n7) Correlation \\ntheorem\\n†\\n() ( , ) ( ) ( , )\\n() ( , ) ( ) ( ) ( , )\\n*\\n*\\nfh x y F H\\nfh x y M N FH\\n/H22845\\n/H22845\\n⇔\\n⇔\\n[]\\ni\\ni\\nuv\\nuv\\n1\\n8) Discrete unit \\nimpulse\\nd\\n(,\\n)\\nxy\\n⇔\\n1\\n1\\n⇔\\nMN\\nd\\n(,)\\nuv\\n9) Rectangle\\nrec\\nab a b\\na\\na\\nb\\nb\\ne\\njab\\n,\\nsin( )\\n()\\nsin( )\\n()\\n()\\n[]\\n⇔\\n−+\\np\\np\\np\\np\\np\\nu\\nu\\nv\\nv\\nuv\\n10) Sine\\nsin(\\n) ( , ) ( , )\\n22\\n2\\n0 0\\n00 00\\npp d d\\nu v\\nuu vv uu vv\\nxM yN\\njMN\\n+⇔ + + − − −\\n[]\\n11) Cosine\\ncos(\\n) ( , ) ( , )\\n22\\n1\\n2\\n0 0\\n00 00\\npp d d\\nu v uu vv uu vv\\nxM yN\\n+⇔ + + + − −\\n[]\\nThe following Fourier transform pairs are derivable only for continuous variables, denoted \\nas before by \\nt\\n and \\nz\\n for spatial variables and by \\nm\\n and \\nn\\n for frequency variables. These \\nresults can be used for DFT work by sampling the continuous forms\\n.\\n12) Differentiation \\n(the expressions \\non the right \\nassume that \\nf\\n(,\\n) .\\n±±\\n/H11009/H11009\\n=\\n0\\nab ab\\n∂\\n∂\\n∂\\n∂\\n⇔\\n∂\\n∂\\n⇔\\ntz\\nft z j j F\\nft z\\nt\\nj\\nmn\\nmn\\nm\\nm\\nm\\n(, ) ( ) ( ) ( , )\\n(, )\\n()\\n22\\n2\\npm pn m n\\npm\\nF F\\nft z\\nz\\njF\\nn\\nm\\nn\\n(,) ;\\n(, )\\n() ( , )\\nmn p n mn\\n∂\\n∂\\n⇔\\n2\\n13) Gaussian\\nAe A e A\\ntz\\n2\\n22\\n2\\n22 2 2\\n2 2 2\\nps\\nps\\nm n s\\n−+ − +\\n⇔\\n() ( )\\n(  is a constant)\\n†\\n Assumes that \\nfx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n have been properly padded. Convolution is associative, commutative, and \\ndistributive\\n. Correlation is distributive (see Table 3.5). The products are elementwise products (see Section 2.6).\\nDIP4E_GLOBAL_Print_Ready.indb   259\\n6/16/2017   2:05:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 260}),\n",
       " Document(page_content='260\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.7 THE BASICS OF FILTERING IN THE FREQUENCY DOMAIN  \\nIn this section, we lay the groundwork for all the filtering techniques discussed in the \\nremainder of the chapter. \\nADDITIONAL CHARACTERISTICS OF THE FREQUENCY DOMAIN\\nWe begin by observing in Eq. (4-67) that \\neach\\n term of \\nF\\n(,\\n)\\nuv\\n contains \\nall\\n values of \\nfx y\\n(,\\n) ,\\n modified by the values of the exponential terms. Thus, with the exception \\nof trivial cases\\n, it is usually impossible to make direct associations between specific \\ncomponents of an image and its transform. However, some general statements can \\nbe made about the relationship between the frequency components of the Fourier \\ntransform and spatial features of an image. For instance, because frequency is direct-\\nly related to spatial rates of change, it is not difficult intuitively to associate frequen-\\ncies in the Fourier transform with patterns of intensity variations in an image. We \\nshowed in Section 4.6 that the slowest varying frequency component \\n()\\nuv\\n==\\n0  \\nis proportional to the average intensity of an image\\n. As we move away from the \\norigin of the transform, the low frequencies correspond to the slowly varying inten-\\nsity components of an image. In an image of a room, for example, these might cor-\\nrespond to smooth intensity variations on the walls and floor. As we move further \\naway from the origin, the higher frequencies begin to correspond to faster and faster \\nintensity changes in the image. These are the edges of objects and other components \\nof an image characterized by abrupt changes in intensity.\\nFiltering techniques in the frequency domain are based on modifying the Fourier \\ntransform to achieve a speciﬁc objective, and then computing the inverse DFT to get \\nus back to the spatial domain, as introduced in Section 2.6. It follows from Eq. (4-87) \\nthat the two components of the transform to which we have access are the transform \\nmagnitude (spectrum) and the phase angle. We learned in Section 4.6 that visual \\nanalysis of the phase component generally is not very useful. The spectrum, however, \\nprovides some useful guidelines as to the gross intensity characteristics of the image \\nfrom which the spectrum was generated. For example, consider Fig. 4.28(a), which \\nis a scanning electron microscope image of an integrated circuit, magniﬁed approxi-\\nmately 2500 times. \\nAside from the interesting construction of the device itself, we note two principal \\nfeatures in this image: strong edges that run approximately at \\n±° ,\\n45\\n and two white, \\noxide protrusions resulting from thermally induced failure\\n. The Fourier spectrum \\nin Fig. 4.28(b) shows prominent components along the \\n±°\\n45\\n directions that corre-\\nspond to the edges just mentioned.\\n Looking carefully along the vertical axis in Fig. \\n4.28(b), we see a vertical component of the transform that is off-axis, slightly to the \\nleft. This component was caused by the edges of the oxide protrusions. Note how the \\nangle of the frequency component with respect to the vertical axis corresponds to \\nthe inclination (with respect to the horizontal axis of the image) of the long white \\nelement. Note also the zeros in the vertical frequency component, corresponding to \\nthe narrow vertical span of the oxide protrusions.\\nThese are typical of the types of associations we can make in general between \\nthe frequency and spatial domains. As we will show later in this chapter, even these \\ntypes of gross associations, coupled with the relationships mentioned previously \\n4.7\\nDIP4E_GLOBAL_Print_Ready.indb   260\\n6/16/2017   2:05:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 261}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n261\\nbetween frequency content and rate of change of intensity levels in an image, can \\nlead to some very useful results. We will show in Section 4.8 the effects of modifying \\nvarious frequency ranges in the transform of Fig. 4.28(a). \\nFREQUENCY DOMAIN FILTERING FUNDAMENTALS\\nFiltering in the frequency domain consists of modifying the Fourier transform of an \\nimage, then computing the inverse transform to obtain the spatial domain represen-\\ntation of the processed result. Thus, given (a padded) digital image, \\nfx y\\n(,\\n) ,\\n of size \\nPQ\\n×\\n pixels, the basic filtering equation in which we are interested has the form:\\n \\ngxy\\nH F\\n(,\\n)\\n(,)(,)\\n=\\n[]\\n{}\\n−\\nReal\\n/H5219\\n1\\nuv uv\\n \\n(4-104)\\nwhere \\n/H5219\\n−\\n1\\n is the IDFT, \\nF\\n(,\\n)\\nuv\\n is the DFT of the input image, \\nfx y\\n(,\\n) ,\\n \\nH\\n(,\\n)\\nuv\\n is a \\nfilter transfer function \\n(which we often call just a \\nfilter\\n or \\nfilter function\\n),\\n and \\ngxy\\n(,\\n)\\n \\nis the \\nfiltered\\n (\\noutput\\n) \\nimage\\n.\\n Functions \\nF\\n, \\nH\\n, and \\ng\\n are arrays of size \\nPQ\\n×\\n,\\n the same \\nas the padded input image\\n. The product \\nHF\\n(,\\n) (,)\\nuv uv\\n is formed using elementwise \\nmultiplication,\\n as defined in Section 2.6. The filter transfer function modifies the \\ntransform of the input image to yield the processed output, \\ngxy\\n(,\\n) .\\n The task of speci-\\nfying \\nH\\n(,\\n)\\nuv\\n is simplified considerably by using functions that are symmetric about \\ntheir center\\n, which requires that \\nF\\n(,\\n)\\nuv\\n be centered also. As explained in Section 4.6, \\nthis is accomplished by multiplying the input image by \\n()\\n−\\n+\\n1\\nxy\\n prior to computing \\nits transform.\\n†\\n†\\n Some software implementations of the 2-D DFT (e.g., MATLAB) do not center the transform. This implies \\nthat ﬁlter functions must be arranged to correspond to the same data format as the uncentered transform (i.e., \\nwith the origin at the top left). The net result is that ﬁlter transfer functions are more difﬁcult to generate and \\ndisplay. We use centering in our discussions to aid in visualization, which is crucial in developing a clear under-\\nstanding of ﬁltering concepts. Either method can be used in practice, provided that consistency is maintained. \\nIf \\nH\\n is real and  \\nsymmetric and \\nf\\n is real \\n(as is typically the case), \\nthen the IDFT in Eq. \\n(4-104) should yield \\nreal quantities in theory. \\nIn practice, the inverse \\noften contains para-\\nsitic complex terms from \\nroundoff error and other \\ncomputational inaccura-\\ncies. Thus, it is customary \\nto take the real part of \\nthe IDFT to form \\ng\\n.\\nb a\\nFIGURE 4.28\\n (a) SEM image of a damaged integrated circuit. (b) Fourier spectrum of (a).  \\n(Original image courtesy of Dr. J. M. Hudak, Brockhouse Institute for Materials Research, \\nMcMaster University, Hamilton, Ontario, Canada.) \\nDIP4E_GLOBAL_Print_Ready.indb   261\\n6/16/2017   2:05:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 262}),\n",
       " Document(page_content='262\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nWe are now in a position to consider ﬁltering in  detail. One of the simplest ﬁlter \\ntransfer functions we can construct is a function \\nH\\n(,\\n)\\nuv\\n that is 0 at the center of \\nthe (centered) transform,\\n and 1’s elsewhere. This ﬁlter would reject the dc term and \\n“pass” (i.e., leave unchanged) all other terms of \\nF\\n(,\\n)\\nuv\\n when we form the product \\nHF\\n(,\\n) (,) .\\nuv uv\\n We know from property 7 in Table 4.3 that the dc term is responsible \\nfor the average intensity of an image\\n, so setting it to zero will reduce the average \\nintensity of the output image to zero. Figure 4.29 shows the result of this operation \\nusing Eq. (4-104). As expected, the image became much darker. An average of zero \\nimplies the existence of negative intensities. Therefore, although it illustrates the \\nprinciple, Fig. 4.29 is not a true representation of the original, as all negative intensi-\\nties were clipped (set to 0) by the display.\\nAs noted earlier, low frequencies in the transform are related to slowly varying \\nintensity components in an image, such as the walls of a room or a cloudless sky in \\nan outdoor scene. On the other hand, high frequencies are caused by sharp transi-\\ntions in intensity, such as edges and noise. Therefore, we would expect that a func-\\ntion \\nH\\n(,\\n)\\nuv\\n that attenuates high frequencies while passing low frequencies (called a \\nlo\\nwpass ﬁlter\\n, as noted before) would blur an image, while a ﬁlter with the opposite \\nproperty (called a \\nhighpass ﬁlter\\n) would enhance sharp detail, but cause a reduction \\nin contrast in the image. Figure 4.30 illustrates these effects. For example, the ﬁrst \\ncolumn of this ﬁgure shows a lowpass ﬁlter transfer function and the corresponding \\nﬁltered image. The second column shows similar results for a highpass ﬁlter. Note \\nthe similarity between Figs. 4.30(e) and Fig. 4.29. The reason is that the highpass \\nﬁlter function shown eliminates the dc term, resulting in the same basic effect that \\nled to Fig. 4.29. As illustrated in the third column, adding a small constant to the \\nﬁlter does not affect sharpening appreciably, but it does prevent elimination of the \\ndc term and thus preserves tonality.\\nEquation (4-104) involves the product of two functions in the frequency domain \\nwhich, by the convolution theorem, implies convolution in the spatial domain. We \\nknow from the discussion in Section 4.6 that we can expect wraparound error if \\nthe functions in question are not padded. Figure 4.31 shows what happens when \\nFIGURE 4.29\\nResult of ﬁlter-\\ning the image in \\nFig. 4.28(a) with \\na ﬁlter transfer \\nfunction that sets \\nto 0 the dc term, \\nFP Q\\n(, ) ,\\n22\\n \\nin the centered \\nF\\nourier transform, \\nwhile leaving all \\nother transform \\nterms unchanged.\\nDIP4E_GLOBAL_Print_Ready.indb   262\\n6/16/2017   2:05:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 263}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n263\\nv\\nv\\nu\\nu\\na\\nH\\n(\\nu\\n, \\nv\\n)\\nH\\n(\\nu\\n, \\nv\\n)\\nM\\n/\\n2\\nM\\n/\\n2\\nN\\n/\\n2\\nN\\n/\\n2\\nH\\n(\\nu\\n, \\nv\\n)\\nv\\nu\\nM\\n/\\n2\\nN\\n/\\n2\\nb a\\nc\\ne d\\nf\\nFIGURE 4.30\\n Top row: Frequency domain ﬁlter transfer functions of (a) a lowpass ﬁlter, (b) a highpass ﬁlter, and (c) \\nan offset highpass ﬁlter. Bottom row: Corresponding ﬁltered images obtained using Eq. (4-104). The offset in (c) is \\na\\n=\\n08\\n5\\n.,\\n and the height of \\nH\\n(,\\n)\\nuv\\n is 1. Compare (f) with Fig. 4.28(a). \\nb a\\nc\\nFIGURE 4.31\\n (a) A simple image. (b) Result of blurring with a Gaussian lowpass ﬁlter without padding. (c) Result of \\nlowpass ﬁltering with zero padding. Compare the vertical edges in (b) and (c). \\nDIP4E_GLOBAL_Print_Ready.indb   263\\n6/16/2017   2:05:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 264}),\n",
       " Document(page_content='264\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nFIGURE 4.32\\n (a) Image periodicity without image padding. (b) Periodicity after padding with 0’s (black). The dashed \\nareas in the center correspond to the image in Fig. 4.31(a). Periodicity is inherent when using the DFT. (The thin \\nwhite lines in both images are superimposed for clarity; they are not part of the data.) \\nwe apply Eq. (4-104) without padding. Figure 4.31(a) shows a simple image, and \\nFig. 4.31(b) is the result of lowpass ﬁltering the image with a Gaussian lowpass ﬁlter \\nof the form shown in Fig. 4.30(a). As expected, the image is blurred. However, the \\nblurring is not uniform; the top white edge is blurred, but the sides are not. Pad-\\nding the input image with zeros according to Eqs. (4-98) and (4-99) before applying \\nEq. (4-104) resulted in the ﬁltered image in Fig. 4.31(c). This result is as expected, \\nwith a uniform dark border resulting from zero padding (see Fig. 3.33 for an expla-\\nnation of this effect).\\nFigure 4.32 illustrates the reason for the discrepancy between Figs. 4.31(b) and (c). \\nThe dashed area in Fig. 4.32(a) corresponds to the image in Fig. 4.31(a). The other \\ncopies of the image are due to the implied periodicity of the image (and its trans-\\nform) implicit when we use the DFT, as explained in Section 4.6. Imagine convolving \\nthe spatial representation of the blurring ﬁlter (i.e., the corresponding spatial ker-\\nnel) with this image. When the kernel is centered on the top of the dashed image, it \\nwill encompass part of the image and also part of the bottom of the periodic image \\nimmediately above it. When a dark and a light region reside under the ﬁlter, the \\nresult is a mid-gray, blurred output. However, when the kernel is centered on the top \\nright side of the image, it will encompass only light areas in the image and its right \\nregion. Because the average of a constant value is that same value, ﬁltering will have \\nno effect in this area, giving the result in Fig. 4.31(b). Padding the image with 0’s cre-\\nates a uniform border around each image of the periodic sequence, as Fig. 4.32(b) \\nshows. Convolving the blurring kernel with the padded “mosaic” of Fig. 4.32(b) gives \\nthe correct result in Fig. 4.31(c). You can see from this example that failure to pad an \\nimage prior to ﬁltering can lead to unexpected results. \\nThus far, the discussion has centered on padding the input image. However, \\nEq. (4-104) also involves a ﬁlter transfer function that can be speciﬁed either in the \\nDIP4E_GLOBAL_Print_Ready.indb   264\\n6/16/2017   2:05:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 265}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n265\\nspatial or in the frequency domain. But padding is done in the spatial domain, which \\nraises an important question about the relationship between \\nspatial\\n padding and \\nﬁlter functions speciﬁed directly in the frequency domain.\\nIt would be reasonable to conclude that the way to handle padding of a frequency \\ndomain transfer function is to construct the function the same size as the unpad-\\nded image, compute the IDFT of the function to obtain the corresponding spatial \\nrepresentation, pad that representation in the spatial domain, and then compute its \\nDFT to return to the frequency domain. The 1-D example in Fig. 4.33 illustrates the \\npitfalls in this approach. \\nFigure 4.33(a) shows a 1-D ideal lowpass ﬁlter transfer function in the frequency \\ndomain. The function is real and has even symmetry, so we know from property 8 \\nin Table 4.1 that its IDFT will be real and symmetric also. Figure 4.33(b) shows the \\nresult of multiplying the elements of the transfer function by \\n()\\n−\\n1\\nu\\n and computing \\nits IDFT to obtain the corresponding spatial ﬁlter kernel. The result is shown in \\nFig. 4.33(b). It is evident in this ﬁgure that the extremes of this spatial function are \\nnot zero. Zero-padding the function would create two discontinuities, as Fig. 4.33(c) \\nshows. To return to the frequency domain, we compute the forward DFT of the \\nspatial, padded function. As Fig. 4.33(d) shows, the discontinuities in the padded \\nfunction caused ringing in its frequency domain counterpart. \\nPadding the two ends of \\na function is the same \\nas padding one end, \\nprovided that the total \\nnumber of zeros is the \\nsame.\\n0.01\\n0.02\\n0.03\\n0.04\\n0 128 256 384 511\\n0 128 255\\n0\\n1.2\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n/H11002\\n0.2\\n/H11002\\n0.01\\n0 128 255\\n0 128 256 384 511\\n1.2\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n/H11002\\n0.2\\n0.01\\n0.02\\n0.03\\n0.04\\n0\\n/H11002\\n0.01\\nb\\na\\nd\\nc\\nFIGURE 4.33\\n(a) Filter transfer \\nfunction speciﬁed in \\nthe (centered)  \\nfrequency domain. \\n(b) Spatial  \\nrepresentation (ﬁlter \\nkernel) obtained by  \\ncomputing the IDFT \\nof (a).  \\n(c) Result of  \\npadding (b) to twice \\nits length (note the \\ndiscontinuities).  \\n(d) Corresponding \\nﬁlter in the frequen-\\ncy domain obtained \\nby computing the \\nDFT of (c). Note the \\nringing caused by \\nthe discontinuities \\nin (c). Part (b) of the \\nﬁgure is below (a), \\nand (d) is below (c).\\nDIP4E_GLOBAL_Print_Ready.indb   265\\n6/16/2017   2:05:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 266}),\n",
       " Document(page_content='266\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe preceding results tell us that we cannot pad the spatial representation of a \\nfrequency domain transfer function in order to avoid wraparound error. Our objec-\\ntive is to work with speciﬁed ﬁlter shapes in the frequency domain without having to \\nbe concerned with truncation issues. An alternative is to pad images and then create \\nthe desired ﬁlter transfer function directly in the frequency domain, this function \\nbeing of the same size as the padded images (remember, images and ﬁlter transfer \\nfunctions must be of the same size when using the DFT). Of course, this will result \\nin wraparound error because no padding is used for the ﬁlter transfer function, but \\nthis error is mitigated signiﬁcantly by the separation provided by padding the image, \\nand it is preferable to ringing. Smooth transfer functions (such as those in Fig. 4.30) \\npresent even less of a problem. Speciﬁcally, then, the approach we will follow in this \\nchapter is to pad images to size \\nPQ\\n×\\n and construct ﬁlter transfer functions of the \\nsame dimensions directly in the frequenc\\ny domain. As explained earlier, \\nP\\n and \\nQ\\n \\nare given by Eqs. (4-100) and (4-101).\\nWe conclude this section by analyzing the phase angle of ﬁltered images. We can \\nexpress the DFT in terms of its real and imaginary parts: \\nFRj I\\n(,\\n) (,) (,) .\\nuv uv uv\\n=+\\n \\nEquation (4-104) then becomes\\n \\ngxy H R j H I\\n(\\n,) (,)(,) (,) (,)\\n=+\\n[]\\n−\\n/H5219\\n1\\nuv uv uv uv\\n \\n(4-105)\\nThe phase angle is computed as the arctangent of the ratio of the imaginary and the \\nreal parts of a complex number [see Eq.\\n (4-88)]. Because \\nH\\n(,\\n)\\nuv\\n multiplies both \\nR\\n and \\nI\\n,\\n it will cancel out when this ratio is formed. Filters that affect the real and \\nimaginary parts equally, and thus have no effect on the phase angle, are appropri-\\nately called \\nzero-phase-shift\\n filters. These are the only types of filters considered in \\nthis chapter. \\nThe importance of the phase angle in determining the spatial structure of an \\nimage was vividly illustrated in Fig. 4.26. Thus, it should be no surprise that even \\nsmall changes in the phase angle can have dramatic (and usually undesirable) effects \\non the ﬁltered output. Figures 4.34(b) and (c) illustrate the effect of changing the \\nphase angle array of the DFT of Fig. 4.34(a) (the \\nF\\n(,)\\nuv\\n term was not changed in \\neither case).\\n Figure 4.34(b) was obtained by multiplying the phase angle, \\nf\\n(,\\n) ,\\nuv\\n in \\nEq.\\n (4-86) by \\n−\\n1\\n and computing the IDFT.  The net result is a reﬂection of every pixel \\nin the image about both coordinate axes\\n. Figure 4.34(c) was obtained by multiply-\\ning the phase term by 0.25 and computing the IDFT. Even a scale change rendered \\nthe image almost unrecognizable. These two results illustrate the advantage of using \\nfrequency-domain ﬁlters that do not alter the phase angle.\\nSUMMARY OF STEPS FOR FILTERING IN THE FREQUENCY DOMAIN\\nThe process of filtering in the frequency domain can be summarized as follows:\\n1. \\nGiven an input image \\nfx y\\n(,\\n)\\n of size \\nMN\\n×\\n,\\n obtain the padding sizes \\nP\\n and \\nQ\\n \\nusing Eqs\\n. (4-102) and (4-103); that is, \\nPM\\n=\\n2  and \\nQN\\n=\\n2.\\nDIP4E_GLOBAL_Print_Ready.indb   266\\n6/16/2017   2:05:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 267}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n267\\n2. \\nForm a padded\\n†\\n image \\nfx y\\np\\n(,)\\n of size \\nPQ\\n×\\n using zero-, mirror-, or replicate \\npadding (see F\\nig. 3.39\\n \\nfor a comparison of padding methods).\\n3. \\nMultiply \\nfx y\\np\\n(,)\\n by \\n()\\n−\\n1\\nxy\\n+\\n to center the Fourier transform on the \\nPQ\\n×\\n fre-\\nquenc\\ny rectangle.\\n4. \\nCompute the DFT, \\nF\\n(,\\n) ,\\nuv\\n of the image from Step 3.\\n5. \\nConstruct a real, symmetric ﬁlter transfer function, \\nH\\n(,\\n) ,\\nuv\\n of size \\nPQ\\n×\\n with \\ncenter at \\n(, ) .\\nPQ\\n22\\n6. \\nForm the product \\nGH F\\n(,\\n) (,) (,)\\nuv uv uv\\n=\\n using elementwise multiplication; that \\ni\\ns, \\nGik HikFik\\n(,\\n) (, ) (, )\\n=\\n for \\niM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nk\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n7. \\nObtain the ﬁltered image (of size \\nP\\nQ\\n×\\n) by computing the IDFT of \\nG\\n(,\\n) :\\nuv\\n \\ngx y\\nG\\np\\nxy\\n(,)\\n(,) ( )\\n=\\n{}\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n−+\\nQR\\nreal\\n/H5219\\n1\\n1\\nuv\\n8. \\nObtain the ﬁnal ﬁltered result, \\ngxy\\n(,\\n) ,\\n of the same size as the input image, by \\nextracting the \\nMN\\n×\\n region from the top, left quadrant of \\ngx y\\np\\n(,) .\\n \\nW\\ne will discuss the construction of filter transfer functions (Step 5) in the following \\nsections of this chapter. In theory, the IDFT in Step 7 should be real because \\nfx y\\n(,\\n)\\n \\nis real and \\nH\\n(,\\n)\\nuv\\n is real and symmetric. However, parasitic complex terms in the \\nIDFT resulting from computational inaccuracies are not uncommon.\\n Taking the real \\npart of the result takes care of that. Multiplication by \\n()\\n−\\n+\\n1\\nxy\\n cancels out the multi-\\nplication by this factor in Step 3.\\n†\\n  Sometimes we omit padding when doing “quick” experiments to get an idea of ﬁlter performance, or when \\ntrying to determine quantitative relationships between spatial features and their effect on frequency domain \\ncomponents, particularly in band and notch ﬁltering, as explained later in Section 4.10 and in Chapter 5. \\nSee Section 2.6 for a \\ndeﬁnition of elementwise \\noperations.\\nb a\\nc\\nFIGURE 4.34\\n (a) Original image. (b) Image obtained by multiplying the phase angle array by \\n−\\n1\\n in Eq. (4-86) and \\ncomputing the IDFT\\n. (c) Result of multiplying the phase angle by 0.25 and computing the IDFT. The magnitude of \\nthe transform, \\nF\\n(,) ,\\nuv\\n used in (b) and (c) was the same.\\nDIP4E_GLOBAL_Print_Ready.indb   267\\n6/16/2017   2:05:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 268}),\n",
       " Document(page_content='268\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nFigure 4.35 illustrates the preceding steps using zero padding. The ﬁgure legend \\nexplains the source of each image. If enlarged, Fig. 4.35(c) would show black dots \\ninterleaved in the image because negative intensities, resulting from the multiplica-\\ntion of \\nf\\np\\n by \\n() ,\\n−\\n+\\n1\\nxy\\n are clipped at 0 by the display. Note in Fig. 4.35(h) the charac-\\nteristic dark border of  by lowpass ﬁltered images obtained using zero padding.\\nCORRESPONDENCE BETWEEN FILTERING IN THE SPATIAL AND  \\nFREQUENCY DOMAINS \\nAs mentioned several times before, the link between filtering in the spatial and fre-\\nquency domains is the convolution theorem. Earlier in this section, we defined fil-\\ntering in the frequency domain as the elementwise product of a filter transfer func-\\ntion, \\nH\\n(,\\n) ,\\nuv\\n and \\nF\\n(,\\n) ,\\nuv\\n the Fourier transform of the input image. Given \\nH\\n(,\\n) ,\\nuv\\n \\nsuppose that we want to find its equivalent kernel in the spatial domain.\\n If we let \\nfx y x y\\n(,\\n) (,) ,\\n=\\nd\\n it follows from Table 4.4 that \\nF\\n(,\\n) .\\nuv\\n=\\n1\\n Then, from Eq. (4-104), \\nthe filtered output is \\n/H5219\\n−\\n{}\\n1\\nH\\n(,).\\nuv\\n This expression as the inverse transform of the \\nfrequenc\\ny domain filter transfer function, which is the corresponding kernel in the \\nSee Section 2.6 for a \\ndeﬁnition of elementwise \\noperations.\\nb a\\nc\\ne d\\nf\\nh\\ng\\nFIGURE 4.35\\n(a) An \\nMN\\n×\\n \\nimage\\n, \\nf\\n.  \\n(b) Padded image\\n, \\nf\\np\\n of size \\nPQ\\n×\\n. \\n(c) Result of \\nmultiplying \\nf\\np\\n by \\n() .\\n−\\n+\\n1\\nxy\\n \\n(d) Spectrum of \\nF\\n. (e) Centered \\nGaussian lowpass \\nﬁlter transfer \\nfunction,\\n \\nH\\n, of size \\nPQ\\n×\\n.  \\n(f) Spectrum of \\nthe product \\nHF\\n. \\n(g) Image \\ng\\np\\n, the \\nreal part of the \\nIDFT of \\nHF\\n,\\n mul-\\ntiplied by \\n() .\\n−\\n+\\n1\\nxy\\n  \\n(h) Final result, \\ng\\n, obtained by \\nextracting the ﬁrst \\nM\\n rows and \\nN\\n \\ncolumns of \\ng\\np\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   268\\n6/16/2017   2:05:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 269}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n269\\nspatial domain. Conversely, it follows from a similar analysis and the convolution \\ntheorem that, given a spatial filter kernel, we obtain its frequency domain repre-\\nsentation by taking the forward Fourier transform of the kernel. Therefore, the two \\nfilters form a Fourier transform pair:\\n \\nhxy H\\n(,\\n) (,)\\n⇔\\nuv\\n \\n(4-106)\\nwhere \\nhxy\\n(,\\n)\\n is the spatial kernel. Because this kernel can be obtained from the \\nresponse of a frequenc\\ny domain filter to an impulse, \\nhxy\\n(,\\n)\\n sometimes is referred to \\nas the \\nimpulse response\\n of \\nH\\n(,\\n) .\\nuv\\n Also, because all quantities in a discrete imple-\\nmentation of Eq.\\n (4-106) are finite, such filters are called \\nfinite impulse response\\n \\n(FIR) filters. These are the only types of linear spatial filters considered in this book.\\nWe discussed spatial convolution in Section 3.4,\\n \\nand its implementation in \\nEq.\\n (3-35), which involved convolving functions of different sizes. When we use the \\nDFT to compute the transforms used in the convolution theorem, it is implied that \\nwe are convolving periodic functions of the \\nsame\\n size, as explained in Fig. 4.27. For \\nthis reason, as explained earlier, Eq. (4-94) is referred to as \\ncircular convolution\\n.\\nWhen computational speed, cost, and size are important parameters, spatial con-\\nvolution ﬁltering using Eq. (3-35) is well suited for small kernels using hardware \\nand/or ﬁrmware, as explained in Section 4.1. However, when working with general-\\npurpose machines, frequency-domain methods in which the DFT is computed using \\na fast Fourier transform (FFT) algorithm can be hundreds of times faster than using \\nspatial convolution, depending on the size of the kernels used, as you saw in Fig. 4.2. \\nWe will discuss the FFT and its computational advantages in Section 4.11.\\nFiltering concepts are more intuitive in the frequency domain, and ﬁlter design \\noften is easier there. One way to take advantage of the properties of both domains \\nis to specify a ﬁlter in the frequency domain, compute its IDFT, and then use the \\nproperties of the resulting, full-size spatial kernel as a guide for constructing smaller \\nkernels. This is illustrated next (keep in mind that the Fourier transform and its \\ninverse are linear processes (see Problem 4.24), so the discussion is limited to linear \\nﬁltering). In Example 4.15, we illustrate the converse, in which a spatial kernel is \\ngiven, and we obtain its full-size frequency domain representation. This approach is \\nuseful for analyzing the behavior of small spatial kernels in the frequency domain. \\nFrequency domain ﬁlters can be used as guides for specifying the coefﬁcients of \\nsome of the small kernels we discussed in Chapter 3. Filters based on Gaussian func-\\ntions are of particular interest because, as noted in Table 4.4, both the forward and \\ninverse Fourier transforms of a Gaussian function are real Gaussian functions. We \\nlimit the discussion to 1-D to illustrate the underlying principles. Two-dimensional \\nGaussian transfer functions are discussed later in this chapter.\\nLet \\nHu\\n()\\n denote the 1-D frequency domain Gaussian transfer function\\n \\nHu A e\\nu\\n()\\n=\\n−\\n22\\n2\\ns\\n \\n(4-107)\\nwhere \\ns\\n is the standard deviation of the Gaussian curve. The kernel in the spatial \\ndomain is obtained by taking the inverse DFT of \\nHu\\n()\\n (see Problem\\n \\n4.48):\\n \\nhx A e\\nx\\n()\\n=\\n−\\n2\\n2\\n22 2\\nps\\nps\\n \\n(4-108)\\nAs mentioned in Table \\n4.4, the forward and \\ninverse Fourier trans-\\nforms of Gaussians are \\nvalid only for continuous \\nvariables. To use discrete \\nformulations, we sample \\nthe continuous forms.\\nDIP4E_GLOBAL_Print_Ready.indb   269\\n6/16/2017   2:05:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 270}),\n",
       " Document(page_content='270\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThese two equations are important for two reasons: (1) They are a Fourier trans-\\nform pair, both components of which are Gaussian \\nand\\n real. This facilitates analysis \\nbecause we do not have to be concerned with complex numbers. In addition, Gauss-\\nian curves are intuitive and easy to manipulate. (2) The functions behave recipro-\\ncally. When \\nHu\\n()\\n has a broad profile (large value of \\ns\\n),\\n \\nhx\\n()\\n has a narrow profile, \\nand vice versa.\\n In fact, as \\ns\\n approaches infinity, \\nHu\\n()\\n tends toward a constant func-\\ntion and \\nhx\\n()\\n tends toward an impulse, which implies no filtering in either domain.\\nF\\nigures 4.36(a) and (b) show plots of a Gaussian lowpass ﬁlter transfer function \\nin the frequency domain and the corresponding function in the spatial domain. Sup-\\npose that we want to use the shape of \\nhx\\n()\\n in Fig. 4.36(b) as a guide for specifying \\nthe coefﬁcients of a small kernel in the spatial domain.\\n The key characteristic of the \\nfunction in Fig. 4.36(b) is that all its values are positive. Thus, we conclude that we \\ncan implement lowpass ﬁltering in the spatial domain by using a kernel with all posi-\\ntive coefﬁcients (as we did in Section 3.5). For reference, Fig. 4.36(b) also shows two \\nof the kernels discussed in that section. Note the reciprocal relationship between \\nthe width of the Gaussian functions, as discussed in the previous paragraph. The nar-\\nrower the frequency domain function, the more it will attenuate the low frequencies, \\nresulting in increased blurring. In the spatial domain, this means that a larger kernel \\nmust be used to increase blurring, as we illustrated in Example 3.11.\\nAs you know from Section 3.7, we can construct a highpass ﬁlter from a lowpass \\nﬁlter by subtracting a lowpass function from a constant. We working with Gauss-\\nian functions, we can gain a little more control over ﬁlter function shape by using \\na so-called \\ndifference of Gaussians\\n, which involves two lowpass functions. In the \\nfrequency domain, this becomes\\n \\nHu A e B e\\nuu\\n()\\n//\\n=−\\n−−\\n2\\n1\\n22\\n2\\n2\\n22\\nss\\n \\n(4-109)\\nwith \\nAB\\n≥\\n and \\nss\\n12\\n>\\n. The corresponding function in the spatial domain is\\nH\\n(\\nu\\n)\\nuu\\nx\\nx\\nH\\n(\\nu\\n)\\nh\\n(\\nx\\n)\\n1\\n16\\n––\\n/H11003\\nh\\n(\\nx\\n)\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n8\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n0\\n/H11002\\n1\\n0\\n/H11002\\n1\\n4\\n/H11002\\n1\\n0\\n/H11002\\n1\\n0\\n1\\n2\\n1\\n2\\n1\\n9\\n––\\n/H11003\\n4\\n2\\n1\\n2\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\nb\\na\\nd\\nc\\nFIGURE 4.36\\n(a) A 1-D Gaussian \\nlowpass transfer \\nfunction in the \\nfrequency domain. \\n(b) Corresponding \\nkernel in the spatial \\ndomain. (c) Gauss-\\nian highpass trans-\\nfer function in the \\nfrequency domain. \\n(d) Corresponding \\nkernel. The small \\n2-D kernels shown \\nare kernels we used \\nin Chapter 3. \\nDIP4E_GLOBAL_Print_Ready.indb   270\\n6/16/2017   2:05:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 271}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n271\\n \\nhx A e\\nB e\\nxx\\n()\\n=−\\n−−\\n22\\n1\\n2\\n2\\n2\\n2\\n1\\n22 2\\n2\\n22\\nps\\nps\\nps\\nps\\n \\n(4-110)\\nFigures 4.36(c) and (d) show plots of these two equations. We note again the reci-\\nprocity in width,\\n but the most important feature here is that \\nhx\\n()\\n has a positive cen-\\nter term with negative terms on either side\\n. The small kernels shown in Fig. 4.36(d), \\nwhich we used in Chapter 3 for sharpening, “capture” this property, and thus illus-\\ntrate how knowledge of frequency domain filtering can be used as the basis for \\nchoosing coefficients of spatial kernels.\\nAlthough we have gone through signiﬁcant effort to get here, be assured that it is \\nimpossible to truly understand ﬁltering in the frequency domain without the foun-\\ndation we have just established. In practice, the frequency domain can be viewed as \\na “laboratory” in which we take advantage of the correspondence between frequen-\\ncy content and image appearance. As will be demonstrated numerous times later in \\nthis chapter, some tasks that would be exceptionally difﬁcult to formulate direct-\\nly in the spatial domain become almost trivial in the frequency domain. Once we \\nhave selected a speciﬁc ﬁlter transfer function via experimentation in the frequency \\ndomain, we have the option of implementing the ﬁlter directly in that domain using \\nthe FFT, or we can take the IDFT of the transfer function to obtain the equivalent \\nspatial domain function. As we showed in Fig. 4.36, one approach is to specify a \\nsmall spatial kernel that attempts to capture the “essence” of the \\nfull\\n ﬁlter function \\nin the spatial domain. A more formal approach is to design a 2-D digital ﬁlter by \\nusing approximations based on mathematical or statistical criteria, as we discussed \\nin Section 3.7. \\nEXAMPLE 4.15 :  Obtaining a frequency domain transfer function from a spatial kernel.\\nIn this example, we start with a spatial kernel and show how to generate its corresponding ﬁlter trans-\\nfer function in the frequency domain. Then, we compare the ﬁltering results obtained using frequency \\ndomain and spatial techniques. This type of analysis is useful when one wishes to compare the perfor-\\nmance of a given kernel against one or more “full” ﬁlter candidates in the frequency domain, or to gain a \\ndeeper understanding about the performance of a kernel in the spatial domain. To keep matters simple, \\nwe use the \\n33\\n×\\n vertical Sobel kernel from Fig. 3.50(e). Figure 4.37(a) shows a \\n600 600\\n×\\n-p\\nixel\\n image, \\nfx y\\n(,\\n) ,\\n that we wish to ﬁlter, and Fig. 4.37(b) shows its spectrum.\\nF\\nigure 4.38(a) shows the Sobel kernel, \\nhxy\\n(,\\n)\\n (the perspective plot is explained below). Because \\nthe input image is of size \\n600 600\\n×\\n pixels and the kernel is of size \\n33\\n×\\n,\\n we avoid wraparound error in \\nthe frequenc\\ny domain by padding \\nf\\n and \\nh\\n with zeros to size \\n602 602\\n×\\n pixels, according to Eqs. (4-100) \\nand (4-101).\\n At ﬁrst glance, the Sobel kernel appears to exhibit odd symmetry. However, its ﬁrst element \\nis not 0, as required by Eq. (4-81). To convert the kernel to the smallest size that will satisfy Eq. (4-83), \\nwe have to add to it a leading row and column of 0’s, which turns it into an array of size \\n44\\n×\\n.\\n We can \\nembed this array into a larger array of zeros and still maintain its odd symmetry if the larger array is of \\neven dimensions (as is the \\n44\\n×\\n kernel) \\nand\\n their centers coincide\\n, as explained in Example 4.10. The \\npreceding comments are an important aspect of ﬁlter generation. If we preserve the odd symmetry with \\nrespect to the padded array in forming \\nhx y\\np\\n(,) ,\\n we know from property 9 in Table 4.1 that \\nH\\n(,\\n)\\nuv\\n will \\nbe purely imaginary\\n. As we show at the end of this example, this will yield results that are identical to \\nﬁltering the image spatially using the original kernel \\nhxy\\n(,\\n) .\\n If the symmetry were not preserved, the \\nresults would no longer be the same\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   271\\n6/16/2017   2:05:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='272\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe procedure used to generate \\nH\\n(,\\n)\\nuv\\n is: (1) multiply \\nhx y\\np\\n(,)\\n by \\n()\\n−\\n+\\n1\\nxy\\n to center the frequency \\ndomain ﬁlter; (2) compute the forward DFT of the result in (1) to generate \\nH\\n(,\\n) ;\\nuv\\n (3) set the real \\npart of \\nH\\n(,\\n)\\nuv\\n to 0 to account for parasitic real parts (we know that \\nH\\n has to be purely imaginary \\nbecause \\nh\\np\\n is real and odd); and (4) multiply the result by \\n() .\\n−\\n+\\n1\\nu\\nv\\n This last step reverses the multiplica-\\ntion of \\nH\\n(,\\n)\\nuv\\n by \\n() ,\\n−\\n+\\n1\\nu\\nv\\n which is implicit when \\nhxy\\n(,\\n)\\n was manually placed in the center of \\nhx y\\np\\n(,) .\\n \\nF\\nigure 4.38(a) shows a perspective plot of \\nH\\n(,\\n) ,\\nuv\\n and Fig. 4.38(b) shows \\nH\\n(,\\n)\\nuv\\n as an image. Note \\nthe antisymmetry in this image about its center\\n, a result of \\nH\\n(,\\n)\\nuv\\n being odd. Function \\nH\\n(,\\n)\\nuv\\n is used \\nas any other frequenc\\ny domain ﬁlter transfer function. Figure 4.38(c) is the result of using the ﬁlter \\ntransfer function just obtained to ﬁlter the image in Fig. 4.37(a) in the frequency domain, using the step-\\nby-step ﬁltering procedure outlined earlier. As expected from a derivative ﬁlter, edges were enhanced \\nand all the constant intensity areas were reduced to zero (the grayish tone is due to scaling for display). \\nFigure 4.38(d) shows the result of ﬁltering the same image in the spatial domain with the Sobel kernel \\nhxy\\n(,\\n) ,\\n using the procedure discussed in Section 3.6. The results are identical.\\n4.8  IMAGE SMOOTHING USING LOWPASS FREQUENCY DOMAIN  \\nFILTERS  \\nThe remainder of this chapter deals with various filtering techniques in the frequency \\ndomain, beginning with lowpass filters. Edges and other sharp intensity transitions \\n(such as noise) in an image contribute significantly to the high frequency content \\nof its Fourier transform. Hence, smoothing (blurring) is achieved in the frequency \\ndomain by high-frequency attenuation; that is, by \\nlowpass\\n filtering. In this section, \\nwe consider three types of lowpass filters: \\nideal\\n, \\nButterworth\\n, and \\nGaussian\\n. These \\nthree categories cover the range from very sharp (ideal) to very smooth (Gaussian) \\nfiltering. The shape of a Butterworth filter is controlled by a parameter called the \\nfilter order\\n. For large values of this parameter, the Butterworth filter approaches \\nthe ideal filter. For lower values, the Butterworth filter is more like a Gaussian filter. \\nThus, the Butterworth filter provides a transition between two “extreme\\ns.” \\nAll filter-\\ning in this section follows the procedure outlined in the previous section, so all filter \\ntransfer functions, \\nH\\n(,\\n) ,\\nuv\\n are understood to be of size \\nPQ\\n×\\n;\\n that is, the discrete \\n4.8\\nb a\\nFIGURE 4.37\\n(a) Image of a \\nbuilding, and  \\n(b) its Fourier \\nspectrum.\\nDIP4E_GLOBAL_Print_Ready.indb   272\\n6/16/2017   2:05:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 273}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n273\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n0\\n0\\n0\\n1\\n2\\n1\\nb a\\nd c\\nFIGURE 4.38\\n(a) A spatial \\nkernel and per-\\nspective plot of \\nits corresponding \\nfrequency domain \\nﬁlter transfer \\nfunction.  \\n(b) Transfer  \\nfunction shown as \\nan image.  \\n(c) Result of  \\nﬁltering \\nFig. 4.37(a) in the \\nfrequency domain \\nwith the transfer \\nfunction in (b).  \\n(d) Result of \\nﬁltering the same \\nimage in the  \\nspatial domain \\nwith the kernel \\nin (a). The results \\nare identical. \\nfrequency variables are in the range \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nP\\n and \\nv\\n=−\\n012\\n1\\n,,, , ,\\n…\\nQ\\n \\nwhere \\nP\\n and \\nQ\\n are the padded sizes given by Eqs\\n. (4-100) and (4-101).\\nIDEAL LOWPASS FILTERS\\nA 2-D lowpass filter that passes without attenuation all frequencies within a circle of \\nradius from the origin, and “cuts off” all frequencies outside this, circle is called an \\nideal lowpass filter\\n (ILPF); it is specified by the transfer function\\n \\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n1\\n0\\n0\\n0\\nif \\nif \\n≤\\n>\\n \\n(4-111)\\nwhere \\nD\\n0\\n is a positive constant, and \\nD\\n(,\\n)\\nuv\\n is the distance between a point \\n(,)\\nuv\\n in \\nthe frequenc\\ny domain and the center of the \\nPQ\\n×\\n frequency rectangle; that is,\\n \\nDu P Q\\n(,)\\n/\\nuv\\nv\\n=−\\n()\\n+−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n12\\n \\n(4-112)\\nDIP4E_GLOBAL_Print_Ready.indb   273\\n6/16/2017   2:06:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 274}),\n",
       " Document(page_content='274\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere, as before, \\nP\\n and \\nQ\\n are the padded sizes from Eqs. (4-102) and (4-103). \\nFigure 4.39(a) shows a perspective plot of transfer function \\nH\\n(,\\n)\\nuv\\n and Fig. 4.39(b) \\nshows it displayed as an image\\n. As mentioned in Section 4.3, the name \\nideal\\n indicates \\nthat all frequencies on or inside a circle of radius \\nD\\n0\\n are passed without attenuation, \\nwhereas all frequencies outside the circle are completely attenuated (filtered out). \\nThe ideal lowpass filter transfer function is radially symmetric about the origin. This \\nmeans that it is defined completely by a radial cross section, as Fig. 4.39(c) shows. A \\n2-D representation of the filter is obtained by rotating the cross section 360°.\\nFor an ILPF cross section, the point of transition between the values \\nH\\n(,\\n)\\nuv\\n=\\n1 \\nand \\nH\\n(,\\n)\\nuv\\n=\\n0\\n is called the\\n cutoff frequency\\n.\\n In Fig. 4.39, the cutoff frequency is \\nD\\n0\\n. \\nT\\nhe sharp cutoff frequency of an ILPF cannot be realized with electronic compo-\\nnents, although they certainly can be simulated in a computer (subject to the con-\\nstrain that the fastest possible transition is limited by the distance between pixels). \\nThe lowpass ﬁlters in this chapter are compared by studying their behavior as a \\nfunction of the same cutoff frequencies. One way to establish standard cutoff fre-\\nquency loci using circles that enclose speciﬁed amounts of total \\nimage power\\n \\nP\\nT\\n, \\nwhich we obtain by summing the components of the power spectrum of the padded \\nimages at each point \\n(,) ,\\nuv\\n for \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nP\\n and \\nv\\n=−\\n012\\n1\\n,,, , ;\\n…\\nQ\\n that is,\\n \\nPP\\nT\\nQ\\nu\\nP\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n(,)\\nuv\\nv\\n0\\n1\\n0\\n1\\n \\n(4-113)\\nwhere \\nP\\n(,\\n)\\nuv\\n is given by Eq. (4-89). If the DFT has been centered, a circle of radius \\nD\\n0\\n with origin at the center of the frequency rectangle encloses \\na\\n percent of the \\npower\\n, where\\n \\na\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n∑ ∑\\n100\\nPP\\nT\\nu\\n(,)\\nuv\\nv\\n \\n(4-114)\\nand the summation is over values of \\n(,)\\nuv\\n that lie inside the circle or on its boundary.\\nF\\nigures 4.40(a) and (b) show a test pattern image and its spectrum. The cir-\\ncles superimposed on the spectrum have radii of 10, 30, 60, 160, and 460 pixels, \\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nD\\n0\\n1\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nb a\\nc\\nFIGURE 4.39\\n (a) Perspective plot of an ideal lowpass-ﬁlter transfer function. (b) Function displayed as an image. \\n \\n(c) Radial cross section. \\nDIP4E_GLOBAL_Print_Ready.indb   274\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 275}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n275\\nrespectively, and enclosed the percentages of total power listed in the ﬁgure caption. \\nThe spectrum falls off rapidly, with close to 87% of the total power being enclosed \\nby a relatively small circle of radius 10. The signiﬁcance of this will become evident \\nin the following example.\\nEXAMPLE 4.16 :  Image smoothing in the frequency domain using lowpass ﬁlters.\\nFigure 4.41 shows the results of applying ILPFs with cutoff frequencies at the radii shown in Fig. 4.40(b). \\nFigure 4.41(b) is useless for all practical purposes, unless the objective of blurring is to eliminate all \\ndetail in the image, except the “blobs” representing the largest objects. The severe blurring in this image \\nis a clear indication that most of the sharp detail information in the image is contained in the 13% power \\nremoved by the ﬁlter. As the ﬁlter radius increases, less and less power is removed, resulting in less blur-\\nring. Note that the images in Figs. 4.41(c) through (e) contain signiﬁcant “ringing,” which becomes ﬁner \\nin texture as the amount of high frequency content removed decreases. Ringing is visible even in the \\nimage in which only 2% of the total power was removed [Fig. 4.41(e)]. This ringing behavior is a char-\\nacteristic of ideal ﬁlters, as we have mentioned several times before. Finally, the result for \\na\\n=\\n99\\n4\\n.%\\n in \\nF\\nig. 4.41(f) shows very slight blurring and almost imperceptible ringing but, for the most part, this image \\nis close to the original. This indicates that little edge information is contained in the upper 0.6% of the \\nspectrum power removed by the ILPF.\\nIt is clear from this example that ideal lowpass ﬁltering is not practical. However, it is useful to study \\nthe behavior of ILPFs as part of our development of ﬁltering concepts. Also, as shown in the discussion \\nthat follows, some interesting insight is gained by attempting to explain the ringing property of ILPFs \\nin the spatial domain.\\nb a\\nFIGURE 4.40\\n (a) Test pattern of size \\n688 688\\n×\\n pixels, and (b) its spectrum. The spectrum is dou-\\nble the image size as a result of padding\\n, but is shown half size to ﬁt. The circles have radii of \\n10, 30, 60, 160, and 460 pixels with respect to the full-size spectrum. The radii enclose 86.9, 92.8, \\n95.1, 97.6, and 99.4% of the padded image power, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   275\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 276}),\n",
       " Document(page_content='276\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe blurring and ringing properties of ILPFs can be explained using the convolu-\\ntion theorem. Figure 4.42(a) shows an image of a frequency-domain ILPF transfer \\nfunction of radius 15 and size \\n1000 1000\\n×\\n pixels. Figure 4.42(b) is the spatial repre-\\nsentation,\\n \\nhxy\\n(,\\n) ,\\n of the ILPF, obtained by taking the IDFT of (a) (note the ringing). \\nF\\nigure 4.42(c) shows the intensity proﬁle of a line passing through the center of (b).\\nThis proﬁle resembles a sinc function.\\n†\\n Filtering in the spatial domain is done by \\nconvolving the function in Fig. 4.42(b) with an image. Imagine each pixel in an image \\nas being a discrete impulse whose strength is proportional to the intensity of the \\nimage at that location. Convolving this sinc-like function with an impulse copies (i.e., \\nshifts the origin of) the function to the location of the impulse. That is, convolution \\n†\\n Although this proﬁle resembles a sinc function, the transform of an ILPF is actually a Bessel function whose \\nderivation is beyond the scope of this discussion. The important point to keep in mind is that the inverse propor-\\ntionality between the “width” of the ﬁlter function in the frequency domain, and the “spread” of the width of the \\nlobes in the spatial function, still holds. \\nb a\\nc\\ne d\\nf\\nFIGURE 4.41\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using ILPFs with cutoff frequencies \\nset at radii values 10,\\n 30, 60, 160, and 460, as shown in Fig. 4.40(b). The power removed by these ﬁlters was 13.1, 7.2, \\n4.9, 2.4, and 0.6% of the total, respectively. We used mirror padding to avoid the black borders characteristic of zero \\npadding, as illustrated in Fig. 4.31(c).\\nDIP4E_GLOBAL_Print_Ready.indb   276\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 277}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n277\\nmakes a copy of the function in Fig. 4.42(b) centered on each pixel location in the \\nimage. The center lobe of this spatial function is the principal cause of blurring, while \\nthe outer, smaller lobes are mainly responsible for ringing. Because the “spread” of \\nthe spatial function is inversely proportional to the radius of \\nH\\n(,\\n) ,\\nuv\\n the larger \\nD\\n0\\n \\nbecomes (i,e, the more frequencies that are passed), the more the spatial function \\napproaches an impulse which, in the limit, causes no blurring at all when convolved \\nwith the image. The converse happens as \\nD\\n0\\n becomes smaller. This type of recipro-\\ncal behavior should be routine to you by now. In the next two sections, we show that \\nit is possible to achieve blurring with little or no ringing, an important objective in \\nlowpass ﬁltering. \\nGAUSSIAN LOWPASS FILTERS\\nGaussian lowpass filter (GLPF) transfer functions have the form\\n \\nHe\\nD\\n(,)\\n(,) /\\nuv\\nuv\\n=\\n−\\n22\\n2\\ns\\n \\n(4-115)\\nwhere, as in Eq. (4-112), \\nD\\n(,\\n)\\nuv\\n is the distance from the center of the \\nPQ\\n×\\n fre-\\nquenc\\ny rectangle to any point, \\n(,)\\nuv\\n, contained by the rectangle. Unlike our earlier \\nexpressions for Gaussian functions\\n, we do not use a multiplying constant here in \\norder to be consistent with the filters discussed in this and later sections, whose \\nhighest value is 1. As before, \\ns\\n is a measure of spread about the center. By letting \\ns\\n=\\nD\\n0\\n,\\n we can express the Gaussian transfer function in the same notation as other \\nfunctions in this section:\\n \\nHe\\nDD\\n(,)\\n(,) /\\nuv\\nuv\\n=\\n−\\n2\\n0\\n2\\n2\\n \\n(4-116)\\nwhere \\nD\\n0\\n is the cutoff frequency. When \\nDD\\n(,\\n) ,\\nuv\\n=\\n0\\n the GLPF transfer function is \\ndown to 0.607 of its maximum value of 1.0.\\nFrom Table 4.4, we know that the inverse Fourier transform of a frequency-\\ndomain Gaussian function is Gaussian also. This means that a spatial Gaussian ﬁlter \\nkernel, obtained by computing the IDFT of Eq. (4-115) or (4-116), will have no \\nringing. As property 13 of Table 4.4 shows, the same inverse relationship explained \\nearlier for ILPFs is true also of GLPFs. Narrow Gaussian transfer functions in the \\nfrequency domain imply broader kernel functions in the spatial domain, and vice \\nb a\\nc\\nFIGURE 4.42\\n  \\n(a) Frequency  \\ndomain ILPF \\ntransfer function. \\n(b) Corresponding \\nspatial domain  \\nkernel function.  \\n(c) Intensity proﬁle \\nof a horizontal line \\nthrough the center \\nof (b).\\nDIP4E_GLOBAL_Print_Ready.indb   277\\n6/16/2017   2:06:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 278}),\n",
       " Document(page_content='278\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nversa. Figure 4.43 shows a perspective plot, image display, and radial cross sections \\nof a GLPF transfer function.\\nEXAMPLE 4.17 :  Image smoothing in the frequency domain using Gaussian lowpass ﬁlters.\\nFigure 4.44 shows the results of applying the GLPF of Eq. (4-116) to Fig. 4.44(a), with \\nD\\n0\\n equal to the ﬁve \\nradii in Fig. 4.40(b). Compared to the results obtained with an ILPF (Fig. 4.41), we note a smooth transi-\\ntion in blurring as a function of increasing cutoff frequency. The GLPF achieved slightly less smoothing \\nthan the ILPF. The key difference is that we are assured of no ringing when using a GLPF. This is an \\nimportant consideration in practice, especially in situations in which any type of artifact is unacceptable, \\nas in medical imaging. In cases where more control of the transition between low and high frequencies \\nabout the cutoff frequency are needed, the Butterworth lowpass ﬁlter discussed next presents a more \\nsuitable choice. The price of this additional control over the ﬁlter proﬁle is the possibility of ringing, as \\nyou will see shortly.\\nBUTTERWORTH LOWPASS FILTERS\\nThe transfer function of a Butterworth lowpass filter (BLPF) of order \\nn\\n, with cutoff \\nfrequency at a distance \\nD\\n0\\n from the center of the frequency rectangle, is defined as\\n \\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\n \\n(4-117)\\nwhere \\nD\\n(,\\n)\\nuv\\n is given by Eq. (4-112). Figure 4.45 shows a perspective plot, image \\ndisplay\\n, and radial cross sections of the BLPF function. Comparing the cross section \\nplots in Figs. 4.39, 4.43, and 4.45, we see that the BLPF function can be controlled to \\napproach the characteristics of the ILPF using higher values of \\nn\\n, and the GLPF for \\nlower values of \\nn\\n, while providing a smooth transition in from low to high frequen-\\ncies. Thus, we can use a BLPF to approach the sharpness of an ILPF function with \\nconsiderably less ringing. \\nu\\nv\\n1.0\\n0.607\\nD\\n0\\n \\n/H11005 \\n10\\nD\\n0\\n \\n/H11005 \\n20\\nD\\n0\\n \\n/H11005 \\n40\\nD\\n0\\n \\n/H11005 \\n60\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\n0\\nb a\\nc\\nFIGURE 4.43\\n (a) Perspective plot of a GLPF transfer function. (b) Function displayed as an image. (c) Radial cross \\nsections for various values of \\nD\\n0\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   278\\n6/16/2017   2:06:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 279}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n279\\nb a\\nc\\ne d\\nf\\nFIGURE 4.44\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using GLPFs with cutoff frequen-\\ncies at the radii shown in F\\nig. 4.40. Compare with Fig. 4.41. We used mirror padding to avoid the black borders \\ncharacteristic of zero padding.\\n0.5\\nD\\n0\\nn \\n/H11005 \\n1\\nn \\n/H11005 \\n2\\nn \\n/H11005 \\n3\\nn \\n/H11005 \\n4\\n1.0\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nb a\\nc\\nFIGURE 4.45\\n (a) Perspective plot of a Butterworth lowpass-ﬁlter transfer function. (b) Function displayed as an image. \\n(c) Radial cross sections of BLPFs of orders 1 through 4. \\nDIP4E_GLOBAL_Print_Ready.indb   279\\n6/16/2017   2:06:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 280}),\n",
       " Document(page_content='280\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nc\\ne d\\nf\\nFIGURE 4.46\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using BLPFs with cutoff frequen-\\ncies at the radii shown in F\\nig. 4.40 and \\nn\\n=\\n22\\n5\\n..\\n Compare with Figs. 4.41 and 4.44. We used mirror padding to avoid \\nthe black borders characteristic of zero padding\\n.\\nEXAMPLE 4.18 :  Image smoothing using a Butterworth lowpass ﬁlter.\\nFigures 4.46(b)-(f) show the results of applying the BLPF of Eq. (4-117) to Fig. 4.46(a), with cutoff \\nfrequencies equal to the ﬁve radii in Fig. 4.40(b), and with \\nn\\n=\\n22\\n5\\n..\\n The results in terms of blurring are \\nbetween the results obtained with using ILPFs and GLPFs\\n. For example, compare Fig. 4.46(b), with \\nFigs. 4.41(b) and 4.44(b). The degree of blurring with the BLPF was less than with the ILPF, but more \\nthan with the GLPF. \\nThe spatial domain kernel obtainable from a BLPF of order 1 has no ringing. \\nGenerally, ringing is imperceptible in ﬁlters of order 2 or 3, but can become sig-\\nniﬁcant in ﬁlters of higher orders. Figure 4.47 shows a comparison between the spa-\\ntial representation (i.e., spatial kernels) corresponding to BLPFs of various orders \\n(using a cutoff frequency of 5 in all cases). Shown also is the intensity proﬁle along \\nThe kernels in Figs. 4.47(a)  \\nthrough (d) were obtained \\nusing the procedure out-\\nlined in the explanation of \\nFig. 4.42.\\nDIP4E_GLOBAL_Print_Ready.indb   280\\n6/16/2017   2:06:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 281}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n281\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 4.47\\n (a)–(d) Spatial representations (i.e., spatial kernels) corresponding to BLPF transfer functions of size \\n1000 1000\\n×\\n pixels, cut-off frequency of 5, and order 1, 2, 5, and 20, respectively. (e)–(h) Corresponding intensity \\nproﬁles through the center of the ﬁlter functions\\n. \\na horizontal scan line through the center of each spatial kernel. The kernel corre-\\nsponding to the BLPF of order 1 [see Fig. 4.47(a)] has neither ringing nor negative \\nvalues. The kernel corresponding to a BLPF of order 2 does show mild ringing and \\nsmall negative values, but they certainly are less pronounced than would be the case \\nfor an ILPF. As the remaining images show, ringing becomes signiﬁcant for higher-\\norder ﬁlters. A BLPF of order 20 has a spatial kernel that exhibits ringing charac-\\nteristics similar to those of the ILPF (in the limit, both ﬁlters are identical). BLPFs \\nof orders 2 to 3 are a good compromise between effective lowpass ﬁltering and \\nacceptable spatial-domain ringing. Table 4.5 summarizes the lowpass ﬁlter transfer \\nfunctions discussed in this section.\\nADDITIONAL EXAMPLES OF LOWPASS FILTERING\\nIn the following discussion, we show several practical applications of lowpass filter-\\ning in the frequency domain. The first example is from the field of machine per-\\nception with application to character recognition; the second is from the printing \\nand publishing industry; and the third is related to processing satellite and aerial \\nimages. Similar results can be obtained using the lowpass spatial filtering techniques \\ndiscussed in Section 3.5. We use GLPFs in all examples for consistency, but simi-\\nlar results can be obtained using BLPFs. Keep in mind that images are padded to \\ndouble size for filtering, as indicated by Eqs. (4-102) and (4-103), and filter transfer \\nfunctions have to match padded-image size. The values of \\nD\\n0\\n used in the following \\nexamples reflect this doubled filter size.\\nDIP4E_GLOBAL_Print_Ready.indb   281\\n6/16/2017   2:06:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 282}),\n",
       " Document(page_content='282\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nIdeal\\nGaussian\\nButterworth\\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n≤\\n>\\n⎧\\n⎨\\n⎩\\n1\\n0\\n0\\n0\\nif \\nif \\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=\\n−\\n2\\n0\\n2\\n2\\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\nTABLE \\n4.5\\nLowpass ﬁlter transfer functions. \\nD\\n0\\n is the cutoff frequency, and \\nn\\n is the order of the Butterworth ﬁlter.\\nFigure 4.48 shows a sample of text of low resolution. One encounters text like \\nthis, for example, in fax transmissions, duplicated material, and historical records. \\nThis particular sample is free of additional difﬁculties like smudges, creases, and \\ntorn sections. The magniﬁed section in Fig. 4.48(a) shows that the characters in this \\ndocument have distorted shapes due to lack of resolution, and many of the charac-\\nters are broken. Although humans ﬁll these gaps visually without difﬁculty, machine \\nrecognition systems have real difﬁculties reading broken characters. One approach \\nfor handling this problem is to bridge small gaps in the input image by blurring \\nit. Figure 4.48(b) shows how well characters can be “repaired” by this simple pro-\\ncess using a Gaussian lowpass ﬁlter with \\nD\\n0\\n120\\n=\\n.\\n It is typical to follow the type of \\n“repair”\\n just described with additional processing, such as thresholding and thinning, \\nto yield cleaner characters. We will discuss thinning in Chapter 9 and thresholding \\nin Chapter 10.\\nLowpass ﬁltering is a staple in the printing and publishing industry, where it is \\nused for numerous preprocessing functions, including unsharp masking, as discussed \\nin Section 3.6. “Cosmetic” processing is another use of lowpass ﬁltering prior to print-\\ning. Figure 4.49 shows an application of lowpass ﬁltering for producing a smoother, \\nsofter-looking result from a sharp original. For human faces, the typical objective is \\nto reduce the sharpness of ﬁne skin lines and small blemishes. The magniﬁed sec-\\ntions in Figs. 4.49(b) and (c) clearly show a signiﬁcant reduction in ﬁne skin lines \\naround the subject’s eyes. In fact, the smoothed images look quite soft and pleasing.\\nFigure 4.50 shows two applications of lowpass ﬁltering on the same image, but \\nwith totally different objectives. Figure 4.50(a) is an \\n808 754\\n×\\n segment of a very high \\nWe will cover unsharp \\nmasking in the frequency \\ndomain in Section 4.9.\\nb a\\nFIGURE 4.48\\n(a) Sample text \\nof low resolution \\n(note the broken \\ncharacters in the  \\nmagniﬁed view). \\n(b) Result of \\nﬁltering with a \\nGLPF,  \\nshowing that gaps \\nin the broken \\ncharacters were \\njoined. \\nDIP4E_GLOBAL_Print_Ready.indb   282\\n6/16/2017   2:06:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 283}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n283\\nb a\\nc\\nFIGURE 4.49\\n (a) Original \\n785 732\\n×\\n image. (b) Result of ﬁltering using a GLPF with \\nD\\n0\\n150\\n=\\n.\\n (c) Result of ﬁltering \\nusing a GLPF with \\nD\\n0\\n130\\n=\\n.\\n Note the reduction in ﬁne skin lines in the magniﬁed sections in (b) and (c). \\nresolution radiometer (VHRR) image showing part of the Gulf of Mexico (dark) \\nand Florida (light) (note the horizontal sensor scan lines). The boundaries between \\nbodies of water were caused by loop currents. This image is illustrative of remotely \\nsensed images in which sensors have the tendency to produce pronounced scan lines \\nalong the direction in which the scene is being scanned. (See Example 4.24 for an \\nb a\\nc\\nFIGURE 4.50\\n (a) \\n808 754\\n×\\n satellite image showing prominent horizontal scan lines. (b) Result of ﬁltering using a \\nGLPF with \\nD\\n0\\n50\\n=\\n. (c) Result of using a GLPF with \\nD\\n0\\n20\\n=\\n. (Original image courtesy of NOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   283\\n6/16/2017   2:06:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 284}),\n",
       " Document(page_content='284\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nillustration of imaging conditions that can lead for such degradations.) Lowpass ﬁl-\\ntering is a crude (but simple) way to reduce the effect of these lines, as Fig. 4.50(b) \\nshows (we consider more effective approaches in Sections 4.10 and 5.4). This image \\nwas obtained using a GLFP with \\nD\\n0\\n50\\n=\\n.\\n The reduction in the effect of the scan \\nlines in the smoothed image can simplify the detection of macro features\\n, such as the \\ninterface boundaries between ocean currents.\\nFigure 4.50(c) shows the result of signiﬁcantly more aggressive Gaussian lowpass \\nﬁltering with \\nD\\n0\\n20\\n=\\n.\\n Here, the objective is to blur out as much detail as possible \\nwhile leaving large features recognizable\\n. For instance, this type of ﬁltering could be \\npart of a preprocessing stage for an image analysis system that searches for features \\nin an image bank. An example of such features could be lakes of a given size, such \\nas Lake Okeechobee in the lower eastern region of Florida, shown in Fig. 4.50(c) as \\na nearly round dark region surrounded by a lighter region. Lowpass ﬁltering helps \\nto simplify the analysis by averaging out features smaller than the ones of interest.\\n4.9 IMAGE SHARPENING USING HIGHPASS FILTERS  \\nWe showed in the previous section that an image can be smoothed by attenuating \\nthe high-frequency components of its Fourier transform. Because edges and other \\nabrupt changes in intensities are associated with high-frequency components, image \\nsharpening can be achieved in the frequency domain by highpass filtering, which \\nattenuates low-frequencies components without disturbing high-frequencies in the \\nFourier transform. As in Section 4.8, we consider only zero-phase-shift filters that \\nare radially symmetric. All filtering in this section is based on the procedure outlined \\nin Section 4.7, so all images are assumed be padded to size \\nPQ\\n×\\n [see Eqs. (4-102) \\nand (4-103)],\\n and filter transfer functions, \\nH\\n(,\\n) ,\\nuv\\n are understood to be centered, \\ndiscrete functions of size \\nPQ\\n×\\n. \\nIDEAL, GAUSSIAN, AND BUTTERWORTH HIGHPASS FILTERS FROM \\nLOWPASS FILTERS\\nAs was the case with kernels in the spatial domain (see Section 3.7), subtracting a \\nlowpass filter transfer function from 1 yields the corresponding highpass filter trans-\\nfer function in the frequency domain:\\n \\nHH\\nHP\\nLP\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-118)\\nwhere \\nH\\nLP\\n(,)\\nuv\\n is the transfer function of a lowpass filter. Thus, it follows from \\nEq.\\n (4-111) that an ideal highpass filter (IHPF) transfer function is given by\\n \\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n0\\n1\\n0\\n0\\nif \\nif \\n≤\\n>\\n \\n(4-119)\\nwhere, as before, \\nD\\n(,\\n)\\nuv\\n is the distance from the center of the \\nPQ\\n×\\n frequency rect-\\nangle\\n, as given in Eq. (4-112). Similarly, it follows from Eq. (4-116) that the transfer \\nfunction of a Gaussian highpass filter (GHPF) transfer function is given by\\n4.9\\nIn some applications of \\nhighpass ﬁltering, it is \\nadvantageous to enhance \\nthe high-frequencies of \\nthe Fourier transform.\\nDIP4E_GLOBAL_Print_Ready.indb   284\\n6/16/2017   2:06:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 285}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n285\\n  \\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n1\\n2\\n0\\n2\\n2\\n \\n(4-120)\\nand, from Eq. (4-117), that the transfer function of a Butterworth highpass filter \\n(BHPF) is\\n \\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\n \\n(4-121)\\nFigure 4.51 shows 3-D plots, image representations, and radial cross sections for \\nthe preceding transfer functions\\n. As before, we see that the BHPF transfer function \\nin the third row of the ﬁgure represents a transition between the sharpness of the \\nIHPF and the broad smoothness of the GHPF transfer function.\\nIt follows from Eq. (4-118) that the spatial kernel corresponding to a highpass \\nﬁlter transfer function in the frequency domain is given by\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nu\\nv\\nu\\nv\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 4.51\\nTop row:  \\nPerspective plot, \\nimage, and, radial \\ncross section of \\nan IHPF transfer \\nfunction. Middle \\nand bottom \\nrows: The same \\nsequence for \\nGHPF and BHPF \\ntransfer functions. \\n(The thin image \\nborders were \\nadded for clarity. \\nThey are not part \\nof the data.)\\nDIP4E_GLOBAL_Print_Ready.indb   285\\n6/16/2017   2:06:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 286}),\n",
       " Document(page_content='286\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nhx y H\\nH\\nxy\\nh xy\\nHP\\nHP\\nLP\\nLP\\n(,) (,)\\n(,)\\n(,) (,)\\n=\\n[]\\n=−\\n[]\\n=−\\n−\\n−\\n/H5219\\n/H5219\\n1\\n1\\n1\\nuv\\nuv\\nd\\n \\n(4-122)\\nwhere we used the fact that the IDFT of 1 in the frequency domain is a unit impulse \\nin the spatial domain (see \\nTable 4.4). This equation is precisely the foundation for \\nthe discussion in\\n \\nSection 3.7, in which we showed how to construct a highpass kernel \\nby subtracting a lowpass kernel from a unit impulse\\n.\\nFigure 4.52 shows highpass spatial kernels constructed in just this manner, using \\nEq. (4-122) with ILPF, GLPF, and BLPF transfer functions (the values of \\nM\\n, \\nN\\n, and \\nD\\n0\\n used in this ﬁgure are the same as those we used for Fig. 4.42, and the BLPF is of \\norder 2). Figure 4.52(a) shows the resulting ideal highpass kernel obtained using Eq. \\n(4-122), and Fig. 4.52(b) is a horizontal intensity proﬁle through the center of the ker-\\nnel. The center element of the proﬁle is a unit impulse, visible as a bright dot in the \\ncenter of Fig. 4.52(a). Note that this highpass kernel has the same ringing properties \\nillustrated in Fig. 4.42(b) for its corresponding lowpass counterpart. As you will see \\nshortly, ringing is just as objectionable as before, but this time in images sharpened \\nwith ideal highpass ﬁlters. The other images and proﬁles in Fig. 4.52 are for Gaussian \\nand Butterworth kernels. We know from Fig. 4.51 that GHPF transfer functions in \\nthe frequency domain tend to have a broader “skirt” than Butterworth functions of \\ncomparable size and cutoff frequency. Thus, we would expect Butterworth spatial \\nRecall that a unit impulse \\nin the spatial domain is \\nan array of 0’s with a 1 in \\nthe center.\\nb a\\nc\\ne d\\nf\\nFIGURE 4.52\\n (a)–(c): Ideal, Gaussian, and Butterworth highpass spatial kernels obtained from \\nIHPF, GHPF, and BHPF frequency-domain transfer functions. (The thin image borders are \\nnot part of the data.) (d)–(f): Horizontal intensity proﬁles through the centers of the kernels.\\nDIP4E_GLOBAL_Print_Ready.indb   286\\n6/16/2017   2:06:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 287}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n287\\nkernels to be “broader” than comparable Gaussian kernels, a fact that is conﬁrmed \\nby the images and their proﬁles in Figs. 4.52. Table 4.6 summarizes the three highpass \\nﬁlter transfer functions discussed in the preceding paragraphs.\\nEXAMPLE 4.19 :  Highpass ﬁltering of the character test pattern.\\nThe ﬁrst row of Fig. 4.53 shows the result of ﬁltering the test pattern in Fig. 4.37(a) using IHPF, GHPF, and \\nBHPF transfer functions with \\nD\\n0\\n60\\n=\\n [see Fig. 4.37(b)] and \\nn\\n=\\n2\\n for the Butterworth ﬁlter. We know \\nfrom Chapter 3 that highpass ﬁltering produces images with negative values\\n. The images in Fig.  4.53 are \\nnot scaled, so the negative values are clipped by the display at 0 (black). The key objective of highpass \\nﬁltering is to sharpen. Also, because the highpass ﬁlters used here set the DC term to zero, the images \\nhave essentially no tonality, as explained earlier in connection with Fig. 4.30.\\nOur main objective in this example is to compare the behavior of the three highpass ﬁlters. As \\nFig. 4.53(a) shows, the ideal highpass ﬁlter produced results with severe distortions caused by ringing. \\nFor example, the blotches inside the strokes of the large letter “a” are ringing artifacts. By comparison, \\nneither Figs. 4.53(b) or (c) have such distortions. With reference to Fig. 4.37(b), the ﬁlters removed or \\nattenuated approximately 95% of the image energy. As you know, removing the lower frequencies of an \\nimage reduces its gray-level content signiﬁcantly, leaving mostly edges and other sharp transitions, as is \\nevident in Fig. 4.53. The details you see in the ﬁrst row of the ﬁgure are contained in only the upper 5% \\nof the image energy.\\nThe second row, obtained with \\nD\\n0\\n160\\n=\\n,\\n is more interesting. The remaining energy of those images \\nis about 2.5%,\\n or half, the energy of the images in the ﬁrst row. However, the difference in ﬁne detail \\nis striking. See, for example, how much cleaner the boundary of the large “a” is now, especially in the \\nGaussian and Butterworth results. The same is true for all other details, down to the smallest objects. \\nThis is the type of result that is considered acceptable when detection of edges and boundaries is impor-\\ntant.\\nFigure 4.54 shows the images in the second row of Fig. 4.53, scaled using Eqs. (2-31) and (2-32) to \\ndisplay the full intensity range of both positive and negative intensities. The ringing in Fig. 4.54(a) shows \\nthe inadequacy of ideal highpass ﬁlters. In contrast, notice the smoothness of the background on the \\nother two images, and the crispness of their edges.\\nEXAMPLE 4.20 :  Using highpass ﬁltering and thresholding for image enhancement.\\nFigure 4.55(a) is a \\n962 1026\\n×\\n image of a thumbprint in which smudges (a typical problem) are evident. \\nA key step in automated ﬁngerprint recognition is enhancement of print ridges and the reduction of \\nsmudges\\n. In this example, we use highpass ﬁltering to enhance the ridges and reduce the effects of \\nIdeal\\nGaussian\\nButterworth\\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n0\\n1\\n0\\n0\\nif \\nif \\n≤\\n>\\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n1\\n2\\n0\\n2\\n2\\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\nTABLE \\n4.6\\nHighpass ﬁlter transfer functions. \\nD\\n0\\n is the cutoff frequency and \\nn\\n is the order of the Butterworth transfer function.\\nDIP4E_GLOBAL_Print_Ready.indb   287\\n6/16/2017   2:06:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 288}),\n",
       " Document(page_content='288\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nc\\ne d\\nf\\nFIGURE 4.53\\n Top row: The image from Fig. 4.40(a) ﬁltered with IHPF, GHPF, and BHPF transfer functions using \\nD\\n0\\n60\\n=\\n in all cases (\\nn\\n=\\n2 for the BHPF). Second row: Same sequence, but using \\nD\\n0\\n160\\n=\\n.\\nb a\\nc\\nFIGURE 4.54\\n The images from the second row of Fig. 4.53 scaled using Eqs. (2-31) and (2-32) to show both positive \\nand negative values.\\nDIP4E_GLOBAL_Print_Ready.indb   288\\n6/16/2017   2:06:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 289}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n289\\nsmudging. Enhancement of the ridges is accomplished by the fact that their boundaries are character-\\nized by high frequencies, which are unchanged by a highpass ﬁlter. On the other hand, the ﬁlter reduces \\nlow frequency components, which correspond to slowly varying intensities in the image, such as the \\nbackground and smudges. Thus, enhancement is achieved by reducing the effect of all features except \\nthose with high frequencies, which are the features of interest in this case.\\nFigure 4.55(b) is the result of using a Butterworth highpass ﬁlter of order 4 with a cutoff frequency \\nof 50. A fourth-order ﬁlter provides a sharp (but smooth) transition from low to high frequencies, with \\nﬁltering characteristics between an ideal and a Gaussian ﬁlter. The cutoff frequency chosen is about 5% \\nof the long dimension of the image. The idea is for \\nD\\n0\\n to be close to the origin so that low frequencies are \\nattenuated but not completely eliminated, except for the DC term which is set to 0, so that tonality dif-\\nferences between the ridges and background are not lost completely. Choosing a value for \\nD\\n0\\n between \\n5% and 10% of the long dimension of the image is a good starting point. Choosing a large value of \\nD\\n0\\n would highlight ﬁne detail to such an extent that the deﬁnition of the ridges would be affected. As \\nexpected, the highpass ﬁltered image has negative values, which are shown as black by the display.\\nA simple approach for highlighting sharp features in a highpass-ﬁltered image is to threshold it by set-\\nting to black (0) all negative values and to white (1) the remaining values. Figure 4.55(c) shows the result \\nof this operation. Note how the ridges are clear, and how the effect of the smudges has been reduced \\nconsiderably. In fact, ridges that are barely visible in the top, right section of the image in Fig. 4.55(a) are \\nnicely enhanced in Fig. 4.55(c). An automated algorithm would ﬁnd it much easier to follow the ridges \\non this image than it would on the original. \\nTHE LAPLACIAN IN THE FREQUENCY DOMAIN\\nIn Section 3.6, we used the Laplacian for image sharpening in the spatial domain. In \\nthis section, we revisit the Laplacian and show that it yields equivalent results using \\nfrequency domain techniques. It can be shown (see Problem 4.52) that the Laplacian \\ncan be implemented in the frequency domain using the filter transfer function\\n \\nHu\\n(,\\n) ( )\\nuv v\\n=− +\\n4\\n22 2\\np\\n \\n(4-123)\\nb a\\nc\\nFIGURE 4.55\\n (a) Smudged thumbprint. (b) Result of highpass ﬁltering (a). (c) Result of thresholding (b). (Original \\nimage courtesy of the U.S. National Institute of Standards and Technology.) \\nDIP4E_GLOBAL_Print_Ready.indb   289\\n6/16/2017   2:06:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 290}),\n",
       " Document(page_content='290\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor, with respect to the center of the frequency rectangle, using the transfer function\\n \\nHu\\nP Q\\nD\\n(,)\\n(,)\\nuv\\nv\\nuv\\n=− −\\n()\\n+−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n42 2\\n4\\n2\\n22\\n22\\np\\np\\n \\n(4-124)\\nwhere \\nD\\n(,\\n)\\nuv\\n is the distance function defined in Eq. (4-112). Using this transfer \\nfunction,\\n the Laplacian of an image, \\nfx y\\n(,\\n) ,\\n is obtained in the familiar manner:\\n \\n/H11612\\n21\\nfx y H F\\n(,) (,)(,)\\n=\\n[]\\n−\\n/H5219\\nuv uv\\n \\n(4-125)\\nwhere \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) .\\n As in Eq. (3-54), enhancement is implemented \\nusing the equation \\n \\ngxy f xy c f xy\\n(,\\n) (,) (,)\\n=+ ∇\\n2\\n \\n(4-126)\\nHere, \\nc\\n=−\\n1\\n because \\nH\\n(,\\n)\\nuv\\n is negative. In Chapter 3, \\nfx y\\n(,\\n)\\n and \\n/H11612\\n2\\nfx y\\n(,)\\n had \\ncomparable values\\n. However, computing \\n/H11612\\n2\\nfx y\\n(,)\\n with Eq. (4-125) introduces DFT \\nscaling factors that can be several orders of magnitude larger than the maximum \\nvalue of \\nf\\n.\\n Thus, the differences between \\nf\\n and its Laplacian must be brought into \\ncomparable ranges. The easiest way to handle this problem is to normalize the val-\\nues of \\nfx y\\n(,\\n)\\n to the range \\n[, ]\\n01\\n (before computing its DFT) and divide \\n/H11612\\n2\\nfx y\\n(,)\\n by \\nits maximum value\\n, which will bring it to the approximate range \\n[, ] .\\n−\\n11\\n (Remember, \\nthe Laplacian has negative values\\n.) Equation (4-126) can then be used.\\nWe can write Eq. (4-126) directly in the frequency domain as\\n  \\ngxy F H F\\nHF\\n(\\n,) (,) (,)(,)\\n(,) (,)\\n=−\\n{}\\n=−\\n[]\\n{}\\n=\\n−\\n−\\n−\\n/H5219\\n/H5219\\n/H5219\\n1\\n1\\n1\\n1\\n1\\nuv uv uv\\nuv uv\\n+ +\\n⎡\\n⎣\\n⎤\\n⎦\\n{}\\n4\\n22\\np\\nDF\\n(,) (,)\\nuv uv\\n \\n(4-127)\\nAlthough this result is elegant, it has the same scaling issues just mentioned, com-\\npounded by the fact that the normalizing factor is not as easily computed.\\n For this \\nreason, Eq. (4-126) is the preferred implementation in the frequency domain, with \\n/H11612\\n2\\nfx y\\n(,)\\n computed using Eq. (4-125) and scaled using the approach mentioned in \\nthe previous paragraph.\\nEXAMPLE 4.21 :  Image sharpening in the frequency domain using the Laplacian.\\nFigure 4.56(a) is the same as Fig. 3.46(a), and Fig. 4.56(b) shows the result of using Eq. (4-126), in which \\nthe Laplacian was computed in the frequency domain using Eq. (4-125). Scaling was done as described \\nin connection with Eq. (4-126). We see by comparing Figs. 4.56(b) and 3.46(d) that the frequency-domain \\nresult is superior. The image in Fig. 4.56(b) is much sharper, and shows details that are barely visible in \\n3.46(d), which was obtained using the Laplacian kernel in Fig. 3.45(b), with a \\n−\\n8\\n in the center. The sig-\\nniﬁcant improvement achieved in the frequenc\\ny domain is not unexpected. The spatial Laplacian kernel \\nDIP4E_GLOBAL_Print_Ready.indb   290\\n6/16/2017   2:06:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 291}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n291\\nencompasses a very small neighborhood, while the formulation in Eqs. (4-125) and (4-126) encompasses \\nthe entire image.\\nUNSHARP MASKING, HIGH-BOOST FILTERING, AND HIGH- \\nFREQUENCY-EMPHASIS FILTERING\\nIn this section, we discuss frequency domain formulations of the unsharp mask-\\ning and high-boost filtering image sharpening techniques introduced in Section 3.6. \\nUsing frequency domain methods, the mask defined in Eq. (3-55) is given by \\n \\ngx y f x y f x y\\nmask\\nLP\\n(,) (,) (,)\\n=−\\n \\n(4-128)\\nwith\\n \\nfx y H F\\nLP\\nLP\\n(,) (,)(,)\\n=\\n[]\\n−\\n/H5219\\n1\\nuv uv\\n \\n(4-129)\\nwhere \\nH\\nLP\\n(,)\\nuv\\n is a lowpass filter transfer function, and \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) .\\n \\nHere\\n, \\nfx y\\nLP\\n(,)\\n is a smoothed image analogous to \\nfx y\\n(,)\\n in Eq. (3-55). Then, as in \\nEq.\\n (3-56),\\n \\ngxy f xy k g xy\\n(,\\n) (,) (,)\\n=+\\nmask\\n \\n(4-130)\\nThis expression defines \\nunsharp masking\\n when \\nk\\n=\\n1\\n and high-boost filtering when \\nk\\n>\\n1.\\n Using the preceding results, we can express Eq. (4-130) entirely in terms of \\nfrequenc\\ny domain computations involving a lowpass filter:\\n \\ngxy\\nk H F\\n(,\\n)\\n(,) (,)\\n=+ −\\n[]\\n{}\\n−\\n/H5219\\n1\\n11\\nQR\\nLP\\nuv uv\\n \\n(4-131)\\nb a\\nFIGURE 4.56\\n(a) Original, \\nblurry image.  \\n(b) Image \\nenhanced using \\nthe Laplacian in \\nthe frequency  \\ndomain.  \\nCompare with \\nFig. 3.46(d). \\n(Original image \\ncourtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   291\\n6/16/2017   2:06:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 292}),\n",
       " Document(page_content='292\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nWe can express this result in terms of a highpass filter using Eq. (4-118):\\n \\ngxy\\nk H F\\nP\\n(,)\\n(,) (,)\\n=+\\n[]\\n{}\\n−\\n/H5219\\n1\\n1\\nH\\nuv uv\\n \\n(4-132)\\nThe expression contained within the square brackets is called a \\nhigh-frequency-\\nemphasis filter transfer function\\n.\\n As noted earlier, highpass filters set the dc term \\nto zero, thus reducing the average intensity in the filtered image to 0. The high-fre-\\nquency-emphasis filter does not have this problem because of the 1 that is added to \\nthe highpass filter transfer function. Constant \\nk\\n gives control over the proportion of \\nhigh frequencies that influences the final result. A slightly more general formulation \\nof high-frequency-emphasis filtering is the expression\\n \\ngxy k kH F\\n(,\\n)\\n(,) (,)\\n=+\\n[]\\n{}\\n−\\n/H5219\\n1\\n12 H P\\nuv uv\\n \\n(4-133)\\nwhere \\nk\\n1\\n0\\n≥\\n offsets the value the transfer function so as not to zero-out the dc term \\n[see F\\nig. 4.30(c)], and \\nk\\n2\\n0\\n>\\n controls the contribution of high frequencies.\\nEXAMPLE 4.22 :  Image enhancement using high-frequency-emphasis ﬁltering.\\nFigure 4.57(a) shows a \\n503 720\\n×\\n-p\\nixel\\n chest X-ray image with a narrow range of intensity levels. The \\nobjective of this example is to enhance the image using high-frequenc\\ny-emphasis ﬁltering. X-rays can-\\nnot be focused in the same manner that optical lenses can, and the resulting images generally tend to be \\nslightly blurred. Because the intensities in this particular image are biased toward the dark end of the \\nb a\\nd c\\nFIGURE 4.57\\n(a) A chest X-ray.\\n(b) Result of  \\nﬁltering with a \\nGHPF function.  \\n(c) Result of \\nhigh-frequency-\\nemphasis ﬁltering \\nusing the same \\nGHPF. (d) Result \\nof performing  \\nhistogram  \\nequalization on (c). \\n(Original image \\ncourtesy of Dr. \\nThomas R. Gest, \\nDivision of  \\nAnatomical  \\nSciences,  \\nUniversity of \\nMichigan Medical \\nSchool.)\\nDIP4E_GLOBAL_Print_Ready.indb   292\\n6/16/2017   2:06:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 293}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n293\\ngray scale, we also take this opportunity to give an example of how spatial domain processing can be \\nused to complement frequency-domain ﬁltering.\\nImage artifacts, such as ringing, are unacceptable in medical image processing, so we use a Gaussian \\nhighpass ﬁlter transfer function. Because the spatial representation of a GHPF function is Gaussian also, \\nwe know that ringing will not be an issue. The value chosen for \\nD\\n0\\n should provide enough ﬁltering to \\nsharpen boundaries while at the same time not over-sharpening minute details (such as noise). We used \\nD\\n0\\n70\\n=\\n,\\n approximately 10% of the long image dimension, but other similar values would work also. \\nF\\nigure 4.57(b) is the result of highpass ﬁltering the original image (scaled as the images in Fig. 4.54). As \\nexpected, the image is rather featureless, but the important boundaries (e.g., the edges of the ribs) are \\nclearly delineated. Figure 4.57(c) shows the advantage of high-frequency-emphasis ﬁltering, where we \\nused Eq. (4-133) with \\nk\\n1\\n05\\n=\\n.\\n and \\nk\\n2\\n07 5\\n=\\n..\\n Although the image is still dark, the gray-level tonality has \\nbeen restored,\\n with the added advantage of sharper features. \\nAs we discussed in Section 3.3, an image characterized by intensity levels in a narrow range of the \\ngray scale is an ideal candidate for histogram equalization. As Fig. 4.57(d) shows, this was indeed an \\nappropriate method to further enhance the image. Note the clarity of the bone structure and other \\ndetails that simply are not visible in any of the other three images. The ﬁnal enhanced image is a little \\nnoisy, but this is typical of X-ray images when their gray scale is expanded. The result obtained using a \\ncombination of high-frequency-emphasis and histogram equalization is superior to the result that would \\nbe obtained by using either method alone.\\nHOMOMORPHIC FILTERING\\nThe illumination-reflectance model introduced in Section 2.3 can be used to develop \\na frequency domain procedure for improving the appearance of an image by simul-\\ntaneous intensity range compression and contrast enhancement. From the discus-\\nsion in that section, an image \\nfx y\\n(,\\n)\\n can be expressed as the product of its illumina-\\ntion,\\n \\nixy\\n(,\\n) ,\\n and reflectance, \\nrxy\\n(,\\n) ,\\n components:\\n \\nfx y i x y rx y\\n(,\\n) (,) (,)\\n=\\n \\n(4-134)\\nThis equation cannot be used directly to operate on the frequency components of \\nillumination and reflectance because the F\\nourier transform of a product is not the \\nproduct of the transforms:\\n \\n/H5219/H5219 /H5219\\nfx\\ny i x y rx y\\n(,) (,) (,)\\n[] [ ] [ ]\\n≠\\n \\n(4-135)\\nHowever, suppose that we define\\n \\nzxy f xy\\nix\\ny rxy\\n(,) l n (,)\\nln ( , ) ln ( , )\\n=\\n=+\\n \\n(4-136)\\nThen,\\n \\n/H5219/H5219\\n/H5219/H5219\\nzx\\ny f xy\\nix\\nyr x y\\n(,) l n (,)\\nln ( , ) ln ( , )\\n[]\\n=\\n[]\\n=\\n[]\\n+\\n[]\\n \\n(4-137)\\nIf \\nf\\n(\\nx\\n, \\ny\\n) has any zero \\nvalues, a 1 must be added \\nto the image to avoid \\nhaving to deal with ln(0). \\nThe 1 is then subtracted \\nfrom the ﬁnal result.\\nDIP4E_GLOBAL_Print_Ready.indb   293\\n6/16/2017   2:06:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 294}),\n",
       " Document(page_content='294\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor\\n \\nZF F\\nir\\n(,) (,) (,)\\nuv uv uv\\n=+\\n \\n(4-138)\\nwhere \\nF\\ni\\n(,)\\nuv\\n and \\nF\\nr\\n(,)\\nuv\\n are the Fourier transforms of \\nln ( , )\\nix\\ny\\n and \\nln ( , ),\\nrx\\ny\\n \\nrespectively\\n.\\nWe can ﬁlter \\nZ\\n(,\\n)\\nuv\\n using a ﬁlter transfer function \\nH\\n(,\\n)\\nuv\\n so that\\n \\nSH Z\\nHF\\nHF\\nir\\n(,) (,) (,)\\n(,) (,) (,) (,)\\nuv uv uv\\nuv uv uv uv\\n=\\n=+\\n \\n(4-139)\\nThe filtered image in the spatial domain is then\\n \\nsxy S\\nHF\\nHF\\nir\\n(,) (,)\\n(,) (,) (,) (\\n,)\\n=\\n[]\\n=\\n[]\\n+\\n[]\\n−\\n−−\\n/H5219\\n/H5219/H5219\\n1\\n11\\nuv\\nuv uv\\nuv uv\\n \\n(4-140)\\nBy defining\\n \\n′\\n=\\n[]\\n−\\nix y H F\\ni\\n(,) (,) (,)\\n/H5219\\n1\\nuv uv\\n \\n(4-141)\\nand\\n \\n′\\n=\\n[]\\n−\\nrx y H F\\nr\\n(,) (,) (,)\\n/H5219\\n1\\nuv uv\\n \\n(4-142)\\nwe can express Eq. (4-140) in the form\\n \\nsxy i xy r xy\\n(,\\n) (,) (,)\\n=\\n′\\n+\\n′\\n \\n(4-143)\\nFinally, because \\nzxy\\n(,\\n)\\n was formed by taking the natural logarithm of the input \\nimage\\n, we reverse the process by taking the exponential of the ﬁltered result to form \\nthe output image:\\n \\ngxy e\\nee\\nix\\ny rx y\\nsx y\\nix y rx y\\n(,)\\n(,) (\\n,)\\n(,)\\n(,) (,)\\n=\\n=\\n=\\n′′\\n00\\n \\n(4-144)\\nwhere\\n \\nix y e\\nix y\\n0\\n(,)\\n(,)\\n=\\n′\\n \\n(4-145)\\nand\\n \\nrx y e\\nrx y\\n0\\n(,)\\n(,)\\n=\\n′\\n \\n(4-146)\\nare the illumination and reflectance components of the output (processed) image.\\nF\\nigure 4.58 is a summary of the ﬁltering approach just derived. This method is \\nbased on a special case of a class of systems known as \\nhomomorphic systems\\n. In this \\nparticular application, the key to the approach is the separation of the illumination \\nDIP4E_GLOBAL_Print_Ready.indb   294\\n6/16/2017   2:06:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 295}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n295\\nand reﬂectance components achieved in the form shown in Eq. (4-138). The \\nhomo-\\nmorphic ﬁlter transfer function\\n, \\nH\\n(,\\n) ,\\nuv\\n then can operate on these components sepa-\\nrately\\n, as indicated by Eq. (4-139).\\nThe illumination component of an image generally is characterized by slow spa-\\ntial variations, while the reﬂectance component tends to vary abruptly, particularly \\nat the junctions of dissimilar objects. These characteristics lead to associating the low \\nfrequencies of the Fourier transform of the logarithm of an image with illumination, \\nand the high frequencies with reﬂectance. Although these associations are rough \\napproximations, they can be used to advantage in image ﬁltering, as illustrated in \\nExample 4.23.\\nA good deal of control can be gained over the illumination and reﬂectance com-\\nponents with a homomorphic ﬁlter. This control requires speciﬁcation of a ﬁlter \\ntransfer function \\nH\\n(,\\n)\\nuv\\n that affects the low- and high-frequency components of \\nthe F\\nourier transform in different, controllable ways. Figure 4.59 shows a cross sec-\\ntion of such a function. If the parameters \\ng\\nL\\n and \\ng\\nH\\n are chosen so that \\ng\\nL\\n<\\n1\\n and \\ng\\nH\\n≥\\n1,\\n the ﬁlter function in Fig. 4.59 will attenuate the contribution made by the \\nlow frequencies (illumination) and amplify the contribution made by high frequen-\\ncies (reﬂectance).\\n The net result is simultaneous dynamic range compression and \\ncontrast enhancement.\\nThe shape of the function in Fig. 4.59 can be approximated using a highpass ﬁlter \\ntransfer function. For example, using a slightly modiﬁed form of the GHPF function \\nyields the homomorphic function\\nHe\\nHL\\ncD D\\nL\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n()\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n+\\n−\\ngg\\ng\\n1\\n2\\n0\\n2\\n(4-147)\\nwhere \\nD\\n(,\\n)\\nuv\\n is defined in Eq. (4-112) and constant \\nc\\n controls the sharpness of the \\nslope of the function as it transitions between \\ng\\nL\\n and \\ng\\nH\\n.\\n This filter transfer function \\nis similar to the high-frequenc\\ny-emphasis function discussed in the previous section.\\nA BHPF function would \\nwork well too, with the \\nadded advantage of more \\ncontrol over the sharp-\\nness of the transition \\nbetween \\ng\\nL\\n \\nand\\n g\\nH\\n. The \\ndisadvantage is the  \\npossibility of ringing for \\nhigh values of \\nn\\n.\\nln\\nexp\\nDFT\\n(DFT)\\n/H11002\\n1\\nH\\n(u, \\nv\\n)\\ng\\n(\\nx\\n, \\ny\\n)\\nf\\n(\\nx\\n, \\ny\\n)\\nFIGURE 4.58\\nSummary of steps \\nin homomorphic \\nﬁltering.\\ng\\nH\\ng\\nL\\n(,)\\nD\\nuv\\n(,)\\nH\\nuv\\nFIGURE 4.59\\nRadial cross  \\nsection of a  \\nhomomorphic \\nﬁlter transfer \\nfunction..\\nDIP4E_GLOBAL_Print_Ready.indb   295\\n6/16/2017   2:06:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 296}),\n",
       " Document(page_content='296\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nEXAMPLE 4.23 :  Homomorphic ﬁltering.\\nFigure 4.60(a) shows a full body PET (Positron Emission Tomography) scan of size \\n1162 746\\n×\\n pixels. \\nT\\nhe image is slightly blurred and many of its low-intensity features are obscured by the high intensity of \\nthe “hot spots” dominating the dynamic range of the display. (These hot spots were caused by a tumor in \\nthe brain and one in the lungs.) Figure 4.60(b) was obtained by homomorphic ﬁltering Fig. 4.60(a) using \\nthe ﬁlter transfer function in Eq. (4-147) with \\ng\\nL\\n=\\n04\\n.,\\n \\ng\\nH\\n=\\n30\\n.,\\n \\nc\\n=\\n5,\\n and \\nD\\n0\\n20\\n=\\n.\\n A radial cross sec-\\ntion of this function looks just like F\\nig. 4.59, but with a much sharper slope, and the transition between \\nlow and high frequencies much closer to the origin.\\nNote in Fig. 4.60(b) how much sharper the hot spots, the brain, and the skeleton are in the processed \\nimage, and how much more detail is visible in this image, including, for example, some of the organs, the \\nshoulders, and the pelvis region. By reducing the effects of the dominant illumination components (the \\nhot spots), it became possible for the dynamic range of the display to allow lower intensities to become \\nmore visible. Similarly, because the high frequencies are enhanced by homomorphic ﬁltering, the reﬂec-\\ntance components of the image (edge information) were sharpened considerably. The enhanced image \\nin Fig. 4.60(b) is a signiﬁcant improvement over the original. \\n4.10  SELECTIVE FILTERING  \\nThe filters discussed in the previous two sections operate over the entire frequency \\nrectangle. There are applications in which it is of interest to process specific bands of \\nfrequencies or small regions of the frequency rectangle. Filters in the first category \\n4.10\\nb a\\nFIGURE 4.60\\n(a) Full body PET \\nscan. (b) Image \\nenhanced using \\nhomomorphic \\nﬁltering. (Original \\nimage courtesy \\nof Dr. Michael E. \\nCasey, CTI Pet \\nSystems.)\\nDIP4E_GLOBAL_Print_Ready.indb   296\\n6/16/2017   2:06:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 297}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n297\\nare called \\nband filters. \\nIf frequencies in the band are filtered out, the band filter is \\ncalled a \\nbandreject\\n filter; similarly, if the frequencies are passed, the filter is called \\na \\nbandpass\\n filter. Filters in the second category are called \\nnotch filters\\n. These filters \\nare further qualified as being \\nnotch reject\\n or \\nnotch pass\\n filters, depending on whether \\nfrequencies in the notch areas are rejected or passed.\\nBANDREJECT AND BANDPASS FILTERS\\nAs you learned in\\n \\nSection 3.7, bandpass and bandreject filter transfer functions in \\nthe frequenc\\ny domain can be constructed by combining lowpass and highpass filter \\ntransfer functions, with the latter also being derivable from lowpass functions (see \\nFig. 3.52). In other words, lowpass filter transfer functions are the basis for forming \\nhighpass, bandreject, and bandpass filter functions. Furthermore, a bandpass filter \\ntransfer function is obtained from a bandreject function in the same manner that we \\nobtained a highpass from a lowpass transfer function:\\n \\nHH\\nBP\\nBR\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-148)\\nFigure 4.61(a) shows how to construct an ideal bandreject ﬁlter (IBRF) transfer \\nfunction.\\n It consists of an ILPF and an IHPF function with different cutoff frequen-\\ncies. When dealing with bandpass functions, the parameters of interest are the width, \\nW\\n, and the center, \\nC\\n0\\n,\\n of the band. An equation for the IBRF function is easily \\nobtained by inspection from F\\nig, 4.61(a), as the leftmost entry in Table 4.7 shows. \\nThe key requirements of a bandpass transfer function are: (1) the values of the func-\\ntion must be in the range \\n[, ] ;\\n01\\n (2) the value of the function must be zero at a dis-\\ntance \\nC\\n0\\n from the origin (center) of the function; and (3) we must be able to specify \\na value for \\nW\\n. Clearly, the IBRF function just developed satisﬁes these requirements.\\nAdding lowpass and highpass transfer functions to form Gaussian and Butter-\\nworth bandreject functions presents some difﬁculties. For example, Fig. 4.61(b) \\nshows a bandpass function formed as the sum of lowpass and highpass Gaussian \\nfunctions with different cutoff points. Two problems are immediately obvious: we \\nhave no direct control over \\nW\\n, and the value of \\nH\\n(,\\n)\\nuv\\n is not 0 at \\nC\\n0\\n.\\n We could \\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\nW\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\nb\\na\\nc\\nd\\nFIGURE 4.61\\n Radial cross sections. (a) Ideal bandreject ﬁlter transfer function. (b) Bandreject transfer function formed \\nby the sum of Gaussian lowpass and highpass ﬁlter functions. (The minimum is not 0 and does not align with \\nC\\n0\\n.)\\n  \\n(c) Radial plot of Eq.\\n (4-149). (The minimum is 0 and is properly aligned with \\nC\\n0\\n,\\n but the value at the origin is \\nnot 1.) (d) Radial plot of Eq.\\n (4-150); this Gaussian-shape plot meets all the requirements of a bandreject ﬁlter \\ntransfer function.\\nDIP4E_GLOBAL_Print_Ready.indb   297\\n6/16/2017   2:06:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 298}),\n",
       " Document(page_content='298\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\noffset the function and scale it so that values fall in the range \\n[, ] ,\\n01\\n but ﬁnding an \\nanalytical solution for the point where the lowpass and highpass Gaussian functions \\nintersect is impossible\\n, and this intersection would be required to solve for the cutoff \\npoints in terms of \\nC\\n0\\n.\\n The only alternatives are trial-and-error or numerical methods.\\nF\\nortunately, instead of adding lowpass and highpass transfer function, an alterna-\\ntive is to modify the expressions for the Gaussian and Butterworth highpass transfer \\nfunctions so that they will satisfy the three requirements stated earlier. We illustrate \\nthe procedure for a Gaussian function. In this case, we begin by changing the point \\nat which \\nH\\n(,)\\nuv\\n=\\n0 from \\nD\\n(,\\n)\\nuv\\n=\\n0 to \\nDC\\n(,\\n)\\nuv\\n=\\n0\\n in Eq. (4-120):\\n \\nHe\\nDC\\nW\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n−\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n0\\n2\\n2\\n \\n(4-149)\\nA plot of this function [Fig. 4.61(c)] shows that, below \\nC\\n0\\n,\\n the function behaves as a \\nlowpass Gaussian function,\\n at \\nC\\n0\\n the function will always be 0, and for values higher \\nthan \\nC\\n0\\n the function behaves as a highpass Gaussian function. Parameter \\nW\\n is pro-\\nportional to the standard deviation and thus controls the “width” of the band. The \\nonly problem remaining is that the function is not always 1 at the origin. A simple \\nmodification of Eq. (4-149) removes this shortcoming:\\n \\nHe\\nDC\\nDW\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=−\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n0\\n2\\n2\\n \\n(4-150)\\nNow, the exponent is infinite when \\nD\\n(,\\n) ,\\nuv\\n=\\n0\\n which makes the exponential term go \\nto zero and \\nH\\n(,\\n)\\nuv\\n=\\n1\\n at the origin, as desired. In this modification of Eq. (4-149), \\nthe basic Gaussian shape is preserved and the three requirements stated earlier are \\nsatisfied.\\n Figure 4.61(d) shows a plot of Eq. (4-150). A similar analysis leads to the \\nform of a Butterworth bandreject filter transfer function shown in Table 4.7. \\nFigure 4.62 shows perspective plots of the ﬁlter transfer functions just discussed. \\nAt ﬁrst glance the Gaussian and Butterworth functions appear to be about the same, \\nbut, as before, the behavior of the Butterworth function is between the ideal and \\nGaussian functions. As Fig. 4.63 shows, this is easier to see by viewing the three ﬁlter \\nThe overall ratio in this \\nequation is squared so \\nthat, as the distance \\nincreases, Eqs. (4-149) \\nand (4-150) behave  \\napproximately the same.\\nIdeal (IBRF)\\nGaussian (GBRF)\\nButterworth (BBRF)\\nH\\nC\\nW\\nDC\\nW\\n(,)\\n(,)\\nuv\\nuv\\n=\\n−+\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n0\\n22\\n1\\n00\\nif \\notherwise\\n≤≤\\nHe\\nDC\\nDW\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=−\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n0\\n2\\n2\\nH\\nDW\\nDC\\nn\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n+\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n1\\n2\\n0\\n2\\n2\\nTABLE \\n4.7\\nBandreject ﬁlter transfer functions. \\nC\\n0\\n is the center of the band, \\nW\\n is the width of the band, and \\nD\\n(,\\n)\\nuv\\n is the dis-\\ntance from the center of the transfer function to a point \\n(,)\\nuv\\n in the frequency rectangle.\\nDIP4E_GLOBAL_Print_Ready.indb   298\\n6/16/2017   2:06:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 299}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n299\\nfunctions as images. Increasing the order of the Butterworth function would bring it \\ncloser to the ideal bandreject transfer function. \\nNOTCH FILTERS\\nNotch filters are the most useful of the selective filters. A notch filter rejects (or \\npasses) frequencies in a predefined neighborhood of the frequency rectangle. Zero-\\nphase-shift filters must be symmetric about the origin (center of the frequency \\nrectangle), so a notch filter transfer function with center at \\n(,)\\nuv\\n00\\n must have a \\ncorresponding notch at location \\n(,) .\\n−−\\nuv\\n00\\n \\nNotch reject\\n filter transfer functions are \\nconstructed as products of highpass filter transfer functions whose centers have \\nbeen translated to the centers of the notches. The general form is:\\n \\nHH H\\nk\\nk\\nQ\\nk\\nNR\\n(,) (,) (,)\\nuv uv uv\\n=\\n=\\n−\\n∏\\n1\\n \\n(4-151)\\nwhere \\nH\\nk\\n(,)\\nuv\\n and \\nH\\nk\\n−\\n(,)\\nuv\\n are highpass filter transfer functions whose centers are \\nat \\n(,)\\nuv\\nkk\\n and \\n(,) ,\\n−−\\nuv\\nkk\\n respectively. These centers are specified with respect to \\nthe center of the frequency rectangle, \\nMN\\n22\\n,,\\n()\\n where, as usual, \\nM\\n and \\nN\\n are the \\nb a\\nc\\nFIGURE 4.62\\n Perspective plots of (a) ideal, (b) modiﬁed Gaussian, and (c) modiﬁed Butterworth (of order 1) bandre-\\nject ﬁlter transfer functions from Table 4.7. All transfer functions are of size \\n512 512\\n×\\n elements, with \\nC\\n0\\n128\\n=\\n and \\nW\\n=\\n60.\\nu\\nv\\n(,)\\nHuv\\nu\\nv\\nHuv\\nu\\nv\\n(,)\\nHuv\\n(,)\\nb a\\nc\\nFIGURE 4.63\\n(a) The ideal,  \\n(b) Gaussian, and \\n(c) Butterworth \\nbandpass transfer \\nfunctions from \\nFig. 4.62, shown \\nas images. (The \\nthin border lines \\nare not part of the \\nimage data.) \\nDIP4E_GLOBAL_Print_Ready.indb   299\\n6/16/2017   2:06:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 300}),\n",
       " Document(page_content='300\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nnumber of rows and columns in the input image. Thus, the distance computations for \\neach filter transfer function are given by\\n \\nDu M u N\\nkk k\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n=− − + − −\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n12\\n \\n(4-152)\\nand\\n \\nDu M u N\\nkk k\\n−\\n=− + + − +\\n⎡\\n⎣\\n⎤\\n⎦\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n22\\n22\\n12\\n \\n(4-153)\\nFor example, the following is a Butterworth notch reject filter transfer function of \\norder \\nn\\n,\\n containing three notch pairs:\\n \\nH\\nDD\\nDD\\nkk\\nn\\nk\\nkk\\nn\\nNR\\n(,)\\n(,\\n)(\\n,\\n)\\nuv\\nuv\\nuv\\n=\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n=\\n−\\n∏\\n1\\n1\\n1\\n1\\n0\\n1\\n3\\n0\\n⎢ ⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n \\n(4-154)\\nwhere \\nD\\nk\\n(,)\\nuv\\n and \\nD\\nk\\n−\\n(,)\\nuv\\n are given by Eqs. (4-152) and (4-153). The constant \\nD\\nk\\n0\\n \\nis the same for each pair of notches, but it can be different for different pairs. Other \\nnotch reject filter functions are constructed in the same manner, depending on the \\nhighpass filter function chosen. As with the filters discussed earlier, a notch pass \\nfilter transfer function is obtained from a notch reject function using the expression\\n \\nHH\\nNP\\nNR\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-155)\\nAs the next two examples show, one of the principal applications of notch ﬁlter-\\ning is for selectively modifying local regions of the DFT\\n. Often, this type of pro-\\ncessing is done interactively, working directly with DFTs obtained without padding. \\nThe advantages of working interactively with actual DFTs (as opposed to having to \\n“translate” from padded to actual frequency values) generally outweigh any wrap-\\naround errors that may result from not using padding in the ﬁltering process. If nec-\\nessary, after an acceptable solution is obtained, a ﬁnal result using padding can be \\ngenerated by adjusting all ﬁlter parameters to compensate for the padded DFT size. \\nThe following two examples were done without padding. To get an idea of how DFT \\nvalues change as a function of padding, see Problem 4.42. \\nEXAMPLE 4.24 :  Using notch ﬁltering to remove moiré patterns from digitized printed media images.\\nFigure 4.64(a) is the scanned newspaper image used in Fig. 4.21, showing a prominent moiré pattern, and \\nFig. 4.64(b) is its spectrum. The Fourier transform of a pure sine, which is a periodic function, is a pair of \\nconjugate symmetric impulses (see Table 4.4). The symmetric “impulse-like” bursts in Fig. 4.64(b) are a \\nresult of the near periodicity of the moiré pattern. We can attenuate these bursts by using notch ﬁltering. \\nFigure 4.64(c) shows the result of multiplying the DFT of Fig. 4.64(a) by a Butterworth notch reject \\ntransfer function with \\nD\\n0\\n9\\n=\\n and \\nn\\n=\\n4\\n for all notch pairs (the centers of the notches are coincide with \\nthe centers of the black circular regions in the ﬁgure).\\n The value of the radius was selected (by visual \\ninspection of the spectrum) to encompass the energy bursts completely, and the value of \\nn\\n was selected \\nto produce notches with sharp transitions. The locations of the center of the notches were determined \\nDIP4E_GLOBAL_Print_Ready.indb   300\\n6/16/2017   2:06:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 301}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n301\\ninteractively from the spectrum. Figure 4.64(d) shows the result obtained with this ﬁlter transfer func-\\ntion, using the ﬁltering procedure outlined in Section 4.7. The improvement is signiﬁcant, considering \\nthe low resolution and degree of degradation of the original image. \\nb a\\nd c\\nFIGURE 4.64\\n (a) Sampled \\nnewspaper  \\nimage showing a \\nmoiré pattern.  \\n(b) Spectrum.  \\n(c) Fourier  \\ntransform \\nmultiplied by \\na Butterworth \\nnotch reject ﬁlter \\ntransfer function. \\n(d) Filtered image. \\nDIP4E_GLOBAL_Print_Ready.indb   301\\n6/16/2017   2:06:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 302}),\n",
       " Document(page_content='302\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nEXAMPLE 4.25 :  Using notch ﬁltering to remove periodic interference.\\nFigure 4.65(a) shows an image of part of the rings surrounding the planet Saturn. This image was cap-\\ntured by \\nCassini\\n, the ﬁrst spacecraft to enter the planet’s orbit. The nearly sinusoidal pattern visible in \\nthe image was caused by an AC signal superimposed on the camera video signal just prior to digitizing \\nthe image. This was an unexpected problem that corrupted some images from the mission. Fortunately, \\nthis type of interference is fairly easy to correct by postprocessing. One approach is to use notch ﬁltering. \\nFigure 4.65(b) shows the DFT spectrum. Careful analysis of the vertical axis reveals a series of \\nsmall bursts of energy near the origin which correspond to the nearly sinusoidal interference. A simple \\napproach is to use a narrow notch rectangle ﬁlter starting with the lowest frequency burst, and extending \\nfor the remainder of the vertical axis. Figure 4.65(c) shows the transfer function of such a ﬁlter (white \\nrepresents 1 and black 0). Figure 4.65(d) shows the result of processing the corrupted image with this \\nﬁlter. This result is a signiﬁcant improvement over the original image.\\nTo obtain and image of just the interference pattern, we isolated the frequencies in the vertical axis \\nusing a notch pass transfer function, obtained by subtracting the notch reject function from 1 [see \\nFig. 4.66(a)]. T\\nhen, as Fig. 4.66(b) shows, the IDFT of the ﬁltered image is the spatial interference pattern.\\nb a\\nd c\\nFIGURE 4.65\\n(a) Image of  \\nSaturn rings \\nshowing nearly \\nperiodic  \\ninterference.  \\n(b) Spectrum. \\n(The bursts of \\nenergy in the  \\nvertical axis \\nnear the origin \\ncorrespond to \\nthe interference \\npattern).  \\n(c) A vertical \\nnotch reject ﬁlter \\ntransfer function.  \\n(d) Result of \\nﬁltering.  \\n(The thin black \\nborder in (c) is \\nnot part of the \\ndata.) (Original \\nimage courtesy \\nof Dr. Robert A. \\nWest, NASA/\\nJPL.) \\nDIP4E_GLOBAL_Print_Ready.indb   302\\n6/16/2017   2:06:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 303}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n303\\n4.11 THE FAST FOURIER TRANSFORM \\nWe have focused attention thus far on theoretical concepts and on examples of fil-\\ntering in the frequency domain. One thing that should be clear by now is that com-\\nputational requirements in this area of image processing are not trivial. Thus, it is \\nimportant to develop a basic understanding of methods by which Fourier transform \\ncomputations can be simplified and speeded up. This section deals with these issues. \\nSEPARABILITY OF THE 2-D DFT\\nAs mentioned in Table 4.3, the 2-D DFT is separable into 1-D transforms. We can \\nwrite Eq. (4-67) as\\n \\nFef x y e\\nFx e\\nju x M\\ny\\nN\\nx\\nM\\njy N\\nju x M\\n(,)\\n(,)\\n(,)\\nuv\\nv\\nv\\n=\\n=\\n−\\n=\\n−\\n=\\n−\\n−\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\n2\\n2\\npp\\np\\nx x\\nM\\n=\\n−\\n∑\\n0\\n1\\n \\n(4-156)\\nwhere\\n \\nFx fx ye\\ny\\nN\\njy N\\n(,) (,)\\nv\\nv\\n=\\n=\\n−\\n−\\n∑\\n0\\n1\\n2\\np\\n \\n(4-157)\\nFor one value of \\nx,\\n and for \\nv\\n=−\\n012\\n1\\n,,, , ,\\n…\\nN\\n we see that \\nFx\\n(,\\n)\\nv\\n is the 1-D DFT of \\none\\n row of \\nfx y\\n(,\\n) .\\n By varying \\nx\\n from 0 to \\nM\\n−\\n1 in Eq. (4-157), we compute a set of \\n1-D DFTs for all rows of \\nfx y\\n(,\\n) .\\n The computations in Eq. (4-156) similarly are 1-D \\ntransforms of the columns of \\nFx\\n(,\\n) .\\nv\\n Thus, we conclude that the 2-D DFT of \\nfx y\\n(,\\n)\\n \\ncan be obtained by computing the 1-D transform of each row of \\nfx y\\n(,\\n)\\n and then \\ncomputing the 1-D transform along each column of the result.\\n This is an important \\nsimplification because we have to deal only with one variable at a time. A similar \\ndevelopment applies to computing the 2-D IDFT using the 1-D IDFT. However, \\nas we show in the following section, we can compute the IDFT using an algorithm \\n4.11\\nWe could have formu-\\nlated the preceding \\ntwo equations to show \\nthat a 2-D DFT can be \\nobtained by computing \\nthe 1-D DFT of each \\ncolumn\\n of the input \\nimage followed by 1-D \\ncomputations on the \\nrows of the result. \\nb a\\nFIGURE 4.66\\n(a) Notch pass \\nﬁlter function \\nused to isolate \\nthe vertical axis \\nof the DFT of Fig. \\n4.65(a).  \\n(b) Spatial pattern \\nobtained by  \\ncomputing the \\nIDFT of (a). \\nDIP4E_GLOBAL_Print_Ready.indb   303\\n6/16/2017   2:06:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 304}),\n",
       " Document(page_content='304\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ndesigned to compute the forward DFT, so all 2-D Fourier transform computations \\nare reduced to multiple passes of a 1-D algorithm designed for computing the 1-D \\nDFT.\\nCOMPUTING THE IDFT USING A DFT ALGORITHM\\nTaking the complex conjugate of both sides of Eq. (4-68) and multiplying the results \\nby \\nMN\\n yields\\n \\nMNf x y F e\\nju x My N\\nN\\nu\\nM\\n** ( )\\n(,) (,)\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\nv\\n2\\n0\\n1\\n0\\n1\\np\\n \\n(4-158)\\nBut, we recognize the form of the right side of this result as the DFT of \\nF\\n*\\n(,) .\\nuv\\n There-\\nfore\\n, Eq. (4-158) indicates that if we substitute \\nF\\n*\\n(,)\\nuv\\n into an algorithm designed to \\ncompute the 2-D forward F\\nourier transform, the result will be \\nMNf x y\\n*\\n(,) .\\n Taking \\nthe complex conjugate and dividing this result by \\nMN\\n yields \\nfx y\\n(,\\n) ,\\n which is the \\ninverse of \\nF\\n(,\\n) .\\nuv\\n \\nComputing the 2-D inverse from a 2-D forward DFT algorithm that is based on \\nsuccessive passes of 1-D transforms (as in the previous section) is a frequent source \\nof confusion involving the complex conjugates and multiplication by a constant,\\n nei-\\nther of which is done in the 1-D algorithms. The key concept to keep in mind is that \\nwe simply input \\nF\\n*\\n(,)\\nuv\\n into whatever forward algorithm we have. The result will be \\nMNf x y\\n*\\n(,) .\\n All we have to do with this result to obtain \\nfx y\\n(,\\n)\\n is to take its complex \\nconjugate and divide it by the constant \\nMN\\n.\\n Of course, when \\nfx y\\n(,\\n)\\n is real, as typi-\\ncally is the case\\n, then \\nfx y f x y\\n*\\n(,) (,) .\\n=\\nTHE FAST FOURIER TRANSFORM (FFT)\\nWork in the frequency domain would not be practical if we had to implement \\nEqs. (4-67) and (4-68) directly. Brute-force implementation of these equations \\nrequires on the order of \\nMN\\n()\\n2\\n multiplications and additions. For images of moder-\\nate size (say, \\n2048 2048\\n×\\n pixels), this means on the order of 17 trillion multiplica-\\ntions and additions for just one 2-D DFT\\n, excluding the exponentials, which could be \\ncomputed once and stored in a look-up table. Without the discovery of the \\nfast Fou-\\nrier transform\\n (FFT), which reduces computations to the order of \\nMN MN\\nlog\\n2\\n mul-\\ntiplications and additions, it is safe to say that the material presented in this chapter \\nwould be of little practical value. The computational reductions afforded by the FFT \\nare impressive indeed. For example, computing the 2-D FFT of a \\n2048 2048\\n×\\n image \\nwould require on the order of 92 million multiplication and additions\\n, which is a \\nsignificant reduction from the one trillion computations mentioned above.\\nAlthough the FFT is a topic covered extensively in the literature on signal pro-\\ncessing, this subject matter is of such signiﬁcance in our work that this chapter would \\nbe incomplete if we did not provide an introduction explaining why the FFT works \\nas it does. The algorithm we selected to accomplish this objective is the so-called \\nsuccessive-doubling method\\n, which was the original algorithm that led to the birth \\nof an entire industry. This particular algorithm assumes that the number of samples \\nis an integer power of 2, but this is not a general requirement of other approaches \\nDIP4E_GLOBAL_Print_Ready.indb   304\\n6/16/2017   2:06:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 305}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n305\\n(Brigham [1988]).We know from the previous section that 2-D DFTs can be imple-\\nmented by successive passes of the 1-D transform, so we need to focus only on the \\nFFT of one variable.\\nIn derivations of the FFT, it is customary to express Eq. (4-44) in the form\\n \\nFu f xW\\nM\\nux\\nx\\nM\\n()\\n=\\n()\\n=\\n−\\n∑\\n0\\n1\\n \\n(4-159)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nM\\n where\\n \\nWe\\nM\\njM\\n=\\n−\\n2\\np\\n \\n(4-160)\\nand \\nM\\n is assumed to be of the form\\n \\nM\\np\\n=\\n2  \\n(4-161)\\nwhere \\np\\n is a positive integer\\n. Then it follows that \\nM\\n can be expressed as\\n \\nMK\\n=\\n2  \\n(4-162)\\nwith \\nK\\n being a positive integer also\\n. Substituting Eq. (4-162) into Eq. (4-159) yields\\n \\nFu fxW\\nfx\\nW fx W\\nK\\nux\\nx\\nK\\nK\\nux\\nx\\nK\\nK\\nux\\n() ()\\n() ( )\\n=\\n=+ +\\n=\\n−\\n()\\n=\\n−\\n+\\n∑\\n∑\\n2\\n0\\n21\\n2\\n2\\n0\\n1\\n2\\n2\\n22\\n1\\n1 1\\n0\\n1\\n()\\n=\\n−\\n∑\\nx\\nK\\n \\n(4-163)\\nHowever, it can be shown using Eq. (4-160) that \\nWW\\nK\\nux\\nK\\nux\\n2\\n2\\n=\\n,\\n so Eq. (4-163) can be \\nwritten as\\n \\nFu f x W f x W W\\nK\\nux\\nK\\nux\\nx\\nK\\nK\\nu\\nx\\nK\\n() ( ) ( )\\n=+ +\\n=\\n−\\n=\\n−\\n∑\\n∑\\n22 1\\n0\\n1\\n2\\n0\\n1\\n \\n(4-164)\\nDefining\\n \\nFu f x W\\nK\\nux\\nx\\nK\\neven\\n() ( )\\n=\\n=\\n−\\n∑\\n2\\n0\\n1\\n \\n(4-165)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nK\\n and\\n \\nFu f x W\\nK\\nux\\nx\\nK\\nodd\\n() ( )\\n=+\\n=\\n−\\n∑\\n21\\n0\\n1\\n \\n(4-166)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nK\\n reduces Eq. (4-164) to\\n \\nFu F u F uW\\nK\\nu\\n() () ()\\n=+\\neven\\nodd 2\\n \\n(4-167)\\nDIP4E_GLOBAL_Print_Ready.indb   305\\n6/16/2017   2:06:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 306}),\n",
       " Document(page_content='306\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nAlso, because \\nWW\\nM\\nuK\\nK\\nu\\n+\\n=\\n and \\nWW\\nK\\nuK\\nK\\nu\\n22\\n+\\n=−\\n, it follows that\\n \\nFu K F u F uW\\nK\\nu\\n( ) () ()\\n+= −\\neven\\nodd 2\\n \\n(4-168)\\nAnalysis of Eqs. (4-165) through (4-168) reveals some important (and surprising) \\nproperties of these expressions\\n. An \\nM\\n-point DFT can be computed by dividing the \\noriginal expression into two parts, as indicated in Eqs. (4-167) and (4-168). Comput-\\ning the first half of \\nFu\\n()\\n requires evaluation of the two \\n()\\nM\\n2 -point\\n transforms giv-\\nen in Eqs\\n. (4-165) and (4-166). The resulting values of \\nFu\\neven\\n()\\n and \\nFu\\nodd\\n()\\n are then \\nsubstituted into Eq.\\n (4-167) to obtain \\nFu\\n()\\n for \\nu\\n=−\\n012 2 1\\n,,, , ( ) .\\n…\\nM\\n The other \\nhalf then follows directly from Eq.\\n (4-168) \\nwithout\\n additional transform evaluations.\\n It is of interest to examine the computational implications of the preceding pro-\\ncedure. Let \\n/H5258\\n()\\np\\n and \\n/H5246\\n()\\np\\n represent the number of complex multiplications and \\nadditions\\n, respectively, required to implement the method. As before, the number \\nof samples is \\n2\\np\\n,\\n where \\np\\n is a positive integer\\n. Suppose ﬁrst that \\np\\n=\\n1\\n so that the \\nnumber of samples is two\\n. A two-point transform requires the evaluation of \\nF\\n()\\n;\\n0  \\nthen \\nF\\n()\\n1\\n follows from Eq. (4-168). To obtain \\nF\\n()\\n0\\n requires computing \\nF\\neven\\n()\\n0\\n and \\nF\\nodd\\n() .\\n0\\n In this case \\nK\\n=\\n1\\n and Eqs. (4-165) and (4-166) are one-point transforms. \\nHowever\\n, because the DFT of a single sample point is the sample itself, no multipli-\\ncations or additions are required to obtain \\nF\\neven\\n()\\n0\\n and \\nF\\nodd\\n() .\\n0\\n One multiplication \\nof \\nF\\nodd\\n()\\n0\\n by \\nW\\n2\\n0\\n and one addition yields \\nF\\n()\\n0\\n from Eq. (4-167). Then \\nF\\n()\\n1\\n follows \\nfrom Eq.\\n (4-168) with one more addition (subtraction is considered to be the same \\nas addition). Because \\nFW\\nodd\\n()\\n0\\n2\\n0\\n has been computed already, the total number of \\noperations required for a two-point transform consists of \\n/H5258\\n()\\n11\\n=\\n multiplication \\nand \\n/H5246\\n()\\n12\\n=\\n additions.\\nT\\nhe next allowed value for \\np\\n is 2. According to the preceding development, a four-\\npoint transform can be divided into two parts. The ﬁrst half of \\nFu\\n()\\n requires evaluation \\nof two\\n, two-point transforms, as given in Eqs. (4-165) and (4-166) for \\nK\\n=\\n2.\\n A two-point \\ntransform requires \\n/H5258\\n()\\n1\\n multiplications and \\n/H5246\\n()\\n1\\n additions. Therefore, evaluation of \\nthese two equations requires a total of \\n21\\n/H5258\\n()\\n multiplications and \\n21\\n/H5246\\n()\\n additions. Two \\nfurther multiplications and additions are necessary to obtain \\nF\\n()\\n0\\n and \\nF\\n()\\n1\\n from Eq. \\n(4-167).\\n Because \\nFu W\\nK\\nu\\nodd\\n()\\n2\\n has been computed already for \\nu\\n=\\n{}\\n01\\n,,\\n two more \\nadditions give \\nF\\n()\\n2\\n and \\nF\\n()\\n.\\n3\\n The total is then \\n/H5258/H5258\\n()\\n()\\n22 1 2\\n=+\\n and \\n/H5246/H5246\\n()\\n() .\\n22 1 4\\n=+\\nWhen \\np\\n is equal to 3,\\n two four-point transforms are needed to evaluate \\nFu\\neven\\n()\\n \\nand \\nFu\\nodd\\n() .\\n They require \\n22\\n/H5258\\n()\\n multiplications and \\n22\\n/H5246\\n()\\n additions. Four more \\nmultiplications and eight more additions yield the complete transform.\\n The total \\nthen is then \\n/H5258/H5258\\n()\\n()\\n32 24\\n=+\\n multiplication and \\n/H5246/H5246\\n()\\n()\\n32 28\\n=+\\n additions.\\nContinuing this argument for any positive integer \\np\\n leads to recursive expressions \\nfor the number of multiplications and additions required to implement the FFT\\n:\\n \\n/H5258/H5258\\n()\\n( )\\npp p\\np\\n=− +\\n−\\n21 2 1\\n1\\n≥\\n \\n(4-169)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   306\\n6/16/2017   2:06:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 307}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n307\\n \\n/H5246/H5246\\n()\\n( )\\npp p\\np\\n=− +\\n21 2 1\\n≥\\n \\n(4-170)\\nwhere \\n/H5258\\n()\\n00\\n=\\n and \\n/H5246\\n()\\n00\\n=\\n because the transform of a single point does not \\nrequire any multiplication or additions\\n.\\nThe method just developed is called the \\nsuccessive doubling FFT algorithm\\n \\nbecause it is based on computing a two-point transform from two one-point trans-\\nforms, a four-point transform from two two-point transforms, and so on, for any \\nM\\n \\nequal to an integer power of 2. It is left as an exercise (see Problem 4.63) to show \\nthat\\n \\n/H5258\\n() l o g\\npM M\\n=\\n1\\n2\\n2\\n \\n(4-171)\\nand\\n \\n/H5246\\n()\\nl o g\\nnM M\\n=\\n2\\n \\n(4-172)\\nwhere \\nM\\np\\n=\\n2.\\nThe computational advantage of the FFT over a direct implementation of the 1-D \\nDFT is deﬁned as\\n \\nCM\\nM\\nMM\\nM\\nM\\n()\\nlog\\nlog\\n=\\n=\\n2\\n2\\n2\\n \\n(4-173)\\nwhere \\nM\\n2\\n is the number of operations required for a “brute force” implementation \\nof the 1-D DFT. Because it is assumed that \\nM\\np\\n=\\n2,\\n we can write Eq. (4-173) in \\nterms of \\np\\n:\\n \\nCp\\np\\np\\n()\\n=\\n2\\n \\n(4-174)\\nA plot of this function (Fig. 4.67) shows that the computational advantage increases \\nrapidly as a function of \\np\\n.\\n For example, when \\np\\n=\\n15\\n (32,768 points), the FFT has \\nnearly a 2,200 to 1 advantage over a brute-force implementation of the DFT\\n. Thus, \\nwe would expect that the FFT can be computed nearly 2,200 times faster than the \\nDFT on the same machine. As you learned in Section 4.1, the FFT also offers signifi-\\ncant computational advantages over spatial filtering, with the cross-over between \\nthe two approaches being for relatively small kernels.\\nThere are many excellent sources that cover details of the FFT so we will not \\ndwell on this topic further (see, for example, Brigham [1988]). Most comprehensive \\nsignal and image processing software packages contain generalized implementa-\\ntions of the FFT that do not require the number of points to be an integer power \\nDIP4E_GLOBAL_Print_Ready.indb   307\\n6/16/2017   2:06:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 308}),\n",
       " Document(page_content='308\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n1\\nC\\n(\\np\\n)\\n0\\n600\\n1200\\n1800\\n2400\\n234567\\np\\n8 9 10 11 12 13 14 15\\n2\\n()\\np\\nCp\\np\\n=\\nFIGURE 4.67\\nComputational \\nadvantage of the \\nFFT over a direct \\nimplementation \\nof the 1-D DFT. \\nThe number of \\nsamples is \\nM\\np\\n=\\n2.\\n \\nT\\nhe computational \\nadvantage increases \\nrapidly as a  \\nfunction of \\np\\n. \\nof 2 (at the expense of slightly less efﬁcient computation). Free FFT programs also \\nare readily available, principally over the internet.\\nSummary, References, and Further Reading\\n \\nThe material in this chapter is a progression from sampling to the Fourier transform, and then to ﬁltering in the \\nfrequency domain. Some of the concepts, such as the sampling theorem, make very little sense if not explained in \\nthe context of the frequency domain. The same is true of effects such as aliasing. Thus, the material developed in \\nthe preceding sections is a solid foundation for understanding the fundamentals of 2-D digital signal processing. We \\ntook special care to develop the material starting with basic principles, so that any reader with a modest mathemati-\\ncal background would be in a position not only to absorb the material, but also to apply it.\\nFor complementary reading on the 1-D and 2-D continuous Fourier transforms, see the books by Bracewell \\n[1995, 2003]. These two books, together with Castleman [1996], Petrou and Petrou [2010], Brigham [1988], and \\nSmith [2003], provide additional background for the material in Sections 4.2 through 4.6. Sampling phenomena \\nsuch as aliasing and moiré patterns are topics amply illustrated in books on computer graphics, as exempliﬁed by \\nHughes and Andries [2013]. For additional general background on the material in Sections 4.7 through 4.11 see \\nHall [1979], Jain [1989], Castleman [1996], and Pratt [2014]. For details on the software aspects of many of the ex-\\namples in this chapter, see Gonzalez, Woods, and Eddins [2009].\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com)\\n4.1 \\nAnswer the following:\\n(a) * \\nGive an equation similar to Eq. (4-10), but \\nfor an impulse located at \\ntt\\n=\\n0\\n.\\n(b) \\nRepeat for Eq. (4-15).\\n(c) * \\nIs it correct to say that \\ndd\\n()\\n()\\nta at\\n−= −\\n in \\ngeneral? Explain.\\n4.2 \\nRepeat Example 4.1, but using the function \\nft A\\n()\\n=\\n for \\n0\\n≤<\\ntT\\n and \\nft\\n()\\n=\\n0\\n for all other \\nvalues of \\nt\\n.\\n Explain the reason for any differences \\nbetween your results and the results in the exam-\\nple.\\n4.3 \\nWhat is the convolution of two, 1-D impulses: \\n(a) * \\nd\\n()\\nt\\n and \\nd\\n()\\n?\\ntt\\n−\\n0\\n(b) \\nd\\n()\\ntt\\n−\\n0\\n and \\nd\\n() ?\\ntt\\n+\\n0\\nDIP4E_GLOBAL_Print_Ready.indb   308\\n6/16/2017   2:06:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 309}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n309\\n4.4 * \\nUse the sifting property of the impulse to show \\nthat convolving a 1-D continuous function,\\n \\nft\\n()\\n,\\n \\nwith an impulse located at \\nt\\n0\\n shifts the function \\nso that its origin is moved to the location of the \\nimpulse (if the impulse is at the origin, the func-\\ntion is not shifted).\\n4.5 * \\nWith reference to Fig. 4.9, give a graphical illustra-\\ntion of an aliased pair of functions that are not \\nperiodic\\n.\\n4.6 \\nWith reference to Fig. 4.11:\\n(a) * \\nRedraw the ﬁgure, showing what the dots \\nwould look like for a sampling rate that \\nexceeds the Nyquist rate slightly\\n.\\n(b) \\nWhat is the \\nappro\\nximate\\n sampling rate repre-\\nsented by the large dots in Fig. 4.11?\\n(c) \\nApproximately\\n,\\n what would be the lowest \\nsampling rate that you would use so that (1) \\nthe Nyquist rate is satisﬁed, and (2) the sam-\\nples look like a sine wave?\\n4.7 \\nA function, \\nft\\n()\\n,\\n is formed by the sum of three \\nfunctions\\n, \\nft A t\\n1\\n( ) sin( ),\\n=\\np\\n \\nft B t\\n2\\n4\\n( ) sin( ),\\n=\\np\\n \\nand \\nft C t\\n3\\n8\\n( ) cos( ).\\n=\\np\\n(a) \\nAssuming that the functions extend to inﬁn-\\nity in both directions\\n, what is the highest fre-\\nquency of \\nft\\n()\\n?\\n (\\nHint:\\n Start by ﬁnding the \\nperiod of the sum of the three functions\\n.)\\n(b) * \\nWhat is the Nyquist rate corresponding to \\nyour result in (a)? (Give a numerical answer\\n.)\\n(c) \\nAt what rate would you sample \\nft\\n()\\n so that \\nperfect recovery of the function from its \\nsamples is possible?\\n4.8 * \\nShow that \\n/H5219\\n{}\\n() ,\\net\\njt t\\n2\\n0\\n0\\np\\ndm\\n=−\\n where \\nt\\n0\\n is a con-\\nstant. (\\nHint:\\n Study Example 4.2.) \\n4.9 \\nShow that the following expressions are true. \\n(\\nHint:\\n Make use of the solution to Problem 4.8):\\n(a) * \\n/H5219\\ncos( ) ( ) ( )\\n2\\n1\\n2\\n00 0\\npm d m m d m m\\nt\\n{}\\n=− + +\\n[]\\n(b) \\n/H5219\\nsin( ) ( ) ( )\\n2\\n1\\n2\\n00 0\\npm d m m d m m\\nt\\nj\\n{}\\n=− − +\\n[]\\n4.10 \\nConsider the function \\nft n t\\n(\\n) sin( ),\\n=\\n2\\np\\n where \\nn\\n is an integer\\n. Its Fourier transform, \\nF\\n()\\n,\\nm\\n is \\npurely imaginary (see Problem 4.9).\\n Because the \\ntransform, \\n/tildenosp\\nF\\n()\\n,\\nm\\n of sampled data consists of peri-\\nodic copies of \\nF\\n()\\n,\\nm\\n it follows that \\n/tildenosp\\nF\\n()\\nm\\n will also \\nbe purely imaginary\\n. Draw a diagram similar to \\nFig. 4.6, and answer the following questions based \\non your diagram (assume that sampling starts at \\nt\\n=\\n0)\\n.\\n(a) * \\nWhat is the period of \\nft\\n()\\n?\\n(b) * \\nWhat is the frequency of \\nft\\n()\\n?\\n(c) * \\nWhat would the sampled function and its \\nF\\nourier transform look like in general if \\nft\\n()\\n \\nis sampled at a rate higher than the Nyquist \\nrate?\\n(d) \\nWhat would the sampled function look like \\nin general if \\nft\\n()\\n is sampled at a rate lower \\nthan the Nyquist rate?\\n(e) \\nWhat would the sampled function look like \\nif \\nft\\n()\\n is sampled at the Nyquist rate, with \\nsamples taken at \\ntT T\\n=\\n02\\n,,\\n,\\n±±\\n/H9004/H9004\\n…\\n ?\\n4.11 * \\nProve the validity of the convolution theorem of \\none continuous variable\\n, as given in Eqs. (4-25) \\nand (4-26).\\n4.12 \\nWe explained in the paragraph after Eq. (4-36) that \\narbitrarily limiting the duration of a band-limit-\\ned function by multiplying it by a box function \\nwould cause the function to cease being band \\nlimited.\\n Show graphically why this is so by limit-\\ning the duration of the function \\nft\\nt\\n()\\nc o s ( )\\n=\\n2\\n0\\npm\\n \\n[the F\\nourier transform of this function is given in \\nProblem 4.9(a)]. (\\nHint:\\n The transform of a box \\nfunction is given in Example 4.1. Use that result \\nin your solution, and also the fact that convolu-\\ntion of a function with an impulse shifts the func-\\ntion to the location of the impulse, in the sense \\ndiscussed in the solution of Problem 4.4.)\\n4.13 * \\nComplete the steps that led from Eq. (4-37) to \\nEq.\\n (4-38).\\n4.14 \\nShow that \\n/tildenosp\\nF\\n()\\nm\\n in Eq. (4-40) is inﬁnitely periodic \\nin both directions\\n, with period \\n1\\n/H9004\\nT\\n.\\n4.15 \\nDo the following:\\n(a) \\nShow that Eqs. (4-42) and (4-43) are a Fou-\\nrier transform pair:\\n \\nfF\\nnm\\n⇔\\n.\\n(b) * \\nShow that Eqs. (4-44) and (4-45) also are a \\nF\\nourier transform pair: \\nfx Fu\\n()\\n() .\\n⇔\\nYou will need the following orthogonality prop-\\nerty in both parts of this problem:\\n \\nee\\nMr u\\njr x M\\nx\\nM\\nju x M\\n2\\n0\\n1\\n2\\n0\\npp\\n=\\n−\\n−\\n∑\\n=\\n=\\n⎧\\n⎨\\n⎩\\nif \\notherwise\\nDIP4E_GLOBAL_Print_Ready.indb   309\\n6/16/2017   2:06:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='310\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.16 \\nShow that both \\nFu\\n()\\n and \\nfx\\n()\\n in Eqs. (4-44) and \\n(4-45) are inﬁnitely periodic with period \\nM\\n;\\n that is, \\nFu Fu k M\\n()\\n( )\\n=+\\n and \\nfx fx M\\n()\\n( ) ,\\n=+\\n where \\nk\\n \\nis an integer\\n. [See Eqs. (4-46) and (4-47).]\\n4.17 \\nDemonstrate the validity of the translation (shift) \\nproperties of the following 1-D\\n, discrete Fourier \\ntransform pairs. (\\nHint:\\n It is easier in part (b) to \\nwork with the IDFT.)\\n(a) * \\nfxe Fu u\\nju x M\\n()\\n( )\\n2\\n0\\n0\\np\\n⇔−\\n(b) \\nfx x Fue\\nju x M\\n() ( )\\n−⇔\\n−\\n0\\n2\\n0\\np\\n4.18 \\nShow that the 1-D convolution theorem given in \\nEqs\\n. (4-25) and (4-26) also holds for discrete vari-\\nables, but with the right side of Eq. (4-26) multi-\\nplied by \\n1\\nM\\n. That is, show that\\n(a) * \\n \\n() ( ) () ( ) ,\\nfh x F H u\\n/H22841\\n⇔\\ni\\n and\\n(b) \\n() ( ) ( ) ( )\\nfhx\\nM\\nFH u\\ni\\n⇔\\n1\\n/H22841\\n \\n4.19 * \\nExtend the expression for 1-D convolution [see \\nEq.\\n (4-24)] to two continuous variables. Use \\nt\\n and \\nz\\n for the variables on the left side of the expression \\nand \\na\\n and \\nb\\n for the variables in the 2-D integral. \\n4.20 \\nUse the sifting property of the 2-D impulse to \\nshow that convolution of a 2-D continuous func-\\ntion,\\n \\nft z\\n(,\\n) ,\\n with an impulse shifts the function \\nso that its origin is located at the location of the \\nimpulse\\n. (If the impulse is at the origin, the func-\\ntion is copied exactly as it was.) (\\nHint:\\n Study the \\nsolution to Problem 4.4).\\n4.21 \\nThe image on the left in the ﬁgure below consists \\nof alternating stripes of black/white\\n, each stripe \\nbeing two pixels wide. The image on the right \\nis the Fourier spectrum of the image on the left, \\nshowing the dc term and the frequency terms cor-\\nresponding to the stripes. (Remember, the spec-\\ntrum is symmetric so all components, other than \\nthe dc term, appear in two symmetric locations.)\\n(a) * \\nSuppose that the stripes of an image of the \\nsame size are four pixels wide\\n. Sketch what \\nthe spectrum of the image would look like, \\nincluding only the dc term and the two high-\\nest-value frequency terms, which correspond \\nto the two spikes in the spectrum above.\\n(b) \\nWhy are the components of the spectrum \\nlimited to the horizontal axis?\\n(c) \\nWhat would the spectrum look like for an \\nimage of the same size but having stripes that \\nare one pixel wide? Explain the reason for \\nyour answer\\n.\\n(d) \\nAre the dc terms in (a) and (c) the same, or \\nare they different? Explain.\\n4.22 \\nA high-technology company specializes in devel-\\noping imaging systems for digitizing images of \\ncommercial cloth.\\n The company has a new order \\nfor 1,000 systems for digitizing cloth consisting of \\nrepeating black and white vertical stripes, each \\nof width 2 cm. Optical and mechanical engineers \\nhave already designed the front-end optics and \\nmechanical positioning mechanisms so that you \\nare guaranteed that every image your system digi-\\ntizes starts with a complete black vertical stripe \\nand ends with a complete white stripe. Every \\nimage acquired will contain exactly 250 vertical \\nstripes. Noise and optical distortions are negligi-\\nble. Having learned of your success in taking an \\nimage processing course, the company employs \\nyou to specify the resolution of the imaging chip \\nto be used in the new system. The optics can be \\nadjusted to project the ﬁeld of view accurately \\nonto the area deﬁned by the size of the chip you \\nspecify. Your design will be implemented in hun-\\ndreds of locations, so cost is an important consid-\\neration. What resolution chip (in terms of number \\nof imaging elements per horizontal line) would \\nyou specify to avoid aliasing? \\n4.23 * \\nWe know from the discussion in Section 4.5 that \\nzooming or shrinking a digital image generally \\ncauses aliasing\\n. Give an example of an image that \\nwould be free of aliasing if it were zoomed by \\npixel replication.\\n4.24 \\nWith reference to the discussion on linearity in \\nSection 2.6,\\n demonstrate that\\n(a) * \\nThe 2-D continuous Fourier transform is a \\nlinear operator\\n.\\n(b) \\nThe 2-D DFT is a linear operator also.\\nDIP4E_GLOBAL_Print_Ready.indb   310\\n6/16/2017   2:06:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n311\\n4.25 \\nWith reference to Eqs. (4-59) and (4-60), show the \\nvalidity of the following translation (shift) prop-\\nerties of 2-D\\n, \\ncontinuous\\n Fourier transform pairs. \\n(\\nHint:\\n Study the solutions to Problem 4.11.)\\n(a) * \\nft ze\\nF\\njt z\\n(, )\\n( , )\\n()\\n2\\n00\\n00\\npm n\\nmm nn\\n+\\n⇔−−\\n(b) \\nft t z z F e\\njt z\\n(, ) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nmn\\npm n\\n4.26 \\nShow the validity of the following 2-D \\ncontinuous \\nF\\nourier transform pairs.\\n(a) * \\nd\\n(,\\n)\\ntz\\n⇔\\n1\\n(b) * \\n1\\n⇔\\ndmn\\n(,)\\n(c) * \\nd\\npm n\\n(, )\\n()\\ntt zz e\\njt z\\n−− ⇔\\n−+\\n00\\n2\\n00\\n(d) \\net z\\njt t z z\\n2\\n00\\n00\\np\\ndm n\\n()\\n(,)\\n+\\n⇔−−\\n(e) * \\ncos( )\\n22\\n00\\npm pn\\ntz\\n+⇔\\n() ( , )( , )\\n12\\n00 00\\ndm m n n dm m n n\\n−− + ++\\n[]\\n(f) \\nsin( )\\n22\\n00\\npm pn\\ntz\\n+⇔\\n() ( , ) ( , )\\n12\\n00 00\\nj\\ndm m n n dm m n n\\n−− − ++\\n[]\\n4.27 \\nWith reference to Eqs. (4-71) and (4-72), dem-\\nonstrate the validity of the following translation \\n(shifting) properties of 2-D\\n, \\ndiscrete\\n Fourier trans-\\nform pairs from Table 4.4. (\\nHint:\\n Study the solu-\\ntions to Problem 4.17.) \\n(a) \\nfx y e\\nFu u\\njx M y N\\n(,)\\n( , )\\n()\\n2\\n00\\n00\\np\\nuv\\nvv\\n+\\n⇔− −\\n(b) * \\nfx x y y F e\\njx M y N\\n(,) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nuv\\nuv\\np\\n4.28 \\nShow the validity of the following 2-D \\ndiscrete \\nF\\nourier transform pairs from Table 4.4: \\n(a) * \\nd\\n(,\\n)\\nxy\\n⇔\\n1\\n(b) * \\n1\\n⇔\\nMN\\nd\\n(,)\\nuv\\n(c) \\nd\\np\\n(,)\\n()\\nxx yy e\\nj u xM yN\\n−− ⇔\\n−+\\n00\\n2\\n00\\nv\\n(d) * \\neM\\nN\\nu\\nu\\nj uxM yN\\n2\\n00\\n0\\np\\nd\\n()\\n(,)\\n+\\n⇔− −\\nv\\n0\\nvv\\n(e) \\ncos(\\n)\\n22\\n00\\npm pn\\nxM yN\\n+⇔\\n() ( , ) ( , )\\nMN u u u\\n2\\n00 00\\ndm d\\n++ + − −\\n[]\\nvv vv\\n(f) * \\nsin(\\n)\\n22\\n00\\npm pn\\nxM yN\\n+⇔\\n() ( , ) ( , )\\njMN u u u\\n2\\n00 00\\ndm d\\n++ − − −\\n[]\\nvv vv\\n4.29 \\nYou are given a “canned” program that computes \\nthe 2-D\\n, DFT pair. However, it is not known \\nin which of the two equations the \\n1\\nMN\\n term \\nis included or if it was split as two constants\\n, \\n1\\nMN\\n,\\n in front of both the forward and inverse \\ntransforms\\n. How can you ﬁnd where the term(s) \\nis (are) included if this information is not avail-\\nable in the documentation?\\n4.30 \\nWhat is period and frequency of each of following \\ndigital sequences (\\nHint:\\n \\nThink of these as square \\nwaves.)\\n(a) * \\n0 1 0 1 0 1 0 1 . . .\\n(b) \\n0 0 1 0 0 1 0 0 1 . . . .\\n(c) \\n0 0 1 1 0 0 1 1 0 0 1 1 . . .\\n4.31 \\nWith reference to the 1-D sequences in Example \\n4.10:\\n(a) * \\nWhen \\nM\\n is even,\\n why is the point at \\nM\\n2\\n in \\nan even sequence always arbitrary?\\n(b) \\nWhen \\nM\\n is even,\\n why is the point at \\nM\\n2\\n in \\nan odd sequence always 0?\\n4.32 \\nWe mentioned in Example 4.10 that embedding a \\n2-D array of even (odd) dimensions into a larger \\narray of zeros of even (odd) dimensions keeps the \\nsymmetry of the original array\\n, provided that the \\ncenters coincide. Show that this is true also for \\nthe following 1-D arrays (i.e., show that the larger \\narrays have the same symmetry as the smaller \\narrays). For arrays of even length, use arrays of \\n0’s ten elements long. For arrays of odd lengths, \\nuse arrays of 0’s nine elements long.\\n(a) * \\na\\nbccb\\n,,,,\\n{}\\n(b) \\n00\\n,,, , ,\\n−−\\n{}\\nbc c b\\n(c) \\nabcdcb\\n,,,,,\\n{}\\n(d) \\n0, , , ,\\n−−\\n{}\\nb ccb\\n4.33 \\nIn Example 4.10 we showed a Sobel kernel \\nembedded in a ﬁeld of zeros\\n. The kernel is of size \\n33\\n×\\n and its structure appears to be odd. However, \\nits ﬁrst element is \\n−\\n1,\\n and we know that in order \\nto be odd,\\n the ﬁrst (top, left) element a 2-D array \\nmust be zero. Show the smallest ﬁeld of zeros in \\nwhich you can embed the Sobel kernel so that it \\nsatisﬁes the condition of oddness.\\n4.34 \\nDo the following:\\n(a) * \\nShow that the \\n66\\n×\\n array in Example 4.10 is \\nodd.\\n(b) \\nWhat would happen if the minus signs are \\nchanged to pluses?\\n(c) \\nExplain why, as stated at the end of the exam-\\nple\\n, adding to the array another row of 0’s on \\nthe top and column of 0’s to the left would \\ngive a result that is neither even nor odd.\\n(d) \\nSuppose that the row is added to the bot-\\nDIP4E_GLOBAL_Print_Ready.indb   311\\n6/16/2017   2:06:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 312}),\n",
       " Document(page_content='312\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ntom and the column to the right? Would that \\nchange your answer in (c)?\\n4.35 \\nThe following problems are related to the proper-\\nties in \\nTable 4.1.\\n(a) * \\nDemonstrate the validity of property 2.\\n(b) * \\nDemonstrate the validity of property 4.\\n(c) \\nDemonstrate the validity of property 5.\\n(d) * \\nDemonstrate the validity of property 7.\\n(e) \\nDemonstrate the validity of property 9.\\n4.36 \\nYou know from Table 4.3 that the dc term, \\nF\\n(,\\n) ,\\n00\\n \\nof a DFT is proportional to the average value of \\nits corresponding spatial image\\n. Assume that the \\nimage is of size \\nMN\\n×\\n.\\n Suppose that you pad the \\nimage with zeros to size \\nPQ\\n×\\n,\\n where \\nP\\n and \\nQ\\n \\nare given in Eqs. (4-102) and (4-103). Let \\nF\\np\\n(,)\\n00\\n \\ndenote the dc term of the DFT of the padded \\nfunction.\\n(a) * \\nWhat is the ratio of the average values of the \\noriginal and padded images?\\n(b) \\nIs \\nFF\\np\\n(,) (,) ?\\n00 00\\n=\\n Support your answer \\nmathematically\\n.\\n4.37 \\nDemonstrate the validity of the periodicity prop-\\nerties (entry 8) in \\nTable 4.3. \\n4.38 \\nWith reference to the 2-D discrete convolution \\ntheorem in Eqs\\n. (4-95) and (4-96) (entry 6 in \\nTable 4.4), show that\\n(a) \\n \\n(( )\\n(\\n,\\n)\\n)( , )\\nfF\\nH\\nhx y\\n/H22841\\n⇔\\ni\\nuv\\n(b) * \\n \\n() ( , ) ( ) ( ) ( , )\\nfhx y M N F H\\ni\\n⇔\\n[]\\n1\\n/H22841\\nuv\\n(\\nHint:\\n Study the solution to Problem 4.18.)\\n4.39 \\nWith reference to the 2-D discrete correlation \\ntheorem (entry 7 in \\nTable 4.4), show that\\n(a) * \\n \\n(( )\\n(\\n,\\n)\\n)( , )\\n*\\nfF H\\nhx y\\n/H22845\\n⇔\\ni\\nuv\\n(b) \\n \\n() ( , ) ( ) ( ) ( , )\\n*\\nfhx y M N F H\\ni\\n⇔\\n[]\\n1\\n/H22845\\nuv\\n4.40 * \\nDemonstrate validity of the differentiation pairs \\nin entry 12 of \\nTable 4.4. \\n4.41 \\nWe discussed in Section 4.6 the need for image \\npadding when ﬁltering in the frequenc\\ny domain. \\nWe showed in that section that images could be \\npadded by appending zeros to the ends of rows \\nand columns in the image (see the following \\nimage, on the left). Do you think it would make a \\ndifference if we centered the image and surround-\\ned it by a border of zeros instead (see image on \\nthe right), but without changing the total number \\nof zeros used? Explain.\\n4.42 * \\nThe two Fourier spectra shown are of the same \\nimage\\n. The spectrum on the left corresponds to \\nthe original image, and the spectrum on the right \\nwas obtained after the image was padded with \\nzeros. Explain the signiﬁcant increase in signal \\nstrength along the vertical and horizontal axes of \\nthe spectrum shown on the right.\\n4.43 \\nConsider the images shown. The image on the \\nright was obtained by:\\n (a) multiplying the image \\non the left by \\n() ;\\n−\\n+\\n1\\nxy\\n (b) computing the DFT; (c) \\ntaking the complex conjugate of the transform; \\n(d) computing the inverse DFT; and (e) multiply-\\ning the real part of the result by \\n() .\\n−\\n+\\n1\\nxy\\n Explain \\n(mathematically) why the image on the right \\nappears as it does.\\nDIP4E_GLOBAL_Print_Ready.indb   312\\n6/16/2017   2:06:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 313}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n313\\n4.44 * \\nThe image in Fig. 4.34(b) was obtained by mul-\\ntiplying by \\n−\\n1\\n the phase angle of the image in \\nF\\nig. 4.34(a), and then computing the IDFT. With \\nreference to Eq. (4-86) and entry 5 in Table 4.1, \\nexplain why this operation caused the image to be \\nreﬂected about both coordinate axes.\\n4.45 \\nIn Fig. 4.34(b) we saw that multiplying the phase \\nangle by \\n−\\n1\\n ﬂipped the image with respect to both \\ncoordinate axes\\n. \\nSuppose that instead we multi-\\nplied the magnitude of the transform by \\n−\\n1\\n and \\nthen took the inverse DFT using the equation:\\n \\ngxy F e\\nj\\n(,) (,) .\\n(,)\\n=−\\n{}\\n−\\n/H5219\\n1\\nuv\\nuv\\nf\\n(a) * \\nWhat would be the difference between the \\ntwo images \\ngxy\\n(,\\n)\\n and \\nfx y\\n(,\\n) ?\\n [Remember, \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) . ]\\n(b) \\nAssuming that they are both 8-bit images, \\nwhat would \\ngxy\\n(,\\n)\\n look like in terms of \\nfx y\\n(,\\n)\\n if we scaled the intensity values of \\ngxy\\n(,\\n)\\n using Eqs. (2-31) and (2-32), with \\nK\\n=\\n255\\n?\\n4.46 \\nWhat is the source of the nearly periodic bright \\nspots on the horizontal axis of F\\nig. 4.40(b)?\\n4.47 * \\nConsider a \\n33\\n×\\n spatial kernel that averages \\nthe four closest neighbors of a point \\n(,) ,\\nxy\\n but \\nexcludes the point itself from the average\\n.\\n(a) \\nFind the equivalent ﬁlter transfer function, \\nH\\n(,\\n) ,\\nuv\\n in the frequency domain.\\n(b) \\nShow that your result is a lowpass ﬁlter trans-\\nfer function.\\n4.48 * \\nA continuous Gaussian lowpass ﬁlter in the con-\\ntinuous frequenc\\ny domain has the transfer func-\\ntion\\nHA e\\n(,)\\n()\\nmn\\nmn s\\n=\\n−+\\n22 2\\n2\\nShow that the corresponding ﬁlter kernel in the \\ncontinuous spatial domain is\\nhtz A e\\ntz\\n(, )\\n()\\n=\\n−+\\n2\\n22\\n22 2 2\\nps\\nps\\n4.49 \\nGiven an image of size \\nMN\\n×\\n,\\n you are asked to \\nperform an experiment that consists of repeat-\\nedly lowpass ﬁltering the image in the frequenc\\ny \\ndomain using a Gaussian lowpass ﬁlter transfer \\nfunction with a cutoff frequency, \\nD\\n0\\n.\\n You may \\nignore computational round-off errors\\n. \\n(a) * \\nLet \\nK\\n denote the number of applications of \\nthe ﬁlter\\n. Can you predict (without doing the \\nexperiment) what the result (image) will be \\nfor a sufﬁciently large value of \\nK\\n? If so, what \\nis that result?\\n(b) \\nLet \\nc\\nmin\\n denote the smallest positive num-\\nber representable in the machine in which \\nthe proposed experiment will be conducted \\n(any number \\n<\\nc\\nmin\\n is automatically set to 0). \\nDerive an expression (in terms of \\nc\\nmin\\n)\\n for \\nthe minimum value of \\nK\\n that will guarantee \\nthe result that you predicted in (a).\\n \\n4.50 \\nAs explained in Section 3.6, ﬁrst-order deriva-\\ntives can be approximated by the spatial differ\\n-\\nences \\ngf x y x f x y f x y\\nx\\n=∂ ∂ = + −\\n(, ) ( , ) (, )\\n1\\n and \\ng f xy y f xy f xy\\ny\\n=∂ ∂ = + −\\n(, ) (, ) (, ) .\\n1\\n(a) \\nFind the equivalent ﬁlter transfer func-\\ntions \\nH\\nx\\n(,)\\nuv\\n and \\nH\\ny\\n(,)\\nuv\\n in the frequency \\ndomain.\\n(b) \\nShow that these are highpass ﬁlter transfer \\nfunctions\\n.\\n(\\nHint:\\n Study the solution to Problem 4.47.)\\n4.51 \\nFind the equivalent frequency-domain ﬁlter \\ntransfer function for the Laplacian kernel shown \\nin F\\nig. 3.45(a). Show that your result behaves as a \\nhighpass ﬁlter transfer function. (\\nHint:\\n Study the \\nsolution to Problem 4.47.)\\n4.52 \\nDo the following:\\n(a) \\nShow that the Laplacian of a continuous \\nfunction \\nft z\\n(,\\n)\\n of two continuous variables, \\nt\\n and \\nz\\n,\\n satisﬁes the following Fourier trans-\\nform pair:\\n/H11612\\n22\\n2\\n2\\n4\\nft z F\\n(, ) ( ) ( , )\\n⇔− +\\npm n m n\\n(\\nHint:\\n See Eq.\\n (3-50) and study entry 12 in \\nTable 4.4.)\\n(b) * \\nThe result in (a) is valid only for continuous \\nvariables\\n. How would you implement the \\ncontinuous frequency domain transfer func-\\ntion \\nH\\n(,\\n) ( )\\nmn p m n\\n=− +\\n4\\n22 2\\n for discrete \\nvariables?\\n(c) \\nAs you saw in Example 4.21, the Laplacian \\nresult in the frequenc\\ny domain was similar to \\nthe result in Fig. 3.46(d), which was obtained \\nusing a spatial kernel with a center coefﬁ-\\ncient equal to \\n−\\n8.\\n Explain why the frequency \\ndomain result was not similar instead to the \\nDIP4E_GLOBAL_Print_Ready.indb   313\\n6/16/2017   2:06:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='314\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nresult in Fig. 3.46(c), which was obtained \\nusing a kernel with a center coefﬁcient of \\n−\\n4.\\n4.53 * \\nCan you think of a way to use the Fourier trans-\\nform to compute (or partially compute) the \\nmagnitude of the gradient [Eq.\\n (3-58)] for use in \\nimage differentiation? If your answer is yes, give \\na method to do it. If your answer is no, explain \\nwhy.\\n4.54 \\nAs explained in Eq. (4-118), it is possible to obtain \\nthe transfer function of a highpass ﬁlter from the \\ntransfer function of a lowpass ﬁlter by subtract-\\ning the latter from 1.\\n What is the highpass spatial \\nkernel corresponding to the lowpass Gaussian \\ntransfer function given in Problem 4.48?\\n4.55 \\nEach spatial highpass kernel in Fig. \\n4.52\\n has a \\nstrong spike in the center. Explain the source of \\nthis spikes.\\n4.56 * \\nShow how the Butterworth highpass ﬁlter trans-\\nfer function in Eq.\\n (4-121) follows from its low-\\npass counterpart in Eq. (4-117).\\n4.57 \\nConsider the hand X-ray images shown below. \\nT\\nhe image on the right was obtained by lowpass\\n(Original image courtesy of Dr. Thomas R. Gest, Division \\nof Anatomical Sciences, University of Michigan Medical \\nSchool.) \\nﬁltering the image on the left with a Gaussian \\nlowpass ﬁlter, and then highpass ﬁltering the \\nresult with a Gaussian highpass ﬁlter. The images \\nare of size \\n420 344\\n×\\n pixels and \\nD\\n0\\n25\\n=\\n was used \\nfor both ﬁlter transfer functions\\n.\\n(a) * \\nExplain why the center part of the ﬁnger ring \\nin the ﬁgure on the right appears so bright \\nand solid,\\n considering that the dominant \\ncharacteristic of the ﬁltered image consists \\nof edges of the ﬁngers and wrist bones, with \\ndarker areas in between. In other words, \\nwould you not expect the highpass ﬁlter to \\nrender the constant area inside the ring as \\ndark, since a highpass ﬁlter eliminates the dc \\nterm and reduces low frequencies?\\n(b) \\n Do you think the result would have been dif-\\nferent if the order of the ﬁltering process had \\nbeen reversed?\\n4.58 \\nConsider the sequence of images shown below. \\nT\\nhe image on the top left is a segment of an X-ray \\nimage of a commercial printed circuit board. The \\nimages following it are, respectively, the results of \\nsubjecting the image to 1, 10, and 100 passes of a \\nGaussian highpass ﬁlter with \\nD\\n0\\n30\\n=\\n.\\n The images \\nare of size \\n330 334\\n×\\n pixels, with each pixel being \\nrepresented by 8 bits of gray\\n. The images were \\nscaled for display, but this has no effect on the \\nproblem statement.\\n(Original image courtesy of Mr. Joseph E. Pascente, Lixi, Inc.)\\n(a) \\nIt appears from the images that changes will \\ncease to take place after a ﬁnite number of \\npasses\\n. Show whether or not this is the case. \\nYou may ignore computational round-off \\nerrors. Let \\nc\\nmin\\n denote the smallest positive \\nnumber representable in the machine in \\nwhich the computations are conducted. \\n(b) \\nIf you determined in (a) that changes would \\ncease after a ﬁnite number of iterations\\n, \\ndetermine the minimum value of that num-\\nber.\\n(\\nHint: \\nStudy the solution to Problem 4.49.)\\n4.59 \\nAs illustrated in Fig. 4.57, combining high-fre-\\nquenc\\ny emphasis and histogram equalization is \\nDIP4E_GLOBAL_Print_Ready.indb   314\\n6/16/2017   2:06:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 315}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n315\\nan effective method for achieving edge sharpen-\\ning and contrast enhancement.\\n(a) * \\nShow whether or not it matters which pro-\\ncess is applied ﬁrst.\\n(b) \\nIf the order does matter, give a rationale for \\nusing one or the other method ﬁrst.\\n4.60 \\nUse a Butterworth highpass ﬁlter to construct a \\nhomomorphic ﬁlter transfer function that has the \\nsame general shape as the function in F\\nig. 4.59.\\n4.61 \\nSuppose that you are given a set of images gener-\\nated by an experiment dealing with the analysis of \\nstellar events\\n. Each image contains a set of bright, \\nwidely scattered dots corresponding to stars in \\na sparsely occupied region of the universe. The \\nproblem is that the stars are barely visible as a \\nresult of superimposed illumination from atmo-\\nspheric dispersion. If these images are modeled as \\nthe product of a constant illumination component \\nwith a set of impulses, give an enhancement pro-\\ncedure based on homomorphic ﬁltering designed \\nto bring out the image components due to the \\nstars themselves.\\n4.62 \\nHow would you generate an image of only the \\ninterference pattern visible in F\\nig. 4.64(a)?\\n4.63 * \\nShow the validity of Eqs. (4-171) and (4-172). \\n(\\nHint:\\n \\nUse proof by induction.)\\n4.64 \\nA skilled medical technician is assigned the job of \\ninspecting a set of images generated by an elec-\\ntron microscope experiment.\\n In order to simplify \\nthe inspection task, the technician decides to use \\ndigital image enhancement and, to this end, exam-\\nines a set of representative images and ﬁnds the \\nfollowing problems: (1) bright, isolated dots that \\nare of no interest; (2) lack of sharpness; (3) not \\nenough contrast in some images; and (4) shifts \\nin the average intensity to values other than \\nA\\n0\\n, \\nwhich is the average value required to perform \\ncorrectly certain intensity measurements\\n. The \\ntechnician wants to correct these problems and \\nthen display in white all intensities in a band \\nbetween intensities \\nI\\n1\\n and \\nI\\n2\\n,\\n while keeping nor-\\nmal tonality in the remaining intensities\\n. Propose \\na sequence of processing steps that the technician \\ncan follow to achieve the desired goal. You may \\nuse techniques from both Chapters 3 and 4.\\nDIP4E_GLOBAL_Print_Ready.indb   315\\n6/16/2017   2:06:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 316}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 317}),\n",
       " Document(page_content='3175\\nImage Restoration \\nand Reconstruction\\nPreview\\nAs in image enhancement, the principal goal of restoration techniques is to improve an image in some \\npredeﬁned sense. Although there are areas of overlap, image enhancement is largely a subjective pro-\\ncess, while image restoration is for the most part an objective process. Restoration attempts to recover \\nan image that has been degraded by using a priori knowledge of the degradation phenomenon. Thus, \\nrestoration techniques are oriented toward modeling the degradation and applying the inverse process \\nin order to recover the original image. In this chapter, we consider linear, space invariant restoration \\nmodels that are applicable in a variety of restoration situations. We also discuss fundamental tech-\\nniques of image reconstruction from projections, and their application to computed tomography (CT), \\none of the most important commercial applications of image processing, especially in health care.\\nUpon completion of this chapter, readers should:\\n Be familiar with the characteristics of various \\nnoise models used in image processing, and \\nhow to estimate from image data the param-\\neters that deﬁne those models.\\n Be familiar with linear, nonlinear, and adap-\\ntive spatial ﬁlters used to restore (denoise) \\nimages that have been degraded only by noise.\\n Know how to apply notch ﬁltering in the fre-\\nquency domain for removing periodic noise \\nin an image.\\n Understand the foundation of linear, space \\ninvariant system concepts, and how they can \\nbe applied in formulating image restoration \\nsolutions in the frequency domain.\\n Be familiar with direct inverse ﬁltering and its \\nlimitations.\\n Understand minimum mean-square-error (Wie-\\nner) ﬁltering and its advantages over direct \\ninverse ﬁltering.\\n Understand constrained, least-squares ﬁlter-\\ning.\\n Be familiar with the fundamentals of image \\nreconstruction from projections, and their \\napplication to computed tomography.\\nThings which we see are not themselves what we see . . .  \\nIt remains completely unknown to us what the objects may be \\nby themselves and apart from the receptivity of our senses. \\nWe know only but our manner of perceiving them.\\nImmanuel Kant\\nDIP4E_GLOBAL_Print_Ready.indb   317\\n6/16/2017   2:06:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 318}),\n",
       " Document(page_content='318\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n5.1 A MODEL OF THE IMAGE DEGRADATION/RESTORATION  \\nPROCESS  \\nIn this chapter, we model image degradation as an operator \\n/H5108\\n that, together with an \\nadditive noise term,\\n operates on an input image \\nfx y\\n(,\\n)\\n to produce a degraded image \\ngxy\\n(,\\n)\\n (see Fig. 5.1). Given \\ngxy\\n(,\\n) ,\\n some knowledge about \\n/H5108\\n,\\n and some knowledge \\nabout the additive noise term \\nh\\n(,\\n) ,\\nxy\\n the objective of restoration is to obtain an \\nestimate \\nˆ\\n(,\\n)\\nfx y\\n of the original image. We want the estimate to be as close as possible \\nto the original image and,\\n in general, the more we know about \\n/H5108\\n and \\nh\\n,\\n the closer \\nˆ\\n(,\\n)\\nfx y\\n will be to \\nfx y\\n(,\\n) .\\n \\nW\\ne will show in Section 5.5 that, if \\n/H5108\\n is a linear, position-invariant operator, then \\nthe degraded image is given in the spatial domain by\\n \\ngxy h f xy xy\\n(,\\n) ( ) (,) (,)\\n=+\\n/H22841\\nh\\n \\n(5-1)\\nwhere \\nhxy\\n(,\\n)\\n is the spatial representation of the degradation function. As in Chapters \\n3 and 4,\\n the symbol “\\n/H22841\\n” indicates convolution. It follows from the convolution theorem \\nthat the equivalent of Eq. (5-1) in the frequency domain is\\n \\nGH F N\\n(,\\n) (,) (,) (,)\\nuv uv uv uv\\n=+\\n \\n(5-2)\\nwhere the terms in capital letters are the Fourier transforms of the corresponding \\nterms in Eq.\\n (5-1). These two equations are the foundation for most of the restora-\\ntion material in this chapter.\\nIn the following three sections, we work only with degradations caused by noise. \\nBeginning in Section 5.5 we look at several methods for image restoration in the \\npresence of both \\n/H5108\\n and \\nh\\n.\\n5.2 NOISE MODELS  \\nThe principal sources of noise in digital images arise during image acquisition and/or \\ntransmission. The performance of imaging sensors is affected by a variety of environ-\\nmental factors during image acquisition, and by the quality of the sensing elements \\nthemselves. For instance, in acquiring images with a CCD camera, light levels and \\nsensor temperature are major factors affecting the amount of noise in the resulting \\nimage. Images are corrupted during transmission principally by interference in the \\ntransmission channel. For example, an image transmitted using a wireless network \\nmight be corrupted by lightning or other atmospheric disturbance. \\n5.1\\n5.2\\nDegradation\\nDEGRADATION\\nRESTORATION\\nRestoration\\nfilter(s)\\nf\\n(\\nx\\n,\\ny\\n)\\ng\\n(\\nx\\n,\\ny\\n)\\nf\\n(\\nx\\n,\\ny\\n)\\nˆ\\nNoise\\nh\\n(\\nx\\n,\\ny\\n)\\n/H11001\\n/H5108\\nFIGURE 5.1\\nA model of the \\nimage  \\ndegradation/ \\nrestoration  \\nprocess. \\nDIP4E_GLOBAL_Print_Ready.indb   318\\n6/16/2017   2:06:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 319}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n319\\nSPATIAL AND FREQUENCY PROPERTIES OF NOISE\\nRelevant to our discussion are parameters that define the spatial characteristics of \\nnoise, and whether the noise is correlated with the image. Frequency properties refer \\nto the frequency content of noise in the Fourier (frequency) domain discussed in \\ndetail in Chapter 4. For example, when the Fourier spectrum of noise is constant, the \\nnoise is called \\nwhite noise\\n. This terminology is a carryover from the physical prop-\\nerties of white light, which contains all frequencies in the visible spectrum in equal \\nproportions.\\nWith the exception of spatially periodic noise, we assume in this chapter that \\nnoise is independent of spatial coordinates, and that it is uncorrelated with respect \\nto the image itself (that is, there is no correlation between pixel values and the values \\nof noise components). Although these assumptions are at least partially invalid in \\nsome applications (quantum-limited imaging, such as in X-ray and nuclear-medicine \\nimaging, is a good example), the complexities of dealing with spatially dependent \\nand correlated noise are beyond the scope of our discussion.\\nSOME IMPORTANT NOISE PROBABILITY DENSITY FUNCTIONS\\nIn the discussion that follows, we shall be concerned with the statistical behavior of \\nthe intensity values in the noise component of the model in Fig. 5.1. These may be \\nconsidered random variables, characterized by a probability density function (PDF), \\nas noted briefly as noted earlier. The noise component of the model in Fig. 5.1 is an \\nimage, \\nh\\n(,\\n) ,\\nxy\\n of the same size as the input image. We create a noise image for simu-\\nlation purposes by generating an array whose intensity values are random numbers \\nwith a specified probability density function.\\n This approach is true for all the PDFs \\nto be discussed shortly, with the exception of salt-and-pepper noise, which is applied \\ndifferently. The following are among the most common noise PDFs found in image \\nprocessing applications.\\nGaussian Noise\\nBecause of its mathematical tractability in both the spatial and frequency domains, \\nGaussian noise models are used frequently in practice. In fact, this tractability is so \\nconvenient that it often results in Gaussian models being used in situations in which \\nthey are marginally applicable at best. \\nThe PDF of a \\nGaussian\\n random variable, \\nz\\n, is deﬁned by the following familiar \\nexpression:\\n \\npz e\\nz\\nzz\\n()\\n()\\n=−\\n−\\n−\\n1\\n2\\n2\\n2\\n2\\nps\\ns\\n/H11009/H11009\\n<<\\n \\n(5-3)\\nwhere \\nz\\n represents intensity\\n, \\nz\\n is the mean (average) value of \\nz\\n,\\n and \\ns\\n is its standard \\ndeviation.\\n Figure 5.2(a) shows a plot of this function. We know that for a Gaussian \\nrandom variable, the probability that values of \\nz\\n are in the range \\nz\\n±\\ns\\n is approxi-\\nmately 0.68;\\n the probability is about 0.95 that the values of \\nz\\n are in the range \\nz\\n±\\n2\\ns\\n.\\nYou may ﬁnd it helpful \\nto take a look at the \\nTutorials section of the \\nbook website for a brief \\nreview of probability.\\nDIP4E_GLOBAL_Print_Ready.indb   319\\n6/16/2017   2:06:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 320}),\n",
       " Document(page_content='320\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nRayleigh Noise\\nThe PDF of \\nRayleigh\\n noise is given by\\n \\npz\\nb\\nza e za\\nza\\nzab\\n()\\n()\\n()\\n=\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−−\\n2\\n0\\n2\\n≥\\n<\\n \\n(5-4)\\nThe mean and variance of \\nz\\n when this random variable is characterized by a Ray-\\nleigh PDF are\\n \\nza b\\n=+\\np\\n4  \\n(5-5)\\nand\\n \\ns\\np\\n2\\n4\\n4\\n=\\n−\\n()\\nb\\n \\n(5-6)\\nFigure 5.2(b) shows a plot of the Rayleigh density. Note the displacement from the \\norigin,\\n and the fact that the basic shape of the density is skewed to the right. The \\nRayleigh density can be quite useful for modeling the shape of skewed histograms.\\nz\\nRayleigh\\np\\n(\\nz\\n)\\nK\\nErlang (Gamma)\\nz\\n(\\nb\\n \\n/H11002\\n 1)\\n/\\na\\nz\\np\\n(\\nz\\n)\\nz\\na\\nExponential\\np\\n(\\nz\\n)\\nP\\np\\nSalt-and-\\npepper\\np\\n(\\nz\\n)\\n1\\n2\\nps\\n0.607\\n2\\nps\\n_\\nz\\n/H11002\\n \\ns\\n_\\nz\\n/H11001\\n \\ns\\n_\\nz\\np\\n(\\nz\\n)\\n2\\nb\\n0.607\\nz\\na\\nb\\n2\\na\\n \\n/H11001\\na\\n(\\nb\\n \\n/H11002\\n 1)\\nb\\n/H11002\\n1\\n(\\nb\\n \\n/H11002\\n 1)!\\nK\\n \\n/H11005\\ne\\n/H11002\\n(\\nb\\n/H11002\\n1)\\nUniform\\nz\\nab\\np\\n(\\nz\\n)\\n1\\nb\\n \\n/H11002\\n \\na\\nGaussian\\nP\\ns\\n0\\n21\\nk\\n−\\nV\\n1( )\\nsp\\nPP\\n−+\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.2\\n Some important probability density functions.\\nDIP4E_GLOBAL_Print_Ready.indb   320\\n6/16/2017   2:06:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 321}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n321\\nErlang (Gamma) Noise\\nThe PDF of Erlang noise is\\n \\npz\\naz\\nb\\nez\\nz\\nbb\\naz\\n()\\n() !\\n=\\n−\\n<\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−\\n−\\n1\\n1\\n0\\n00\\n≥\\n \\n(5-7)\\nwhere the parameters are such that \\nab\\n>\\n, \\nb\\n is a positive integer\\n, and “!” indicates \\nfactorial. The mean and variance of \\nz\\n are\\n \\nz\\nb\\na\\n=\\n \\n(5-8)\\nand\\n \\ns\\n2\\n2\\n=\\nb\\na\\n \\n(5-9)\\nFigure 5.2(c) shows a plot of this density. Although Eq. (5-9) often is referred to as \\nthe \\ngamma\\n density\\n, strictly speaking this is correct only when the denominator is \\nthe gamma function, \\n/H9003\\n()\\n.\\nb\\n When the denominator is as shown, the density is more \\nappropriately called the \\nErlang\\n density\\n.\\nExponential Noise\\nThe PDF of \\nexponential\\n noise is given by \\n \\npz\\nae\\nz\\nz\\naz\\n()\\n=\\n<\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−\\n≥\\n0\\n00\\n \\n(5-10)\\nwhere \\na\\n>\\n0. The mean and variance of \\nz\\n are\\n \\nz\\na\\n=\\n1\\n \\n(5-11)\\nand\\n \\ns\\n2\\n2\\n1\\n=\\na\\n \\n(5-12)\\nNote that this PDF is a special case of the Erlang PDF with \\nb\\n=\\n1.\\n Figure 5.2(d) \\nshows a plot of the exponential density function.\\nUniform Noise\\nThe PDF of \\nuniform\\n noise is\\n \\npz\\nba\\nazb\\n()\\n=\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n0\\n≤≤\\notherwise\\n \\n(5-13)\\nDIP4E_GLOBAL_Print_Ready.indb   321\\n6/16/2017   2:07:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 322}),\n",
       " Document(page_content='322\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nThe mean and variance of \\nz\\n are\\n \\nz\\nab\\n=\\n+\\n2\\n \\n(5-14)\\nand\\n \\ns\\n2\\n2\\n12\\n=\\n−\\n()\\nba\\n \\n(5-15)\\nFigure 5.2(e) shows a plot of the uniform density.\\nSalt-and-Pepper Noise\\nIf \\nk\\n represents the number of bits used to represent the intensity values in a digital \\nimage, then the range of possible intensity values for that image is \\n[, ]\\n02 1\\nk\\n−\\n (e.g., \\n[, ]\\n0 255\\n for an 8-bit image). The PDF of \\nsalt-and-pepper\\n noise is given by\\n \\npz\\nPz\\nPz\\nPP\\nz V\\ns\\nk\\np\\nsp\\n()\\n()\\n=\\n=−\\n=\\n−+ =\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\nfor \\nfor \\nfor \\n21\\n0\\n1\\n \\n(5-16)\\nwhere \\nV\\n is any integer value in the range \\n02 1\\n<<\\n−\\nV\\nk\\n.\\nLet \\nh\\n(,\\n)\\nxy\\n denote a salt-and-pepper noise image, whose intensity values satisfy \\nEq.\\n (5-16). Given an image, \\nfx y\\n(,\\n) ,\\n of the same size as \\nh\\n(,\\n) ,\\nxy\\n we corrupt it with salt-\\nand-pepper noise by assigning a 0 to all locations in \\nf\\n where a 0 occurs in \\nh\\n.\\n Similarly, \\nwe assign a value of \\n21\\nk\\n−\\n to all location in \\nf\\n where that value appears in \\nh\\n.\\n Finally, \\nwe leave unchanged all location in \\nf\\n where \\nV\\n occurs in \\nh\\n.\\nIf neither \\nP\\ns\\n nor \\nP\\np\\n is zero, and especially if they are equal, noise values satisfy-\\ning Eq. (5-16) will be white \\n()\\n21\\nk\\n−\\n or black (0), and will resemble salt and pepper \\ngranules distributed randomly over the image;\\n hence the name of this type of noise. \\nOther names you will ﬁnd used in the literature are \\nbipolar impulse noise\\n (\\nunipolar\\n \\nif either \\nP\\ns\\n or \\nP\\np\\n is 0), \\ndata-drop-out noise\\n, and \\nspike noise\\n. We use the terms impulse \\nand salt-and-pepper noise interchangeably. \\nThe probability, \\nP\\n, that a pixel is corrupted by salt or pepper noise is \\nP\\nPP\\nsp\\n=+\\n. \\nIt is common terminology to refer to \\nP\\n as the \\nnoise density\\n.\\n If, for example, \\nP\\ns\\n=\\n00 2\\n.  \\nand \\nP\\np\\n=\\n00 1\\n.,\\n then \\nP\\n=\\n00\\n3\\n.\\n and we say that approximately 2% of the pixels in an \\nimage are corrupted by salt noise\\n, 1% are corrupted by pepper noise, and the noise \\ndensity is 3%, meaning that approximately 3% of the pixels in the image are cor-\\nrupted by salt-and-pepper noise.\\nAlthough, as you have seen, salt-and-pepper noise is speciﬁed by the probability \\nof each, and not by the mean and variance, we include the latter here for complete-\\nness. The mean of salt-and-pepper noise is given by\\n \\nzP K P P P\\nps p\\nk\\ns\\n=+ − − + −\\n() ( ) ( )\\n01 2 1\\n \\n(5-17)\\nand the variance by\\nWhen image intensities \\nare scaled to the range \\n[0, 1], we replace by 1 the \\nvalue of salt in this equa-\\ntion. \\nV\\n then becomes a \\nfractional value in the \\nopen interval (0, 1). \\nDIP4E_GLOBAL_Print_Ready.indb   322\\n6/16/2017   2:07:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 323}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n323\\n \\ns\\n22 2\\n2\\n01 2\\n1\\n=− + − −− + −\\n() ( ) ( ) ( )\\nzP K z P P P\\nps\\np\\nk\\ns\\n \\n(5-18)\\nwhere we have included 0 as a value explicit in both equations to indicate that the \\nvalue of pepper noise is assumed to be zero\\n.\\nAs a group, the preceding PDFs provide useful tools for modeling a broad range \\nof noise corruption situations found in practice. For example, Gaussian noise arises \\nin an image due to factors such as electronic circuit noise and sensor noise caused by \\npoor illumination and/or high temperature. The Rayleigh density is helpful in char-\\nacterizing noise phenomena in range imaging. The exponential and gamma densities \\nﬁnd application in laser imaging. Impulse noise is found in situations where quick \\ntransients, such as faulty switching, take place during imaging. The uniform density \\nis perhaps the least descriptive of practical situations. However, the uniform density \\nis quite useful as the basis for numerous random number generators that are used \\nextensively in simulations (Gonzalez, Woods, and Eddins [2009]).\\nEXAMPLE 5.1 :  Noisy images and their histograms.\\nFigure 5.3 shows a test pattern used for illustrating the noise models just discussed. This is a suitable pat-\\ntern to use because it is composed of simple, constant areas that span the gray scale from black to near \\nwhite in only three increments. This facilitates visual analysis of the characteristics of the various noise \\ncomponents added to an image. \\nFigure 5.4 shows the test pattern after addition of the six types of noise in Fig. 5.2. Below each image \\nis the histogram computed directly from that image. The parameters of the noise were chosen in each \\ncase so that the histogram corresponding to the three intensity levels in the test pattern would start to \\nmerge. This made the noise quite visible, without obscuring the basic structure of the underlying image.\\nWe see a close correspondence in comparing the histograms in Fig. 5.4 with the PDFs in Fig. 5.2. \\nThe histogram for the salt-and-pepper example does not contain a speciﬁc peak for \\nV\\n because, as you \\nwill recall, \\nV\\n is used only during the creation of the noise image to leave values in the original image \\nunchanged. Of course, in addition to the salt and pepper peaks, there are peaks for the other intensi-\\nties in the image. With the exception of slightly different overall intensity, it is difﬁcult to differentiate \\nFIGURE 5.3\\nTest pattern used \\nto illustrate the \\ncharacteristics of \\nthe PDFs from \\nFig. 5.2.\\nDIP4E_GLOBAL_Print_Ready.indb   323\\n6/16/2017   2:07:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 324}),\n",
       " Document(page_content='324\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.4\\n Images and histograms resulting from adding Gaussian, Rayleigh, and Erlanga noise to the image in \\nFig. 5.3.\\nvisually between the ﬁrst ﬁve images in Fig. 5.4, even though their histograms are signiﬁcantly different. \\nThe salt-and-pepper appearance of the image in Fig. 5.4(i) is the only one that is visually indicative of \\nthe type of noise causing the degradation.\\nPERIODIC NOISE\\nPeriodic noise in images typically arises from electrical or electromechanical inter-\\nference during image acquisition. This is the only type of spatially dependent noise \\nwe will consider in this chapter. As we will discuss in Section 5.4, periodic noise can \\nbe reduced significantly via frequency domain filtering. For example, consider the \\nimage in Fig. 5.5(a). This image is corrupted by additive (spatial) sinusoidal noise. \\nThe Fourier transform of a pure sinusoid is a pair of conjugate impulses\\n†\\n located at \\n†  Be careful not to confuse the term \\nimpulse\\n in the frequency domain with the use of the same term in impulse \\nnoise discussed earlier, which is in the spatial domain.\\nDIP4E_GLOBAL_Print_Ready.indb   324\\n6/16/2017   2:07:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 325}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n325\\nh\\ng\\ni\\nk\\nl\\nj\\nFIGURE 5.4 \\n(\\ncontinued\\n) Images and histograms resulting from adding exponential, uniform, and salt-and-pepper noise \\nto the image in Fig. 5.3. In the salt-and-pepper histogram, the peaks in the origin (zero intensity) and at the far end \\nof the scale are shown displaced slightly so that they do not blend with the page background.\\nthe conjugate frequencies of the sine wave (see Table 4.4). Thus, if the amplitude of \\na sine wave in the spatial domain is strong enough, we would expect to see in the \\nspectrum of the image a pair of impulses for each sine wave in the image. As shown \\nin Fig. 5.5(b), this is indeed the case. Eliminating or reducing these impulses in the \\nfrequency domain will eliminate or reduce the sinusoidal noise in the spatial domain. \\nWe will have much more to say in Section 5.4 about this and other examples of peri-\\nodic noise.\\nESTIMATING NOISE PARAMETERS\\nThe parameters of periodic noise typically are estimated by inspection of the Fourier \\nspectrum. Periodic noise tends to produce frequency spikes that often can be detect-\\ned even by visual analysis. Another approach is to attempt to infer the periodicity \\nDIP4E_GLOBAL_Print_Ready.indb   325\\n6/16/2017   2:07:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 326}),\n",
       " Document(page_content='326\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.5\\n(a) Image  \\ncorrupted by  \\nadditive  \\nsinusoidal noise. \\n(b) Spectrum \\nshowing two  \\nconjugate  \\nimpulses caused \\nby the sine wave.  \\n(Original  \\nimage courtesy of \\nNASA.) \\nof noise components directly from the image, but this is possible only in simplis-\\ntic cases. Automated analysis is possible in situations in which the noise spikes are \\neither exceptionally pronounced, or when knowledge is available about the general \\nlocation of the frequency components of the interference (see Section 5.4).\\nThe parameters of noise PDFs may be known partially from sensor speciﬁcations, \\nbut it is often necessary to estimate them for a particular imaging arrangement. If \\nthe imaging system is available, one simple way to study the characteristics of system \\nnoise is to capture a set of “ﬂat” images. For example, in the case of an optical sen-\\nsor, this is as simple as imaging a solid gray board that is illuminated uniformly. The \\nresulting images typically are good indicators of system noise.\\nWhen only images already generated by a sensor are available, it is often possible \\nto estimate the parameters of the PDF from small patches of reasonably constant \\nbackground intensity. For example, the vertical strips shown in Fig. 5.6 were cropped \\nfrom the Gaussian, Rayleigh, and uniform images in Fig. 5.4. The histograms shown \\nwere calculated using image data from these small strips. The histograms in Fig. 5.4 \\nthat correspond to the histograms in Fig. 5.6 are the ones in the middle of the group \\nof three in Figs. 5.4(d), (e), and (k).We see that the shapes of these histograms cor-\\nrespond quite closely to the shapes of the corresponding histograms in Fig. 5.6. Their \\nheights are different due to scaling, but the shapes are unmistakably similar.\\nThe simplest use of the data from the image strips is for calculating the mean and \\nvariance of intensity levels. Consider a strip (subimage) denoted by \\nS\\n, and let \\npz\\nSi\\n() ,\\niL\\n=−\\n012 1\\n,,, , ,\\n…\\n denote the probability estimates (normalized histogram values) \\nof the intensities of the pixels in \\nS\\n,\\n where \\nL\\n is the number of possible intensities in \\nthe entire image (e.g., 256 for an 8-bit image). As in Eqs. (2-69) and (2-70), we esti-\\nmate the mean and variance of the pixel values in \\nS\\n as follows:\\n \\nzz p z\\niS i\\ni\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(5-19)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   326\\n6/16/2017   2:07:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 327}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial  Filtering\\n    \\n327\\nb a\\nc\\nFIGURE 5.6\\n Histograms computed using small strips (shown as inserts) from (a) the Gaussian, (b) the Rayleigh, and \\n(c) the uniform noisy images in Fig. 5.4.\\n \\ns\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\nzz p z\\niS i\\ni\\nL\\n \\n(5-20)\\nThe shape of the histogram identifies the closest PDF match. If the shape is approxi-\\nmately Gaussian,\\n then the mean and variance are all we need because the Gaussian \\nPDF is specified completely by these two parameters. For the other shapes discussed \\nearlier, we use the mean and variance to solve for the parameters \\na\\n and \\nb\\n. Impulse \\nnoise is handled differently because the estimate needed is of the actual probability \\nof occurrence of white and black pixels. Obtaining this estimate requires that both \\nblack and white pixels be visible, so a mid-gray, relatively constant area is needed in \\nthe image in order to be able to compute a meaningful histogram of the noise. The \\nheights of the peaks corresponding to black and white pixels are the estimates of \\nP\\na\\nand \\nP\\nb\\n in Eq. (5-16).\\n5.3  RESTORATION IN THE PRESENCE OF NOISE ONLY—SPATIAL  \\nFILTERING  \\nWhen an image is degraded only by additive noise, Eqs. (5-1) and (5-2) become\\n \\ngxy f xy xy\\n(,\\n) (,) (,)\\n=+\\nh\\n \\n(5-21)\\nand\\n \\nGF N\\n(,\\n) (,) (,)\\nuv uv uv\\n=+\\n \\n(5-22)\\nThe noise terms generally are unknown, so subtracting them from \\ngxy\\n(,\\n)\\n \\n[( , ) ]\\nG\\nuv\\n \\nto obtain \\nfx y\\n(,\\n)\\n \\n[( ,) ]\\nF\\nuv\\n typically is not an option. In the case of periodic noise, \\n5.3\\nDIP4E_GLOBAL_Print_Ready.indb   327\\n6/16/2017   2:07:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 328}),\n",
       " Document(page_content='328\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nsometimes it is possible to estimate \\nN\\n(,\\n)\\nuv\\n from the spectrum of \\nG\\n(,\\n) ,\\nuv\\n as noted \\nin Section 5.2.\\n In this case \\nN\\n(,\\n)\\nuv\\n can be subtracted from \\nG\\n(,\\n)\\nuv\\n to obtain an esti-\\nmate of the original image\\n, but this type of knowledge is the exception, rather than \\nthe rule.\\nSpatial ﬁltering is the method of choice for estimating \\nfx y\\n(,\\n)\\n [i.e., \\ndenoising\\n  \\nimage \\ngxy\\n(,\\n)\\n] in situations when only additive random noise is present. Spatial ﬁl-\\ntering was discussed in detail in Chapter 3.\\n With the exception of the nature of the \\ncomputation performed by a speciﬁc ﬁlter, the mechanics for implementing all the \\nﬁlters that follow are exactly as discussed in Sections 3.4 through 3.7.\\nMEAN FILTERS\\nIn this section, we discuss briefly the noise-reduction capabilities of the spatial filters \\nintroduced in Section 3.5 and develop several other filters whose performance is in \\nmany cases superior to the filters discussed in that section.\\nArithmetic Mean Filter\\nThe \\narithmetic mean filter\\n is the simplest of the mean filters (the arithmetic mean \\nfilter is the same as the box filter we discussed in Chapter 3). Let \\nS\\nxy\\n represent the \\nset of coordinates in a rectangular subimage window (neighborhood) of size \\nmn\\n×\\n, \\ncentered on point \\n(,) .\\nxy\\n The arithmetic mean filter computes the average value of \\nthe corrupted image\\n, \\ngxy\\n(,\\n) ,\\n in the area defined by \\nS\\nxy\\n.\\n The value of the restored \\nimage \\nˆ\\nf\\n at point \\n(,)\\nxy\\n is the arithmetic mean computed using the pixels in the \\nregion defined by \\nS\\nxy\\n.In other words,\\n \\nˆ\\n(,\\n)(\\n,\\n)\\n(, )\\nfx y\\nmn\\ngrc\\nrc S\\nxy\\n=\\n∈\\n∑\\n1\\n \\n(5-23)\\nwhere, as in Eq. (2-43), \\nr\\n and \\nc\\n are the row and column coordinates of the pixels \\ncontained in the neighborhood \\nS\\nxy\\n.\\n This operation can be implemented using a spa-\\ntial kernel of size \\nmn\\n×\\n in which all coefficients have value \\n1\\nmn\\n.\\n A mean filter \\nsmooths local variations in an image\\n, and noise is reduced as a result of blurring.\\nGeometric Mean Filter\\nAn image restored using a \\ngeometric\\n \\nmean\\n \\nfilter\\n is given by the expression\\n \\nˆ\\n(,\\n)( , )\\n(, )\\nfx y g r c\\nrc S\\nmn\\nxy\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n∈\\n∏\\n1\\n \\n(5-24)\\nwhere \\n/H9016\\n indicates multiplication. Here, \\neac\\nh\\n restored pixel is given by the product of \\nall\\n the pixels in the subimage area, raised to the power \\n1\\nmn\\n.\\n As Example 5.2 below \\nillustrates\\n, a geometric mean filter achieves smoothing comparable to an arithmetic \\nmean filter, but it tends to lose less image detail in the process.\\nWe assume that \\nm\\n and \\nn\\n are odd integers. The \\nsize\\n of a mean ﬁlter is \\nthe same as the size of \\nneighborhood \\nS\\nxy\\n; that \\nis, \\nm \\n/H11003 \\nn\\n. \\nDIP4E_GLOBAL_Print_Ready.indb   328\\n6/16/2017   2:07:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 329}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n329\\nHarmonic Mean Filter\\nThe \\nharmonic mean\\n filtering operation is given by the expression\\n \\nˆ\\n(,)\\n(, )\\n(, )\\nfx y\\nmn\\ngrc\\nrc S\\nxy\\n=\\n∈\\n∑\\n1\\n \\n(5-25)\\nThe harmonic mean filter works well for salt noise, but fails for pepper noise. It does \\nwell also with other types of noise like Gaussian noise\\n.\\nContraharmonic Mean Filter\\nThe \\ncontraharmonic\\n \\nmean\\n \\nfilter\\n yields a restored image based on the expression\\n \\nˆ\\n(,)\\n(, )\\n(, )\\n(, )\\n(, )\\nfx y\\ngrc\\ngrc\\nQ\\nrc S\\nQ\\nrc S\\nxy\\nxy\\n=\\n+\\n∈\\n∈\\n∑\\n∑\\n1\\n \\n(5-26)\\nwhere \\nQ\\n is called the \\nor\\nder\\n of the filter. This filter is well suited for reducing or vir-\\ntually eliminating the effects of salt-and-pepper noise. For positive values of \\nQ\\n, the \\nfilter eliminates pepper noise. For negative values of \\nQ,\\n it eliminates salt noise. It \\ncannot do both simultaneously. Note that the contraharmonic filter reduces to the \\narithmetic mean filter if \\nQ\\n=\\n0,\\n and to the harmonic mean filter if \\nQ\\n=−\\n1.\\nEXAMPLE 5.2 :  Image denoising using spatial mean ﬁlters.\\nFigure 5.7(a) shows an 8-bit X-ray image of a circuit board, and Fig. 5.7(b) shows the same image, but \\ncorrupted with additive Gaussian noise of zero mean and variance of 400. For this type of image, this is \\na signiﬁcant level of noise. Figures 5.7(c) and (d) show, respectively, the result of ﬁltering the noisy image \\nwith an arithmetic mean ﬁlter of size \\n33\\n×\\n and a geometric mean ﬁlter of the same size. Although both \\nﬁlters did a reasonable job of attenuating the contribution due to noise\\n, the geometric mean ﬁlter did \\nnot blur the image as much as the arithmetic ﬁlter. For instance, the connector ﬁngers at the top of the \\nimage are sharper in Fig. 5.7(d) than in (c). The same is true in other parts of the image.\\nFigure 5.8(a) shows the same circuit image, but corrupted now by pepper noise with probability of \\n0.1. Similarly, Fig. 5.8(b) shows the image corrupted by salt noise with the same probability. Figure 5.8(c) \\nshows the result of ﬁltering Fig. 5.8(a) using a contraharmonic mean ﬁlter with \\nQ\\n=\\n15\\n.,\\n and Fig. 5.8(d) \\nshows the result of ﬁltering F\\nig. 5.8(b) with \\nQ\\n=−\\n15\\n..\\n Both ﬁlters did a good job of reducing the effect of \\nthe noise\\n. The positive-order ﬁlter did a better job of cleaning the background, at the expense of slightly \\nthinning and blurring the dark areas. The opposite was true of the negative order ﬁlter.\\nIn general, the arithmetic and geometric mean ﬁlters (particularly the latter) are well suited for ran-\\ndom noise like Gaussian or uniform noise. The contraharmonic ﬁlter is well suited for impulse noise, but \\nit has the disadvantage that it must be known whether the noise is dark or light in order to select the \\nproper sign for \\nQ\\n. The results of choosing the wrong sign for \\nQ\\n can be disastrous, as Fig. 5.9 shows. Some \\nof the ﬁlters discussed in the following sections eliminate this shortcoming.\\nDIP4E_GLOBAL_Print_Ready.indb   329\\n6/16/2017   2:07:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 330}),\n",
       " Document(page_content='330\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nORDER-STATISTIC FILTERS\\nWe introduced order-statistic filters in Section 3.6. We now expand the discussion \\nin that section and introduce some additional order-statistic filters. As noted in Sec-\\ntion 3.6, order-statistic filters are spatial filters whose response is based on ordering \\n(ranking) the values of the pixels contained in the neighborhood encompassed by \\nthe filter. The ranking result determines the response of the filter.\\nMedian Filter\\nThe best-known order-statistic filter in image processing is the \\nmedian filter\\n, which, \\nas its name implies, replaces the value of a pixel by the median of the intensity levels \\nin a predefined neighborhood of that pixel:\\n \\nˆ\\n(,\\n) ( ,)\\n(, )\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\nmedian\\n \\n(5-27)\\nwhere, as before, \\nS\\nxy\\n is a subimage (neighborhood) centered on point \\n(,) .\\nxy\\n The val-\\nue of the pixel at \\n(,)\\nxy\\n is included in the computation of the median. Median filters \\nb a\\nd c\\nFIGURE 5.7\\n(a) X-ray image \\nof circuit board. \\n(b) Image  \\ncorrupted by  \\nadditive Gaussian \\nnoise. (c) Result \\nof ﬁltering with \\nan arithmetic \\nmean ﬁlter of size \\n33\\n×\\n. (d) Result \\nof ﬁltering with a \\ngeometric mean \\nﬁlter of the same \\nsize\\n. (Original \\nimage courtesy of \\nMr. Joseph E.  \\nPascente, Lixi, \\nInc.)\\nDIP4E_GLOBAL_Print_Ready.indb   330\\n6/16/2017   2:07:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 331}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n331\\nb a\\nd c\\nFIGURE 5.8\\n(a) Image  \\ncorrupted by \\npepper noise with \\na probability of \\n0.1. (b) Image \\ncorrupted by salt \\nnoise with the \\nsame  \\nprobability.  \\n(c) Result of  \\nﬁltering (a) with \\na \\n33\\n×\\n contra-\\nharmonic ﬁlter \\nQ\\n=\\n15\\n..\\n (d) Result \\nof ﬁltering (b) \\nwith \\nQ\\n=−\\n15\\n..\\n \\nb a\\n \\nFIGURE 5.9\\nResults of  \\nselecting the \\nwrong sign in  \\ncontraharmonic \\nﬁltering.  \\n(a) Result of \\nﬁltering Fig. 5.8(a) \\nwith a  \\ncontraharmonic \\nﬁlter of size \\n33\\n×\\n \\nand \\nQ\\n=−\\n15\\n..\\n  \\n(b) Result of  \\nﬁltering F\\nig. 5.8(b) \\nusing \\nQ\\n=\\n15\\n..\\n \\nDIP4E_GLOBAL_Print_Ready.indb   331\\n6/16/2017   2:07:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 332}),\n",
       " Document(page_content='332\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nare quite popular because, for certain types of random noise, they provide excellent \\nnoise-reduction capabilities, with considerably less blurring than linear smoothing \\nfilters of similar size. Median filters are particularly effective in the presence of both \\nbipolar and unipolar impulse noise, as Example 5.3 below shows. Computation of \\nthe median and implementation of this filter are discussed in Section 3.6.\\nMax and Min Filters\\nAlthough the median filter is by far the order-statistic filter most used in image pro-\\ncessing, it is by no means the only one. The median represents the 50th percentile of \\na ranked set of numbers, but you will recall from basic statistics that ranking lends \\nitself to many other possibilities. For example, using the 100th percentile results in \\nthe so-called \\nmax filter\\n, given by\\n \\nˆ\\n(\\n, ) max ( , )\\n(,)\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\n \\n(5-28)\\nThis filter is useful for finding the brightest points in an image or for eroding dark \\nregions adjacent to bright areas\\n. Also, because pepper noise has very low values, it \\nis reduced by this filter as a result of the max selection process in the subimage area \\nS\\nxy\\n.\\nThe 0th percentile ﬁlter is the \\nmin ﬁlter\\n:\\n \\nˆ\\n(,\\n) m i n ( ,)\\n(, )\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\n \\n(5-29)\\nThis filter is useful for finding the darkest points in an image or for eroding light \\nregions adjacent to dark areas\\n. Also, it reduces salt noise as a result of the min opera-\\ntion.\\nMidpoint Filter\\nThe \\nmidpoint filter\\n computes the midpoint between the maximum and minimum \\nvalues in the area encompassed by the filter:\\n \\nˆ\\n( , ) m a x (, ) m i n (, )\\n(, )\\n(, )\\nfx y\\ng r c g r c\\nrc S\\nrc S\\nxy\\nxy\\n=\\n{}\\n+\\n{}\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n∈\\n∈\\n1\\n2\\n \\n(5-30)\\nNote that this filter combines order statistics and averaging. It works best for ran-\\ndomly distributed noise\\n, like Gaussian or uniform noise.\\nAlpha-Trimmed Mean Filter\\nSuppose that we delete the \\nd\\n2\\n lowest and the \\nd\\n2\\n highest intensity values of \\ngrc\\n(,\\n)\\n \\nin the neighborhood \\nS\\nxy\\n.\\n Let \\ngr c\\nR\\n(, )\\n represent the remaining \\nmn d\\n−\\n pixels in \\nS\\nxy\\n. \\nA filter formed by averaging these remaining pixels is called an \\nalpha-trimmed mean \\nfilter\\n.\\n The form of this filter is \\nDIP4E_GLOBAL_Print_Ready.indb   332\\n6/16/2017   2:07:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 333}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n333\\n \\nˆ\\n(,\\n)(\\n,\\n)\\n(, )\\nfx y\\nmn d\\ngr c\\nR\\nrc S\\nxy\\n=\\n−\\n∈\\n∑\\n1\\n \\n(5-31)\\nwhere the value of \\nd\\n can range from 0 to \\nmn\\n−\\n1.\\n When \\nd\\n=\\n0\\n the alpha-trimmed fil-\\nter reduces to the arithmetic mean filter discussed earlier\\n. If we choose \\ndm n\\n=−\\n1,\\n \\nthe filter becomes a median filter\\n. For other values of \\nd\\n, the alpha-trimmed filter is \\nuseful in situations involving multiple types of noise, such as a combination of salt-\\nand-pepper and Gaussian noise.\\nEXAMPLE 5.3 :  Image denoising using order-statistic ﬁlters.\\nFigure 5.10(a) shows the circuit board image corrupted by salt-and-pepper noise with probabilities \\nP\\nP\\nsp\\n==\\n01\\n..\\n Figure 5.10(b) shows the result of median ﬁltering with a ﬁlter of size \\n33\\n×\\n.\\n The improve-\\nment over F\\nig. 5.10(a) is signiﬁcant, but several noise points still are visible. A second pass [on the im-\\nage in Fig. 5.10(b)] with the median ﬁlter removed most of these points, leaving only few, barely visible \\nnoise points. These were removed with a third pass of the ﬁlter. These results are good examples of the \\npower of median ﬁltering in handling impulse-like additive noise. Keep in mind that repeated passes \\nof a median ﬁlter will blur the image, so it is desirable to keep the number of passes as low as possible. \\nFigure 5.11(a) shows the result of applying the max ﬁlter to the pepper noise image of Fig. 5.8(a). The \\nﬁlter did a reasonable job of removing the pepper noise, but we note that it also removed (set to a light \\nintensity level) some dark pixels from the borders of the dark objects. Figure 5.11(b) shows the result \\nof applying the min ﬁlter to the image in Fig. 5.8(b). In this case, the min ﬁlter did a better job than the \\nmax ﬁlter on noise removal, but it removed some white points around the border of light objects. These \\nmade the light objects smaller and some of the dark objects larger (like the connector ﬁngers in the top \\nof the image) because white points around these objects were set to a dark level.\\nThe alpha-trimmed ﬁlter is illustrated next. Figure 5.12(a) shows the circuit board image corrupted \\nthis time by additive, uniform noise of variance 800 and zero mean. This is a high level of noise corrup-\\ntion that is made worse by further addition of salt-and-pepper noise with \\nPP\\nsp\\n==\\n01\\n.,\\n as Fig. 5.12(b) \\nshows\\n. The high level of noise in this image warrants use of larger ﬁlters. Figures 5.12(c) through (f) show \\nthe results, respectively, obtained using arithmetic mean, geometric mean, median, and alpha-trimmed \\nmean (with \\nd\\n=\\n6\\n) ﬁlters of size \\n55\\n×\\n.\\n As expected, the arithmetic and geometric mean ﬁlters (especially \\nthe latter) did not do well because of the presence of impulse noise\\n. The median and alpha-trimmed \\nﬁlters performed much better, with the alpha-trimmed ﬁlter giving slightly better noise reduction. For \\nexample, note in Fig. 5.12(f) that the fourth connector ﬁnger from the top left is slightly smoother in \\nthe alpha-trimmed result. This is not unexpected because, for a high value of \\nd\\n, the alpha-trimmed ﬁlter \\napproaches the performance of the median ﬁlter, but still retains some smoothing capabilities.\\nADAPTIVE FILTERS\\nOnce selected, the filters discussed thus far are applied to an image without regard \\nfor how image characteristics vary from one point to another. In this section, we \\ntake a look at two \\nadaptive\\n filters whose behavior changes based on statistical char-\\nacteristics of the image inside the filter region defined by the \\nmn\\n×\\n rectangular \\nneighborhood \\nS\\nxy\\n.\\n As the following discussion shows, adaptive filters are capable \\nof performance superior to that of the filters discussed thus far\\n. The price paid for \\nDIP4E_GLOBAL_Print_Ready.indb   333\\n6/16/2017   2:07:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='334\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.11\\n(a) Result of \\nﬁltering Fig. 5.8(a) \\nwith a max ﬁlter \\nof size \\n33\\n×\\n. \\n(b) Result of  \\nﬁltering F\\nig. 5.8(b) \\nwith a min ﬁlter of \\nthe same size. \\nb a\\nd c\\nFIGURE 5.10\\n(a) Image  \\ncorrupted by salt-\\nand- pepper noise \\nwith probabilities \\nPP\\nsp\\n==\\n01\\n..\\n \\n(b) Result of one \\npass with a medi-\\nan ﬁlter of size \\n33\\n×\\n. (c) Result \\nof processing (b) \\nwith this ﬁlter\\n.  \\n(d) Result of  \\nprocessing (c) \\nwith the same \\nﬁlter. \\nDIP4E_GLOBAL_Print_Ready.indb   334\\n6/16/2017   2:07:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 335}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n335\\nb a\\nd c\\nf e\\nFIGURE 5.12\\n(a) Image  \\ncorrupted by \\nadditive uniform \\nnoise. (b) Image \\nadditionally  \\ncorrupted by \\nadditive salt-and-\\npepper noise. \\n(c)-(f) Image (b) \\nﬁltered with a \\n55\\n×\\n: \\n(c) arithmetic \\nmean ﬁlter;\\n  \\n(d) geometric \\nmean ﬁlter;  \\n(e) median ﬁlter; \\n(f) alpha-trimmed \\nmean ﬁlter, with \\nd\\n=\\n6. \\nDIP4E_GLOBAL_Print_Ready.indb   335\\n6/16/2017   2:07:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 336}),\n",
       " Document(page_content='336\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nimproved filtering power is an increase in filter complexity. Keep in mind that we \\nstill are dealing with the case in which the degraded image is equal to the original \\nimage plus noise. No other types of degradations are being considered yet.\\nAdaptive, Local Noise Reduction Filter\\nThe simplest statistical measures of a random variable are its mean and variance. \\nThese are reasonable parameters on which to base an adaptive filter because they \\nare quantities closely related to the appearance of an image. The mean gives a mea-\\nsure of average intensity in the region over which the mean is computed, and the \\nvariance gives a measure of image contrast in that region.\\nOur ﬁlter is to operate on a neighborhood, \\nS\\nxy\\n,\\n centered on coordinates \\n(,) .\\nxy\\nThe response of the ﬁlter at \\n(,)\\nxy\\n is to be based on the following quantities: \\ngxy\\n(,\\n) ,\\n \\nthe value of the noisy image at \\n(,) ;\\nxy\\n \\ns\\nh\\n2\\n,\\n the variance of the noise; \\nz\\nS\\nxy\\n,\\n the local \\naverage intensity of the pixels in \\nS\\nxy\\n;\\n and \\ns\\nS\\nxy\\n2\\n,\\n the local variance of the intensities of \\npixels in \\nS\\nxy\\n. We want the behavior of the ﬁlter to be as follows:\\n1. \\nIf \\ns\\nh\\n2\\n is zero, the ﬁlter should return simply the value of \\ng \\nat \\n(,) .\\nxy\\n This is the \\ntrivial,\\n zero-noise case in which \\ng\\n is equal to \\nf\\n at \\n(,) .\\nxy\\n2.\\n \\nIf the local variance \\ns\\nS\\nxy\\n2\\n is high relative to \\ns\\nh\\n2\\n,\\n the ﬁlter should return a value \\nclose to \\ng\\n at \\n(,) .\\nxy\\n A high local variance typically is associated with edges, and \\nthese should be preserved.\\n3. \\nIf the two variances are equal, we want the ﬁlter to return the arithmetic mean \\nvalue of the pixels in \\nS\\nxy\\n.\\n This condition occurs when the local area has the same \\nproperties as the overall image\\n, and local noise is to be reduced by averaging.\\nAn adaptive expression for obtaining \\nˆ\\n(,\\n)\\nfx y\\n based on these assumptions may be \\nwritten as\\n \\nˆ\\n(,) (,) (,)\\nfx y g x y g x\\nyz\\nS\\nS\\nxy\\nxy\\n=− −\\n⎡\\n⎣\\n⎤\\n⎦\\ns\\ns\\nh\\n2\\n2\\n \\n(5-32)\\nThe only quantity that needs to be known a priori is \\ns\\nh\\n2\\n,\\n the variance of the noise \\ncorrupting image \\nfx y\\n(,\\n) .\\n This is a constant that can be estimated from sample noisy \\nimages using Eq.\\n (3-26). The other parameters are computed from the pixels in \\nneighborhood \\nS\\nxy\\n using Eqs. (3-27) and (3-28).\\nAn assumption in Eq. (5-32) is that the ratio of the two variances does not exceed 1, \\nwhich implies that \\nss\\nh\\n22\\n≤\\nS\\nxy\\n.\\n The noise in our model is additive and position indepen-\\ndent,\\n so this is a reasonable assumption to make because \\nS\\nxy\\n is a subset of \\ngxy\\n(,\\n) .\\n \\nHowever\\n, we seldom have exact knowledge of \\ns\\nh\\n2\\n.\\n Therefore, it is possible for this \\ncondition to be violated in practice\\n. For that reason, a test should be built into an \\nimplementation of Eq. (5-32) so that the ratio is set to 1 if the condition \\nss\\nh\\n22\\n>\\nS\\nxy\\n \\noccurs. This makes this ﬁlter nonlinear. However, it prevents nonsensical results (i.e., \\nnegative intensity levels, depending on the value of \\nz\\nS\\nxy\\n) due to a potential lack of \\nknowledge about the variance of the image noise. Another approach is to allow the \\nnegative values to occur, and then rescale the intensity values at the end. The result \\nthen would be a loss of dynamic range in the image.\\nDIP4E_GLOBAL_Print_Ready.indb   336\\n6/16/2017   2:07:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 337}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n337\\nb a\\nd c\\nFIGURE 5.13\\n(a) Image  \\ncorrupted by  \\nadditive  \\nGaussian noise of \\nzero mean and a \\nvariance of 1000. \\n(b) Result of \\narithmetic mean \\nﬁltering.  \\n(c) Result of  \\ngeometric mean \\nﬁltering.  \\n(d) Result of \\nadaptive noise- \\nreduction ﬁltering. \\nAll ﬁlters used \\nwere of size \\n77\\n×\\n. \\nEXAMPLE 5.4 :  Image denoising using adaptive, local noise-reduction ﬁltering.\\nFigure 5.13(a) shows the circuit-board image, corrupted this time by additive Gaussian noise of zero \\nmean and a variance of 1000. This is a signiﬁcant level of noise corruption, but it makes an ideal test bed \\non which to compare relative ﬁlter performance. Figure 5.13(b) is the result of processing the noisy im-\\nage with an arithmetic mean ﬁlter of size \\n77\\n×\\n.\\n The noise was smoothed out, but at the cost of signiﬁcant \\nblurring\\n. Similar comments apply to Fig. 5.13(c), which shows the result of processing the noisy image \\nwith a geometric mean ﬁlter, also of size \\n77\\n×\\n.\\n The differences between these two ﬁltered images are \\nanalogous to those we discussed in Example 5.2;\\n only the degree of blurring is different. \\nFigure 5.13(d) shows the result of using the adaptive ﬁlter of Eq. (5-32) with \\ns\\nh\\n2\\n1000\\n=\\n.\\n The improve-\\nments in this result compared with the two previous ﬁlters are signiﬁcant.\\n In terms of overall noise \\nreduction, the adaptive ﬁlter achieved results similar to the arithmetic and geometric mean ﬁlters. How-\\never, the image ﬁltered with the adaptive ﬁlter is much sharper. For example, the connector ﬁngers at the \\ntop of the image are signiﬁcantly sharper in Fig. 5.13(d). Other features, such as holes and the eight legs \\nof the dark component on the lower left-hand side of the image, are much clearer in Fig. 5.13(d).These \\nresults are typical of what can be achieved with an adaptive ﬁlter. As mentioned earlier, the price paid \\nfor the improved performance is additional ﬁlter complexity.\\nDIP4E_GLOBAL_Print_Ready.indb   337\\n6/16/2017   2:07:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 338}),\n",
       " Document(page_content='338\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nThe preceding results used a value for \\ns\\nh\\n2\\n that matched the variance of the noise exactly. If this \\nquantity is not known, and the estimate used is too low, the algorithm will return an image that closely \\nresembles the original because the corrections will be smaller than they should be. Estimates that are \\ntoo high will cause the ratio of the variances to be clipped at 1.0, and the algorithm will subtract the \\nmean from the image more frequently than it would normally. If negative values are allowed and the \\nimage is rescaled at the end, the result will be a loss of dynamic range, as mentioned previously.\\nAdaptive Median Filter\\nThe median filter in Eq. (5-27) performs well if the spatial density of the salt-and-\\npepper noise is low (as a rule of thumb, \\nP\\ns\\n and \\nP\\np\\n less than 0.2). We show in the fol-\\nlowing discussion that adaptive median filtering can handle noise with probabilities \\nlarger than these. An additional benefit of the adaptive median filter is that it seeks \\nto preserve detail while simultaneously smoothing non-impulse noise, something \\nthat the “traditional” median filter does not do. As in all the filters discussed in the \\npreceding sections, the adaptive median filter also works in a rectangular neighbor-\\nhood, \\nS\\nxy\\n.\\n Unlike those filters, however, the adaptive median filter changes (increas-\\nes) the size of \\nS\\nxy\\n during filtering, depending on certain conditions to be listed short-\\nly. Keep in mind that the output of the filter is a single value used to replace the \\nvalue of the pixel at \\n(,) ,\\nxy\\n the point on which region \\nS\\nxy\\n is centered at a given time.\\nWe use the following notation:\\n \\nzS\\nz\\nxy\\nmin\\nmax\\n=\\n=\\nminimum intensity value in \\nmaximum intensity v\\nvalue in \\nmedian of intensity values in \\nint\\nmed\\nS\\nzS\\nz\\nxy\\nxy\\nxy\\n=\\n=\\ne\\nensity at coordinates \\nmaximum allowed size of \\n(,)\\nmax\\nxy\\nSS\\n=\\nx xy\\nThe adaptive median-filtering algorithm uses two processing levels, denoted level\\n A\\nand level\\n B\\n, at each point \\n(,) :\\nxy\\n \\nLevel If \\n go to Level \\nElse, increase th\\ne\\nmed\\nAz z z B\\n:,\\nmin\\nmax\\n<<\\n  size of \\nIf , repeat level \\nElse, output \\nmax\\nmed\\nS\\nSS A\\nz\\nxy\\nxy\\n≤\\n.\\nL\\nLevel If  output \\nElse output\\nmed\\nBz z z z\\nz\\nxy\\nxy\\n:,\\n.\\nmin\\nmax\\n<<\\nwhere \\nS\\nxy\\n and \\nS\\nmax\\n are odd, positive integers greater than 1. Another option in the \\nlast step of level \\nA\\n is to output \\nz\\nxy\\n instead of \\nz\\nmed\\n.\\n This produces a slightly less \\nblurred result,\\n but can fail to detect salt (pepper) noise embedded in a constant \\nbackground having the same value as pepper (salt) noise.\\nDIP4E_GLOBAL_Print_Ready.indb   338\\n6/16/2017   2:07:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 339}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n339\\nThis algorithm has three principal objectives: to remove salt-and-pepper (impulse) \\nnoise, to provide smoothing of other noise that may not be impulsive, and to reduce \\ndistortion, such as excessive thinning or thickening of object boundaries. The values \\nz\\nmin\\n and \\nz\\nmax\\n are considered statistically by the algorithm to be “impulse-like” noise \\ncomponents in region \\nS\\nxy\\n,\\n even if these are not the lowest and highest possible pixel \\nvalues in the image\\n.\\nWith these observations in mind, we see that the purpose of level \\nA\\n is to deter-\\nmine if the median ﬁlter output, \\nz\\nmed\\n,\\n is an impulse (salt \\nor\\n pepper) or not.\\n If the \\ncondition \\nzz z\\nmin\\nmax\\n<<\\nmed\\n holds, then \\nz\\nmed\\n cannot be an impulse for the reason \\nmentioned in the previous paragraph. In this case, we go to level \\nB\\n and test to see \\nif the point in the center of the neighborhood is itself an impulse (recall that \\n(,)\\nxy\\n \\nis the location of the point being processed,\\n and \\nz\\nxy\\n is its intensity). If the condition \\nzz z\\nxy\\nmin max\\n<<\\n is true, then the pixel at \\nz\\nxy\\n cannot be the intensity of an impulse for \\nthe same reason that \\nz\\nmed\\n was not. In this case, the algorithm outputs the unchanged \\npixel value, \\nz\\nxy\\n.\\n By not changing these “intermediate-level” points, distortion is \\nreduced in the ﬁltered image\\n. If the condition \\nzz z\\nxy\\nmin max\\n<<\\n is false, then either \\nzz\\nxy\\n=\\nmin\\n or \\nzz\\nxy\\n=\\nmax\\n.\\n In either case, the value of the pixel is an extreme value and \\nthe algorithm outputs the median value\\n, \\nz\\nmed\\n,\\n which we know from level \\nA\\n is not a \\nnoise impulse\\n. The last step is what the standard median ﬁlter does. The problem is \\nthat the standard median ﬁlter replaces every point in the image by the median of \\nthe corresponding neighborhood. This causes unnecessary loss of detail.\\nContinuing with the explanation, suppose that level \\nA\\n \\ndoes\\n ﬁnd an impulse (i.e., \\nit fails the test that would cause it to branch to level \\nB\\n). The algorithm then increas-\\nes the size of the neighborhood and repeats level \\nA\\n. This looping continues until \\nthe algorithm either ﬁnds a median value that is not an impulse (and branches to \\nstage \\nB\\n), or the maximum neighborhood size is reached. If the maximum size is \\nreached, the algorithm returns the value of \\nz\\nmed\\n.\\n Note that there is no guarantee \\nthat this value is not an impulse\\n. The smaller the noise probabilities \\nP\\na\\n and/or \\nP\\nb\\n are, \\nor the larger \\nS\\nmax\\n is allowed to be, the less likely it is that a premature exit will occur. \\nThis is plausible. As the density of the noise impulses increases, it stands to reason \\nthat we would need a larger window to “clean up” the noise spikes.\\nEvery time the algorithm outputs a value, the center of neighborhood \\nS\\nxy\\n is \\nmoved to the next location in the image. The algorithm then is reinitialized and \\napplied to the pixels in the new region encompassed by the neighborhood. As indi-\\ncated in Problem 3.37, the median value can be updated iteratively from one loca-\\ntion to the next, thus reducing computational load.\\nEXAMPLE 5.5 :  Image denoising using adaptive median ﬁltering.\\nFigure 5.14(a) shows the circuit-board image corrupted by salt-and-pepper noise with probabilities \\nPP\\nsp\\n==\\n02 5\\n.,\\n which is 2.5 times the noise level used in Fig. 5.10(a). Here the noise level is high enough \\nto obscure most of the detail in the image\\n. As a basis for comparison, the image was ﬁltered ﬁrst using a \\n77\\n×\\n median ﬁlter, the smallest ﬁlter required to remove most visible traces of impulse noise in this case. \\nF\\nigure 5.14(b) shows the result. Although the noise was effectively removed, the ﬁlter caused signiﬁcant \\nDIP4E_GLOBAL_Print_Ready.indb   339\\n6/16/2017   2:07:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 340}),\n",
       " Document(page_content='340\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nloss of detail in the image. For instance, some of the connector ﬁngers at the top of the image appear \\ndistorted or broken. Other image details are similarly distorted.\\nFigure 5.14(c) shows the result of using the adaptive median ﬁlter with \\nS\\nmax\\n.\\n=\\n7\\n Noise removal \\nperformance was similar to the median ﬁlter\\n. However, the adaptive ﬁlter did a much better job of pre-\\nserving sharpness and detail. The connector ﬁngers are less distorted, and some other features that were \\neither obscured or distorted beyond recognition by the median ﬁlter appear sharper and better deﬁned \\nin Fig. 5.14(c). Two notable examples are the feed-through small white holes throughout the board, and \\nthe dark component with eight legs in the bottom, left quadrant of the image.\\nConsidering the high level of noise in Fig. 5.14(a), the adaptive algorithm performed quite well. The \\nchoice of maximum allowed size for \\nS\\nxy\\n depends on the application, but a reasonable starting value can \\nbe estimated by experimenting with various sizes of the standard median ﬁlter ﬁrst. This will establish a \\nvisual baseline regarding expectations on the performance of the adaptive algorithm.\\n5.4 PERIODIC NOISE REDUCTION USING FREQUENCY DOMAIN  \\nFILTERING  \\nPeriodic noise can be analyzed and filtered quite effectively using frequency domain \\ntechniques. The basic idea is that periodic noise appears as concentrated bursts of \\nenergy in the Fourier transform, at locations corresponding to the frequencies of \\nthe periodic interference. The approach is to use a selective filter (see\\n \\nSection 4.10) \\nto isolate the noise\\n. The three types of selective filters (bandreject, bandpass, and \\nnotch) were discussed in detail in Section 4.10. There is no difference between how \\nthese filters were used in Chapter 4, and the way they are used for image restora-\\ntion. In restoration of images corrupted by periodic interference, the tool of choice \\nis a notch filter. In the following discussion we will expand on the notch filtering \\napproach introduced in Section 4.10, and also develop a more powerful optimum \\nnotch filtering method. \\n5.4\\nb a\\nc\\nFIGURE 5.14\\n (a) Image corrupted by salt-and-pepper noise with probabilities \\nPP\\nsp\\n==\\n02 5\\n..\\n (b) Result of ﬁltering \\nwith a \\n77\\n×\\n median ﬁlter. (c) Result of adaptive median ﬁltering with \\nS\\nmax\\n.\\n=\\n7  \\nDIP4E_GLOBAL_Print_Ready.indb   340\\n6/16/2017   2:07:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 341}),\n",
       " Document(page_content='5.4\\n  \\nPeriodic Noise Reduction Using Frequency Domain Filtering\\n    \\n341\\nMORE ON NOTCH FILTERING\\nAs explained in Section 4.10, notch reject filter transfer functions are constructed \\nas products of highpass filter transfer functions whose centers have been translated \\nto the centers of the notches. The general form of a notch filter transfer function is\\n \\nHH H\\nk\\nk\\nQ\\nk\\nNR\\n(,) (,) (,)\\nuv uv uv\\n=\\n=\\n−\\n∏\\n1\\n \\n(5-33)\\nwhere \\nH\\nk\\n(,)\\nuv\\n and \\nH\\nk\\n−\\n(,)\\nuv\\n are highpass filter transfer functions whose centers \\nare at \\n(,)\\nuv\\nkk\\n and \\n(,) ,\\n−−\\nuv\\nkk\\n respectively.\\n†\\n These centers are specified with respect \\nto the center of the frequency rectangle, \\nfloor( floor\\nMN\\n22\\n), ( ) ,\\n[]\\n where, as usual, \\nM\\n and \\nN\\n are the number of rows and columns in the input image. Thus, the distance \\ncomputations for the filter transfer functions are given by\\n \\nDu M u N\\nkk k\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n=− − + − −\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n12\\n \\n(5-34)\\nand\\n \\nDu M u N\\nkk k\\n−\\n=− + + − +\\n⎡\\n⎣\\n⎤\\n⎦\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n22\\n22\\n12\\n \\n(5-35)\\nFor example, the following is a Butterworth notch reject filter transfer function of \\norder \\nn\\n with three notch pairs:\\n \\nH\\nDD\\nDD\\nkk\\nn\\nk\\nkk\\nn\\nNR\\n(,)\\n(,\\n)(\\n,\\n)\\nuv\\nuv\\nuv\\n=\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n=\\n−\\n∏\\n1\\n1\\n1\\n1\\n0\\n1\\n3\\n0\\n⎢ ⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n \\n(5-36)\\nBecause notches are specified as symmetric pairs, the constant \\nD\\nk\\n0\\n is the same for \\neach pair. However, this constant can be different from one pair to another. Other \\nnotch reject filter functions are constructed in the same manner, depending on the \\nhighpass filter function chosen. As explained in Section 4.10, a notch pass filter \\ntransfer function is obtained from a notch reject function using the expression\\n \\nHH\\nNP\\nNR\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(5-37)\\nwhere \\nH\\nNP\\n(,)\\nuv\\n is the transfer function of the notch pass filter corresponding to \\nthe notch reject filter with transfer function \\nH\\nNR\\n(,) .\\nuv\\n Figure 5.15 shows perspec-\\ntive plots of the transfer functions of ideal,\\n Gaussian, and Butterworth notch reject \\nfilters with one notch pair. As we discussed in Chapter 4, we see again that the shape \\nof the Butterworth transfer function represents a transition between the sharpness \\nof the ideal function and the broad, smooth shape of the Gaussian transfer function.\\nAs we show in the second part of the following example, we are not limited to \\nnotch ﬁlter transfer functions of the form just discussed. We can construct notch \\n† Remember, frequency domain transfer functions are symmetric about the center of the frequency rectangle, so \\nthe notches are speciﬁed as symmetric pairs. Also, recall from Section 4.10 that we use unpadded images when \\nworking with notch ﬁlters in order to simplify the speciﬁcation of notch locations.\\nDIP4E_GLOBAL_Print_Ready.indb   341\\n6/16/2017   2:07:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 342}),\n",
       " Document(page_content=\"342\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nﬁlters of arbitrary shapes, provided that they are zero-phase-shift functions, as \\ndeﬁned in Section 4.7.\\nEXAMPLE 5.6 :  Image denoising (interference reduction) using notch ﬁltering.\\nFigure 5.16(a) is the same as Fig. 2.45(a), which we used in Section 2.6 to introduce the concept of ﬁlter-\\ning in the frequency domain. We now look in more detail at the process of denoising this image, which is \\ncorrupted by a single, 2-D additive sine wave. You know from Table 4.4 that the Fourier transform of a \\npure sine wave is a pair of complex, conjugate impulses, so we would expect the spectrum to have a pair \\nof bright dots at the frequencies of the sine wave. As Fig. 5.16(b) shows, this is indeed is the case. Because \\nwe can determine the location of these impulses accurately, eliminating them is a simple task, consisting \\nof using a notch ﬁlter transfer function whose notches coincide with the location of the impulses. \\nFigure 5.16(c) shows an ideal notch reject ﬁlter transfer function, which is an array of 1's (shown in \\nwhite) and two small circular regions of 0's (shown in black). Figure 5.16(d) shows the result of ﬁltering \\nthe noisy image this transfer function. The sinusoidal noise was virtually eliminated, and a number of \\ndetails that were previously obscured by the interference are clearly visible in the ﬁltered image (see, for \\nexample, the thin ﬁducial marks and the ﬁne detail in the terrain and rock formations). As we showed \\nin Example 4.25, obtaining an image of the interference pattern is straightforward. We simply turn the \\nreject ﬁlter into a pass ﬁlter by subtracting it from 1, and ﬁlter the input image with it. Figure 5.17 shows \\nthe result.\\nFigure 5.18(a) shows the same image as Fig. 4.50(a), but covering a larger area (the interference \\npattern is the same). When we discussed lowpass ﬁltering of that image in Chapter 4, we indicated that \\nthere were better ways to reduce the effect of the scan lines. The notch ﬁltering approach that follows \\nreduces the scan lines signiﬁcantly, without introducing blurring. Unless blurring is desirable for reasons \\nwe discussed in Section 4.9, notch ﬁltering generally gives much better results. \\nJust by looking at the nearly horizontal lines of the noise pattern in Fig. 5.18(a), we expect its con-\\ntribution in the frequency domain to be concentrated along the vertical axis of the DFT. However, \\nthe noise is not dominant enough to have a clear pattern along this axis, as is evident in the spectrum \\nshown in Fig. 5.18(b). The approach to follow in cases like this is to use a narrow, rectangular notch ﬁlter \\nfunction that extends along the vertical axis, and thus eliminates all components of the interference \\nalong that axis. We do not ﬁlter near the origin to avoid eliminating the dc term and low frequencies, \\n(,)\\nH\\nuv\\nu\\nv\\n(,)\\nH\\nuv\\nu\\nv\\n(,)\\nH\\nuv\\nu\\nv\\nIdeal\\nGaussian\\nButterworth\\nb a\\nc\\nFIGURE 5.15\\n Perspective plots of (a) ideal, (b) Gaussian, and (c) Butterworth notch reject ﬁlter transfer functions.\\nDIP4E_GLOBAL_Print_Ready.indb   342\\n6/16/2017   2:07:20 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 343}),\n",
       " Document(page_content='5.4\\n  \\nPeriodic Noise Reduction Using Frequency Domain Filtering\\n    \\n343\\nb a\\nd c\\nFIGURE 5.16\\n(a) Image cor-\\nrupted by sinusoi-\\ndal interference.  \\n(b) Spectrum  \\nshowing the \\nbursts of energy \\ncaused by the \\ninterference. (The \\nbursts were  \\nenlarged for \\ndisplay purposes.) \\n(c) Notch ﬁlter \\n(the radius of the \\ncircles is 2 pixels) \\nused to eliminate \\nthe energy bursts. \\n(The thin borders \\nare not part of the \\ndata.)  \\n(d) Result of  \\nnotch reject  \\nﬁltering.  \\n(Original  \\nimage courtesy of \\nNASA.) \\nFIGURE 5.17\\nSinusoidal  \\npattern extracted \\nfrom the DFT  \\nof Fig. 5.16(a) \\nusing a notch pass \\nﬁlter.\\nwhich, as you know from Chapter 4, are responsible for the intensity differences between smooth areas. \\nFigure 5.18(c) shows the ﬁlter transfer function we used, and Fig. 5.18(d) shows the ﬁltered result. Most \\nof the ﬁne scan lines were eliminated or signiﬁcantly attenuated. In order to get an image of the noise \\npattern, we proceed as before by converting the reject ﬁlter into a pass ﬁlter, and then ﬁltering the input \\nimage with it. Figure 5.19 shows the result.\\nDIP4E_GLOBAL_Print_Ready.indb   343\\n6/16/2017   2:07:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 344}),\n",
       " Document(page_content='344\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nFIGURE 5.19\\nNoise pattern  \\nextracted from \\nFig. 5.18(a) by \\nnotch pass  \\nﬁltering.\\nb a\\nd c\\nFIGURE 5.18\\n(a) Satellite image \\nof Florida and the \\nGulf of Mexico. \\n(Note horizontal \\nsensor scan lines.) \\n(b) Spectrum of \\n(a). (c) Notch  \\nreject ﬁlter  \\ntransfer  \\nfunction. (The \\nthin black border \\nis not part of the \\ndata.) (d) Filtered \\nimage. (Original \\nimage courtesy of \\nNOAA.)  \\nDIP4E_GLOBAL_Print_Ready.indb   344\\n6/16/2017   2:07:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 345}),\n",
       " Document(page_content='5.4\\n  \\nPeriodic Noise Reduction Using Frequency Domain Filtering\\n    \\n345\\nOPTIMUM NOTCH FILTERING\\nIn the examples of notch filtering given thus far, the interference patterns have been \\nsimple to identify and characterize in the frequency domain, leading to the specifica-\\ntion of notch filter transfer functions that also are simple to define heuristically.\\nWhen several interference components are present, heuristic speciﬁcations of \\nﬁlter transfer functions are not always acceptable because they may remove too \\nmuch image information in the ﬁltering process (a highly undesirable feature when \\nimages are unique and/or expensive to acquire). In addition, the interference com-\\nponents generally are not single-frequency bursts. Instead, they tend to have broad \\nskirts that carry information about the interference pattern. These skirts are not \\nalways easily detectable from the normal transform background. Alternative ﬁlter-\\ning methods that reduce the effect of these degradations are quite useful in practice. \\nThe method discussed next is optimum, in the sense that it minimizes local variances \\nof the restored estimate \\nˆ\\n(,\\n) .\\nfx y\\nThe procedure consists of ﬁrst isolating the principal contributions of the interfer-\\nence pattern and then subtracting a variable\\n, weighted portion of the pattern from \\nthe corrupted image. Although we develop the procedure in the context of a speciﬁc \\napplication, the basic approach is general and can be applied to other restoration \\ntasks in which multiple periodic interference is a problem.\\nWe begin by extracting the principal frequency components of the interfer-\\nence pattern. As before, we do this by placing a notch pass ﬁlter transfer function, \\nH\\nNP\\n(,) ,\\nuv\\n at the location of each spike. If the ﬁlter is constructed to pass only com-\\nponents associated with the interference pattern,\\n then the Fourier transform of the \\ninterference noise pattern is given by the expression\\n \\nNHG\\n(,\\n) (,) (,)\\nuv uv uv\\n=\\nNP\\n \\n(5-38)\\nwhere, as usual, \\nG\\n(,\\n)\\nuv\\n is the DFT of the corrupted image.\\nSpecifying \\nH\\nNP\\n(,)\\nuv\\n requires considerable judgment about what is or is not an \\ninterference spike\\n. For this reason, the notch pass ﬁlter generally is constructed inter-\\nactively by observing the spectrum of \\nG\\n(,\\n)\\nuv\\n on a display. After a particular ﬁlter \\nfunction has been selected,\\n the corresponding noise pattern in the spatial domain is \\nobtained using the familiar expression\\n \\nh\\n(,\\n) (,) (,)\\nxy H G\\n=\\n{}\\n−\\n/H5219\\n1\\nNP\\nuv uv\\n \\n(5-39)\\nBecause the corrupted image is assumed to be formed by the addition of the uncor-\\nrupted image \\nfx y\\n(,\\n)\\n and the interference, \\nh\\n(,\\n) ,\\nxy\\n if the latter were known com-\\npletely\\n, subtracting the pattern from \\ngxy\\n(,\\n)\\n to obtain \\nfx y\\n(,\\n)\\n would be a simple mat-\\nter\\n. The problem, of course, is that this filtering procedure usually yields only an \\napproximation of the true noise pattern. The effect of incomplete components not \\npresent in the estimate of \\nh\\n(,\\n)\\nxy\\n can be minimized by subtracting from \\ngxy\\n(,\\n)\\n a \\nweighted\\n portion of \\nh\\n(,\\n)\\nxy\\n to obtain an estimate of \\nfx y\\n(,\\n) :\\n \\nˆ\\n(,\\n) (,) (,)(,)\\nf xy gxy xy xy\\n=−\\nw\\nh\\n \\n(5-40)\\nDIP4E_GLOBAL_Print_Ready.indb   345\\n6/16/2017   2:07:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 346}),\n",
       " Document(page_content=\"346\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nwhere, as before, \\nˆ\\n(,\\n)\\nfx y\\n is the estimate of \\nfx y\\n(,\\n)\\n and \\nw\\n(,\\n)\\nxy\\n is to be determined. \\nT\\nhis function is called a \\nweighting\\n or \\nmodulation function\\n, and the objective of the \\nprocedure is to select \\nw\\n(,\\n)\\nxy\\n so that the result is optimized in some meaningful way. \\nOne approach is to select \\nw\\n(,\\n)\\nxy\\n so that the variance of \\nˆ\\n(,\\n)\\nfx y\\n is minimized over a \\nspecified neighborhood of every point \\n(,) .\\nxy\\n \\nConsider a neighborhood \\nS\\nxy\\n of (odd) size \\nmn\\n×\\n,\\n centered on \\n(,) .\\nxy\\n The “local” \\nvariance of \\nˆ\\n(,\\n)\\nfx y\\n at point \\n(,)\\nxy\\n can be estimated using the samples in \\nS\\nxy\\n,\\n as fol-\\nlows:\\n \\ns\\n2\\n2\\n1\\n(,) [\\n(, )\\n]\\n_\\n(,)\\nxy\\nmn\\nf\\nrc f\\nrc S\\nxy\\n=\\n−\\n∈\\n∑\\n^\\n^\\n \\n(5-41)\\nwhere \\nˆ\\nf\\n is the average value of \\nˆ\\nf\\n in neighborhood \\nS\\nxy\\n; that is,\\n \\nˆˆ\\n(, )\\n(, )\\nf\\nmn\\nfr c\\nrc S\\nxy\\n=\\n∈\\n∑\\n1\\n \\n(5-42)\\nPoints on or near the edge of the image can be treated by considering partial neigh-\\nborhoods or by padding the border with 0's\\n.\\nSubstituting Eq. (5-40) into Eq. (5-41) we obtain\\n \\nsh\\nh\\n2\\n1\\n( , )\\n[ (, ) (, )(, ) ]\\n[]\\n____\\n(, )\\nxy\\nmn\\ngrc rc rc g\\nrc S\\nxy\\n=− −\\n−\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n∈\\nww\\n∑ ∑\\n2\\n \\n(5-43)\\nwhere \\ng\\n and \\nw\\nh\\n____\\n denote the average values of \\ng\\n and of the product \\nw\\nh\\n in neighbor-\\nhood \\nS\\nxy\\n, respectively.\\nIf we assume that \\nw\\n is approximately constant in \\nS\\nxy\\n we can replace \\nw\\n(,\\n)\\nrc\\n by \\nthe value of \\nw\\n at the center of the neighborhood:\\n \\nww\\n(,\\n) ( , )\\nrc xy\\n=\\n \\n(5-44)\\nBecause \\nw\\n(,\\n)\\nxy\\n is assumed to be constant in \\nS\\nxy\\n,\\n it follows that \\nw=w\\n__\\n(,)\\nxy\\n and, \\ntherefore\\n, that\\n \\nww\\nhh\\n____\\n__\\n(,)\\n=\\nxy\\n \\n(5-45)\\nin \\nS\\nxy\\n,\\n where \\nh\\n is the average value of \\nh\\n in the neighborhood. Using these approxi-\\nmations, Eq. (5-43) becomes\\nsh\\nh\\n2\\n1\\n( , )\\n[ (, ) ( , )(, ) ] [ ( , ) ]\\n__\\n(, )\\nxy\\nmn\\ngrc xy rc g xy\\nrc\\n=− −\\n−\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n∈\\nww\\nS S\\nxy\\n∑\\n2\\n \\n(5-46)\\nDIP4E_GLOBAL_Print_Ready.indb   346\\n6/16/2017   2:07:26 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 347}),\n",
       " Document(page_content='5.4\\n  \\nPeriodic Noise Reduction Using Frequency Domain Filtering\\n    \\n347\\nb a\\nFIGURE 5.20\\n(a) Image of the \\nMartian  \\nterrain taken by \\nMariner 6.  \\n(b) Fourier \\nspectrum showing \\nperiodic  \\ninterference. \\n(Courtesy of \\nNASA.) \\nTo minimize \\ns\\n2\\n(,)\\nxy\\n with respect to \\nw\\n(,\\n)\\nxy\\n we solve\\n \\n∂\\n∂\\n=\\ns\\n2\\n0\\n(,)\\n(,)\\nxy\\nxy\\nw\\n \\n(5-47)\\nfor \\nw\\n(,\\n) .\\nxy\\n The result is (see Problem 5.17):\\n \\nw\\n(,)\\n____\\n___\\n__\\nxy\\ngg\\n=\\n−\\n−\\nhh\\nhh\\n22\\n \\n(5-48)\\nTo obtain the value of the restored image at point \\n(,)\\nxy\\n we use this equation to com-\\npute \\nw\\n(,\\n)\\nxy\\n and then substitute it into Eq. (5-40). To obtain the complete restored \\nimage\\n, we perform this procedure at every point in the noisy image, \\ng\\n.\\nEXAMPLE 5.7 :  Denoising (interference removal) using optimum notch ﬁltering.\\nFigure 5.20(a) shows a digital image of the Martian terrain taken by the Mariner 6 spacecraft. The image \\nis corrupted by a semi-periodic interference pattern that is considerably more complex (and much more \\nsubtle) than those we have studied thus far. The Fourier spectrum of the image, shown in Fig. 5.20(b), \\nhas a number of “starlike” bursts of energy caused by the interference. As expected, these components \\nare more difﬁcult to detect than those we have seen before. Figure 5.21 shows the spectrum again, but \\nwithout centering. This image offers a somewhat clearer view of the interference components because \\nthe more prominent dc term and low frequencies are “out of way,” in the top left of the spectrum.\\nFigure 5.22(a) shows the spectrum components that, in the judgement of an experienced image ana-\\nlyst, are associated with the interference. Applying a notch pass ﬁlter to these components and using \\nEq. (5-39) yielded the spatial noise pattern, \\nh\\n(,\\n) ,\\nxy\\n shown in Fig. 5.22(b). Note the similarity between \\nthis pattern and the structure of the noise in F\\nig. 5.20(a).\\nDIP4E_GLOBAL_Print_Ready.indb   347\\n6/16/2017   2:07:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 348}),\n",
       " Document(page_content='348\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.22\\n(a) Fourier spec-\\ntrum of \\nN\\n(,\\n) ,\\nuv\\nand  \\n(b) corresponding \\nspatial noise  \\ninterference  \\npattern,\\n \\nh\\n(,\\n) .\\nxy\\n(Courtesy of \\nN\\nASA.) \\nFinally, Fig. 5.23 shows the restored image, obtained using Eq. (5-40) with the interference pattern just \\ndiscussed. Function \\nw\\n(,\\n)\\nxy\\n was computed using the procedure explained in the preceding paragraphs. \\nAs you can see\\n, the periodic interference was virtually eliminated from the noisy image in Fig. 5.20(a).\\n5.5 LINEAR, POSITION-INVARIANT DEGRADATIONS  \\nThe input-output relationship in Fig. 5.1 before the restoration stage is expressed as\\n \\ngxy f xy xy\\n(,\\n) (,) (,)\\n=\\n[]\\n+\\n/H5108\\nh\\n \\n(5-49)\\nFor the moment, let us assume that \\nh\\n(,\\n)\\nxy\\n=\\n0\\n so that \\ngxy f xy\\n(,\\n) (,) .\\n=\\n[]\\n/H5108\\n Based on \\nthe discussion in Section 2.6,\\n \\n/H5108\\n is \\nlinear\\n if\\n5.5\\nFIGURE 5.21\\nUncentered  \\nFourier spectrum \\nof the image \\nin Fig. 5.20(a). \\n(Courtesy of \\nNASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   348\\n6/16/2017   2:07:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 349}),\n",
       " Document(page_content='5.5\\n  \\nLinear, Position-Invariant Degradations\\n    \\n349\\nFIGURE 5.23\\nRestored image. \\n(Courtesy of \\nNASA.) \\n \\n/H5108/H5108\\n/H5108\\na\\nf xy b f xy a f xy b f xy\\n12 1 2\\n(,) (,) (,) (,)\\n+\\n[]\\n=\\n[]\\n+\\n[]\\n \\n(5-50)\\nwhere \\na\\n and \\nb\\n are scalars and \\nfx y\\n1\\n(,)\\n and \\nfx y\\n2\\n(,)\\n are any two input images.\\nIf \\nab\\n==\\n1,\\n Eq. (5-50) becomes\\n \\n/H5108/H5108 /H5108\\nfx\\ny fx y fx y fx y\\n12 1 2\\n(,) (,) (,) (,)\\n+\\n[]\\n=\\n[]\\n+\\n[]\\n \\n(5-51)\\nwhich is called the property of \\nadditivity\\n.\\n This property says that, if \\n/H5108\\n is a linear \\noperator\\n, the response to a sum of two inputs is equal to the sum of the two responses.\\nWith \\nfx y\\n2\\n0\\n(,) ,\\n=\\n Eq. (5-50) becomes\\n \\n/H5108/H5108\\na\\nf xy a f xy\\n11\\n(,) (,)\\n[]\\n=\\n[]\\n \\n(5-52)\\nwhich is called the property of \\nhomogeneity\\n.\\n It says that the response to a constant \\nmultiple of any input is equal to the response to that input multiplied by the same \\nconstant. Thus, a linear operator possesses both the property of additivity and the \\nproperty of homogeneity. \\nAn operator having the input-output relationship \\ngxy f xy\\n(,\\n) (,)\\n=\\n[]\\n/H5108\\n is said to \\nbe \\nposition\\n (or \\nspace\\n) \\ninvariant\\n if\\n \\n/H5108\\nfx\\ny g x y\\n(, ) (, )\\n−−\\n[]\\n=−−\\nab ab\\n \\n(5-53)\\nfor any \\nfx y\\n(,\\n)\\n and any two scalars \\na\\n and \\nb\\n.\\n This definition indicates that the \\nresponse at any point in the image depends only on the \\nvalue\\n of the input at that \\npoint,\\n not on its \\nposition\\n. \\nUsing the sifting property of the 2-D continuous impulse [see Eq. (4-55)], we can \\nwrite \\nfx y\\n(,\\n)\\n as\\nDIP4E_GLOBAL_Print_Ready.indb   349\\n6/16/2017   2:07:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 350}),\n",
       " Document(page_content='350\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n \\nfx y f x y dd\\n(,\\n) (, )( , )\\n=− −\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nabd a b ab\\n \\n(5-54)\\nAssuming again that \\nh\\n(,\\n) ,\\nxy\\n=\\n0  substituting this equation into Eq. (5-49) yields\\n \\ngxy f xy\\nf x y d d\\n(,\\n) (,)\\n(, )( , )\\n=\\n[]\\n=−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n/H5108/H5108\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nabd a b ab\\n \\n(5-55)\\nIf \\n/H5108\\n is a linear operator and we extend the additivity property to integrals, then\\n \\ngxy\\nf x y d d\\n(,\\n)\\n(, )( , )\\n=−\\n−\\n[]\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n/H5108\\nabd a b ab\\n \\n(5-56)\\nBecause \\nf\\n(,\\n)\\nab\\n is independent of \\nx\\n and \\ny\\n,\\n and using the homogeneity property, it \\nfollows that\\n \\ngxy f x y d d\\n(,\\n) (, ) ( , )\\n=−\\n−\\n[]\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nab d a b ab\\n/H5108\\n \\n(5-57)\\nThe term\\n \\nhx y x y\\n(,\\n, , ) ( , )\\nab d a b\\n=− −\\n[]\\n/H5108\\n \\n(5-58)\\nis called the \\nimpulse response\\n of \\n/H5108\\n.\\n In other words, if \\nh\\n(,\\n)\\nxy\\n=\\n0\\n in Eq. (5-49), then \\nhx y\\n(,\\n, , )\\nab\\n is the response of \\n/H5108\\n to an impulse at coordinates \\n(,) .\\nxy\\n In optics, the \\nimpulse becomes a point of light and \\nhx y\\n(,\\n, , )\\nab\\n is commonly referred to as the \\npoint spread function\\n (PSF).\\n This name is based on the fact that all physical optical \\nsystems blur (spread) a point of light to some degree, with the amount of blurring \\nbeing determined by the quality of the optical components.\\nSubstituting Eq. (5-58) into Eq. (5-57) we obtain the expression\\n \\ngxy f hx y d d\\n(,\\n) (, )(, , , )\\n=\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nab a b ab\\n \\n(5-59)\\nwhich is called the \\nsuperposition\\n (or \\nF\\nredholm\\n) \\nintegral of the first kind\\n. This expres-\\nsion is a fundamental result that is at the core of linear system theory. It states that \\nif the response of \\n/H5108\\n to an impulse is known, the response to \\nany\\n input \\nf\\n(,\\n)\\nab\\n can \\nbe calculated using Eq.\\n (5-59). In other words, a linear system \\n/H5108\\n is \\nc\\nharacterized\\n \\ncompletely \\nby its impulse response.\\nIf \\n/H5108\\n is position invariant, then it follows from Eq. (5-53) that\\n \\n/H5108\\nda\\nb ab\\n(, ) (, )\\nxy h xy\\n−−\\n[]\\n=− −\\n \\n(5-60)\\nIn this case, Eq. (5-59) reduces to \\n \\ngxy f hx y d d\\n(,\\n) (, )( , )\\n=−\\n−\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nab a b ab\\n \\n(5-61)\\nDIP4E_GLOBAL_Print_Ready.indb   350\\n6/16/2017   2:07:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 351}),\n",
       " Document(page_content='5.5\\n  \\nLinear, Position-Invariant Degradations\\n    \\n351\\nThis expression is the \\nconvolution integral\\n introduced for one variable in Eq. (4-24) \\nand extended to 2-D in Problem 4.19. Equation (5-61) tells us that the output of a \\nlinear, position invariant system to \\nany\\n input, is obtained by convolving the input \\nand the system’s impulse response.\\nIn the presence of additive noise, the expression of the linear degradation model \\n[Eq. (5-59)] becomes\\n \\ngxy f hx y d d xy\\n(,\\n) (, )(, , , ) (,)\\n=+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nab a b ab h\\n \\n(5-62)\\nIf \\n/H5108\\n is position invariant, then this equation becomes\\n \\ngxy f hx y d d xy\\n(,\\n) (, )( , ) (,)\\n=− − +\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nab a b ab h\\n \\n(5-63)\\nThe values of the noise term \\nh\\n(,\\n)\\nxy\\n are random, and are assumed to be independent \\nof position.\\n Using the familiar notation for convolution introduced in Chapters 3 \\nand 4, we can write Eq. (5-63) as\\n \\ngxy h\\nxy\\nfx y\\n(,) (\\n(,)\\n)( , )\\n=+\\n/H22841\\nh\\n \\n(5-64)\\nor, using the convolution theorem, we write the equivalent result in the frequency \\ndomain as\\n \\nGH F N\\n(,\\n) (,) (,) (,)\\nuv uv uv uv\\n=+\\n \\n(5-65)\\nThese two expressions agree with Eqs. (5-1) and (5-2). Keep in mind that, for dis-\\ncrete quantities\\n, all products are elementwise products, as defined in Section 2.6. \\nIn summary, the preceding discussion indicates that a linear, spatially invariant \\ndegradation system with additive noise can be modeled in the spatial domain as \\nthe convolution of an image with the system’s degradation (point spread) function, \\nfollowed by the addition of noise. Based on the convolution theorem, the same pro-\\ncess can be expressed in the frequency domain as the product of the transforms of \\nthe image and degradation, followed by the addition of the transform of the noise. \\nWhen working in the frequency domain, we make use of an FFT algorithm. Howev-\\ner, unlike in Chapter 4, we do not use image padding in the implementation of any of \\nthe frequency domain restoration ﬁlters discussed in this chapter. The reason is that \\nin restoration work we usually have access only to degraded images. For padding \\nto be effective, it would have to be applied to images before they were degraded, a \\ncondition that obviously cannot be met in practice. If we had access to the original \\nimages, then restoration would be a mute point.\\nMany types of degradations can be approximated by linear, position-invariant \\nprocesses. The advantage of this approach is that the extensive tools of linear sys-\\ntem theory then become available for the solution of image restoration problems. \\nDIP4E_GLOBAL_Print_Ready.indb   351\\n6/16/2017   2:07:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 352}),\n",
       " Document(page_content='352\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nNonlinear and position-dependent techniques, although more general (and usually \\nmore accurate), introduce difﬁculties that often have no known solution or are very \\ndifﬁcult to solve computationally. This chapter focuses on linear, space-invariant res-\\ntoration techniques. Because degradations are modeled as being the result of convo-\\nlution, and restoration seeks to ﬁnd ﬁlters that apply the process in reverse, the term \\nimage deconvolution\\n is used frequently to signify linear image restoration. Similarly, \\nthe ﬁlters used in the restoration process often are called \\ndeconvolution ﬁlters\\n.\\n5.6 ESTIMATING THE DEGRADATION FUNCTION  \\nThere are three principal ways to estimate the degradation function for use in image \\nrestoration: (1) observation, (2) experimentation, and (3) mathematical modeling. \\nThese methods are discussed in the following sections. The process of restoring \\nan image by using a degradation function that has been estimated by any of these \\napproaches sometimes is called \\nblind deconvolution\\n, to emphasize the fact that the \\ntrue degradation function is seldom known completely. \\nESTIMATION BY IMAGE OBSERVATION\\nSuppose that we are given a degraded image without any knowledge about the degra- \\ndation function \\n/H5108\\n.\\n Based on the assumption that the image was degraded by a lin-\\near\\n, position-invariant process, one way to estimate \\n/H5108\\n is to gather information from \\nthe image itself\\n. For example, if the image is blurred, we can look at a small rectan-\\ngular section of the image containing sample structures, like part of an object and \\nthe background. In order to reduce the effect of noise, we would look for an area in \\nwhich the signal content is strong (e.g., an area of high contrast). The next step would \\nbe to process the subimage to arrive at a result that is as unblurred as possible. \\nLet the observed subimage be denoted by \\ngx y\\ns\\n(,) ,\\n and let the processed subimage \\n(which in reality is our estimate of the original image in that area) be denoted by \\nˆ\\n(,\\n) .\\nfx y\\ns\\n Then, assuming that the effect of noise is negligible because of our choice of \\na strong-signal area, it follows from Eq. (5-65) that\\n \\nH\\nG\\nF\\ns\\ns\\ns\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⁄\\n \\n(5-66)\\nFrom the characteristics of this function, we then deduce the complete degradation \\nfunction \\nH\\n(,\\n)\\nuv\\n based on our assumption of position invariance. For example, sup-\\npose that a radial plot of \\nH\\ns\\n(,)\\nuv\\n has the approximate shape of a Gaussian curve. We \\ncan use that information to construct a function \\nH\\n(,\\n)\\nuv\\n on a larger scale, but having \\nthe same basic shape\\n. We then use \\nH\\n(,\\n)\\nuv\\n in one of the restoration approaches to \\nbe discussed in the following sections\\n. Clearly, this is a laborious process used only in \\nvery specific circumstances, such as restoring an old photograph of historical value.\\nESTIMATION BY EXPERIMENTATION\\nIf equipment similar to the equipment used to acquire the degraded image is avail-\\nable, it is possible in principle to obtain an accurate estimate of the degradation. \\nImages similar to the degraded image can be acquired with various system settings \\n5.6\\nDIP4E_GLOBAL_Print_Ready.indb   352\\n6/16/2017   2:07:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 353}),\n",
       " Document(page_content='5.6\\n  \\nEstimating the Degradation Function\\n    \\n353\\nb a\\nFIGURE 5.24\\nEstimating a \\ndegradation by \\nimpulse  \\ncharacterization. \\n(a) An impulse \\nof light (shown \\nmagniﬁed).  \\n(b) Imaged  \\n(degraded)  \\nimpulse. \\nuntil they are degraded as closely as possible to the image we wish to restore. Then \\nthe idea is to obtain the impulse response of the degradation by imaging an impulse \\n(small dot of light) using the same system settings. As noted in Section 5.5, a linear, \\nspace-invariant system is characterized completely by its impulse response.\\nAn impulse is simulated by a bright dot of light, as bright as possible to reduce the \\neffect of noise to negligible values. Then, recalling that the Fourier transform of an \\nimpulse is a constant, it follows from Eq. (5-65) that\\n \\nH\\nG\\nA\\n(,)\\n(,)\\nuv\\nuv\\n=\\n \\n(5-67)\\nwhere, as before, \\nG\\n(,\\n)\\nuv\\n is the Fourier transform of the observed image, and \\nA\\n is a \\nconstant describing the strength of the impulse\\n. Figure 5.24 shows an example.\\nESTIMATION BY MODELING\\nDegradation modeling has been used for many years because of the insight it affords \\ninto the image restoration problem. In some cases, the model can even take into \\naccount environmental conditions that cause degradations. For example, a degrada-\\ntion model proposed by Hufnagel and Stanley [1964] is based on the physical char-\\nacteristics of atmospheric turbulence. This model has a familiar form:\\n \\nHe\\nku\\n(,)\\n()\\n/\\nuv\\nv\\n=\\n−+\\n22 5 6\\n \\n(5-68)\\nwhere \\nk\\n is a constant that depends on the nature of the turbulence\\n. With the excep-\\ntion of the \\n56\\n power in the exponent, this equation has the same form as the Gauss-\\nian lowpass filter transfer function discussed in Section 4.8.\\n In fact, the Gaussian \\nLPF is used sometimes to model mild, uniform blurring. Figure 5.25 shows examples \\nobtained by simulating blurring an image using Eq. (5-68) with values \\nk\\n=\\n0\\n0025 .\\n \\nDIP4E_GLOBAL_Print_Ready.indb   353\\n6/16/2017   2:07:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 354}),\n",
       " Document(page_content='354\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nd c\\nFIGURE 5.25\\nModeling  \\nturbulence.  \\n(a) No visible \\nturbulence.  \\n(b) Severe  \\nturbulence, \\nk\\n=\\n0\\n0025\\n..\\n  \\n(c) Mild  \\nturbulence\\n, \\nk\\n=\\n0\\n001\\n..\\n  \\n(d) Low  \\nturbulence\\n, \\nk\\n=\\n0\\n00025\\n..\\n \\nAll images are \\nof size \\n480 480\\n×\\n \\npixels\\n. \\n(Original  \\nimage courtesy of \\nNASA.) \\n(severe turbulence), \\nk\\n=\\n0\\n001 .\\n (mild turbulence), and \\nk\\n=\\n0\\n00025 .\\n (low turbulence). \\nW\\ne restore these images using various methods later in this chapter.\\nAnother approach used frequently in modeling is to derive a mathematical model \\nstarting from basic principles. We illustrate this procedure by treating in some detail \\nthe case in which an image has been blurred by uniform linear motion between \\nthe image and the sensor during image acquisition. Suppose that an image \\nfx y\\n(,\\n)\\n \\nundergoes planar motion and that \\nxt\\n0\\n()\\n and \\nyt\\n0\\n()\\n are the time-varying components \\nof motion in the \\nx\\n- and \\ny\\n-directions\\n, respectively. We obtain the total exposure at \\nany point of the recording medium (say, ﬁlm or digital memory) by integrating the \\ninstantaneous exposure over the time interval during which the imaging system \\nshutter is open.\\nAssuming that shutter opening and closing takes place instantaneously, and that \\nthe optical imaging process is perfect, lets us isolate the effects due to image motion. \\nThen, if \\nT\\n is the duration of the exposure, it follows that\\nDIP4E_GLOBAL_Print_Ready.indb   354\\n6/16/2017   2:07:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 355}),\n",
       " Document(page_content='5.6\\n  \\nEstimating the Degradation Function\\n    \\n355\\n \\ngxy fx x t y y t d t\\nT\\n(,) [ ( ) , ( ) ]\\n=− −\\n0\\n00\\n2\\n \\n(5-69)\\nwhere \\ngxy\\n(,\\n)\\n is the blurred image.\\nT\\nhe continuous Fourier transform of this expression is\\n \\nGg\\nx\\ny\\ne d\\nx\\nd\\ny\\nju x y\\n(,) (,)\\n()\\nuv\\nv\\n=\\n−+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\np\\n \\n(5-70)\\nSubstituting Eq. (5-69) into Eq. (5-70) yields\\nGf\\nx\\nx\\nt\\ny\\ny\\nt\\nd\\nt\\ne\\nT\\nju x y\\n(,)\\n[ ( ) , ( ) ]\\n()\\nuv\\nv\\n=− −\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22 2\\n0\\n00\\n2\\np\\nd\\ndx dy\\n \\n(5-71)\\nReversing the order of integration results in the expression\\nGf\\nx\\nx\\nt\\ny\\ny\\nt\\ne\\nd\\nx\\nd\\ny\\nT\\nju x y\\n(,)\\n[ ( ) , ( ) ]\\n()\\nuv\\nv\\n=− −\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n−+\\n0\\n00\\n2\\n22 2\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\np\\n⎥ ⎥\\n⎥\\ndt\\n \\n(5-72)\\nThe term inside the outer brackets is the Fourier transform of the displaced function \\nfx xty yt\\n−−\\n[]\\n00\\n() , ().\\n Using entry 3 in Table 4.4 then yields the expression\\n \\nGF d t\\nF\\nT\\nT\\nju x t y t\\nju x t\\n(,) (,) e\\n(,) e\\n() ()\\n(\\nuv uv\\nuv\\nv\\n=\\n=\\n−+\\n[]\\n−\\n0\\n0\\n2\\n2\\n00\\n0\\n2\\n2\\np\\np\\n)\\n)( )\\n+\\n[]\\nv\\nyt\\ndt\\n0\\n \\n(5-73)\\nBy defining\\n \\nHd\\nt\\nT\\nju x t y t\\n(,) e\\n() ()\\nuv\\nv\\n=\\n−+\\n[]\\n0\\n2\\n00\\n2\\np\\n \\n(5-74)\\nwe can express Eq. (5-73) in the familiar form\\n \\nGH F\\n(,\\n) (,) (,)\\nuv uv uv\\n=\\n \\n(5-75)\\nIf the motion variables \\nxt\\n0\\n()\\n and \\nyt\\n0\\n()\\n are known, the transfer function \\nH\\n(,\\n)\\nuv\\n can \\nbe obtained directly from Eq.\\n (5-74). As an illustration, suppose that the image in \\nquestion undergoes uniform linear motion in the \\nx\\n-direction only (i.e., \\nyt\\n0\\n0\\n() ) ,\\n=\\n at \\na rate \\nx t at T\\n0\\n() .\\n=\\n When \\ntT\\n=\\n,\\n the image has been displaced by a total distance \\na\\n. \\nW\\nith \\nyt\\n0\\n0\\n() ,\\n=\\n Eq. (5-74) yields\\n \\nHd\\nt d\\nt\\nT\\nua\\nua e\\nTT\\nju x t\\nj uat T\\nju a\\n(,) e\\ne\\nsin( )\\n()\\nuv\\n==\\n=\\n−\\n−\\n−\\n00\\n2\\n2\\n0\\n22\\np\\np\\np\\np\\np\\n \\n(5-76)\\nDIP4E_GLOBAL_Print_Ready.indb   355\\n6/16/2017   2:07:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 356}),\n",
       " Document(page_content='356\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.26\\n(a) Original  \\nimage. (b) Result \\nof blurring using \\nthe function in \\nEq. (5-77) with \\nab\\n==\\n01\\n.  and \\nT\\n=\\n1. \\nIf we allow the \\ny\\n-component to vary as well, with the motion given by \\nyt b t T\\n0\\n() ,\\n=\\n \\nthen the degradation function becomes\\n \\nH\\nT\\nua b\\nua b e\\nju a b\\n(,)\\n()\\nsin ( )\\n()\\nuv\\nv\\nv\\nv\\n=\\n+\\n+\\n[]\\n−+\\np\\np\\np\\n \\n(5-77)\\nTo generate a discrete filter transfer function of size \\nMN\\n×\\n,\\n we sample this equation \\nfor \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n and \\nv\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n \\nEXAMPLE 5.8 : Image blurring caused by motion.\\nFigure 5.26(b) is an image blurred by computing the Fourier transform of the image in Fig. 5.26(a), mul-\\ntiplying the transform by \\nH\\n(,\\n)\\nuv\\n from Eq. (5-77), and taking the inverse transform. The images are of \\nsize 688 688\\n×\\n pixels, and we used \\nab\\n==\\n01\\n.\\n and \\nT\\n=\\n1\\n in Eq. (5-77). As we will discuss in Sections 5.8 \\nand 5.9,\\n recovery of the original image from its blurred counterpart presents some interesting challenges, \\nparticularly when noise is present in the degraded image. As mentioned at the end of Section 5.5, we \\nperform all DFT computations without padding.\\n5.7 INVERSE FILTERING  \\nThe material in this section is our first step in studying restoration of images degrad-\\ned by a degradation function \\n/H5108\\n,\\n which is given, or is obtained by a method such \\nas those discussed in the previous section.\\n The simplest approach to restoration is \\ndirect inverse filtering, where we compute an estimate, \\nˆ\\n(,\\n) ,\\nF\\nuv\\n of the transform of \\nthe original image by dividing the transform of the degraded image\\n, \\nG\\n(,\\n) ,\\nuv\\n by the \\ndegradation transfer function:\\n \\n \\nˆ\\n(,)\\n(,)\\n(,)\\nF\\nG\\nH\\nuv\\nuv\\nuv\\n=\\n \\n(5-78)\\n5.7\\nDIP4E_GLOBAL_Print_Ready.indb   356\\n6/16/2017   2:07:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 357}),\n",
       " Document(page_content='5.7\\n  \\nInverse Filtering\\n    \\n357\\nThe division is elementwise, as defined in Section 2.6 and in connection with Eq. \\n(5-65). Substituting the right side of Eq. (5-2) for \\nG\\n(,\\n)\\nuv\\n in Eq. (5-78) yields\\n \\nˆ\\n(,) (,)\\n(,)\\n(,)\\nFF\\nN\\nH\\nuv uv\\nuv\\nuv\\n=+\\n \\n(5-79)\\nThis is an interesting expression. It tells us that, even if we know the degradation \\nfunction,\\n we cannot recover the undegraded image [the inverse Fourier transform \\nof \\nF\\n(,\\n)\\nuv\\n] exactly because \\nN\\n(,\\n)\\nuv\\n is not known. There is more bad news. If the deg-\\nradation function has zero or very small values\\n, then the ratio \\nNH\\n(,) (,)\\nuv uv\\n could \\neasily dominate the term \\nF\\n(,\\n) .\\nuv\\n In fact, this is frequently the case, as you will see \\nshortly\\n.\\nOne approach to get around the zero or small-value problem is to limit the ﬁlter \\nfrequencies to values near the origin. From the discussion of Eq. (4-92), we know \\nthat \\nH\\n(,\\n)\\n00\\n is usually the highest value of \\nH\\n(,\\n)\\nuv\\n in the frequency domain. Thus, \\nby limiting the analysis to frequencies near the origin,\\n we reduce the likelihood of \\nencountering zero values. The following example illustrates this approach.\\nEXAMPLE 5.9 :  Image deblurring by inverse ﬁltering.\\nThe image in Fig. 5.25(b) was inverse ﬁltered with Eq. (5-78) using the exact inverse of the degradation \\nfunction that generated that image. That is, the degradation function used was\\n \\nHe\\nku M N\\n(,)\\n() ( )\\n/\\nuv\\nv\\n=\\n−+ + −\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n56\\nwith \\nk\\n=\\n0\\n0025\\n..\\n The \\nM\\n2\\n and \\nN\\n2\\n constants are offset values; they center the function so that it will \\ncorrespond with the centered F\\nourier transform, as discussed in the previous chapter. (Remember, we \\ndo not use padding with these functions.) In this case, \\nMN\\n==\\n480.\\n We know that a Gaussian function \\nhas no zeros\\n, so that will not be a concern here. However, despite this, the degradation values became so \\nsmall that the result of full inverse ﬁltering [Fig. 5.27(a)] is useless. The reasons for this poor result are \\nas discussed in connection with Eq. (5-79). \\nFigures 5.27(b) through (d) show the results of cutting off values of the ratio \\nGH\\n(,) (,)\\nuv uv\\n outside \\na radius of 40,\\n 70, and 85, respectively. The cut off was implemented by applying to the ratio a Butter-\\nworth lowpass function of order 10. This provided a sharp (but smooth) transition at the desired radius. \\nRadii near 70 yielded the best visual results [Fig. 5.27(c)]. Radii below 70 resulted in blurred images, as \\nin Fig. 5.27(b), which was obtained using a radius of 40. Values above 70 started to produce degraded \\nimages, as illustrated in Fig. 5.27(d), which was obtained using a radius of 85. The image content is almost \\nvisible in this image behind a “curtain” of noise, but the noise deﬁnitely dominates the result. Further \\nincreases in radius values produced images that looked more and more like Fig. 5.27(a).\\nThe results in the preceding example are illustrative of the poor performance of \\ndirect inverse ﬁltering in general. The basic theme of the three sections that follow is \\nhow to improve on direct inverse ﬁltering.\\nDIP4E_GLOBAL_Print_Ready.indb   357\\n6/16/2017   2:07:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 358}),\n",
       " Document(page_content='358\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n5.8 MINIMUM MEAN SQUARE ERROR (WIENER) FILTERING  \\nThe inverse filtering approach discussed in the previous section makes no explicit \\nprovision for handling noise. In this section, we discuss an approach that incorpo-\\nrates both the degradation function and statistical characteristics of noise into the \\nrestoration process. The method is founded on considering images and noise as ran-\\ndom variables, and the objective is to find an estimate \\nˆ\\nf\\n of the uncorrupted image \\nf\\n \\nsuch that the mean square error between them is minimized.\\n This error measure is \\ndefined as\\n \\neE f f\\n22\\n=−\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n()\\n⁄\\n \\n(5-80)\\nwhere \\nE\\ni\\n{}\\n is the expected value of the argument. We assume that the noise and the \\nimage are uncorrelated, that one or the other has zero mean, and that the intensity \\nlevels in the estimate are a linear function of the levels in the degraded image. Based \\n5.8\\nb a\\nd c\\nFIGURE 5.27\\nRestoring  \\nFig. 5.25(b)  \\nusing Eq. (5-78).  \\n(a) Result of using \\nthe full ﬁlter.  \\n(b) Result with \\nH\\n \\ncut off outside a \\nradius of 40.  \\n(c) Result with \\nH\\n \\ncut off outside a \\nradius of 70.  \\n(d) Result with \\nH\\n \\ncut off outside a \\nradius of 85.\\nDIP4E_GLOBAL_Print_Ready.indb   358\\n6/16/2017   2:07:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 359}),\n",
       " Document(page_content='5.8\\n  \\nMinimum Mean Square Error (Wiener) Filtering\\n    \\n359\\non these assumptions, the minimum of the error function in Eq. (5-80) is given in the \\nfrequency domain by the expression \\n \\nˆ\\n(,)\\n(,) (,)\\n(,) (,) (,)\\n(,)\\n*\\nF\\nHu Su\\nSu H u Su v\\nGu\\nf\\nf\\nuv\\nvv\\nvv\\nv\\n=\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n2\\nh\\n= =\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\nHu\\nHu S u S u\\nGu\\nHu\\nHu\\nf\\n*\\n(,)\\n(,) (,) (,)\\n(,)\\n(,)\\n(,\\nv\\nvv v\\nv\\nv\\nv\\n2\\n1\\nh\\n) )\\n(,) (,) (,)\\n(,)\\n2\\n2\\nHu S u S u\\nGu\\nf\\nvv v\\nv\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\nh\\n \\n(5-81)\\nwhere we used the fact that the product of a complex quantity with its conjugate \\nis equal to the magnitude of the complex quantity squared.\\n This result is known as \\nthe \\nWiener filter\\n, after N. Wiener [1942], who first proposed the concept in the year \\nshown. The filter, which consists of the terms inside the brackets, also is commonly \\nreferred to as the \\nminimum mean square error filter\\n or the \\nleast square error filter\\n. \\nWe include references at the end of the chapter to sources containing detailed deri-\\nvations of the Wiener filter. Note from the first line in Eq. (5-81) that the Wiener \\nfilter does not have the same problem as the inverse filter with zeros in the degrada-\\ntion function, unless the entire denominator is zero for the same value(s) of \\nu\\n and \\nv\\n.\\nThe terms in Eq. (5-81) are as follows:\\n1. \\nˆ\\n(,\\n)\\nF\\nuv\\n=\\n Fourier transform of the estimate of the undegraded image.\\n2. \\nG\\n(,\\n)\\nuv\\n=\\n Fourier transform of the degraded image.\\n3. \\nH\\n(,\\n)\\nuv\\n = degradation transfer function (Fourier transform of the spatial \\ndegradation).\\n4. \\nH\\n∗\\n(,)\\nuv\\n = complex conjugate of \\nH\\n(,\\n)\\nuv\\n.\\n5. \\nHH H\\n(,) (,) (,)\\nuv uv uv\\n2\\n=\\n∗\\n.\\n6. \\nSN\\nh\\n(,) (,)\\nuv uv\\n==\\n2\\n power spectrum of the noise [see Eq. (4-89)]\\n†\\n7. \\nSF\\nf\\n(,) (,)\\nuv uv\\n==\\n2\\n power spectrum of the undegraded image.\\nThe restored image in the spatial domain is given by the inverse Fourier transform \\nof the frequency-domain estimate \\nˆ\\n(,\\n) .\\nF\\nuv\\n Note that if the noise is zero, then the \\nnoise power spectrum vanishes and the \\nWiener filter reduces to the inverse filter. \\nAlso, keep in mind the discussion at the end of Section 5.5 regarding the fact that all \\ntransform work in this chapter is done without padding.\\n†\\n  The term \\nN\\n(,)\\nuv\\n2\\n also is referred to as the \\nautocorrelation\\n of the noise. This term comes from the correlation \\ntheorem (ﬁrst line of entry 7 in Table 4.4). When the two functions are the same, correlation becomes \\nautocorrela-\\ntion\\n and the right side of that entry becomes \\nHH\\n∗\\n(,) (,) ,\\nuv uv\\n which is equal to \\nH\\n(,).\\nuv\\n2\\n Similar comments apply \\nto \\nF\\n(,) ,\\nuv\\n2\\n which is the autocorrelation of the image. We will discuss correlation in more detail in Chapter 12. \\nDIP4E_GLOBAL_Print_Ready.indb   359\\n6/16/2017   2:07:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 360}),\n",
       " Document(page_content='360\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nA number of useful measures are based on the power spectra of noise and of the \\nundegraded image. One of the most important is the \\nsignal-to-noise ratio\\n, approxi-\\nmated using frequency domain quantities such as\\n \\nSNR\\n=\\n=\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n∑ ∑\\n∑ ∑\\nF\\nN\\nN\\nu\\nM\\nN\\nu\\nM\\n(,)\\n(,)\\nuv\\nuv\\nv\\nv\\n2\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\n \\n(5-82)\\nThis ratio gives a measure of the level of information-bearing signal power (i.e., of \\nthe original,\\n undegraded image) to the level of noise power. An image with low \\nnoise would tend to have a high SNR and, conversely, the same image with a higher \\nlevel of noise would have a lower SNR. This ratio is an important measure used in \\ncharacterizing the performance of restoration algorithms.\\nThe \\nmean square error\\n given in statistical form in Eq. (5-80) can be approximated \\nalso in terms of a summation involving the original and restored images:\\n \\nMSE\\n=−\\n[]\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n0\\n1\\n0\\n1\\n2\\nMN\\nfx y fx y\\ny\\nN\\nx\\nM\\n(,) (,)\\n⁄\\n \\n(5-83)\\nIn fact, if one considers the restored image to be “signal” and the difference between \\nthis image and the original to be \\n“noise,” we can define a signal-to-noise ratio in the \\nspatial domain as\\n \\nSNR\\n=\\n()\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n∑\\n∑∑\\nˆ\\n,(\\n,\\n)\\nˆ\\n(,)\\nfx y fx y fx y\\ny\\nN\\nx\\nM\\ny\\nM\\nx\\nM\\n2\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\n∑ ∑\\n \\n(5-84)\\nThe closer \\nf\\n and \\nˆ\\nf\\n are, the larger this ratio will be. Sometimes the square root of the \\npreceding two measures is used instead,\\n in which case they are referred to as the \\nroot-mean-square-error\\n and the \\nroot-mean-square-signal-to-noise ratio\\n, respectively. \\nAs we have mentioned before, keep in mind that quantitative measures do not nec-\\nessarily relate well to perceived image quality.\\nWhen dealing with white noise, the spectrum is a constant, which simpliﬁes things \\nconsiderably. However, the power spectrum of the undegraded image seldom is \\nknown. An approach frequently used when these quantities are not known, or can-\\nnot be estimated, is to approximate Eq. (5-81) by the expression\\n \\nˆ\\n(,)\\n(,)\\n(,)\\n(,)\\n(,)\\nF\\nH\\nH\\nHK\\nG\\nuv\\nuv\\nuv\\nuv\\nuv\\n=\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n2\\n \\n(5-85)\\nwhere \\nK\\n is a specified constant that is added to all terms of \\nH\\n(,).\\nuv\\n2\\n The following \\nexamples illustrate the use of this expression.\\nDIP4E_GLOBAL_Print_Ready.indb   360\\n6/16/2017   2:07:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 361}),\n",
       " Document(page_content='5.8\\n  \\nMinimum Mean Square Error (Wiener) Filtering\\n    \\n361\\nb a\\nc\\nFIGURE 5.28\\n  Comparison of inverse and Wiener ﬁltering. (a) Result of full inverse ﬁltering of Fig. 5.25(b). (b) Radially \\nlimited inverse ﬁlter result. (c) Wiener ﬁlter result.\\nEXAMPLE 5.10 :  Comparison of deblurring by inverse and Wiener ﬁltering.\\nFigure 5.28 illustrates the advantage of Wiener ﬁltering over direct inverse ﬁltering. Figure 5.28(a) is the \\nfull inverse-ﬁltered result from Fig. 5.27(a). Similarly, Fig. 5.28(b) is the radially limited inverse ﬁlter result \\nof Fig, 5.27(c). These images are duplicated here for convenience in making comparisons. Figure 5.28(c) \\nshows the result obtained using Eq. (5-85) with the degradation function used in Example 5.9. The value \\nof \\nK\\n was chosen interactively to yield the best visual results. The advantage of Wiener ﬁltering over the \\ndirect inverse approach is evident in this example. By comparing Figs. 5.25(a) and 5.28(c), we see that \\nthe Wiener ﬁlter yielded a result very close in appearance to the original, undegraded image. \\nEXAMPLE 5.11 :  More deblurring examples using Wiener ﬁltering.\\nThe ﬁrst row of Fig. 5.29 shows, from left to right, the blurred image of Fig. 5.26(b) heavily corrupted by \\nadditive Gaussian noise of zero mean and variance of 650; the result of direct inverse ﬁltering; and the \\nresult of Wiener ﬁltering. The Wiener ﬁlter of Eq. (5-85) was used, with \\nH\\n(,\\n)\\nuv\\n from Example 5.8, and \\nwith \\nK\\n chosen interactively to give the best possible visual result.\\n As expected, direct inverse ﬁltering \\nproduced an unusable image. Note that the noise in the inverse ﬁltered image is so strong that it masks \\ncompletely the content of the image. The Wiener ﬁlter result is by no means perfect, but it does give us \\na hint as to image content. The text can be read with moderate effort. \\nThe second row of Fig. 5.29 shows the same sequence just discussed, but with the level of the noise \\nvariance reduced by one order of magnitude. This reduction had little effect on the inverse ﬁlter, but \\nthe Wiener results are considerably improved. For example, the text is much easier to read now. In the \\nthird row of Fig. 5.29, the noise variance was reduced more than ﬁve orders of magnitude from the ﬁrst \\nrow. In fact, image in Fig. 5.29(g) has no visible noise. The inverse ﬁlter result is interesting in this case. \\nThe noise is still quite visible, but the text can be seen through a “curtain” of noise (see Problem 5.30). \\nThe Wiener ﬁlter result in Fig. 5.29(i) is excellent, being quite close visually to the original image in Fig. \\nDIP4E_GLOBAL_Print_Ready.indb   361\\n6/16/2017   2:07:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 362}),\n",
       " Document(page_content='362\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 5.29\\n (a) 8-bit image corrupted by motion blur and additive noise. (b) Result of inverse ﬁltering. (c) Result of \\nWiener ﬁltering. (d)–(f) Same sequence, but with noise variance one order of magnitude less. (g)–(i) Same sequence, \\nbut noise variance reduced by ﬁve orders of magnitude from (a). Note in (h) how the deblurred image is quite vis-\\nible through a “curtain” of noise.  \\nDIP4E_GLOBAL_Print_Ready.indb   362\\n6/16/2017   2:07:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 363}),\n",
       " Document(page_content='5.9\\n  \\nConstrained Least Squares Filtering\\n    \\n363\\n5.26(a). In practice, the results of restoration ﬁltering are seldom this close to the original images. This \\nexample, and Example 5.12 in the next section, were idealized slightly to focus on the effects of noise \\non restoration algorithms. \\n5.9 CONSTRAINED LEAST SQUARES FILTERING \\nThe problem of having to know something about the degradation function \\nH\\n is com-\\nmon to all methods discussed in this chapter. However, the Wiener filter presents \\nan additional difficulty: the power spectra of the undegraded image and noise must \\nbe known also. We showed in the previous section that in some cases it is possible \\nto achieve acceptable results using the approximation in Eq. (5-85), but a constant \\nvalue for the ratio of the power spectra is not always a suitable solution. \\nThe method discussed in this section requires knowledge of only the mean and \\nvariance of the noise. As discussed in Section 5.2, these parameters generally can be \\ncalculated from a given degraded image, so this is an important advantage. Another \\ndifference is that the Wiener ﬁlter is based on minimizing a statistical criterion and, \\nas such, it is optimal in an average sense. The algorithm presented in this section \\nhas the notable feature that it yields an optimal result for each image to which it \\nis applied. Of course, it is important to keep in mind that these optimality criteria, \\nalthough they are comforting from a theoretical point of view, are not related to \\nthe dynamics of visual perception. As a result, the choice of one algorithm over the \\nother will almost always be determined by the perceived visual quality of the result-\\ning images.\\nBy using the deﬁnition of convolution given in Eq. (4-94), and as explained in \\nSection 2.6, we can express Eq. (5-64) in vector-matrix form:\\n \\ngH f\\n=+\\nH\\n \\n(5-86)\\nFor example, suppose that \\ngxy\\n(,\\n)\\n is of size \\nMN\\n×\\n.\\n We can form the first \\nN\\n elements \\nof vector \\ng\\n by using the image elements in the first row of \\ngxy\\n(,\\n) ,\\n the next \\nN\\n ele-\\nments from the second row\\n, and so on. The dimensionality of the resulting vector will \\nbe \\nMN\\n×\\n1.\\n These are also the dimensions of \\nf\\n and \\nH\\n,\\n as these vectors are formed in \\nthe same manner\\n. Matrix \\nH\\n then has dimensions \\nMN MN\\n×\\n.\\n Its elements are given \\nby the elements of the convolution in Eq.\\n (4-94). \\nIt would be reasonable to arrive at the conclusion that the restoration problem \\ncan now be reduced to simple matrix manipulations. Unfortunately, this is not the \\ncase. For instance, suppose that we are working with images of medium size, say \\nMN\\n==\\n512.\\n Then the vectors in Eq. (5-86) would be of dimension \\n262 144 1 ,\\n×\\n \\nand matrix \\nH\\n would be of dimension \\n262 144 262 144\\n,,\\n.\\n×\\n Manipulating vectors and \\nmatrices of such sizes is not a trivial task.\\n The problem is complicated further by \\nthe fact that \\nH\\n is highly sensitive to noise (after the experiences we had with the \\neffect of noise in the previous two sections, this should not be a surprise). The key \\nadvantage of formulating the restoration problem in matrix form is that it facilitates \\nderivation of restoration algorithms.\\nAlthough we do not fully derive the method of constrained least squares that \\nwe are about to present, this method has its roots in a matrix formulation. We give \\n5.9\\nSee Gonzalez and Woods \\n[1992] for an entire chap-\\nter devoted to the topic \\nof algebraic techniques \\nfor image restoration. \\nDIP4E_GLOBAL_Print_Ready.indb   363\\n6/16/2017   2:07:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 364}),\n",
       " Document(page_content='364\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nreferences at the end of the chapter to sources where derivations are covered in \\ndetail. Central to the method is the issue of the sensitivity of \\nH\\n to noise. One way \\nto reduce the effects of noise sensitivity, is to base optimality of restoration on a \\nmeasure of smoothness, such as the second derivative of an image (our old friend, \\nthe Laplacian). To be meaningful, the restoration must be constrained by the param-\\neters of the problems at hand. Thus, what is desired is to find the minimum of a \\ncriterion function, \\nC\\n, defined as\\n \\nCf\\nx\\ny\\ny\\nN\\nx\\nM\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n=\\n−\\n∑ ∑\\n/H11612\\n2\\n0\\n1\\n0\\n1\\n2\\n(,)\\n \\n(5-87)\\nsubject to the constraint\\n \\ngH f\\n−=\\nˆ\\n2\\n2\\nH\\n \\n(5-88)\\nwhere \\naa a\\n2\\n≜\\nT\\n is the Euclidean norm (see Section 2.6), and \\nˆ\\nf\\n is the estimate of the \\nundegraded image\\n. The Laplacian operator \\n/H11612\\n2\\n is defined in Eq. (3-50).\\nThe frequency domain solution to this optimization problem is given by the \\nexpression\\n \\nˆ\\n(,)\\n(,)\\n(,) (,)\\n(,)\\nF\\nH\\nHP\\nG\\nuv\\nuv\\nuv uv\\nuv\\n=\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n∗\\n22\\ng\\n \\n(5-89)\\nwhere \\ng\\n is a parameter that must be adjusted so that the constraint in Eq. (5-88) is \\nsatisfied, and \\nP\\n(,\\n)\\nuv\\n is the Fourier transform of the function\\n \\npxy\\n(,\\n)\\n=\\n−\\n−−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n01 0\\n14 1\\n01 0\\n \\n(5-90)\\nWe recognize this function as a Laplacian kernel from Fig. 3.45. Note that Eq. (5-89) \\nreduces to inverse filtering if \\ng\\n=\\n0.\\nFunctions \\nP\\n(,\\n)\\nuv\\n and \\nH\\n(,\\n)\\nuv\\n must be of the same size. If \\nH\\n is of size \\nMN\\n×\\n,\\n this \\nmeans that \\npxy\\n(,\\n)\\n must be embedded in the center of an \\nMN\\n×\\n array of zeros. In \\norder to preserve the even symmetry of \\npxy\\n(,\\n) ,\\n \\nM\\n and \\nN\\n must be even integers\\n, as \\nexplained in Examples 4.10 and 4.15. If a given degraded image from which \\nH\\n is \\nobtained is not of even dimensions, then a row and/or column, as appropriate, must \\nbe deleted before computing \\nH\\n for use in Eq. (5-89).\\nEXAMPLE 5.12 :  Comparison of deblurring by Wiener and constrained least squares ﬁltering.\\nFigure 5.30 shows the result of processing Figs. 5.29(a), (d), and (g) with constrained least squares ﬁl-\\nters, in which the values of \\ng\\n were selected manually to yield the best visual results. This is the same \\nprocedure we used to generate the Wiener ﬁlter results in Fig. 5.29(c), (f), and (i). By comparing the \\nconstrained least squares and Wiener results, we see that the former yielded better results (especially in \\nterms of noise reduction) for the high- and medium-noise cases, with both ﬁlters generating essentially \\nThe quantity in brackets \\nis the transfer function \\nof the constrained least \\nsquares ﬁlter. Note that \\nit reduces to the inverse \\nﬁlter transfer function \\nwhen \\ng\\n = 0.\\nDIP4E_GLOBAL_Print_Ready.indb   364\\n6/16/2017   2:07:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 365}),\n",
       " Document(page_content='5.9\\n  \\nConstrained Least Squares Filtering\\n    \\n365\\nb a\\nc\\nFIGURE 5.30\\n  Results of constrained least squares ﬁltering. Compare (a), (b), and (c) with the Wiener ﬁltering results \\nin Figs. 5.29(c), (f), and (i), respectively. \\nequal results for the low-noise case. This is not surprising because parameter \\ng\\n in Eq. (5-89) is a true \\nscalar, whereas the value of \\nK\\n in Eq. (5-85) is a scalar approximation to the ratio of two unknown fre-\\nquency domain \\nfunctions\\n of size \\nMN\\n×\\n.\\n Thus, it stands to reason that a result based on manually select-\\ning \\ng\\n would be a more accurate estimate of the undegraded image. As in Example 5.11, the results in \\nthis example are better than one normally ﬁnds in practice. Our focus here was on the effects of noise \\nblurring on restoration. As noted earlier, you will encounter situations in which the restoration solutions \\nare not quite as close to the original images as we have shown in these two examples.\\nAs discussed in the preceding example, it is possible to adjust the parameter \\ng\\n \\ninteractively until acceptable results are achieved. However, if we are interested in \\nmathematical optimality, then this parameter must be adjusted so that the constraint \\nin Eq. (5-88) is satisﬁed. A procedure for computing \\ng\\n by iteration is as follows.\\nDeﬁne a “residual” vector \\nr\\n as\\n \\nrg H f\\n=−\\nˆ\\n \\n(5-91)\\nFrom Eq. (5-89), we see that \\nˆ\\n(,\\n)\\nF\\nuv\\n (and by implication \\nˆ\\nf\\n) is a function of \\ng\\n.\\n Then \\nit follows that \\nr\\n also is a function of this parameter\\n. It can be shown (Hunt [1973], \\nGonzalez and Woods [1992]) that\\n \\nfg\\n()\\n=\\n=\\nrr\\nr\\nT\\n2\\n \\n(5-92)\\nis a monotonically increasing function of \\ng\\n. What we want to do is adjust \\ng\\n so that\\n \\nr\\n22\\n=\\nH\\n±α\\n \\n(5-93)\\nDIP4E_GLOBAL_Print_Ready.indb   365\\n6/16/2017   2:07:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 366}),\n",
       " Document(page_content='366\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nwhere \\na\\n is an accuracy factor. In view of Eq. (5-91), if \\nr\\n22\\n=\\nH\\n,\\n the constraint in \\nEq.\\n (5-88) will be strictly satisfied.\\nBecause \\nfg\\n()\\n is monotonic, ﬁnding the desired value of \\ng\\n is not difﬁcult. One \\napproach is to\\n1. \\nSpecify an initial value of \\ng\\n.\\n2.\\n \\nCompute \\nr\\n2\\n.\\n3. \\nStop if Eq. (5-93) is satisﬁed; otherwise return to Step 2 after increasing \\ng\\n if \\nr\\n22\\n<\\n()\\nHa\\n−\\n or decreasing \\ng\\n if \\nr\\n22\\n>\\n() .\\nHa\\n+\\n Use the new value of \\ng\\n in \\nEq. (5-89) to recompute the optimum estimate \\nˆ\\n(,\\n) .\\nF\\nuv\\n \\nOther procedures\\n, such as a Newton–Raphson algorithm, can be used to improve \\nthe speed of convergence.\\nIn order to use this algorithm, we need the quantities \\nr\\n2\\n and \\nH\\n2\\n.\\n To compute \\nr\\n2\\n, we note from Eq. (5-91) that\\n \\nRG H F\\n(,\\n) (,) (,) (,)\\nuv uv uv uv\\n=−\\n \\n(5-94)\\nfrom which we obtain \\nrxy\\n(,\\n)\\n by computing the inverse Fourier transform of \\nR\\n(,\\n) .\\nuv\\n \\nT\\nhen, from the definition of the Euclidean norm, it follows that\\n \\nrr r\\n2\\n2\\n0\\n1\\n0\\n1\\n==\\n=\\n−\\n=\\n−\\n∑ ∑\\nT\\ny\\nN\\nx\\nM\\nrx y\\n(,)\\n \\n(5-95)\\nComputation of \\nH\\n2\\n leads to an interesting result. First, consider the variance \\nof the noise over the entire image, which we estimate from the samples using the \\nexpression\\n \\nsh h\\nh\\n2\\n2\\n0\\n1\\n0\\n1\\n1\\n=−\\n[]\\n=\\n−\\n=\\n−\\n∑ ∑\\nMN\\nxy\\ny\\nN\\nx\\nM\\n(,)\\n \\n(5-96)\\nwhere\\n \\nhh\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n0\\n1\\n0\\n1\\nMN\\nxy\\ny\\nN\\nx\\nM\\n(,)\\n \\n(5-97)\\nis the sample mean. With reference to the \\nform\\n of Eq.\\n (5-95), we note that the dou-\\nble summation in Eq. (5-96) is proportional to \\nH\\n2\\n. This leads to the expression\\n \\nHs h\\nh\\n2\\n22\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\nMN\\n \\n(5-98)\\nThis is a most useful result. It tells us that we can estimate the unknown quantity \\nH\\n2\\n by having knowledge of only the mean and variance of the noise. These quanti-\\nties are not difficult to estimate (see Section 5.2), assuming that the noise and image \\nDIP4E_GLOBAL_Print_Ready.indb   366\\n6/16/2017   2:07:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 367}),\n",
       " Document(page_content='5.10\\n  \\nGeometric Mean Filter\\n    \\n367\\nb a\\nFIGURE \\n5.31\\n(a) Iteratively \\ndetermined \\nconstrained \\nleast squares \\nrestoration of \\nFig. 5.25(b), using \\ncorrect noise \\nparameters. (b) \\nResult obtained \\nwith wrong noise \\nparameters. \\nintensity values are not correlated. This is an assumption of all the methods dis\\n-\\ncussed in this chapter. \\nEXAMPLE \\n5.13\\n \\n:\\n  Iterative estimation of the optimum constrained least squares filter\\n. \\nFigure 5.31(a) shows the result obtained using the algorithm just described to estimate the optimum \\nfilter for restoring Fig. 5.25(b). The initial value used for \\ng\\n was \\n10\\n5\\n−\\n,\\n the correction factor for adjusting \\ng\\n \\nwas \\n10\\n6\\n−\\n,\\n and the value for \\na\\n was 0.25. The noise parameters specified were the same used to generate \\nFig. 5.25(a): a noise variance of \\n10\\n5\\n−\\n,\\n and zero mean. The restored result is comparable to Fig. 5.28(c), \\nwhich was obtained by Wiener filtering with \\nK\\n manually specified for best visual results. Figure 5.31(b) \\nshows what can happen if the wrong estimate of noise parameters are used. In this case, the noise vari\\n-\\nance specified was \\n10\\n2\\n−\\n and the mean was left at 0. The result in this case is considerably more blurred. \\n5.10\\n \\n \\nGEOMETRIC MEAN FIL\\nTER  \\nIt is possible to generalize slightly the Wiener filter discussed in Section 5.8. The \\ngeneralization is in the form of the so-called \\ngeometric mean filter\\n: \\n \\nˆ\\n(,\\n)\\n(,\\n)\\n(,\\n)\\n(,\\n)\\n(,\\n)\\n(,\\n)\\n(\\n**\\nF\\nH\\nH\\nH\\nH\\nS\\nS\\nf\\nuv\\nuv\\nuv\\nuv\\nuv\\nuv\\nu\\n=\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n+\\n2\\n2\\na\\nh\\nb\\n,,\\n)\\n(,\\n)\\nv\\nuv\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n−\\n1\\na\\nG\\n \\n(5-99)\\nwhere \\na\\n and \\nb\\n are nonnegative, real constants. The geometric mean filter transfer \\nfunction consists of the two expressions in brackets raised to the powers \\na\\n and \\n1\\n−\\na\\n,\\n \\nrespectively.\\nWhen \\na\\n=\\n1\\n the geometric mean filter reduces to the inverse filter. With \\na\\n=\\n0\\n the \\nfilter becomes the so-called \\nparametric Wiener filter\\n, which reduces to the “standard” \\n5.10\\nDIP4E_GLOBAL_Print_Ready.indb   367\\n6/16/2017   2:07:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 368}),\n",
       " Document(page_content='368\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nWiener ﬁlter when \\nb\\n=\\n1.\\n If \\na\\n=\\n12 ,\\n the ﬁlter becomes a product of the two quanti-\\nties raised to the same power\\n, which is the deﬁnition of the geometric mean, thus \\ngiving the ﬁlter its name. With \\nb\\n=\\n1,\\n as \\na\\n increases above \\n12 ,\\n the ﬁlter performance \\nwill tend more toward the inverse ﬁlter\\n. Similarly, when \\na\\n decreases below \\n12 ,\\n the \\nﬁlter will behave more like a \\nWiener ﬁlter. When \\na\\n=\\n12\\n and \\nb\\n=\\n1\\n the ﬁlter is com-\\nmonly referred to as a \\nspectrum equalization ﬁlter\\n.\\n Equation (5-99) is useful when \\nimplementing restoration ﬁlters because it represents a family of ﬁlters combined \\ninto a single expression.\\n5.11 IMAGE RECONSTRUCTION FROM PROJECTIONS  \\nIn the previous sections of this chapter we discussed techniques for restoring degrad-\\ned images. In this section, we examine the problem of \\nreconstructing\\n an image from \\na series of projections, with a focus on X-ray \\ncomputed tomography\\n (CT). This is the \\nearliest and still the most-widely used type of CT, and is currently one of the princi-\\npal applications of digital image processing in medicine.\\nINTRODUCTION\\nThe reconstruction problem is simple in principle, and can be explained qualitatively \\nin a straightforward, intuitive manner, without using equations (we will deal with the \\nmath later in this section. To begin, consider Fig. 5.32(a), which consists of a single \\nobject on a uniform background. In order to bring physical meaning to the following \\nexplanation, suppose that this image is a \\ncross-section\\n of a 3-D region of a human \\nbody. Assume also that the background in the image represents soft, uniform tissue, \\nwhile the round object is a tumor, also uniform, but with higher X-ray absorption \\ncharacteristics.\\nSuppose next that we pass a thin, ﬂat beam of X-rays from left to right (through \\nthe plane of the image), as Fig. 5.32(b) shows, and assume that the energy of the \\nbeam is absorbed more by the object than by the background, as typically is the case. \\nUsing a strip of X-ray absorption detectors on the other side of the region will yield \\nthe signal (\\nabsorption proﬁle\\n) shown, whose amplitude (intensity) is proportional to \\nabsorption.\\n†\\n We may view any point in the signal as the sum of the absorption values \\nacross the single ray in the beam corresponding spatially to that point (such a sum \\noften is referred to as a \\nraysum\\n). At this juncture, all the information we have about \\nthe object is this 1-D absorption signal.\\nWe have no way of determining from a single projection whether we are dealing \\nwith a single object, or a multitude of objects along the path of the beam, but we \\nbegin the reconstruction by creating an \\nimage\\n based only on this information. The \\napproach is to project the 1-D signal \\nback\\n in the opposite direction from which the \\nbeam came, as Fig. 5.32(c) shows. The process of backprojecting a 1-D signal across a \\n2-D area sometimes is referred to as \\nsmearing\\n the projection back across the area. In \\n†\\n A treatment of the physics of X-ray sources and detectors is beyond the scope of our discussion, which focuses \\non the image processing aspects of CT. See Prince and Links [2006] for an excellent introduction to the physics \\nof X-ray image formation.\\n5.11\\nAs noted in Chapter 1, \\nthe term \\ncomputerized \\naxial tomography\\n (CAT) \\nis used interchangeably \\nto denote CT.\\nDIP4E_GLOBAL_Print_Ready.indb   368\\n6/16/2017   2:07:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 369}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n369\\nterms of digital images, this means duplicating the same 1-D signal across the image, \\nperpendicularly to the direction of the beam. For example, Fig. 5.32(c) was created \\nby duplicating the 1-D signal in all columns of the reconstructed image. For obvious \\nreasons, the approach just described is called \\nbackprojection\\n.\\nNext, suppose that we rotate the position of the source-detector pair by 90°, as \\nin Fig. 5.32(d). Repeating the procedure explained in the previous paragraph yields \\na backprojection image in the vertical direction, as Fig. 5.32(e) shows. We continue \\nthe reconstruction by \\nadding\\n this result to the previous backprojection, resulting in \\nFig. 5.32(f). Now, we begin to suspect that the object of interest is contained in the \\nsquare shown, whose amplitude is twice the amplitude of the individual backprojec-\\ntions because the signals were added. We should be able to learn more about the \\nshape of the object in question by taking more views in the manner just described, \\nas Fig. 5.33 shows. As the number of projections increases, the amplitude strength \\nof non-intersecting backprojections decreases relative to the strength of regions in \\nwhich multiple backprojections intersect. The net effect is that brighter regions will \\ndominate the result, and backprojections with few or no intersections will fade into \\nthe background as the image is scaled for display.\\nFigure 5.33(f), which was formed from 32 backprojections, illustrates this concept. \\nNote, however, that while this reconstructed image is a reasonably good approxi-\\nmation to the shape of the original object, the image is blurred by a “halo” effect, \\nthe formation of which can be seen in progressive stages in Fig. 5.33. For example, \\nthe halo in Fig. 5.33(e) appears as a “star” whose intensity is lower than that of the \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.32\\n(a) Flat region \\nwith a single \\nobject. (b) Parallel \\nbeam, detector \\nstrip, and proﬁle of \\nsensed 1-D  \\nabsorption signal. \\n(c) Result of back-\\nprojecting the \\nabsorption proﬁle. \\n(d) Beam and \\ndetectors rotated \\nby 90°.  \\n(e) Backprojection. \\n(f) The sum of (c) \\nand (e), inten-\\nsity-scaled. The \\nintensity where the \\nbackprojections \\nintersect is twice \\nthe intensity of the \\nindividual back-\\nprojections. \\nAbsorption profile (signal)\\nRay\\nDetector strip\\nBeam\\nDIP4E_GLOBAL_Print_Ready.indb   369\\n6/16/2017   2:07:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 370}),\n",
       " Document(page_content='370\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nobject, but higher than the background. As the number of views increases, the shape \\nof the halo becomes circular, as in Fig. 5.33(f). Blurring in CT reconstruction is an \\nimportant issue, whose solution is addressed later in this section. Finally, we con-\\nclude from the discussion of Figs. 5.32 and 5.33 that backprojections 180° apart are \\nmirror images of each other, so we have to consider only angle increments halfway \\naround a circle in order to generate all the backprojections required for reconstruc-\\ntion.\\nEXAMPLE 5.14 :  Backprojections of a planar region containing two objects.\\nFigure 5.34 illustrates reconstruction using backprojections on a region that contains two objects with \\ndifferent absorption properties (the larger object has higher absorption). Figure 5.34(b) shows the result \\nof using one backprojection. We note three principal features in this ﬁgure, from bottom to top: a thin \\nhorizontal gray band corresponding to the unoccluded portion of the small object, a brighter (more \\nabsorption) band above it corresponding to the area shared by both objects, and an upper band corre-\\nsponding to the rest of the elliptical object. Figures 5.34(c) and (d) show reconstruction using two pro-\\njections 90° apart and four projections 45° apart, respectively. The explanation of these ﬁgures is similar \\nto the discussion of Figs. 5.33(c) through (e). Figures 5.34(e) and (f) show more accurate reconstructions \\nusing 32 and 64 backprojections, respectively. The last two results are quite close visually, and they both \\nshow the blurring problem mentioned earlier.\\nPRINCIPLES OF X-RAY COMPUTED TOMOGRAPHY (CT)\\nAs with the Fourier transform discussed in the last chapter, the basic mathematical \\nconcepts required for CT were in place many years before the availability of digital \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.33\\n(a) Same as  \\nFig. 5.32(a).  \\n(b)-(e) Recon-\\nstruction using 1,  \\n2, 3, and 4 back-\\nprojections 45° \\napart.  \\n(f) Reconstruction \\nwith 32 backpro-\\njections 5.625° \\napart (note the \\nblurring).\\nDIP4E_GLOBAL_Print_Ready.indb   370\\n6/16/2017   2:07:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 371}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n371\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.34\\n(a) Two objects \\nwith different \\nabsorption charac-\\nteristics.  \\n(b)–(d) Recon-\\nstruction using 1, 2, \\nand 4 backprojec-\\ntions, 45° apart.  \\n(e) Reconstruction \\nwith 32 backprojec-\\ntions, 5.625° apart. \\n(f) Reconstruction \\nwith 64 backprojec-\\ntions, 2.8125° apart. \\ncomputers made them practical. The theoretical foundation of CT dates back to \\nJohann Radon, a mathematician from Vienna who derived a method in 1917 for \\nprojecting a 2-D object along parallel rays, as part of his work on line integrals (the \\nmethod now is referred to as the \\nRadon transform\\n, a topic we will discuss shortly). \\nForty-five years later, Allan M. Cormack, a physicist at Tufts University, partially \\n“rediscovered” these concepts and applied them to CT. Cormack published his initial \\nfindings in 1963 and 1964 and showed how his results could be used to reconstruct \\ncross-sectional images of the body from X-ray images taken in different angular \\ndirections. He gave the mathematical formulae needed for the reconstruction and \\nbuilt a CT prototype to show the practicality of his ideas. Working independently, \\nelectrical engineer Godfrey N. Hounsfield and his colleagues at EMI in London \\nformulated a similar solution and built the first medical CT machine. Cormack and \\nHounsfield shared the 1979 Nobel Prize in Medicine for their contributions to medi-\\ncal uses of tomography.\\nThe goal of X-ray computed tomography is to obtain a 3-D representation of the \\ninternal structure of an object by X-raying the object from many different directions. \\nImagine a traditional chest X-ray, obtained by placing the subject against an X-ray \\nsensitive plate and “illuminating” the individual with an X-ray beam in the form of \\na cone. The X-ray plate would produce an image whose intensity at a point would \\nbe proportional to the X-ray energy impinging on that point after it passed through \\nthe subject. This image is the 2-D equivalent of the projections we discussed in the \\nprevious section. We could back-project this entire image and create a 3-D volume. \\nRepeating this process through many angles and adding the backprojections would \\nresult in 3-D rendition of the structure of the chest cavity. Computed tomography \\nattempts to get that same information (or localized parts of it) by generating slices \\nDIP4E_GLOBAL_Print_Ready.indb   371\\n6/16/2017   2:07:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 372}),\n",
       " Document(page_content='372\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nDetector\\nSubject\\nSource\\nb a\\nd c\\nFIGURE 5.35\\nFour generations \\nof CT scanners. \\nThe dotted arrow \\nlines indicate \\nincremental linear \\nmotion. The  \\ndotted arrow arcs \\nindicate  \\nincremental \\nrotation. The \\ncross-mark on \\nthe subject’s \\nhead indicates \\nlinear motion \\nperpendicular to \\nthe plane of the \\npaper. The double \\narrows in (a) \\nand (b) indicate \\nthat the source/\\ndetector unit is \\ntranslated and \\nthen brought back \\ninto its original \\nposition. \\nthrough the body. A 3-D representation then can be obtained by stacking the slices. \\nA CT implementation is much more economical because the number of detectors \\nrequired to obtain a high resolution slice is much smaller than the number of detec-\\ntors needed to generate a complete 2-D projection of the same resolution. Compu-\\ntational burden and X-ray dosages are similarly reduced, making the 1-D projection \\nCT a more practical approach.\\nFirst-generation (G1) CT scanners\\n employ a “pencil” X-ray beam and a single \\ndetector, as Fig. 5.35(a) shows. For a given angle of rotation, the source/detector \\npair is translated incrementally along the linear direction shown. A projection (like \\nthe ones in Fig. 5.32), is generated by measuring the output of the detector at each \\nincrement of translation. After a complete linear translation, the source/detector \\nassembly is rotated and the procedure is repeated to generate another projection \\nat a different angle. The procedure is repeated for all desired angles in the range [0°, \\n180°] to generate a complete set of projections images, from which one ﬁnal cross-\\nsectional image (a slice through the 3-D object) is obtained, as explained in the \\nDIP4E_GLOBAL_Print_Ready.indb   372\\n6/16/2017   2:07:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 373}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n373\\nprevious section. A set of cross sectional images (slices) is generated by moving the \\nsubject incrementally (after each complete scan) past the source/detector plane (the \\ncross-mark on the head of the subject indicates motion in a direction perpendicular \\nto the plane of the source/detector pair). Stacking these images computationally \\nproduces a 3-D volume of a section of the body. G1 scanners are no longer manu-\\nfactured for medical imaging, but, because they produce a parallel-ray beam (as in \\nFig. 5.32), their geometry is the one used predominantly for introducing the funda-\\nmentals of CT imaging, and serves as the starting point for deriving the equations \\nnecessary to implement image reconstruction from projections.\\nSecond-generation (G2) CT scanners\\n [Fig. 5.35(b)] operate on the same principle \\nas G1 scanners, but the beam used is in the shape of a fan. This allows the use of mul-\\ntiple detectors, thus requiring fewer translations of the source/detector pair. \\nThird-generation (G3) scanners\\n are a signiﬁcant improvement over the earlier \\ntwo generations of CT geometries. As Fig. 5.35(c) shows, G3 scanners employ a bank \\nof detectors long enough (on the order of 1000 individual detectors) to cover the \\nentire ﬁeld of view of a wider beam. Consequently, each increment of angle pro-\\nduces an entire projection, eliminating the need to translate the source/detector pair, \\nas in G1 and G2 scanners. \\nFourth-generation (G4) scanners\\n go a step further. By employing a circular ring of \\ndetectors (on the order of 5000 individual detectors), only the source has to rotate. \\nThe key advantage of G3 and G4 scanners is speed; key disadvantages are cost and \\ngreater X-ray scatter. The latter implies higher X-ray doses than G1 and G2 scan-\\nners to achieve comparable signal-to-noise characteristics.\\nNewer scanning modalities are beginning to be adopted. For example,  \\nﬁfth-gener-\\nation (G5) CT scanners\\n, also known as \\nelectron beam computed tomography (EBCT) \\nscanners\\n, eliminate all mechanical motion by employing electron beams controlled \\nelectromagnetically. By striking tungsten anodes that encircle the patient, these \\nbeams generate X-rays that are then shaped into a fan beam that passes through the \\npatient and excites a ring of detectors, as in G4 scanners.\\nThe conventional manner in which CT images are obtained is to keep the patient \\nstationary during the scanning time required to generate one image. Scanning is then \\nhalted while the position of the patient is incremented in the direction perpendicu-\\nlar to the imaging plane, using a motorized table. The next image is then obtained \\nand the procedure is repeated for the number of increments required to cover a \\nspeciﬁed section of the body. Although an image may be obtained in less than one \\nsecond, there are procedures (e.g., abdominal and chest scans) that require patient \\nto hold their breath during image acquisition. Completing these procedures for, say, \\n30 images, may require several minutes. An approach for which use is increasing is \\nhelical CT\\n, sometimes referred to as \\nsixth-generation (G6) CT\\n. In this approach, a \\nG3 or G4 scanner is conﬁgured using so-called \\nslip rings\\n that eliminate the need for \\nelectrical and signal cabling between the source/detectors and the processing unit. \\nThe source/detector pair then rotates continuously through 360° while the patient \\nis moved at a constant speed along the axis perpendicular to the scan. The result is \\na continuous helical volume of data that is then processed to obtain individual slice \\nimages.\\nDIP4E_GLOBAL_Print_Ready.indb   373\\n6/16/2017   2:07:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 374}),\n",
       " Document(page_content='374\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\ny\\nx\\nu\\nr\\nFIGURE 5.36\\nNormal  \\nrepresentation of \\na line.\\nSeventh-generation (G7) scanners\\n (also called \\nmultislice CT scanners\\n) are emerg-\\ning in which “thick” fan beams are used in conjunction with parallel banks of detec-\\ntors to collect volumetric CT data simultaneously. That is, 3-D cross-sectional “slabs,” \\nrather than single cross-sectional images are generated per X-ray burst. In addition \\nto a signiﬁcant increase in detail, this approach has the advantage that it utilizes \\nX-ray tubes more economically, thus reducing cost and potentially reducing dosage.\\nIn the following discussion, we develop the mathematical tools necessary for for-\\nmulating image projection and reconstruction algorithms. Our focus is on the image-\\nprocessing fundamentals that underpin all the CT approaches just discussed. Infor-\\nmation regarding the mechanical and source/detector characteristics of CT systems \\nis provided in the references cited at the end of the chapter.\\nPROJECTIONS AND THE RADON TRANSFORM\\nNext, we develop in detail the mathematics needed for image reconstruction in the \\ncontext of X-ray computed tomography. The same basic principles apply to other \\nCT imaging modalities, such as SPECT (single photon emission tomography), PET \\n(positron emission tomography), MRI (magnetic resonance imaging), and some \\nmodalities of ultrasound imaging.\\nA straight line in Cartesian coordinates can be described either by its \\nslope-inter-\\ncept\\n form, \\nya x b\\n=+\\n, or, as in Fig. 5.36, by its \\nnormal\\n representation:\\n \\nxy\\nco\\ns sin\\nuu r\\n+=\\n \\n(5-100)\\nThe projection of a \\nparallel-ra\\ny beam\\n can be modeled by a set of such lines, as \\nFig. 5.37 shows. An arbitrary \\npoint\\n at coordinates \\n(, )\\nru\\njk\\n in the projection profile is \\ngiven by the raysum along the line \\nxy\\nkk j\\ncos sin .\\nuu r\\n+=\\n Working with continuous \\nquantities for the moment,\\n the \\nraysum\\n is a line integral, given by\\n \\ng\\nf x y x y dx dy\\njk\\nk k j\\n(, ) ( , ) (c o s s i n )\\nru d u u r\\n=+ −\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n \\n(5-101)\\nwhere we used the properties of the impulse, \\nd\\n,\\n discussed in Section 4.5. In other \\nwords\\n, the right side of Eq. (5-101) is zero unless the argument of \\nd\\n is zero, indicating \\nThroughout this section, \\nwe follow CT convention \\nand place the origin \\nof the \\nxy\\n-plane in the \\ncenter, instead of at our \\ncustomary top left corner \\n(see Section 2.4). Both \\nare right-handed coor-\\ndinate systems, the only \\ndifference being that our \\nimage coordinate system \\nhas no negative axes. \\nWe can account for the \\ndifference with a simple \\ntranslation of the origin, \\nso both representations \\nare interchangeable. \\nDIP4E_GLOBAL_Print_Ready.indb   374\\n6/16/2017   2:07:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 375}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n375\\nthat the integral is computed only along the line \\nxy\\nkk j\\ncos sin .\\nuu r\\n+=\\n If we con-\\nsider all values of \\nr\\n and \\nu\\n, the preceding equation generalizes to\\n \\ng\\nf x y x y dx dy\\n(,\\n) (,)( c o s s i n )\\nru d u u r\\n=+\\n−\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n \\n(5-102)\\nThis equation, which gives the projection (line integral) of \\nfx y\\n(,\\n)\\n along an arbi-\\ntrary line in the \\nxy\\n-plane\\n, is the \\nRadon transform\\n mentioned earlier. The notation \\n/H5228\\nfx\\ny\\n(,)\\n{}\\n or \\n/H5228\\nf\\n{}\\n is used sometimes in place of \\ng\\n(,\\n)\\nru\\n in Eq. (5-102) to denote \\nthe Radon transform of \\nfx y\\n(,\\n) ,\\n but the type of notation used in Eq. (5-102) is more \\ncustomary\\n. As will become evident in the discussion that follows, the Radon trans-\\nform is the cornerstone of reconstruction from projections, with computed tomogra-\\nphy being its principal application in the field of image processing.\\nIn the discrete case,\\n†\\n the Radon transform of Eq. (5-102) becomes\\n \\ngf\\nx\\ny\\nx y\\ny\\nN\\nx\\nM\\n(, ) (,)( c o s s i n )\\nru d u u r\\n=+ −\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n \\n(5-103)\\nwhere \\nx\\n, \\ny\\n,\\n and are now discrete variables, and \\nM\\n and \\nN\\n are the dimensions of a \\nrectangular area over which the transform is applied. If we fix \\nu\\n and allow \\nr\\n to \\nvary\\n, we see that (5-103) simply sums the pixels of \\nfx y\\n(,\\n)\\n along the line defined by \\nthe specified values of these two parameters\\n. Incrementing through all values of \\nr\\n \\n†\\n  In Chapter 4, we exercised great care in denoting continuous image coordinates by \\n(, )\\ntz\\n and discrete coordi-\\nnates by \\n(, ) .\\nxy\\n At that time, the distinction was important because we were developing basic concepts to take us \\nfrom continuous to sampled quantities\\n. In the present discussion, we go back and forth so many times between \\ncontinuous and discrete coordinates that adhering to this convention is likely to generate unnecessary confusion. \\nFor this reason, and also to follow the published literature in this ﬁeld (e.g., see Prince and Links [2006]), we let \\nthe context determine whether coordinates \\n(, )\\nxy\\n are continuous or discrete. When they are continuous, you will \\nsee integrals;\\n otherwise, you will see summations.\\nFIGURE 5.37\\nGeometry of a \\nparallel-ray beam.\\nr\\nComplete projection, \\ng\\n(\\nr\\n, \\nu\\nk\\n),\\nfor a fixed angle\\nA point \\ng\\n(\\nr\\nj\\n, \\nu\\nk\\n) in\\nthe projection\\n(, )\\ncos sin\\njk\\nkk j\\nL\\nxy\\n=\\n+−\\nru\\nuu r\\ny\\n/H11032\\nx\\n/H11032\\nx\\nr\\nj\\ny\\nu\\nk\\nDIP4E_GLOBAL_Print_Ready.indb   375\\n6/16/2017   2:07:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 376}),\n",
       " Document(page_content='376\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nrequired to span the \\nMN\\n×\\n area (with \\nu\\n fixed) yields \\none\\n projection.\\n Changing \\nu\\n \\nand repeating this procedure yields another projection,\\n and so forth. This is precisely \\nhow the projections in Figs. 5.32-5.34 were generated.\\nEXAMPLE 5.15 :  Using the Radon transform to obtain the projection of a circular region.\\nBefore proceeding, we illustrate how to use the Radon transform to obtain an analytical expression for \\nthe projection of the circular object in Fig. 5.38(a): \\n \\nfx y\\nAx\\nyr\\n(,\\n)\\n=\\n+≤\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n222\\n0 otherwise\\n \\nwhere \\nA\\n is a constant and \\nr\\n is the radius of the object.\\n We assume that the circle is centered on the origin \\nof the \\nxy\\n-plane. Because the object is circularly symmetric, its projections are the same for all angles, so \\nall we have to do is obtain the projection for \\nu\\n=\\n0\\n°\\n. Equation (5-102) then becomes\\n \\ngf\\nx\\ny\\nx\\nd\\nx\\nd\\ny\\nfy\\nd y\\n(, )\\n(,)( )\\n(,)\\nru\\nd r\\nr\\n=−\\n=\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\nwhere the second expression follows from Eq. (4-13). As noted earlier, this is a line integral (along the \\nline \\nL\\n(,\\n)\\nr\\n0\\n in this case). Also, note that \\ng\\n(,\\n)\\nru\\n=\\n0\\n when \\nr\\n>\\nr\\n.\\n When \\nr\\n≤\\nr\\n the integral is evaluated \\nfrom \\ny\\nr\\n=\\n−\\n−\\n()\\n22\\n12\\nr\\n to \\ny\\nr\\n=\\n−\\n() .\\n22\\n12\\nr\\n Therefore,\\ny\\n0\\nr\\nr\\ng\\n(\\nr\\n)\\nx\\nb\\na\\nFIGURE 5.38\\n(a) A disk and,  \\n(b) a plot of its Radon \\ntransform, derived \\nanalytically. Here we \\nwere able to plot the \\ntransform because it  \\ndepends only on one \\nvariable. When \\ng\\n \\ndepends on both \\nr\\n and \\nu\\n, the Radon transform \\nbecomes an image \\nwhose axes are \\nr\\n and \\nu\\n, and the intensity of \\na pixel is proportional \\nto the value of \\ng\\n at the \\nlocation of that pixel.\\n \\nDIP4E_GLOBAL_Print_Ready.indb   376\\n6/16/2017   2:07:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 377}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n377\\n \\ngf\\ny\\nd\\ny\\nAdy\\nr\\nr\\nr\\nr\\n(, )\\n(,)\\nru\\nr\\nr\\nr\\nr\\nr\\n=\\n=\\n−−\\n−\\n−−\\n−\\n22\\n22\\n22\\n22\\n2\\n2\\nCarrying out the integration yields\\n \\ngg\\nAr r\\n(,) ()\\nru r\\nrr\\n==\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n2\\n0\\n22\\n≤\\notherwise\\nwhere we used the fact that \\ng\\n(,\\n)\\nru\\n=\\n0\\n when \\nr\\n>\\nr\\n.\\n Figure 5.38(b) shows a plot of this result. Note that \\ngg\\n(,\\n) () ;\\nru r\\n=\\n that is, \\ng\\n is independent of \\nu\\n because the object is symmetric about the origin.\\nW\\nhen the Radon transform, \\ng\\n(,\\n) ,\\nru\\n is displayed as an image with \\nr\\n and \\nu\\n as recti-\\nlinear coordinates\\n, the result is called a \\nsinogram\\n, similar in concept to displaying the \\nFourier spectrum. Like the Fourier transform, a sinogram contains the data neces-\\nsary to reconstruct \\nfx y\\n(,\\n) .\\n Unlike the Fourier transform, however, \\ng\\n(,\\n)\\nru\\n is always \\na real function.\\n As is the case with displays of the Fourier spectrum, sinograms can \\nbe readily interpreted for simple regions, but become increasingly difﬁcult to “read” \\nas the region being projected becomes more complex. For example, Fig. 5.39(b) is \\nthe sinogram of the rectangle on the left. The vertical and horizontal axes corre-\\nspond to \\nu\\n and \\nr\\n,\\n respectively. Thus, the bottom row is the projection of the rect-\\nangle in the horizontal direction (i.e\\n., \\nu\\n=\\n0\\n°),\\n and the middle row is the projection \\nin the vertical direction (\\n(\\nu\\n=\\n90\\n°).\\n The fact that the nonzero portion of the bottom \\nrow is smaller than the nonzero portion of the middle row tells us that the object is \\nnarrower in the horizontal direction.\\n The fact that the sinogram is symmetric in both \\ndirections about the center of the image tells us that we are dealing with an object \\nthat is symmetric and parallel to the \\nx\\n and \\ny\\n axes. Finally, the sinogram is smooth, \\nindicating that the object has a uniform intensity. Other than these types of general \\nobservations, we cannot say much more about this sinogram.\\nFigure 5.39(c) is an image of the \\nShepp-Logan phantom \\n(Shepp and Logan [1974]), \\na widely used synthetic image designed to simulate the absorption of major areas of \\nthe brain, including small tumors. The sinogram of this image is considerably more \\ndifﬁcult to interpret, as Fig. 5.39(d) shows. We still can infer some symmetry prop-\\nerties, but that is about all we can say. Visual analyses of sinograms are of limited \\npractical use, but they can be helpful in tasks such as algorithm development.\\nBACKPROJECTIONS\\nTo obtain a formal expression for a backprojected image from the Radon transform, \\nlet us begin with a \\nsingle\\n point, \\ng\\njk\\n(, ) ,\\nru\\n of the complete projection, \\ng\\nk\\n(, ) ,\\nru\\n for a \\nfixed value of rotation,\\n \\nu\\nk\\n(see Fig. 5.37). Forming part of an image by backproject-\\ning this single point is nothing more than copying the line \\nL\\njk\\n(, )\\nru\\n onto the image, \\nTo generate arrays with \\nrows of the same size, \\nthe minimum dimen-\\nsion of the \\nr\\n-axis in \\nsinograms corresponds \\nto the largest dimension \\nencountered during \\nprojection. For example, \\nthe minimum size of a \\nsinogram of a square \\nof size \\nM\\n \\n× \\nM\\n obtained \\nusing increments of 1° is \\n180 \\n× \\nQ \\nwhere \\nQ\\n is the \\nsmallest integer greater \\nthan \\n2\\nM\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   377\\n6/16/2017   2:07:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 378}),\n",
       " Document(page_content='378\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nwhere the value (intensity) of each point in that line is \\ng\\njk\\n(, ) .\\nru\\n Repeating this pro-\\ncess of all values of \\nr\\nj\\n in the projected signal (but keeping the value of \\nu\\n fixed at \\nu\\nk\\n) \\nresults in the following expression:\\n \\nfx y g\\ngx\\ny\\nk\\nk\\nkk k\\nu\\nru\\nuu u\\n(,) (, )\\n(c o s s i n , )\\n=\\n=+\\nfor the image due to backprojecting the projection obtained with a fixed angle, \\nu\\nk\\n, \\nas in F\\nig. 5.32(b). This equation holds for an arbitrary value of \\nu\\nk\\n,\\n so we may write \\nin general that the image formed from a \\nsingle\\n backprojection obtained at an angle \\nu\\n is given by\\n \\nfx y g x y\\nu\\nuu u\\n(,) ( c o s s i n, )\\n=+\\n \\n(5-104)\\nWe form the final image by integrating over all the backprojected images:\\n \\nfx y f x yd\\n(,\\n) (,)\\n=\\n0\\np\\nu\\nu\\n2\\n \\n(5-105)\\nIn the discrete case, the integral becomes a sum of all the backprojected images:\\n180\\n135\\n90\\nu\\nu\\n45\\n0\\n180\\n135\\n90\\n45\\n0\\nr\\nr\\nb a\\nd c\\nFIGURE 5.39\\nTwo images and \\ntheir sinograms \\n(Radon  \\ntransforms). Each \\nrow of a sinogram \\nis a projection \\nalong the  \\ncorresponding \\nangle on the  \\nvertical axis. \\n(Note that the \\nhorizontal axis \\nof the sinograms \\nare values of \\nr\\n.)\\nImage (c) is called \\nthe \\nShepp-Logan \\nphantom\\n.\\n In its \\noriginal form, the \\ncontrast of the \\nphantom is quite \\nlow. It is shown \\nenhanced here to \\nfacilitate viewing. \\nDIP4E_GLOBAL_Print_Ready.indb   378\\n6/16/2017   2:07:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 379}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n379\\nb a\\n \\nFIGURE 5.40\\nBackprojections \\nof the sinograms \\nin Fig. 5.39. \\n \\nfx y f x y\\n(,\\n) (,)\\n=\\n=\\n∑\\nu\\nu\\np\\n0\\n \\n(5-106)\\nwhere, \\nx\\n, \\ny\\n,\\n and \\nu\\n are now discrete quantities. As mentioned earlier, the projections \\nat 0\\n°\\n and \\n180\\n°\\n are mirror images of each other, so the summations are carried out \\nto the last angle increment before \\n180\\n°\\n.\\n For example, if \\n05\\n.\\n°\\n increments are being \\nused,\\n the summation is from \\n0\\n°\\n to \\n179 5 .\\n°\\n in half-degree increments. A backpro-\\njected image formed in the manner just described sometimes is referred to as a \\nlaminogram\\n.\\n It is understood implicitly that a laminogram is only an approximation \\nto the image from which the projections were generated, a fact that is illustrated in \\nthe following example.\\nEXAMPLE 5.16 :  Obtaining backprojected images from sinograms. \\nEquation (5-106) was used to generate the backprojected images in Figs. 5.32 through 5.34, from projec-\\ntions obtained with Eq. (5-103). Similarly, these equations were used to generate Figs. 5.40(a) and (b), \\nwhich show the backprojected images corresponding to the sinograms in Figs. 5.39(b) and (d), respec-\\ntively. As with the earlier ﬁgures, we note a signiﬁcant amount of blurring, so it is obvious that a straight \\nuse of Eqs. (5-103) and (5-106) will not yield acceptable results. Early, experimental CT systems were \\nbased on these equations. However, as you will see later in our discussion, signiﬁcant improvements in \\nreconstruction are possible by reformulating the backprojection approach. \\nTHE FOURIER-SLICE THEOREM\\nIn this section, we derive a fundamental equation that establishes a relationship \\nbetween the 1-D Fourier transform of a projection and the 2-D Fourier transform \\nof the region from which the projection was obtained. This relationship is the basis \\nfor reconstruction methods capable of dealing with the blurring problems we have \\nencountered thus far. \\nThe 1-D Fourier transform of a projection with respect to \\nr\\n is\\n \\nGg e d\\nj\\n(,) (,)\\nvu ru r\\npvr\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\n \\n(5-107)\\nThis equation has the \\nsame form as Eq. (4-20).\\nDIP4E_GLOBAL_Print_Ready.indb   379\\n6/16/2017   2:07:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 380}),\n",
       " Document(page_content='380\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nwhere \\nv\\n is the frequency variable, and it is understood that this expression is based \\non a fixed value of \\nu\\n. Substituting Eq. (5-102) for \\ng\\n(,\\n)\\nru\\n we obtain\\nGf\\nx\\ny\\nx\\ny e\\nd\\nx\\nd\\ny\\nd\\nj\\n(,)\\n(,)(c o s s i n )\\nvu\\nd u u r\\nr\\npvr\\n=+\\n−\\n=\\n−\\n---\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n222\\n2\\n2\\n22 2\\n2\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\nfx y x y e d d x d y\\nj\\n(,) ( c o s s i n )\\ndu u r r\\npvr\\n+−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n−\\n2\\n- -\\n/H11009\\n/H11009\\n2\\nf x y e\\ndx dy\\njx y\\n(,)\\n( cos sin )\\n−+\\n2\\npv u u\\n \\n(5-108)\\nwhere the last step follows from the sifting property of the impulse discussed in \\nChapter 4.\\n By letting \\nu\\n=\\nvu\\nco\\ns\\n and \\nv\\n=\\nvu\\nsin\\n,\\n we can write Eq. (5-108) as\\n \\nG\\nf x y e dxdy\\nju x y\\nu\\n(,)\\n(,)\\n()\\ncos ; si\\nvu\\np\\nvuv\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−+\\n==\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\nv\\nv\\nn n\\nu\\n \\n(5-109)\\nWe recognize this expression as the 2-D Fourier transform of \\nfx y\\n(,\\n)\\n \\n[see Eq. (4-59)]\\n \\nevaluated at the values of \\nu\\n and \\nv\\n indicated. That is,\\n \\nGF u\\nF\\nu\\n(,) (,)\\n(c o s , s i n )\\ncos ; sin\\nvu\\nvu vu\\nvuvu\\n=\\n[]\\n=\\n==\\nv\\nv\\n \\n(5-110)\\nwhere, as usual, \\nF\\n(,\\n)\\nuv\\n denotes the 2-D Fourier transform of \\nfx y\\n(,\\n) .\\nThe result in Eq. (5-110) is known as the \\nF\\nourier-slice theorem\\n (or the \\nprojection-\\nslice theorem\\n). It states that the Fourier transform of a projection is a\\n slice\\n of the 2-D \\nFourier transform of the region from which the projection was obtained. The reason \\nfor this terminology can be explained with the aid of Fig. 5.41. As this ﬁgure shows, \\nthe 1-D Fourier transform of an arbitrary projection is obtained by extracting the \\nvalues of \\nF\\n(,\\n)\\nuv\\n along a line oriented at the same angle as the angle used in generat-\\ning the projection.\\n \\nIn principle, we could obtain \\nfx y\\n(,\\n)\\n simply by obtaining the inverse Fourier trans-\\nform of \\nF\\n(,\\n) .\\nuv\\n However, this is expensive computationally, as it involves obtained \\nthe inverse of a 2-D transform.\\n The approach discussed in the following section is \\nmuch more efﬁcient. \\nRECONSTRUCTION USING PARALLEL-BEAM FILTERED  \\nBACKPROJECTIONS\\nAs we saw in Figs. 5.33, 5.34, and 5.40, obtaining backprojections directly yields unac-\\nceptably blurred results. Fortunately, there is a straightforward solution to this prob-\\nlem based simply on filtering the projections before computing the backprojections. \\nFrom Eq. (4-60), the 2-D inverse Fourier transform of \\nF\\n(,\\n)\\nuv\\n is\\nDIP4E_GLOBAL_Print_Ready.indb   380\\n6/16/2017   2:07:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 381}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n381\\n2-D Fourier\\ntransform\\n1-D Fourier\\ntransform\\nF\\n(\\nu\\n, \\nv\\n)\\nf\\n(\\nx\\n, \\ny\\n)\\nv\\nu\\nu\\nu\\nx\\ny\\nProjection\\nSlice of \\nF\\n(\\nu\\n, \\nv\\n)\\n \\nFIGURE 5.41\\nIllustration of \\nthe Fourier-slice \\ntheorem. The 1-D \\nFourier transform \\nof a projection is \\na slice of the 2-D \\nFourier transform \\nof the region from \\nwhich the projec-\\ntion was obtained. \\nNote the corre-\\nspondence of the \\nangle \\nu\\n in the two \\nﬁgures\\n. \\n \\nfx y F e d u d\\nju x y\\n(,)\\n(,)\\n()\\n=\\n+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nuv\\nv\\nv\\n2\\np\\n \\n(5-111)\\nIf, as in Eqs. (5-109) and (5-110), we let \\nu\\n=\\nvu\\nco\\ns\\n and \\nv\\n=\\nvu\\nsin\\n,\\n then the differen-\\ntials become \\ndud d d\\nv\\n=\\nvvu\\n, and we can express Eq. (5-111) in polar coordinates:\\n \\nfx y F\\ne\\nd d\\njx y\\n(,) ( c o s, s i n)\\n( cos sin )\\n=\\n+\\n0\\n2\\n0\\n2\\np\\nv uv u\\nvvu\\npv u u\\n22\\n/H11009\\n \\n(5-112)\\nThen, using the Fourier slice theorem,\\n \\nfx y G e\\nd d\\njx y\\n(,)\\n(, )\\n( cos sin )\\n=\\n+\\n0\\n2\\n0\\n2\\np\\nvu\\nvvu\\npv u u\\n22\\n/H11009\\n \\n(5-113)\\nBy splitting this integral into two expressions, one for \\nu\\n in the range 0° to 180° and \\nthe other in the range 180° to 360°,\\n and using the fact that \\nGG\\n(,\\n) ( ,)\\nvu\\nvu\\n+= −\\n180\\n°\\n \\n(see Problem 5.46),\\n we can express Eq. (5-113) as\\n \\nfx y\\nG e\\nd d\\njx y\\n(,)\\n(, )\\n( cos sin )\\n=\\n−\\n+\\n0\\n2\\np\\nvv\\nu\\nvu\\npv u u\\n22\\n/H11009\\n/H11009\\n \\n(5-114)\\nThe term \\nxy\\nco\\ns sin\\nuu\\n+\\n is a constant with respect to \\nv\\n,\\n and we recognize it as \\nr\\n \\nfrom Eq.\\n (5-100). Therefore, we can write Eq. (5-114) as\\n \\nfx y\\nG e d\\nd\\nj\\nxy\\n(,)\\n(,)\\ncos sin\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=+\\n0\\n2\\np\\nvv u v\\nu\\npvr\\nru u\\n22\\n-\\n/H11009\\n/H11009\\n \\n(5-115)\\nThe relationship  \\ndud d d\\nv\\n=\\nvvu\\n is from \\nbasic integral calculus, \\nwhere the Jacobian is \\nused as the basis for a \\nchange of variables.\\nDIP4E_GLOBAL_Print_Ready.indb   381\\n6/16/2017   2:07:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 382}),\n",
       " Document(page_content='382\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nThe inner expression is in the form of an \\ninverse\\n 1-D Fourier transform [see \\nEq. (4-21)], with the added term \\nv\\n which, based on the discussion in Section 4.7, \\nwe recognize as a 1-D filter transfer function.\\n Observe that \\nv\\n is a \\nramp\\n function \\n[see F\\nig. 5.42(a)]. This function is not integrable because its amplitude extends to \\n+\\n/H11009\\n \\nin both directions\\n, so the inverse Fourier transform is undefined. Theoretically, this \\nis handled by methods such as using so-called \\ngeneralized delta functions\\n. In practice, \\nthe approach is to \\nwindow\\n the ramp so that it becomes zero outside of a defined \\nfrequency interval. That is, a window \\nband-limits\\n the ramp filter transfer function.\\nThe simplest approach to band-limit a function is to use a box in the frequency \\ndomain. However, as we saw in Fig. 4.4, a box has undesirable ringing properties. \\nThis is demonstrated by Figs. 5.42(b) and (c). The former shows a plot of the ramp \\ntransfer function after it was band-limited by a box window, and the latter shows \\nits spatial domain representation, obtained by computing its inverse Fourier trans-\\nform. As expected, the resulting windowed ﬁlter exhibits noticeable ringing in the \\nspatial domain. We know from Chapter 4 that ﬁltering in the frequency domain is \\nequivalent to convolution in the spatial domain, so spatial ﬁltering with a function \\nthat exhibits ringing will produce a result corrupted by ringing also. Windowing with \\na smooth function helps this situation. An \\nM\\n-point discrete window function used \\nfrequently for implementations with the 1-D FFT is given by\\n \\nH\\ncc\\nM\\nM\\n()\\n() c o\\ns(\\n)\\nv\\npv\\nv\\n=\\n+−\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n2\\n01\\n0\\n≤≤\\notherwise\\n \\n(5-116)\\nWhen \\nc\\n=\\n05\\n4\\n.,\\n this function is called the \\nHamming windo\\nw\\n (named after Richard \\nHamming) and, when \\nc\\n=\\n05\\n.\\n it is called the \\nHann windo\\nw\\n (named after Julius von \\nHann). The key difference between the Hamming and Hann windows is that the \\nThe ramp ﬁlter often \\nis referred to as the \\nRam-Lak ﬁlter\\n, after \\nRamachandran and \\nLakshminarayanan \\n[1971] who generally \\nare credited with having \\nbeen ﬁrst to suggest it.\\nSometimes the Hann \\nwindow is referred to as \\nthe \\nHanning\\n window in \\nanalogy to the Hamming \\nwindow. However, this \\nterminology is incorrect \\nand is a frequent source \\nof confusion.\\nFrequency\\ndomain\\nFrequency\\ndomain\\nFrequency\\ndomain\\nSpatial\\ndomain\\nSpatial\\ndomain\\nFrequency\\ndomain\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.42\\n(a) Frequency domain \\nramp ﬁlter transfer \\nfunction. (b) Function \\nafter band-limiting \\nit with a box ﬁlter. \\n(c) Spatial domain \\nrepresentation.  \\n(d) Hamming \\nwindowing func-\\ntion. (e) Windowed \\nramp ﬁlter, formed \\nas the product of (b) \\nand (d). (f) Spatial \\nrepresentation of the \\nproduct. (Note the \\ndecrease in ringing.) \\nDIP4E_GLOBAL_Print_Ready.indb   382\\n6/16/2017   2:07:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 383}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n383\\nend points are zero in the latter. The difference between the two generally is visually \\nimperceptible in image processing applications.\\nFigure 5.42(d) is a plot of the Hamming window, and Fig. 5.42(e) shows the prod-\\nuct of this window and the band-limited ramp ﬁlter transfer function in Fig. 5.42(b). \\nFigure 5.42(f) shows the representation of the product in the spatial domain, \\nobtained as usual by computing the inverse FFT. It is evident by comparing this \\nﬁgure and Fig. 5.42(c) that ringing was reduced in the windowed ramp (the ratios of \\nthe peak to trough in Figs. 5.42(c) and (f) are 2.5 and 3.4, respectively). On the other \\nhand, because the width of the central lobe in Fig. 5.42(f) is slightly wider than in \\nFig. 5.42(c), we would expect backprojections based on using a Hamming window to \\nhave less ringing, but be slightly more blurred. As Example 5.17 below shows, this is \\nindeed the case.\\nRecall from Eq. (5-107) that \\nG\\n(,\\n)\\nvu\\n is the 1-D Fourier transform of \\ng\\n(,\\n) ,\\nru\\n which \\nis a \\nsingle\\n projection obtained at a ﬁxed angle\\n, \\nu\\n.\\n Equation (5-115) states that the \\ncomplete\\n,\\n backprojected image \\nfx y\\n(,\\n)\\n is obtained as follows:\\n1. \\nCompute the 1-D Fourier transform of each projection.\\n2. \\nMultiply each 1-D Fourier transform by the ﬁlter transfer function \\nv\\n which, \\nas explained above\\n, has been multiplied by a suitable (e.g., Hamming) window.\\n3. \\nObtain the inverse 1-D Fourier transform of each resulting ﬁltered transform.\\n4. \\nIntegrate (sum) all the 1-D inverse transforms from Step 3.\\nBecause a ﬁlter function is used,\\n this image reconstruction approach is appropri-\\nately called \\nﬁltered backprojection\\n. In practice, the data are discrete, so all frequency \\ndomain computations are carried out using a 1-D FFT algorithm, and ﬁltering is \\nimplemented using the same basic procedure explained in Chapter 4 for 2-D func-\\ntions. Alternatively, we can implement ﬁltering in the spatial domain using convolu-\\ntion, as explained later.\\n The preceding discussion addresses the windowing aspects of ﬁltered backpro-\\njections. As with any sampled data system, we also need to be concerned about \\nsampling rates. We know from Chapter 4 that the selection of sampling rates has a \\nprofound inﬂuence on image processing results. In the present discussion, there are \\ntwo sampling considerations. The ﬁrst is the number of rays used, which determines \\nthe number of samples in each projection. The second is the number of rotation \\nangle increments, which determines the number of reconstructed images (whose \\nsum yields the ﬁnal image). Under-sampling results in aliasing which, as we saw in \\nChapter 4, can manifest itself as artifacts in the image, such as streaks. We address \\nCT sampling issues in more detail later in our discussion.\\nEXAMPLE 5.17 :  Image reconstruction using ﬁltered backprojections. \\nThe focus of this example is to show reconstruction using ﬁltered backprojections, ﬁrst with a box-\\nlimited ramp transfer function and then using a ramp limited by a Hamming window. These ﬁltered \\nbackprojections are compared against the results of “raw” backprojections from Fig. 5.40. In order to \\nfocus on the difference due only to ﬁltering, the results in this example were generated with 0.5° incre-\\nments of rotation, the same we used to generate Fig. 5.40. The separation between rays was one pixel \\nDIP4E_GLOBAL_Print_Ready.indb   383\\n6/16/2017   2:08:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 384}),\n",
       " Document(page_content='384\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nin both cases. The images in both examples are of size \\n600 600\\n×\\n pixels, so the length of the diagonal \\nis \\n2 600 849\\n×≈\\n.\\nConsequently, 849 rays were used to provide coverage of the entire region when the \\nangle of rotation was 45° and 135°.\\nF\\nigure 5.43(a) shows the rectangle reconstructed using a ramp function band-limited by a box. The \\nmost vivid feature of this result is the absence of any visually detectable blurring. However, as expected, \\nringing is present, visible as faint lines, especially around the corners of the rectangle. These lines are \\nmore visible in the zoomed section in Fig. 5.43(c). Using a Hamming window on the ramp helped con-\\nsiderably with the ringing problem, at the expense of slight blurring, as Figs. 5.43(b) and (d) show. The \\nimprovements (even with the box-windowed ramp) over Fig. 5.40(a) are evident. The phantom image \\ndoes not have transitions that are as sharp and prominent as the rectangle so ringing, even with the \\nbox-windowed ramp, is imperceptible in this case, as you can see in Fig. 5.44(a). Using a Hamming \\nwindow resulted in a slightly smoother image, as Fig. 5.44(b) shows. Both of these results are consider-\\nable improvements over Fig. 5.40(b), illustrating again the signiﬁcant advantage inherent in the ﬁltered \\nbackprojection approach.\\nIn most applications of CT (especially in medicine), artifacts such as ringing are a serious concern, so \\nsigniﬁcant effort is devoted to minimizing them. Tuning the ﬁltering algorithms and, as explained earlier, \\nusing a large number of detectors are among the design considerations that help reduce these effects.\\nThe preceding discussion is based on obtaining ﬁltered backprojections via an \\nFFT implementation. However, we know from the convolution theorem in Chapter 4 \\nthat equivalent results can be obtained using spatial convolution. In particular, note \\nb a\\nd c\\nFIGURE 5.43\\nFiltered backpro-\\njections of the \\nrectangle using \\n(a) a ramp ﬁlter, \\nand  \\n(b) a Hamming  \\nwindowed ramp \\nﬁlter. The second \\nrow shows \\nzoomed details of \\nthe images in the \\nﬁrst row. Compare \\nwith Fig. 5.40(a). \\nDIP4E_GLOBAL_Print_Ready.indb   384\\n6/16/2017   2:08:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 385}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n385\\nb a\\nFIGURE 5.44\\nFiltered backpro-\\njections of the \\nhead phantom \\nusing (a) a ramp \\nﬁlter, and (b) a \\nHamming  \\nwindowed ramp \\nﬁlter. Compare \\nwith Fig. 5.40(b) \\nthat the term inside the brackets in Eq. (5-115) is the inverse Fourier transform of \\nthe product of two frequency domain functions which, according to the convolu-\\ntion theorem, we know to be equal to the convolution of the spatial representa-\\ntions (inverse Fourier transforms) of these two functions. In other words, letting \\ns\\n()\\nr\\n \\ndenote the inverse F\\nourier transform of \\nv\\n,\\n†\\n we write Eq. (5-115) as\\n \\nfx y\\nG e d\\nd\\ns\\nj\\nxy\\n(,)\\n(, )\\n(\\ncos sin\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n=+\\n0\\n0\\n2\\np\\np\\nvv u v u\\npvr\\nruu\\n22\\n2\\n-\\n/H11009\\n/H11009\\nr\\nrr u u\\nru u u r\\nru u\\np\\n)( , )\\n(, ) ( c o s s i n )\\ncos sin\\n/H22841\\ngd\\ngs x y d\\nxy\\n[]\\n=+\\n−\\n=+\\n0\\n22\\n-\\n/H11009\\n/H11009\\nr\\nru\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\nd\\n \\n(5-117)\\nwhere, as in Chapter 4, “\\n/H22841\\n” denotes convolution. The second line follows from the \\nfirst for the reasons explained in the previous paragraph. The third line (including \\nthe \\n−\\nr\\n) follows from the definition of convolution in Eq. (4-24). \\nT\\nhe last two lines of Eq. (5-117) say the same thing: individual backprojections at \\nan angle \\nu\\n can be obtained by convolving the corresponding projection, \\ng\\n(,\\n) ,\\nru\\n and \\nthe inverse F\\nourier transform of the ramp ﬁlter transfer function, \\ns\\n()\\n.\\nr\\n As before, \\nthe complete backprojected image is obtained by integrating (summing) all the indi-\\nvidual backprojected images\\n. With the exception of roundoff differences in compu-\\ntation, the results of using convolution will be identical to the results using the FFT. \\nIn actual CT implementations, convolution generally turns out to be more efﬁcient \\ncomputationally, so most modern CT systems use this approach. The Fourier trans-\\nform does play a central role in theoretical formulations and algorithm development \\n(for example, CT image processing in MATLAB is based on the FFT). Also, we note \\nthat there is no need to store all the backprojected images during reconstruction. \\n†\\n If a windowing function, such as a Hamming window, is used, then the inverse Fourier transform is performed \\non the windowed ramp. \\nDIP4E_GLOBAL_Print_Ready.indb   385\\n6/16/2017   2:08:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 386}),\n",
       " Document(page_content='386\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nInstead, a single running sum is updated with the latest backprojected image. At the \\nend of the procedure, the running sum will equal the sum total of all the backprojec-\\ntions.\\nFinally, we point out that, because the ramp ﬁlter (even when it is windowed) \\nzeros the dc term in the frequency domain, each backprojection image will have \\nzero average value (see Fig. 4.29). This means that the pixels in each backprojec-\\ntion image will have negative and positive values. When all the backprojections are \\nadded to form the ﬁnal image, some negative locations may become positive and the \\naverage value may not be zero, but typically, the ﬁnal image will still have negative \\npixels.\\nThere are several ways to handle this problem. The simplest approach, when \\nthere is no knowledge regarding what the average values should be, is to accept the \\nfact that negative values are inherent in the approach and scale the result using the \\nprocedure described in Eqs. (2-31) and (2-32). This is the approach followed in this \\nsection. When knowledge about what a “typical” average value should be is avail-\\nable, that value can be added to the ﬁlter transfer function in the frequency domain, \\nthus offsetting the ramp and preventing zeroing the dc term [see Fig. 4.30(c)]. When \\nworking in the spatial domain with convolution, the very act of truncating the length \\nof the spatial ﬁlter kernel (inverse Fourier transform of the ramp) prevents it from \\nhaving a zero average value, thus avoiding the zeroing problem altogether.\\nRECONSTRUCTION USING FAN-BEAM FILTERED BACKPROJECTIONS\\nThe discussion thus far has centered on parallel beams. Because of its simplicity and \\nintuitiveness, this is the imaging geometry used traditionally to introduce computed \\ntomography. However, more modern CT systems use a fan-beam geometry (see Fig. \\n5.35), which is the topic of the following discussion.\\nFigure 5.45 shows a basic fan-beam imaging geometry in which the detectors are \\narranged on a circular arc and the angular increments of the source are assumed to \\nbe equal. Let \\np\\n(,\\n)\\nab\\n denote a fan-beam projection, where \\na\\n is the angular position \\nof a particular detector measured with respect to the \\ncenter ra\\ny\\n, and \\nb\\n is the angular \\ndisplacement of the source\\n, measured with respect to the \\ny\\n-axis, as shown in the \\nﬁgure. We also note in Fig. 5.45 that a ray in the fan beam can be represented as a \\nline, \\nL\\n(,\\n) ,\\nru\\n in normal form, which is the approach we used to represent a ray in the \\nparallel-beam imaging geometry discussed earlier\\n. This allows us to utilize parallel-\\nbeam results as the starting point for deriving the corresponding equations for the \\nfan-beam geometry. We proceed to show this by deriving the fan-beam ﬁltered back-\\nprojection based on convolution.\\n†\\nWe begin by noticing in Fig. 5.45 that the parameters of line \\nL\\n(,\\n)\\nru\\n are related to \\nthe parameters of a fan-beam ray by\\n \\nub a\\n=+\\n \\n(5-118)\\n†\\n The Fourier-slice theorem was derived for a parallel-beam geometry and is not directly applicable to fan beams. \\nHowever, Eqs. (5-118) and (5-119) provide the basis for converting a fan-beam geometry to a parallel-beam \\ngeometry, thus allowing us to use the ﬁltered parallel backprojection approach developed in the previous section, \\nfor which the slice theorem is applicable. We will discuss this in more detail at the end of this section.  \\nDIP4E_GLOBAL_Print_Ready.indb   386\\n6/16/2017   2:08:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 387}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n387\\nx\\ny\\nD\\nSource\\nCenter ray\\nL\\n(\\nr\\n, \\nu\\n)\\nr\\nu\\nb\\na\\nFIGURE 5.45\\nBasic fan-beam \\ngeometry. The line \\npassing through \\nthe center of the \\nsource and the \\norigin (assumed \\nhere to be the \\ncenter of rotation \\nof the source) is \\ncalled the \\ncenter \\nray\\n. \\nand\\n \\nra\\n=\\nD\\nsin\\n \\n(5-119)\\nwhere \\nD\\n is the distance from the center of the source to the origin of the \\nxy\\n-plane\\n.\\nThe convolution backprojection formula for the parallel-beam imaging geometry \\nis given by Eq. (5-117). Without loss of generality, suppose that we focus attention \\non objects that are encompassed within a circular area of radius \\nT\\n about the origin \\nof the \\nxy\\n-plane. Then \\ng\\n(,\\n)\\nru\\n=\\n0 for \\nr\\n>\\nT\\n and Eq. (5-117) becomes\\n \\nfx y\\ng s x y dd\\nT\\nT\\n( , )\\n( , ) ( cos sin )\\n=+\\n−\\n−\\n1\\n2\\n0\\n2\\np\\nru u u r ru\\n22\\n \\n(5-120)\\nwhere we used the fact mentioned earlier that projections 180° apart are mirror \\nimages of each other\\n. In this way, the limits of the outer integral in Eq. (5-120) are \\nmade to span a full circle, as required by a fan-beam arrangement in which the \\ndetectors are arranged in a circle. \\nWe are interested in integrating with respect to \\na\\n and \\nb\\n.\\n To do this, we change \\nto polar coordinates\\n, \\n(, ) .\\nr\\nw\\n That is, we let \\nxr\\n=\\nco\\ns\\nw\\n and \\nyr\\n=\\nsin\\n,\\nw\\n from which it \\nfollows that\\n \\nxy r r\\nr\\nco\\ns sin cos cos sin sin\\ncos( )\\nuu w uw u\\nuw\\n+= +\\n=−\\n \\n(5-121)\\nUsing this result we can express Eq. (5-120) as\\nDIP4E_GLOBAL_Print_Ready.indb   387\\n6/16/2017   2:08:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 388}),\n",
       " Document(page_content='388\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n \\nfx y\\ng sr\\ndd\\nT\\nT\\n(,)\\n(, ) c o s ( )\\n=−\\n−\\n()\\n−\\n1\\n2\\n0\\n2\\np\\nru u w r ru\\n22\\n \\n(5-122)\\nThis expression is nothing more than the parallel-beam reconstruction formula writ-\\nten in polar coordinates\\n. However, integration still is with respect to \\nr\\n and \\nu\\n.\\n To \\nintegrate with respect to \\na\\n and \\nb\\n requires a transformation of coordinates using \\nEqs\\n. (5-118) and (5-119):\\n \\nfr\\ngD\\nsr\\nTD\\nTD\\n(, )\\n( s i n , )\\ncos(\\nsin ( / )\\nsin ( / )\\nwa\\na\\nb\\nb\\na\\npa\\n=+\\n+\\n−\\n−\\n−\\n−\\n−\\n1\\n2\\n2\\n1\\n1\\n22\\na\\naw a a a b\\n−−\\n()\\n)s i n c o s\\nDD d d\\n \\n(5-123)\\nwhere we used \\ndd D dd\\nru\\naab\\n=\\ncos\\n [see the explanation of Eq. (5-112)].\\nT\\nhis equation can be simpliﬁed further. First, note that the limits \\n−\\na\\n to \\n2\\npa\\n−\\n \\nfor variable \\nb\\n span the entire range of 360°. Because all functions of \\nb\\n are periodic \\nwith period \\n2\\np\\n,\\n the limits of the outer integral can be replaced by 0 and \\n2\\np\\n,\\n respec-\\ntively\\n. The term \\nsin ( )\\n−\\n1\\nTD\\n has a maximum value, \\na\\nm\\n,\\n corresponding to \\nr\\n>\\nT\\n, \\nbeyond which \\ng\\n=\\n0\\n (see Fig. 5.46), so we can replace the limits of the inner integral \\nby \\n−\\na\\nm\\n and \\na\\nm\\n,\\n respectively. Finally, consider the line \\nL\\n(,\\n)\\nru\\n in Fig. 5.45. A raysum \\nof a fan beam along this line must equal the raysum of a parallel beam along the \\nsame line\\n. This follows from the fact that a raysum is a sum of all values along a \\nline, so the result must be the same for a given ray, regardless of the coordinate sys-\\ntem is which it is expressed. This is true of any raysum for corresponding values of \\n(, )\\nab\\n and \\n(, ) .\\nru\\n Thus, letting \\np\\n(,\\n)\\nab\\n denote a fan-beam projection, it follows that \\npg\\n(,\\n) (, )\\nab ru\\n=\\n and, from Eqs. (5-118) and (5-119), that \\npg D\\n(\\n, ) ( sin , ).\\nab aa b\\n=+\\n \\nIncorporating these observations into Eq.\\n (5-123) results in the expression\\n  \\nfr\\np sr\\nD D d d\\nm\\nm\\n( , )\\n( , ) cos( ) sin cos\\nwa\\nb b\\na\\nw a\\na\\na\\nb\\np\\na\\na\\n=+\\n−\\n−\\n[]\\n−\\n1\\n2\\n0\\n2\\n22\\n \\n(5-124)\\nThis is the fundamental fan-beam \\nreconstruction formula\\n based on filtered backpro-\\njections\\n.\\nEquation (5-124) can be manipulated further to put it in a more familiar convolu-\\ntion form. With reference to Fig. 5.47, it can be shown (see Problem 5.47) that\\n \\nrD\\nR\\nc\\nos( ) sin sin( )\\nbaw a a a\\n+− − =\\n′\\n−\\n \\n(5-125)\\nwhere \\nR\\n is the distance from the source to an arbitrary point in a fan ray\\n, and \\n′\\na\\n is \\nthe angle between this ray and the center ray\\n. Note that \\nR\\n and \\n′\\na\\n are determined by \\nthe values of \\nr\\n, \\nw\\n, and \\nb\\n. Substituting Eq. (5-125) into Eq. (5-124) yields\\n \\nfr\\np sR D d d\\nm\\nm\\n( , )\\n( , ) sin[ ] cos\\nwa\\nb a\\na a\\na\\nb\\np\\na\\na\\n=\\n′\\n−\\n()\\n−\\n1\\n2\\n0\\n2\\n22\\n \\n(5-126)\\nDIP4E_GLOBAL_Print_Ready.indb   388\\n6/16/2017   2:08:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 389}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n389\\nx\\ny\\nD\\na\\nm\\n/H11002\\na\\nm\\nT\\nSource\\nb\\nFIGURE 5.46\\nMaximum value \\nof \\nα\\n needed to  \\nencompass a  \\nregion of interest. \\nIt can be shown (see Problem 5.48) that\\n \\nsR\\nR\\ns\\n(s i n )\\nsin\\n()\\na\\na\\na\\na\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n2\\n \\n(5-127)\\nUsing this expression, we can write Eq. (5-126) as\\n \\nfr\\nR\\nqh d d\\nm\\nm\\n(, )\\n( , )( )\\nwa\\nb\\na\\na\\na\\nb\\np\\na\\na\\n=\\n′\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−\\n1\\n2\\n1\\n0\\n2\\n2\\n22\\n \\n(5-128)\\nwhere\\n \\nhs\\n()\\nsin\\n()\\na\\na\\na\\na\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n2\\n2\\n \\n(5-129)\\nand\\n \\nqp D\\n(,\\n) (, ) c o s\\nab ab a\\n=\\n \\n(5-130)\\nWe recognize the inner integral in Eq. (5-128) as a convolution expression, thus \\nshowing that the image reconstruction formula in Eq.\\n (5-124) can be implemented \\nas the convolution of functions \\nq\\n(,\\n)\\nab\\n and \\nh\\n()\\n.\\na\\n Unlike the reconstruction formula \\nfor parallel projections\\n, reconstruction based on fan-beam projections involves a \\nterm \\n1\\n2\\nR\\n,\\n which is a weighting factor inversely proportional to the distance from \\nthe source\\n. The computational details of implementing Eq. (5-128) are beyond the \\nscope of the present discussion (see Kak and Slaney [2001] for a detailed treatment \\nof this subject). \\nDIP4E_GLOBAL_Print_Ready.indb   389\\n6/16/2017   2:08:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 390}),\n",
       " Document(page_content='390\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nInstead of implementing Eq. (5-128) directly, an approach used often, particularly \\nin software simulations, is to: (1) convert a fan-beam geometry to a parallel-beam \\ngeometry using Eqs. (5-118) and (5-119), and (2) use the parallel-beam reconstruc-\\ntion approach developed earlier. We conclude this section with an example of how to \\ndo this. As noted earlier, a fan-beam projection, \\np\\n, taken at angle \\nb\\n has a correspond-\\ning parallel-beam projection,\\n \\ng\\n, taken at a corresponding angle \\nu\\n and, therefore,\\n \\npg\\ngD\\n(,\\n) (, )\\n(s i n , )\\nab ru\\naa b\\n=\\n=+\\n \\n(5-131)\\nwhere the last line follows from Eqs. (5-118) and (5-119).\\nLet \\n/H9004\\nb\\n denote the angular increment between successive fan-beam projections, \\nand let \\n/H9004\\na\\n be the angular increment between rays, which determines the number of \\nsamples in each projection.\\n We impose the restriction that\\n \\n/H9004/H9004\\nba\\ng\\n==\\n \\n(5-132)\\nThen, \\nbg\\n=\\nm\\n and \\nag\\n=\\nn\\n for some integer values of \\nm\\n and \\nn\\n,\\n and we can write \\nEq. (5-131) as\\n \\np n m g Dnmn\\n(,\\n) s i n , ( )\\ngg g g\\n=+\\n()\\n \\n(5-133)\\nThis equation indicates that the \\nn\\nth ray in the \\nm\\nth radial projection is equal to the \\nn\\nth ray in the \\n()\\nmn\\n+\\nth\\n parallel projection. The \\nD\\nn\\nsin\\ng\\n term on the right side of \\nEq.\\n (5-133) implies that parallel projections converted from fan-beam projections \\nx\\ny\\nD\\nR\\nr\\nSource\\nb\\nw\\na\\n/H11032\\nb\\n \\n/H11002\\n \\nw\\nFIGURE 5.47\\nPolar  \\nrepresentation of \\nan arbitrary point \\non a ray of a fan \\nbeam.  \\nDIP4E_GLOBAL_Print_Ready.indb   390\\n6/16/2017   2:08:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 391}),\n",
       " Document(page_content='5.11\\n  \\nImage Reconstruction from Projections\\n    \\n391\\nb a\\nd c\\nFIGURE 5.48\\nReconstruction of \\nthe rectangle image \\nfrom ﬁltered fan \\nbackprojections.  \\n(a) 1° increments of \\na\\n and \\nb\\n. \\n(b) 0.5° increments\\n. \\n(c) 0.25° increments. \\n(d) 0.125° incre-\\nments.  \\nCompare (d) with \\nFig. 5.43(b). \\nare not sampled uniformly, an issue that can lead to blurring, ringing, and aliasing \\nartifacts if the sampling intervals \\n/H9004\\na\\n and \\n/H9004\\nb\\n are too coarse, as the following exam-\\nple illustrates\\n.\\nEXAMPLE 5.18 :  Image reconstruction using ﬁltered fan backprojections.\\nFigure 5.48(a) shows the results of : (1) generating fan projections of the rectangle image with \\n/H9004/H9004\\nab\\n==\\n1\\n°\\n, \\n(2) converting each fan ray to the corresponding parallel ray using Eq.\\n (5-133), and (3) using the ﬁltered \\nbackprojection approach developed earlier for parallel rays. Figures 5.48(b) through (d) show the results \\nusing 0.5°, 0.25°, and 0.125° increments of \\n/H9004\\na\\n and \\n/H9004\\nb\\n.\\n A Hamming window was used in all cases. We used \\nthis variety of angle increments to illustrate the effects of under\\n-sampling. \\nThe result in Fig. 5.48(a) is a clear indication that 1° increments are too coarse, as blurring and ring-\\ning are quite evident. The result in Fig. 5.48(b) is interesting, in the sense that it compares poorly with \\nFig. 5.43(b), which we generated using the same angle increment of 0.5°. In fact, as Fig. 5.48(c) shows, \\neven with angle increments of 0.25° the reconstruction still is not as good as in Fig. 5.43(b). We have to \\nuse angle increments on the order of 0.125° before the two results become comparable, as Fig. 5.48(d) \\nshows. This angle increment results in projections with \\n180 1 0 125\\n× ( ) = 1440\\n.\\n samples, which is close to \\ndouble the 849 rays used in the parallel projections of Example 5.17.\\n Thus, it is not unexpected that the \\nresults are close in appearance when using \\n/H9004\\na\\n=\\n0\\n125\\n..\\n°\\n \\nSimilar results were obtained with the head phantom,\\n except that aliasing in this case is much more \\nvisible as sinusoidal interference. We see in Fig. 5.49(c) that even with \\n/H9004/H9004\\nab\\n==\\n02\\n5\\n.\\n°\\n signiﬁcant distor-\\ntion still is present,\\n especially in the periphery of the ellipse. As with the rectangle, using increments of \\n0.125° ﬁnally produced results that are comparable with the backprojected image of the head phantom \\nDIP4E_GLOBAL_Print_Ready.indb   391\\n6/16/2017   2:08:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 392}),\n",
       " Document(page_content='392\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nd c\\nFIGURE 5.49\\nReconstruction of \\nthe head phantom \\nimage from ﬁltered \\nfan backprojections.  \\n(a) 1° increments of \\na\\n and \\nb\\n. \\n(b) 0.5° increments\\n. \\n(c) 0.25° increments. \\n(d) 0.125° incre-\\nments.  \\nCompare (d) with \\nFig. 5.44(b). \\nin Fig. 5.44(b). These results illustrate one of the principal reasons why thousands of detectors have to \\nbe used in the fan-beam geometry of modern CT systems in order to reduce aliasing artifacts.\\nSummary, References, and Further Reading\\n  \\nThe restoration results in this chapter are based on the assumption that image degradation can be modeled as a lin-\\near, position invariant process followed by additive noise that is not correlated with image values. Even when these \\nassumptions are not entirely valid, it is often possible to obtain useful results by using the methods developed in the \\npreceding sections. Our treatment of image reconstruction from projections, though introductory, is the foundation \\nfor the image-processing aspects of this ﬁeld. As noted in Section 5.11, computed tomography (CT) is the main ap-\\nplication area of image reconstruction from projections. Although we focused on X-ray tomography, the principles \\nestablished in Section 5.11 are applicable in other CT imaging modalities, such as SPECT (single photon emission \\ntomography), PET (positron emission tomography), MRI (magnetic resonance imaging), and some modalities of \\nultrasound imaging.\\nFor additional reading on the material in Section 5.1 see Pratt [2014]. The books by Ross [2014], and by Mont-\\ngomery and Runger [2011], are good sources for a more in-depth discussion of probability density functions and \\ntheir properties (Section 5.2). See Umbaugh [2010] for complementary reading on the material in Section 5.3, and \\nEng and Ma [2001, 2006] regarding adaptive median ﬁltering. The ﬁlters in Section 5.4 are direct extensions of the \\nmaterial in Chapter 4. The material in Section 5.5 is fundamental linear system theory; for more advanced reading \\non this topic see Hespanha [2009]. The topic of estimating image degradation functions (Section 5.6) is fundamental \\nin the ﬁeld of image restoration. Some of the early techniques for estimating the degradation function are given in \\nAndrews and Hunt [1977], Rosenfeld and Kak [1982]. More recent methods are discussed by Gunturk and Li [2013]. \\nDIP4E_GLOBAL_Print_Ready.indb   392\\n6/16/2017   2:08:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 393}),\n",
       " Document(page_content='  \\n  \\nSummary, References, and Further Reading\\n    \\n393\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n5.1 * \\nThe white bars in the test pattern shown are 7 \\npixels wide and 210 pixels high.\\n The separation \\nbetween bars is 17 pixels. What would this image \\nlook like after application of\\n(a) \\nA \\n33\\n×\\n arithmetic mean ﬁlter?\\n(b) \\nA \\n77\\n×\\n arithmetic mean ﬁlter?\\n(c) \\nA \\n99\\n×\\n arithmetic mean ﬁlter?\\nNote:\\n This problem and the ones that follow it, \\nrelated to ﬁltering this image, may seem a bit \\ntedious. However, they are worth the effort, as \\nthey help develop a real understanding of how \\nthese ﬁlters work. After you understand how a \\nparticular ﬁlter affects the image, your answer \\ncan be a brief verbal description of the result. For \\nexample, “the resulting image will consist of ver-\\ntical bars 3 pixels wide and 206 pixels high.” Be \\nsure to describe any deformation of the bars, such \\nas rounded corners. You may ignore image bor-\\nder effects, in which the ﬁlter neighborhoods only \\npartially contain image pixels.\\n5.2 \\nRepeat Problem 5.1 using a geometric mean ﬁlter.\\n5.3 * \\nRepeat Problem 5.1 using a harmonic mean ﬁlter.\\n5.4 \\nRepeat Problem 5.1 using a contraharmonic \\nmean ﬁlter with \\nQ\\n=\\n1.\\n5.5 * \\nRepeat Problem 5.1 using a contraharmonic \\nmean ﬁlter with \\nQ\\n=−\\n1.\\n5.6 \\nRepeat Problem 5.1 using a median ﬁlter.\\n5.7 * \\nRepeat Problem 5.1 using a max ﬁlter.\\n5.8 \\nRepeat Problem 5.1 using a min ﬁlter.\\n5.9 * \\nRepeat Problem 5.1 using a midpoint ﬁlter.\\n5.10 \\nIn answering the following, refer to the contra-\\nharmonic ﬁlter in Eq.\\n (5-26) :\\n(a) * \\nExplain why the ﬁlter is effective in eliminat-\\ning pepper noise when \\nQ\\n is positive\\n.\\n(b) \\nExplain why the ﬁlter is effective in eliminat-\\ning salt noise when \\nQ\\n is negative\\n.\\n(c) * \\nExplain why the ﬁlter gives poor results \\n(such as the results in F\\nig. 5.9) when the \\nwrong polarity is chosen for \\nQ\\n.\\n(d) \\nDiscuss the expected behavior of the ﬁlter \\nwhen \\nQ\\n=−\\n1.\\n5.11 \\nWe mentioned when discussing Eq. (5-27)] that \\nusing median ﬁlters generally results in less blur\\n-\\nring than using linear smoothing ﬁlters (e.g., box \\nlowpass ﬁlters) of the same size. Explain why this \\nis so. (\\nHint: \\nIn order to focus on the key differ-\\nence between the ﬁlters, assume that noise is neg-\\nligible, and consider the behavior of these ﬁlters \\nin the neighborhood of a binary edge.)\\nThere are two major approaches to the methods developed in Sections 5.7–5.10. One is based on a general for-\\nmulation using matrix theory, as introduced by Andrews and Hunt [1977] and by Gonzalez and Woods [1992]. This \\napproach is elegant and general, but it tends to be difﬁcult for ﬁrst-time readers. Approaches based on frequency \\ndomain ﬁltering (the approach we followed in this chapter) are easier to follow by newcomers to image restoration, \\nbut lack the unifying mathematical rigor of the matrix approach. Both approaches arrive at the same results, but our \\nexperience in teaching this material in a variety of settings indicates that students ﬁrst entering this ﬁeld favor the \\nlatter approach by a signiﬁcant margin. Complementary readings for our coverage of these ﬁltering concepts are \\nCastleman [1996], Umbaugh [2010], Petrou and Petrou [2010] and Gunturk and Li [2013]. For additional reading \\non the material in Section 5.11 see Kak and Slaney [2001], Prince and Links [2006], and Buzug [2008]. For details on \\nthe software aspects of many of the examples in this chapter, see Gonzalez, Woods, and Eddins [2009].\\nDIP4E_GLOBAL_Print_Ready.indb   393\\n6/16/2017   2:08:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 394}),\n",
       " Document(page_content='394\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n5.12 \\nWith reference to the alpha-trimmed ﬁlter deﬁned \\nin Eq.\\n (5-31)]:\\n(a) * \\nExplain why setting \\nd\\n=\\n0\\n in the ﬁlter reduces \\nit to an arithmetic mean ﬁlter\\n.\\n(b) \\nExplain why setting \\ndm n\\n=−\\n1\\n turns the ﬁl-\\nter into a median ﬁlter\\n.\\n5.13 \\nWith reference to the bandreject ﬁlter transfer \\nfunctions in \\nTable 4.7, obtain equations for the \\ntransfer functions of:\\n(a) \\nAn ideal bandpass ﬁlter.\\n(b) * \\nA Gaussian bandpass ﬁlter.\\n(c) \\nA Butterworth bandpass ﬁlter.\\n5.14 \\nWith reference to Eq. (5-33), obtain equations for:\\n(a) * \\nAn ideal notch ﬁlter transfer function.\\n(b) \\nA Gaussian notch ﬁlter transfer function.\\n(c) \\nA Butterworth notch ﬁlter transfer function.\\n5.15 \\nShow that the Fourier transform of the 2-D dis-\\ncrete sine function\\n \\nfx y xM yN\\n( , ) sin( )\\n=+\\n22\\n00\\npm p\\nv\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, ,\\n…\\n \\nis the pair of conjugate impulses\\n \\nF\\njMN\\nuu\\nuu\\n(,) [( , )\\n(,) ]\\nuv\\nv v\\nvv\\n=+ +\\n−− −\\n2\\n00\\n00\\nd\\nd\\n5.16 \\nWith reference to \\nfx y\\n(,\\n)\\n in Problem 5.15, answer \\nthe following:\\n(a) * \\nIf \\nv\\n0\\n0\\n=\\n,\\n and \\nu\\n0\\n and \\nM\\n are integers \\n() ,\\nuM\\n0\\n<\\n \\nwhat would a plot of \\nfx y\\n(,\\n)\\n look like along \\nthe \\nx\\n-axis for \\nxM\\n=−\\n012\\n1\\n,,, , ?\\n…\\n(b) * \\nWhat would a plot of \\nF\\n(,\\n)\\nuv\\n look like for \\nu\\n=−\\n012\\n1\\n,,, , ?\\n…\\nM\\n(c) \\nIf \\nv\\n0\\n0\\n=\\n, \\nM\\n is the same integer as before\\n, \\nbut \\nu\\n0\\n is no longer an integer \\n() ,\\nuM\\n0\\n<\\n how \\nwould a plot of \\nfx y\\n(,\\n)\\n along the \\nx\\n-axis for \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n be different from (a)?\\n5.17 * \\nStart with Eq. (5-46) and derive Eq. (5-48).\\n5.18 \\nAn industrial plant manager has been promoted \\nto a new position.\\n His ﬁrst responsibility is to \\ncharacterize an image ﬁltering system left by his \\npredecessor. In reading the documentation, the \\nmanager discovers that his predecessor estab-\\nlished that the system is linear and position invari-\\nant. Furthermore, he learns that experiments con-\\nducted under negligible-noise conditions resulted \\nin an impulse response that could be expressed \\nanalytically in the frequency domain as\\n \\nHe\\ne\\nu\\nu\\n(,)\\n[]\\n[( ) (\\n)]\\nuv\\nv\\nv\\n=+\\n−\\n−+\\n−− +−\\n22\\n22\\n150 150\\n50 150 50 150\\n1\\nThe manager is not a technical person, so he \\nemploys you as a consultant to determine what,\\n \\nif anything, he needs to do to complete the char-\\nacterization of the system. He also wants to know \\nthe function that the system performs. What (if \\nanything) does the manager need to do to com-\\nplete the characterization of his system? What ﬁl-\\ntering function does the system perform?\\n5.19 \\nA linear, space invariant system has the impulse \\nresponse\\nhxy x ay b\\n(,\\n) ( , )\\n=−−\\nd\\nwhere \\na\\n and \\nb\\n are constants\\n, and \\nx\\n and \\ny\\n are dis-\\ncrete quantities. Answer the following, assuming \\nnegligible noise in each case.\\n(a) * \\nWhat is the system transfer function in the \\nfrequenc\\ny domain?\\n(b) * \\nWhat would the spatial domain system response \\nbe to a constant input,\\n \\nfx y K\\n(,\\n) ?\\n=\\n \\n(c) \\nWhat would the spatial domain system response \\nbe to an impulse input,\\n \\nfx y x y\\n(,\\n) (,) ?\\n=\\nd\\n5.20 * \\nAssuming now that \\nx\\n and \\ny\\n are continuous quanti-\\nties\\n, show how you would solve Problems 5.19(b) \\nand (c) using Eq. (5-61) directly. [\\nHint:\\n Take a \\nlook at the solution to Problem 4.1(c).]\\n5.21 * \\nConsider a linear, position invariant image degra-\\ndation system with impulse response\\nhxy e\\nxy\\n(,)\\n()()\\n=\\n−− +−\\n⎡\\n⎣\\n⎤\\n⎦\\nab\\n22\\nwhere \\nx\\n and \\ny\\n are continuous variables. Suppose \\nthat the input to the system is a binary image con-\\nsisting of a white vertical line of inﬁnitesimal width \\nlocated at \\nxa\\n=\\n,\\n on a black background. Such \\nan image can be modeled as \\nfx y x a\\n(,\\n) ( ) .\\n=−\\nd\\n \\nAssume negligible noise and use Eq.\\n (5-61) to ﬁnd \\nthe output image, \\ngxy\\n(,\\n) .\\n5.22 \\nHow would you solve Problem 5.21 if \\nx\\n and \\ny\\n \\nwere discrete quantities? \\nYou do not need to \\nsolve the problem. All you have to do is list the \\nDIP4E_GLOBAL_Print_Ready.indb   394\\n6/16/2017   2:08:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 395}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n395\\nsteps you would take to solve it. (\\nHint:\\n Refer to \\nentry 13 in Table 4.4.)\\n5.23 \\nThe image shown consists of two inﬁnitesimally \\nthin white lines on a black background,\\n intersect-\\ning at some point in the image. The image is input \\ninto a linear, position invariant system with the \\nimpulse response given in Problem 5.21. Assum-\\ning continuous variables and negligible noise, ﬁnd \\nan expression for the output image, \\ngxy\\n(,\\n) .\\n (\\nHint:\\n \\nReview linear operations in Section 2.6.)\\n5.24 \\nSketch (with arrow lines showing the direction of \\nblur) what the image in F\\nig. 5.26(a) would look \\nlike if it were blurred using the transfer function \\nin Eq. (5-77)\\n(a) * \\nWith \\na\\n=−\\n01\\n.  and \\nb\\n=\\n01\\n..\\n(b) \\nWith \\na\\n=\\n0 and \\nb\\n=−\\n01\\n..\\n5.25 * \\nDuring acquisition, an image undergoes uni-\\nform linear motion in the vertical direction for \\na time \\nT\\n1\\n.\\n The direction of motion then switches \\nto the horizontal direction for a time interval \\nT\\n2\\n.\\n Assuming that the time it takes the image to \\nchange directions is negligible\\n, and that shutter \\nopening and closing times are negligible also, give \\nan expression for the blurring function, \\nH\\n(,\\n) .\\nuv\\n5.26 \\nDuring acquisition, an image undergoes uniform \\nlinear motion in the vertical direction for a time \\nT\\n.\\n The direction of motion then switches \\n180\\n°\\n in \\nthe opposite direction for a time \\nT\\n.\\n Assume that \\nthe time it takes the image to change directions \\nis negligible, and that shutter opening and clos-\\ning times are negligible also. Is the ﬁnal image \\nblurred, or did the reversal in direction “undo” \\nthe ﬁrst blur? Obtain the overall blurring func-\\ntion \\nH\\n(,\\n)\\nuv\\n ﬁrst, and then use it as the basis for \\nyour answer\\n.\\n5.27 * \\nConsider image blurring caused by uniform accel-\\neration in the \\nx\\n-direction.\\n If the image is at rest at \\ntime \\nt\\n=\\n0\\n and accelerates with a uniform acceler-\\nation \\nxt a t\\n0\\n2\\n2\\n()\\n=\\n for a time \\nT\\n,\\n ﬁnd the blurring \\nfunction \\nH\\n(,\\n) .\\nuv\\n You may assume that shutter \\nopening and closing times are negligible\\n.\\n5.28 \\nA space probe is designed to transmit images \\nof a planet as it approaches it for landing\\n. Dur-\\ning the last stages of landing, one of the control \\nthrusters fails, resulting in rotation of the craft \\nabout its vertical axis. The images sent during the \\nlast two seconds prior to landing are blurred as \\na consequence of this circular motion. The cam-\\nera is located in the bottom of the probe, along its \\nvertical axis, and pointing down. Fortunately, the \\nrotation of the craft is also about its vertical axis, \\nso the images are blurred by uniform rotational \\nmotion. During the acquisition time of each image, \\nthe craft rotation was \\np\\n8\\n radians. The image \\nacquisition process can be modeled as an ideal \\nshutter that is open only during the time the craft \\nrotated \\np\\n8\\n radians. You may assume that the \\nvertical motion was negligible during the image \\nacquisition.\\n Formulate a solution for restoring the \\nimages. You do not have to solve the problem, just \\ngive an outline of how you would solve it using \\nthe methods discussed in Section 5.6 through 5.9. \\n(\\nHint:\\n Consider using polar coordinates. The blur \\nwill then appear as one-dimensional, uniform \\nmotion blur along the \\nu\\n-axis.)\\n5.29 * \\nThe image that follows is a blurred, 2-D projection \\nof a volumetric rendition of a heart.\\n It is known \\nthat each of the cross hairs on the right bottom \\npart of the image was (before blurring) 3 pixels \\nwide, 30 pixels long, and had an intensity value of \\n255. Provide a step-by-step procedure indicating \\nhow you would use the information just given to \\nobtain the blurring function \\nH\\n(,\\n) .\\nuv\\n(Original image courtesy of GE Medical Systems.)\\nDIP4E_GLOBAL_Print_Ready.indb   395\\n6/16/2017   2:08:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 396}),\n",
       " Document(page_content='396\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n5.30 \\nThe image in Fig. 5.29(h) was obtained by \\ninverse-ﬁltering the image in F\\nig. 5.29(g), which \\nis a blurred image that, in addition, is corrupted \\nby additive Gaussian noise. The blurring itself \\nis corrected by the inverse ﬁlter, as is evident in \\nFig. 5.29(h). However, the restored image has a \\nstrong streak pattern that is not apparent in Fig. \\n5.29(g) [for example, compare the area of con-\\nstant white in the top right of Fig. 5.29(g) with the \\ncorresponding are in Fig. 5.29(h)]. Explain how \\nthis pattern originated.\\n5.31 \\nA certain X-ray imaging geometry produces a \\nblurring degradation that can be modeled as the \\nconvolution of the sensed image with the spatial,\\n \\ncircularly symmetric function\\nhxy\\nxy\\ne\\nxy\\n(,)\\n()\\n=\\n+−\\n−+\\n22 2\\n4\\n2\\n2\\n22 2\\ns\\ns\\ns\\nAssuming continuous variables, show that the \\ndegradation in the frequency domain is given by \\nthe expression\\nHu e\\nu\\n(,) ( )\\n()\\nuv\\nv\\nv\\n=− +\\n−+\\n8\\n42 2 2 2\\n22 2 2\\nps\\nps\\n(\\nHint:\\n Refer to the discussion of the Laplacian \\nin Section 4.9, entry 13 in Table 4.4, and review \\nProblem 4.52.)\\n5.32 * \\nUsing the transfer function in Problem 5.31, give \\nthe expression for a \\nWiener ﬁlter transfer func-\\ntion, assuming that the ratio of power spectra of \\nthe noise and undegraded images is a constant.\\n5.33 \\nGiven \\npxy\\n(,\\n)\\n in Eq. (5-90), show that\\nPu\\nM N\\n(,) c o s ( ) c o s ( )\\nuv\\nv\\n=−\\n−\\n42 2 2 2\\npp\\n(\\nHint:\\n Study the solution to Problem 4.47.)\\n5.34 \\nShow how Eq. (5-98) follows from Eqs. (5-96) and \\n(5-97).\\n5.35 \\nUsing the transfer function in Problem 5.31, give \\nthe resulting expression for the constrained least \\nsquares ﬁlter transfer function.\\n5.36 * \\nAssume that the model in Fig. 5.1 is linear and \\nposition invariant,\\n and that the noise and image \\nare uncorrelated. Show that the power spectrum \\nof the output is\\nGH F N\\n(,) (,) (,) (,)\\nuv uv uv uv\\n22 2 2\\n=+\\n[\\nHint:\\n Refer to Eqs\\n. (5-65) and (4-89).]\\n5.37 \\nCannon [1974] suggested a restoration ﬁlter \\nR\\n(,\\n)\\nuv\\n \\nsatisfying the condition\\nˆ\\n(,) (,) (,)\\nFR G\\nuv uv uv\\n2\\n22\\n=\\nThe restoration ﬁlter is based on the premise of \\nforcing the power spectrum of the restored image\\n, \\nˆ\\n(,\\n),\\nF\\nuv\\n2\\n to equal the spectrum of the original \\nimage, \\nF\\n(,) .\\nuv\\n2\\n Assume that the image and noise \\nare uncorrelated,\\n(a) * \\nFind \\nR\\n(,\\n)\\nuv\\n in terms of \\nF\\n(,) ,\\nuv\\n2\\n \\nH\\n(,) ,\\nuv\\n2\\n \\nand \\nN\\n(,)\\n.\\nuv\\n2\\n (\\nHint:\\n Take a look at Fig. 5.1, \\nEq. (5-65), and Problem 5.36.)\\n(b) \\nUse your result from (a) to state a result in a \\nform similar to the last line of Eq.\\n (5-81), and \\nusing the same terms.\\n5.38 \\nShow that, when \\na\\n=\\n1\\n in Eq. (5-99), the geomet-\\nric mean ﬁlter reduces to the inverse ﬁlter\\n.\\n5.39 * \\nA professor of archeology doing research on \\ncurrenc\\ny exchange practices during the Roman \\nEmpire recently became aware that four Roman \\ncoins crucial to his research are listed in the hold-\\nings of the British Museum in London. Unfortu-\\nnately, he was told after arriving there that the \\ncoins had been recently stolen. Further research \\non his part revealed that the museum keeps pho-\\ntographs of every item for which it is responsible. \\nUnfortunately, the photos of the coins in question \\nare blurred to the point where the date and other \\nsmall markings are not readable. The cause of the \\nblurring was the camera being out of focus when \\nthe pictures were taken. As an image processing \\nexpert and friend of the professor, you are asked \\nas a favor to determine whether computer pro-\\ncessing can be utilized to restore the images to the \\npoint where the professor can read the markings. \\nYou are told that the original camera used to take \\nthe photos is still available, as are other represen-\\ntative coins of the same era. Propose a step-by-\\nstep solution to this problem.\\n5.40 \\nAn astronomer is working with an optical tele-\\nscope\\n. The telescope lenses focus images onto \\na high-resolution, CCD imaging array, and the \\nimages are then converted by the telescope elec-\\ntronics into digital images. Working late one eve-\\nning, the astronomer notices that her new images \\nare noisy and blurry. The manufacturer tells the \\nastronomer that the unit is operating within speci-\\nﬁcations. Trying to improve the situation by con-\\nDIP4E_GLOBAL_Print_Ready.indb   396\\n6/16/2017   2:08:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 397}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n397\\nducting controlled lab experiments with the lens-\\nes and imaging sensors is not possible because of \\nthe size and weight of the telescope components. \\nHaving heard about your success in restoring the \\nRoman coins, the astronomer calls you to help \\nher formulate a digital image processing solu-\\ntion for sharpening her images. How would you \\ngo about solving this problem, given that the only \\nimages you can obtain are images of stellar bod-\\nies? (\\nHint:\\n A single, bright star that appears as a \\npoint of light in the ﬁeld of view can be used to \\napproximate an impulse.)\\n5.41 * \\nSketch the Radon transform of the \\nMM\\n×\\n binary \\nimage shown below\\n, which consists of a single \\nwhite pixel in the center of the image. Assume a \\nparallel-beam geometry, and label quantitatively \\nall the important elements of your sketch .\\n5.42 * \\nA Sketch a cross section of the Radon transform \\nof the following white disk image containing a \\nsmaller black disk in its center\\n. (\\nHint:\\n Take a look \\nat Fig. 5.38.)\\n5.43 \\nShow that the Radon transform [Eq. (5-102)] of \\nthe Gaussian shape \\nfx y A x y\\n(\\n, ) exp( )\\n=− −\\n22\\n is \\ngiven by \\ngA\\n( , ) exp( ).\\nru p r\\n=−\\n2\\n (\\nHint:\\n Refer to \\nExample 5.15, where we used symmetry to sim-\\nplify integration.)\\n5.44 \\nDo the following:\\n(a) * \\nShow that the Radon transform [Eq. (5-102)] \\nof the unit impulse \\nd\\n(,\\n)\\nxy\\n is a straight ver-\\ntical line passing through the origin of the \\nru\\n-\\nplane\\n .\\n(b) \\nShow that the radon transform of the \\nimpulse \\nd\\n(,\\n)\\nxx yy\\n−−\\n00\\n is a sinusoidal curve \\nin the \\nru\\n-\\nplane.\\n5.45 \\nProve the validity of the following properties of \\nthe Radon transform [Eq.\\n (5-102)]:\\n(a) * \\nLinearity:\\n \\nThe Radon transform is a linear \\noperator. (See Section 2.6 regarding linear-\\nity.)\\n(b) \\nTranslation property:\\n \\nThe radon transform of \\nfx x y y\\n(,\\n)\\n−−\\n00\\n is \\ngx y\\n(\\ncos sin , ).\\nruu u\\n−−\\n00\\n(c) * \\nConvolution property:\\n \\nThe Radon transform \\nof the convolution of two functions is equal \\nto the convolution of the Radon transforms \\nof the two functions.\\n5.46 \\nProvide the steps that lead from Eq. (5-113) to   \\nEq.\\n (5-114). [\\nHint: \\nGG\\n(,\\n) ( ,) . ]\\nvu\\nvu\\n+= −\\n180\\n°\\n5.47 * \\nProve the validity of Eq. (5-125).\\n5.48 \\nProve the validity of Eq. (5-127).\\nDIP4E_GLOBAL_Print_Ready.indb   397\\n6/16/2017   2:08:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 398}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 399}),\n",
       " Document(page_content='3996\\nColor Image Processing\\nPreview\\nUsing color in image processing is motivated by two principal factors. First, color is a powerful descrip-\\ntor that often simpliﬁes object identiﬁcation and extraction from a scene. Second, humans can discern \\nthousands of color shades, compared to only about two dozen shades of gray. The latter factor is par-\\nticularly important in manual image analysis. Color image processing is divided into two major areas: \\npseudo- \\nand \\nfull-color\\n processing. In the ﬁrst category, the issue is one of assigning color(s) to a par-\\nticular grayscale intensity or range of intensities. In the second, images typically are acquired using a \\nfull-color sensor, such as a digital camera, or color scanner. Until just a few years ago, most digital color \\nimage processing was done at the pseudo- or reduced-color level. However, because color sensors and \\nprocessing hardware have become available at reasonable prices, full-color image processing techniques \\nare now used in a broad range of applications. In the discussions that follow, it will become evident that \\nsome of the grayscale methods covered in previous chapters are applicable also to color images.\\nUpon completion of this chapter, readers should:\\n Understand the fundamentals of color and \\nthe color spectrum.\\n Be familiar with several of the color models \\nused in digital image processing. \\n Know how to apply basic techniques in pseudo- \\ncolor image processing, including intensity slic-\\ning and intensity-to-color transformations.\\n Be familiar with how to determine if a gray-\\nscale method is extendible to color images.\\n Understand the basics of working with full-\\ncolor images, including color transformations, \\ncolor complements, and tone/color corrections.\\n Be familiar with the role of noise in color \\nimage processing.\\n Know how to perform spatial ﬁltering on col-\\nor images. \\n Understand the advantages of using color in \\nimage segmentation. \\nIt is only after years of preparation that the young artist should \\ntouch color—not color used descriptively, that is, but as a means of \\npersonal expression.\\nHenri Matisse\\nFor a long time I limited myself to one color—as a form of discipline.\\nPablo Picasso\\nDIP4E_GLOBAL_Print_Ready.indb   399\\n6/16/2017   2:08:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 400}),\n",
       " Document(page_content='400\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\n6.1 COLOR FUNDAMENTALS  \\nAlthough the process employed by the human brain in perceiving and interpreting \\ncolor is a physiopsychological phenomenon that is not fully understood, the physical \\nnature of color can be expressed on a formal basis supported by experimental and \\ntheoretical results.\\nIn 1666, Sir Isaac Newton discovered that when a beam of sunlight passes through \\na glass prism, the emerging light is not white, but consists instead of a continuous \\nspectrum of colors ranging from violet at one end to red at the other. As Fig. 6.1 \\nshows, the color spectrum may be divided into six broad regions: violet, blue, green, \\nyellow, orange, and red. When viewed in full color (see Fig. 6.2), no color in the spec-\\ntrum ends abruptly; rather, each color blends smoothly into the next.\\nBasically, the colors that humans and some other animals perceive in an object \\nare determined by the nature of the light reﬂected from the object. As illustrated in \\nFig. 6.2, visible light is composed of a relatively narrow band of frequencies in the \\nelectromagnetic spectrum. A body that reﬂects light that is balanced in all visible \\nwavelengths appears white to the observer. However, a body that favors reﬂectance \\nin a limited range of the visible spectrum exhibits some shades of color. For example, \\ngreen objects reﬂect light with wavelengths primarily in the 500 to 570 nm range, \\nwhile absorbing most of the energy at other wavelengths.\\nCharacterization of light is central to the science of color. If the light is \\nachro-\\nmatic\\n (void of color), its only attribute is its \\nintensity\\n, or amount. Achromatic light \\nis what you see on movie ﬁlms made before the 1930s. As deﬁned in Chapter 2, and \\nused numerous times since, the term \\ngray (\\nor\\n intensity) level\\n refers to a scalar mea-\\nsure of intensity that ranges from black, to grays, and ﬁnally to white.\\nChromatic\\n \\nlight\\n spans the electromagnetic spectrum from approximately 400 \\nto 700 nm. Three basic quantities used to describe the quality of a chromatic light \\nsource are: radiance, luminance, and brightness. \\nRadiance\\n is the total amount of \\nenergy that ﬂows from the light source, and it is usually measured in watts (W). \\nLuminance\\n, measured in lumens (lm), is a measure of the amount of energy that \\nan observer \\nperceives\\n from a light source. For example, light emitted from a source \\noperating in the far infrared region of the spectrum could have signiﬁcant energy \\n(radiance), but an observer would hardly perceive it; its luminance would be almost \\nzero. Finally, \\nbrightness\\n is a subjective descriptor that is practically impossible to \\nmeasure. It embodies the achromatic notion of intensity, and is one of the key fac-\\ntors in describing color sensation.\\n6.1\\nFIGURE 6.1\\nColor spectrum \\nseen by passing \\nwhite light through \\na prism.  \\n(Courtesy of the \\nGeneral Electric \\nCo., Lighting  \\nDivision.)\\nDIP4E_GLOBAL_Print_Ready.indb   400\\n6/16/2017   2:08:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 401}),\n",
       " Document(page_content='6.1\\n  \\nColor Fundamentals\\n    \\n401\\nAs noted in Section 2.1, cones are the sensors in the eye responsible for color \\nvision. Detailed experimental evidence has established that the 6 to 7 million cones in \\nthe human eye can be divided into three principal sensing categories, corresponding \\nroughly to red, green, and blue. Approximately 65% of all cones are sensitive to red \\nlight, 33% are sensitive to green light, and only about 2% are sensitive to blue. How-\\never, the blue cones are the most sensitive. Figure 6.3 shows average experimental \\ncurves detailing the absorption of light by the red, green, and blue cones in the eye. \\nBecause of these absorption characteristics, the human eye sees colors as variable \\ncombinations of the so-called \\nprimary colors\\n: \\nred\\n (R), \\ngreen\\n (G), and \\nblue\\n (B). \\nFor the purpose of standardization, the CIE (Commission Internationale de \\nl’Eclairage—the International Commission on Illumination) designated in 1931 the \\nfollowing speciﬁc wavelength values to the three primary colors: \\nblue  nm,\\n=\\n435\\n8 .\\n \\ngreen  nm,\\n=\\n546\\n1 .\\n and \\nred  nm.\\n=\\n700\\n This standard was set before results such as \\nthose in F\\nig. 6.3 became available in 1965. Thus, the CIE standards correspond only \\napproximately with experimental data. It is important to keep in mind that deﬁning \\nthree speciﬁc primary color wavelengths for the purpose of standardization does \\nFIGURE 6.2\\nWavelengths compris-\\ning the visible range \\nof the electromagnetic \\nspectrum. (Courtesy of \\nthe General Electric \\nCo., Lighting Division.)\\nFIGURE 6.3\\nAbsorption of \\nlight by the red, \\ngreen, and blue \\ncones in the \\nhuman eye as a \\nfunction of  \\nwavelength.\\nAbsorption (arbitrary units)\\n400 450 500 550 600 650 700 nm\\nBluish purple\\n445 nm\\nBlue\\n535 nm\\nGreen\\n575 nm\\nRed\\nPurplish blue\\nBlue\\nBlue green\\nGreen\\nYellowish green\\nYellow\\nOrange\\nReddish orange\\nRed\\nDIP4E_GLOBAL_Print_Ready.indb   401\\n6/16/2017   2:08:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 402}),\n",
       " Document(page_content='402\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nnot\\n mean that these three ﬁxed RGB components acting alone can generate all \\nspectrum colors. Use of the word \\nprimary\\n has been widely misinterpreted to mean \\nthat the three standard primaries, when mixed in various intensity proportions, can \\nproduce all visible colors. As you will see shortly, this interpretation is not correct \\nunless the wavelength also is allowed to vary, in which case we would no longer have \\nthree ﬁxed primary colors.\\nThe primary colors can be added together to produce the \\nsecondary\\n colors of \\nlight—\\nmagenta\\n (red plus blue), \\ncyan\\n (green plus blue), and \\nyellow\\n (red plus green). \\nMixing the three primaries, or a secondary with its opposite primary color, in the \\nright intensities produces white light. This result is illustrated in Fig. 6.4(a), which \\nshows also the three primary colors and their combinations to produce the second-\\nary colors of light.\\nDifferentiating between the primary colors of light and the primary colors of pig-\\nments or colorants is important. In the latter, a primary color is deﬁned as one that \\nsubtracts or absorbs a primary color of light, and reﬂects or transmits the other two. \\nTherefore, the primary colors of pigments are magenta, cyan, and yellow, and the \\nsecondary colors are red, green, and blue. These colors are shown in Fig. 6.4(b). A \\nproper combination of the three pigment primaries, or a secondary with its opposite \\nprimary, produces black.\\nColor television reception is an example of the additive nature of light colors. \\nThe interior of CRT (cathode ray tube) color TV screens used well into the 1990s is \\ncomposed of a large array of triangular dot patterns of electron-sensitive phosphor. \\nWhen excited, each dot in a triad produces light in one of the primary colors. The \\nIn practice, pigments \\nseldom are pure. This \\nresults in a muddy brown \\ninstead of black when \\nprimaries, or primaries \\nand secondaries, are \\ncombined. We will \\ndiscuss this issue in  \\nSection 6.2\\nb\\na\\nFIGURE 6.4\\nPrimary and \\nsecondary colors \\nof light and  \\npigments.  \\n(Courtesy of the \\nGeneral Electric \\nCo., Lighting  \\nDivision.)\\nDIP4E_GLOBAL_Print_Ready.indb   402\\n6/16/2017   2:08:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 403}),\n",
       " Document(page_content='6.1\\n  \\nColor Fundamentals\\n    \\n403\\nintensity of the red-emitting phosphor dots is modulated by an electron gun inside \\nthe tube, which generates pulses corresponding to the “red energy” seen by the TV \\ncamera. The green and blue phosphor dots in each triad are modulated in the same \\nmanner. The effect, viewed on the television receiver, is that the three primary colors \\nfrom each phosphor triad are received and “added” together by the color-sensitive \\ncones in the eye and perceived as a full-color image. Thirty successive image changes \\nper second in all three colors complete the illusion of a continuous image display on \\nthe screen.\\nCRT displays started being replaced in the late 1990s by ﬂat-panel digital tech-\\nnologies, such as liquid crystal displays (LCDs) and plasma devices. Although they \\nare fundamentally different from CRTs, these and similar technologies use the same \\nprinciple in the sense that they all require three subpixels (red, green, and blue) to \\ngenerate a single color pixel. LCDs use properties of polarized light to block or pass \\nlight through the LCD screen and, in the case of active matrix display technologies, \\nthin ﬁlm transistors (TFTs) are used to provide the proper signals to address each \\npixel on the screen. Light ﬁlters are used to produce the three primary colors of light \\nat each pixel triad location. In plasma units, pixels are tiny gas cells coated with phos-\\nphor to produce one of the three primary colors. The individual cells are addressed \\nin a manner analogous to LCDs. This individual pixel triad coordinate addressing \\ncapability is the foundation of digital displays.\\nThe characteristics generally used to distinguish one color from another are \\nbrightness, hue, and saturation. As indicated earlier in this section, brightness \\nembodies the achromatic notion of intensity. \\nHue\\n is an attribute associated with the \\ndominant wavelength in a mixture of light waves. Hue represents dominant color as \\nperceived by an observer. Thus, when we call an object red, orange, or yellow, we are \\nreferring to its hue. \\nSaturation\\n refers to the relative purity or the amount of white \\nlight mixed with a hue. The pure spectrum colors are fully saturated. Colors such \\nas pink (red and white) and lavender (violet and white) are less saturated, with the \\ndegree of saturation being inversely proportional to the amount of white light added.\\nHue and saturation taken together are called \\nchromaticity\\n and, therefore, a color \\nmay be characterized by its brightness and chromaticity. The amounts of red, green, \\nand blue needed to form any particular color are called the \\ntristimulus\\n values, and \\nare denoted, \\nX\\n, \\nY\\n, and \\nZ\\n, respectively. A color is then speciﬁed by its \\ntrichromatic \\ncoefﬁcients\\n, deﬁned as\\n \\nx\\nX\\nXYZ\\n=\\n++\\n \\n(6-1)\\n \\ny\\nY\\nXYZ\\n=\\n++\\n \\n(6-2)\\nand\\n \\nz\\nZ\\nXYZ\\n=\\n++\\n \\n(6-3)\\nDIP4E_GLOBAL_Print_Ready.indb   403\\n6/16/2017   2:08:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 404}),\n",
       " Document(page_content='404\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nWe see from these equations that\\n \\nxyz\\n++\\n=\\n1  \\n(6-4)\\nFor any wavelength of light in the visible spectrum, the tristimulus values needed \\nto produce the color corresponding to that wavelength can be obtained directly \\nfrom curves or tables that have been compiled from extensive experimental results \\n(P\\noynton [1996, 2012]).\\nAnother approach for specifying colors is to use the CIE \\nchromaticity diagram \\n(see \\nFig. 6.5), which shows color composition as a function of \\nx\\n (red) and \\ny\\n (green). For \\nany value of \\nx\\n and \\ny\\n, the corresponding value of \\nz\\n (blue) is obtained from Eq. (6-4) \\nby noting that \\nzx y\\n=−\\n+\\n1( ) .\\n The point marked green in Fig. 6.5, for example, has \\napproximately 62% green and 25% red content.\\n It follows from Eq. (6-4) that the \\ncomposition of blue is approximately 13%.\\nThe positions of the various spectrum colors—from violet at 380 nm to red at \\n780 nm—are indicated around the boundary of the tongue-shaped chromaticity dia-\\ngram. These are the pure colors shown in the spectrum of Fig. 6.2. Any point not \\nactually on the boundary, but within the diagram, represents some mixture of the \\npure spectrum colors. The \\npoint of equal energy\\n shown in Fig. 6.5 corresponds to \\nequal fractions of the three primary colors; it represents the CIE standard for white \\nlight. Any point located on the boundary of the chromaticity chart is fully saturated. \\nAs a point leaves the boundary and approaches the point of equal energy, more \\nwhite light is added to the color, and it becomes less saturated. The saturation at the \\npoint of equal energy is zero.\\nThe chromaticity diagram is useful for color mixing because a straight-line seg-\\nment joining any two points in the diagram deﬁnes all the different color variations \\nthat can be obtained by combining these two colors additively. Consider, for exam-\\nple, a straight line drawn from the red to the green points shown in Fig. 6.5. If there is \\nmore red than green light, the exact point representing the new color will be on the \\nline segment, but it will be closer to the red point than to the green point. Similarly, a \\nline drawn from the point of equal energy to any point on the boundary of the chart \\nwill deﬁne all the shades of that particular spectrum color.\\nExtending this procedure to three colors is straightforward. To determine the \\nrange of colors that can be obtained from any three given colors in the chromatic-\\nity diagram, we simply draw connecting lines to each of the three color points. The \\nresult is a triangle, and any color inside the triangle, or on its boundary, can be pro-\\nduced by various combinations of the three vertex colors. A triangle with vertices at \\nany three ﬁxed colors cannot enclose the entire color region in Fig. 6.5. This observa-\\ntion supports graphically the remark made earlier that not all colors can be obtained \\nwith three single, \\nﬁxed\\n primaries, because three colors form a triangle.\\nThe triangle in Fig. 6.6 shows a representative range of colors (called the \\ncolor \\ngamut\\n) produced by RGB monitors. The shaded region inside the triangle illustrates \\nthe color gamut of today’s high-quality color printing devices. The boundary of the \\ncolor printing gamut is irregular because color printing is a combination of additive \\nand subtractive color mixing, a process that is much more difﬁcult to control than \\nOur use of \\nx\\n, \\ny\\n, and \\nz\\n in \\nthis context follows con-\\nvention. These should not \\nbe confused with our use \\nof (\\nx\\n, \\ny\\n) throughout the \\nbook to denote spatial \\ncoordinates.\\nDIP4E_GLOBAL_Print_Ready.indb   404\\n6/16/2017   2:08:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 405}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n405\\nthat of displaying colors on a monitor, which is based on the addition of three highly \\ncontrollable light primaries.\\n6.2 COLOR MODELS  \\nThe purpose of a \\ncolor model\\n (also called a \\ncolor space\\n or \\ncolor system\\n) is to facilitate the \\nspecification of colors in some standard way. In essence, a color model is a specification \\nof (1) a coordinate system, and (2) a subspace within that system, such that each color in \\nthe model is represented by a single point contained in that subspace.\\nMost color models in use today are oriented either toward hardware (such as for \\ncolor monitors and printers) or toward applications, where color manipulation is \\na goal (the creation of color graphics for animation is an example of the latter). In \\nterms of digital image processing, the hardware-oriented models most commonly \\nused in practice are the RGB (red, green, blue) model for color monitors and a \\n6.2\\nFIGURE 6.5\\nThe CIE  \\nchromaticity \\ndiagram.  \\n(Courtesy of the \\nGeneral Electric \\nCo., Lighting  \\nDivision.)\\nDIP4E_GLOBAL_Print_Ready.indb   405\\n6/16/2017   2:08:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 406}),\n",
       " Document(page_content='406\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nbroad class of color video cameras; the CMY (cyan, magenta, yellow) and CMYK \\n(cyan, magenta, yellow, black) models for color printing; and the HSI (hue, satura-\\ntion, intensity) model, which corresponds closely with the way humans describe and \\ninterpret color. The HSI model also has the advantage that it decouples the color \\nand gray-scale information in an image, making it suitable for many of the gray-scale \\ntechniques developed in this book. There are numerous color models in use today. \\nThis is a reﬂection of the fact that color science is a broad ﬁeld that encompasses \\nmany areas of application. It is tempting to dwell on some of these models here, sim-\\nply because they are interesting and useful. However, keeping to the task at hand, \\nwe focus attention on a few models that are representative of those used in image \\nprocessing. Having mastered the material in this chapter, you will have no difﬁculty \\nin understanding additional color models in use today.\\nG\\nR\\nB\\n380\\n450\\n460\\n470\\n480\\n490\\n500\\n510\\n530\\n540\\n550\\n560\\n570\\n580\\n590\\n600\\n610\\n620\\n640\\n780\\n520\\nx\\n-axis\\ny\\n-axis\\n0\\n0\\n.1\\n.2\\n.3\\n.4\\n.5\\n.6\\n.7\\n.8\\n.1\\n.2\\n.3\\n.4\\n.5\\n.6\\n.7\\n.8\\n.9\\nFIGURE 6.6\\nIllustrative color \\ngamut of color \\nmonitors  \\n(triangle) and \\ncolor printing \\ndevices (shaded \\nregion).\\nDIP4E_GLOBAL_Print_Ready.indb   406\\n6/16/2017   2:08:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 407}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n407\\nTHE RGB COLOR MODEL\\nIn the RGB model, each color appears in its primary spectral components of red, \\ngreen, and blue. This model is based on a Cartesian coordinate system. The color \\nsubspace of interest is the cube shown in Fig. 6.7, in which RGB primary values are \\nat three corners; the secondary colors cyan, magenta, and yellow are at three other \\ncorners; black is at the origin; and white is at the corner farthest from the origin. In \\nthis model, the grayscale (points of equal RGB values) extends from black to white \\nalong the line joining these two points. The different colors in this model are points \\non or inside the cube, and are defined by vectors extending from the origin. For con-\\nvenience, the assumption is that all color values have been normalized so the cube \\nin Fig. 6.7 is the unit cube. That is, all values of R, G, and B in this representation are \\nassumed to be in the range [0, 1]. Note that the RGB primaries can be interpreted as \\nunit vectors emanating from the origin of the cube. \\nImages represented in the RGB color model consist of three component images, \\none for each primary color. When fed into an RGB monitor, these three images \\ncombine on the screen to produce a composite color image, as explained in Sec-\\ntion 6.1. The number of bits used to represent each pixel in RGB space is called the \\npixel depth\\n. Consider an RGB image in which each of the red, green, and blue imag-\\nes is an 8-bit image. Under these conditions, each RGB \\ncolor\\n \\npixel\\n [that is, a triplet of \\nvalues (R, G, B)] has a depth of 24 bits (3 image planes times the number of bits per \\nplane). The term \\nfull-color\\n image is used often to denote a 24-bit RGB color image. \\nThe total number of possible colors in a 24-bit RGB image is \\n() , , .\\n2\\n16 777 216\\n83\\n=\\n \\nF\\nigure 6.8 shows the 24-bit RGB color cube corresponding to the diagram in Fig. 6.7. \\nNote also that for digital images, the range of values in the cube are scaled to the \\n(1, 0, 0)\\nRed\\nYellow\\nGreen\\nBlack\\nWhite\\nMagenta\\n(0, 1, 0)\\nCyan\\nBlue\\n(0, 0, 1)\\nR\\nG\\nB\\nGrayscale\\nFIGURE 6.7\\nSchematic of the \\nRGB color cube. \\nPoints along the \\nmain diagonal \\nhave gray values, \\nfrom black at the \\norigin to white at \\npoint (1, 1, 1).\\nDIP4E_GLOBAL_Print_Ready.indb   407\\n6/16/2017   2:08:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 408}),\n",
       " Document(page_content='408\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nnumbers representable by the number bits in the images. If, as above, the primary \\nimages are 8-bit images, the limits of the cube along each axis becomes \\n[, ] .\\n0\\n255\\n \\nT\\nhen, for example, white would be at point \\n[, , ]\\n255\\n255 255\\n in the cube.\\nEXAMPLE 6.1 :  Generating a cross-section of the RGB color cube and its thee hidden planes.\\nThe cube in Fig. 6.8 is a solid, composed of the \\n()\\n2\\n83\\n colors mentioned in the preceding paragraph. A \\nuseful way to view these colors is to generate color planes (faces or cross sections of the cube). This is \\ndone by ﬁxing one of the three colors and allowing the other two to vary. For instance, a cross-sectional \\nplane through the center of the cube and parallel to the GB-plane in Fig. 6.8 is the plane (127, G, B) for \\nGB\\n,,\\n, , , .\\n=\\n012 2 5 5\\n…\\n Figure 6.9(a) shows that an image of this cross-sectional plane is generated by feed-\\ning the three individual component images into a color monitor\\n. In the component images, 0 represents \\nblack and 255 represents white. Observe that each component image into the monitor is a grayscale \\nimage. The monitor does the job of combining the intensities of these images to generate an RGB image. \\nFigure 6.9(b) shows the three hidden surface planes of the cube in Fig. 6.8, generated in a similar manner.\\nAcquiring a color image is the process shown in Fig. 6.9(a) in reverse. A color image can be acquired \\nby using three ﬁlters, sensitive to red, green, and blue, respectively. When we view a color scene with a \\nmonochrome camera equipped with one of these ﬁlters, the result is a monochrome image whose inten-\\nsity is proportional to the response of that ﬁlter. Repeating this process with each ﬁlter produces three \\nmonochrome images that are the RGB component images of the color scene. In practice, RGB color \\nimage sensors usually integrate this process into a single device. Clearly, displaying these three RGB \\ncomponent images as in Fig. 6.9(a) would yield an RGB color rendition of the original color scene. \\nTHE CMY AND CMYK COLOR MODELS\\nAs indicated in Section 6.1, cyan, magenta, and yellow are the secondary colors of \\nlight or, alternatively, they are the primary colors of pigments. For example, when \\na surface coated with cyan pigment is illuminated with white light, no red light is \\nreflected from the surface. That is, cyan subtracts red light from reflected white light, \\nwhich itself is composed of equal amounts of red, green, and blue light.\\nMost devices that deposit colored pigments on paper, such as color printers and \\ncopiers, require CMY data input or perform an RGB to CMY conversion internally. \\nThis conversion is performed using the simple operation\\nFIGURE 6.8\\nA 24-bit RGB \\ncolor cube.\\nDIP4E_GLOBAL_Print_Ready.indb   408\\n6/16/2017   2:08:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 409}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n409\\n \\nC\\nM\\nY\\nR\\nG\\nB\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n1\\n1\\n1\\n \\n(6-5)\\nwhere the assumption is that all RGB color values have been normalized to the \\nrange \\n[, ] .\\n01\\n Equation (6-5) demonstrates that light reflected from a surface coated \\nwith pure c\\nyan does not contain red (that is, \\nCR\\n=−\\n1\\n in the equation). Similarly, \\npure magenta does not reflect green,\\n and pure yellow does not reflect blue. Equa-\\ntion (6-5) also reveals that RGB values can be obtained easily from a set of CMY \\nvalues by subtracting the individual CMY values from 1. \\nAccording to Fig. 6.4, equal amounts of the pigment primaries, cyan, magenta, and \\nyellow, should produce black. In practice, because C, M, and Y inks seldom are pure \\ncolors, combining these colors for printing black produces instead a muddy-looking \\nbrown. So, in order to produce true black (which is the predominant color in print-\\ning), a fourth color, \\nblack\\n, denoted by \\nK\\n, is added, giving rise to the CMYK color \\nmodel. The black is added in just the proportions needed to produce true black. Thus, \\nEquation (6-5), as well as \\nall other equations in this \\nsection, are applied on a \\npixel-by-pixel basis.\\nColor\\nmonitor\\nRGB\\nRed\\n(\\nR\\n/H11005\\n 0)\\n(\\nG\\n/H11005\\n 0)\\n(\\nB\\n/H11005 \\n0)\\nGreen\\nBlue\\nb\\na\\nFIGURE 6.9\\n(a) Generating \\nthe RGB image of \\nthe cross-sectional \\ncolor plane  \\n(127, G, B).  \\n(b) The three  \\nhidden surface \\nplanes in the color \\ncube of Fig. 6.8.\\nDIP4E_GLOBAL_Print_Ready.indb   409\\n6/16/2017   2:08:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 410}),\n",
       " Document(page_content='410\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nwhen publishers talk about “four-color printing,” they are referring to the three \\nCMY colors, plus a portion of black.\\nThe conversion from CMY to CMYK begins by letting\\n \\nKC M Y\\n=\\nmi\\nn( , , )\\n \\n(6-6)\\nIf \\nK\\n=\\n1,\\n then we have pure black, with no color contributions, from which it follows \\nthat\\n \\nC\\n=\\n0  \\n(6-7)\\n \\nM\\n=\\n0  \\n(6-8)\\n \\nY\\n=\\n0  \\n(6-9)\\nOtherwise,\\n \\nCC K K\\n=− −\\n() ( )\\n1  \\n(6-10)\\n \\nMM K K\\n=− −\\n() ( )\\n1  \\n(6-11)\\n \\nYY K K\\n=− −\\n() ( )\\n1  \\n(6-12)\\nwhere all values are assumed to be in the range \\n[, ] .\\n01\\n The conversions from CMYK \\nback to CMY are:\\n \\nCC K K\\n=−\\n+\\n*\\n()\\n1  \\n(6-13)\\n \\nMM K K\\n=−\\n+\\n*\\n()\\n1  \\n(6-14)\\n \\nYY Y K\\n=−\\n+\\n*\\n()\\n1  \\n(6-15)\\nAs noted at the beginning of this section, all operations in the preceding equations \\nare performed on a pixel-by-pixel basis\\n. Because we can use Eq. (6-5) to convert \\nboth ways between CMY and RGB, we can use that equation as a “bridge” to con-\\nvert between RGB and CMYK, and vice versa.\\nIt is important to keep in mind that all the conversions just presented to go \\nbetween RGB, CMY, and CMYK are based on the preceding relationships as a \\ngroup. There are many other ways to convert between these color models, so you \\ncannot mix approaches and expect to get meaningful results. Also, colors seen on \\nmonitors generally appear much different when printed, unless these devices are \\ncalibrated (see the discussion of a device-independent color model later in this \\nsection). The same holds true in general for colors converted from one model to \\nanother. However, our interest in this chapter is not on color ﬁdelity; rather, we are \\ninterested in using the properties of color models to facilitate image processing tasks, \\nsuch as region detection. \\nThe \\nC\\n, \\nM\\n, and \\nY\\n on the \\nright side of Eqs. (6-6)-\\n(6-12) are in the CMY \\ncolor system. The \\nC\\n, \\nM\\n, and \\nY\\n on the left of \\nEqs. (6-7)-(6-12) are in \\nthe CMYK system.\\nThe \\nC\\n, \\nM\\n, \\nY\\n, and \\nK\\n \\non the right side of \\nEqs. (6-13)-(6-15) are in \\nthe CMYK color system. \\nThe \\nC\\n, \\nM\\n, and \\nY\\n on the \\nleft of these equations \\nare in the CMY system.\\nDIP4E_GLOBAL_Print_Ready.indb   410\\n6/16/2017   2:08:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 411}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n411\\nTHE HSI COLOR MODEL\\nAs we have seen, creating colors in the RGB, CMY, and CMYK models, and chang-\\ning from one model to the other, is straightforward. These color systems are ideally \\nsuited for hardware implementations. In addition, the RGB system matches nicely \\nwith the fact that the human eye is strongly perceptive to red, green, and blue pri-\\nmaries. Unfortunately, the RGB, CMY, and other similar color models are not well \\nsuited for describing colors in terms that are practical for human interpretation. For \\nexample, one does not refer to the color of an automobile by giving the percentage \\nof each of the primaries composing its color. Furthermore, we do not think of color \\nimages as being composed of three primary images that combine to form a single \\nimage.\\nWhen humans view a color object, we describe it by its hue, saturation, and \\nbrightness. Recall from the discussion in Section 6.1 that hue is a color attribute \\nthat describes a pure color (pure yellow, orange, or red), whereas saturation gives \\na measure of the degree to which a pure color is diluted by white light. Brightness \\nis a subjective descriptor that is practically impossible to measure. It embodies the \\nachromatic notion of \\nintensity\\n and is one of the key factors in describing color sensa-\\ntion. We do know that intensity (gray level) is a most useful descriptor of achromatic \\nimages. This quantity deﬁnitely is measurable and easily interpretable. The mod-\\nel we are about to present, called the \\nHSI\\n (hue, saturation, intensity) \\ncolor model\\n, \\ndecouples the intensity component from the color-carrying information (hue and \\nsaturation) in a color image. As a result, the HSI model is a useful tool for develop-\\ning image processing algorithms based on color descriptions that are natural and \\nintuitive to humans, who, after all, are the developers and users of these algorithms. \\nWe can summarize by saying that RGB is ideal for image color generation (as in \\nimage capture by a color camera or image display on a monitor screen), but its use \\nfor color description is much more limited. The material that follows provides an \\neffective way to do this.\\nWe know from Example 6.1 that an RGB color image is composed three gray-\\nscale intensity images (representing red, green, and blue), so it should come as no \\nsurprise that we can to extract intensity from an RGB image. This becomes clear if \\nwe take the color cube from Fig. 6.7 and stand it on the black, \\n(, , ) ,\\n000\\n vertex, with \\nthe white\\n, (1, 1, 1), vertex directly above it [see Fig. 6.10(a)]. As noted in our discus-\\nsion of Fig. 6.7, the intensity (gray) scale is along the line joining these two vertices. \\nIn Figs. 6.10(a) and (b), the line (intensity axis) joining the black and white vertices is \\nvertical. Thus, if we wanted to determine the intensity component of any color point \\nin Fig. 6.10, we would simply deﬁne a plane that contains the color point and, at the \\nsame time, is perpendicular to the intensity axis. The intersection of the plane with \\nthe intensity axis would give us a point with intensity value in the range [0, 1]. A little \\nthought would reveal that the saturation (purity) of a color increases as a function of \\ndistance from the intensity axis. In fact, the saturation of points on the intensity axis \\nis zero, as evidenced by the fact that all points along this axis are gray.\\nHue can be determined from an RGB value also. To see how, consider Fig. 6.10(b), \\nwhich shows a plane deﬁned by three points (black, white, and cyan). The fact that \\nDIP4E_GLOBAL_Print_Ready.indb   411\\n6/16/2017   2:08:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 412}),\n",
       " Document(page_content='412\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nthe black and white points are contained in the plane tells us that the intensity axis \\nalso is contained in the plane. Furthermore, we see that \\nall\\n points contained in the \\nplane segment deﬁned by the intensity axis and the boundaries of the cube have the \\nsame\\n hue (cyan in this case). We could arrive at the same conclusion by recalling \\nfrom Section 6.1 that all colors generated by three colors lie in the triangle deﬁned \\nby those colors. If two of those points are black and white, and the third is a color \\npoint, all points on the triangle would have the same hue, because the black and \\nwhite components cannot change the hue (of course, the intensity and saturation \\nof points in this triangle would be different). By rotating the shaded plane about \\nthe vertical intensity axis, we would obtain different hues. From these concepts, we \\narrive at the conclusion that the hue, saturation, and intensity values required to \\nform the HSI space can be obtained from the RGB color cube. That is, we can con-\\nvert any RGB point to a corresponding point in the HSI color space by working out \\nthe formulas that describe the reasoning outlined in the preceding discussion.\\nThe key point regarding the cube arrangement in Fig. 6.10, and its corresponding \\nHSI color space, is that the HSI space is represented by a vertical intensity axis, and \\nthe locus of color points that lie on planes perpendicular to that axis. As the planes \\nmove up and down the intensity axis, the boundaries deﬁned by the intersection of \\neach plane with the faces of the cube have either a triangular or a hexagonal shape. \\nThis can be visualized much more readily by looking at the cube straight down its \\ngrayscale axis, as shown in Fig. 6.11(a). We see that the primary colors are separated \\nby 120°. The secondary colors are 60° from the primaries, which means that the angle \\nbetween secondaries is 120° also. Figure 6.11(b) shows the same hexagonal shape \\nand an arbitrary color point (shown as a dot). The hue of the point is determined by \\nan angle from some reference point. Usually (but not always) an angle of 0° from \\nthe red axis designates 0 hue, and the hue increases counterclockwise from there. \\nThe saturation (distance from the vertical axis) is the length of the vector from the \\norigin to the point. Note that the origin is deﬁned by the intersection of the color \\nplane with the vertical intensity axis. The important components of the HSI color \\nspace are the vertical intensity axis, the length of the vector to a color point, and the \\nb a\\nFIGURE 6.10\\nConceptual  \\nrelationships \\nbetween the RGB \\nand HSI color \\nmodels.\\nWhite\\nYellow\\nMagenta\\nCyan\\nBlue Red\\nGreen\\nBlack\\nWhite\\nYellow\\nMagenta\\nCyan\\nBlue Red\\nGreen\\nBlack\\nDIP4E_GLOBAL_Print_Ready.indb   412\\n6/16/2017   2:08:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 413}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n413\\nGreen\\nYellow\\nRed\\nWhite\\nMagenta\\nBlue\\nCyan\\nGreen Yellow\\nRed\\nMagenta\\nBlue\\nCyan\\nS\\nH\\nGreen\\nYellow\\nRed\\nMagenta\\nBlue\\nCyan\\nS\\nH\\nGreen\\nYellow\\nRed\\nMagenta\\nBlue\\nCyan\\nS\\nH\\nb\\na\\nc\\nd\\nFIGURE 6.11\\nHue and saturation \\nin the HSI color \\nmodel. The dot is \\nany color point. \\nThe angle from the \\nred axis gives the \\nhue. The length of \\nthe vector is the \\nsaturation. The \\nintensity of all col-\\nors in any of these \\nplanes is given by \\nthe position of the \\nplane on the verti-\\ncal intensity axis.\\nangle this vector makes with the red axis. Therefore, it is not unusual to see the HSI \\nplanes deﬁned in terms of the hexagon just discussed, a triangle, or even a circle, as \\nFigs. 6.11(c) and (d) show. The shape chosen does not matter because any one of \\nthese shapes can be warped into one of the other two by a geometric transformation. \\nFigure 6.12 shows the HSI model based on color triangles, and on circles.\\nConverting Colors from RGB to HSI\\nGiven an image in RGB color format, the \\nH\\n component of each RGB pixel is \\nobtained using the equation\\n \\nH\\nG\\nG\\n=\\n≤\\n−>\\n⎧\\n⎨\\n⎩\\nu\\nu\\nif\\nif\\nB\\nB\\n360\\n \\n(6-16)\\nwith\\n†\\n \\nu\\n=\\n−\\n()\\n+−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n()\\n+−\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n⎫\\n⎬\\n⎪\\n⎭\\n⎪\\n−\\ncos\\n/\\n1\\n1\\n2\\n2\\n12\\nRG RB\\nRG RB GB\\n \\n(6-17)\\nThe saturation component is given by\\n \\nS\\nRGB\\nRGB\\n=−\\n++\\n()\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n1\\n3\\nmin , ,\\n \\n(6-18)\\n†\\n  It is good practice to add a small number in the denominator of this expression to avoid dividing by 0 when\\nRGB\\n==\\n,\\n in which case \\nu\\n will be 90°. Note that when all RGB components are equal, Eq. (6-18) gives \\nS\\n=\\n0. \\nIn addition,\\n the conversion from HSI back to RGB in Eqs. (6-20) through (6-30) will give \\nRGBI\\n==\\n=\\n,\\n as \\nexpected,\\n because, when \\nRGB\\n==\\n, we are dealing with a grayscale image.\\nComputations from \\nRGB to HSI and back \\nare carried out on a \\npixel-by-pixel basis. We \\nomitted the depen-\\ndence of the conversion \\nequations on (\\nx\\n, \\ny\\n) for \\nnotational clarity. \\nDIP4E_GLOBAL_Print_Ready.indb   413\\n6/16/2017   2:08:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 414}),\n",
       " Document(page_content='414\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nYellow\\nRed\\nBlue\\nCyan\\nWhite\\nBlack\\nS\\nH\\nI\\n/H11005 \\n0.75\\nI\\n/H11005 \\n0.5\\nI\\nMagenta\\nGreen\\nGreen\\nYellow\\nRed\\nMagenta\\nBlue\\nCyan\\nWhite\\nBlack\\nS\\nH\\nI\\n/H11005 \\n0.75\\nI\\n/H11005 \\n0.5\\nI\\nb\\na\\nFIGURE 6.12\\nThe HSI color \\nmodel based on \\n(a) triangular, and \\n(b) circular color \\nplanes. The  \\ntriangles and \\ncircles are  \\nperpendicular to \\nthe vertical  \\nintensity axis.\\nDIP4E_GLOBAL_Print_Ready.indb   414\\n6/16/2017   2:08:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 415}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n415\\nFinally, the intensity component is obtained from the equation\\n \\nIR G B\\n=+ +\\n()\\n1\\n3\\n \\n(6-19)\\nThese equations assume that the RGB values have been normalized to the range \\n[, ] ,\\n01\\n and that angle \\nu\\n is measured with respect to the red axis of the HSI space, as \\nin F\\nig. 6.11. Hue can be normalized to the range \\n[, ]\\n01\\n by dividing by 360° all values \\nresulting from Eq.\\n (6-16). The other two HSI components already are in this range if \\nthe given RGB values are in the interval \\n[, ] .\\n01\\nThe results in Eqs. (6-16) through (6-19) can be derived from the geometry in \\nF\\nigs. 6.10 and 6.11. The derivation is tedious and would not add signiﬁcantly to the \\npresent discussion. You can ﬁnd the proof for these equations (and for the equations \\nthat follow for HSI to RGB conversion) in the \\nTutorials\\n section of the book website.\\nConverting Colors from HSI to RGB\\nGiven values of HSI in the interval \\n[, ] ,\\n01\\n we now want to find the corresponding \\nRGB values in the same range\\n. The applicable equations depend on the values of \\nH\\n. \\nThere are three sectors of interest, corresponding to the 120° intervals in the separa-\\ntion of primaries (see Fig. 6.11). We begin by multiplying \\nH\\n by 360°, which returns \\nthe hue to its original range of \\n[, ] .\\n0\\n360\\n°°\\nRG sector\\n 0 120\\n°≤\\n< °\\n()\\nH\\n:\\n When \\nH\\n is in this sector\\n, the RGB components are given \\nby the equations\\n \\nBI S\\n=−\\n()\\n1  \\n(6-20)\\n \\nRI\\nSH\\nH\\n=+\\n°−\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n60\\ncos\\ncos\\n \\n(6-21)\\nand\\n \\nGI R B\\n=−\\n+\\n()\\n3\\n \\n(6-22)\\nGB sector\\n 120 240\\n°≤\\n< °\\n()\\nH\\n:\\n If the given value of \\nH\\n is in this sector\\n, we first sub-\\ntract 120° from it:\\n \\nHH\\n=−\\n°\\n120  \\n(6-23)\\nThen, the RGB components are\\n \\nRI S\\n=−\\n()\\n1  \\n(6-24)\\n \\nGI\\nSH\\nH\\n=+\\n°−\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n60\\ncos\\ncos\\n \\n(6-25)\\nDIP4E_GLOBAL_Print_Ready.indb   415\\n6/16/2017   2:08:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 416}),\n",
       " Document(page_content='416\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nand\\n \\nBIR G\\n=−\\n+\\n()\\n3\\n \\n(6-26)\\nBR sector\\n 240 360\\n°≤\\n≤ °\\n()\\nH\\n: Finally, if \\nH\\n is in this range\\n, we subtract \\n240\\n°\\n from it:\\n \\nHH\\n=−\\n°\\n240  \\n(6-27)\\nThen, the RGB components are\\n \\nGI S\\n=−\\n()\\n1  \\n(6-28)\\n \\nBI\\nSH\\nH\\n=+\\n°−\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n60\\ncos\\ncos\\n \\n(6-29)\\nand\\n \\nRI G B\\n=−\\n+\\n()\\n3\\n \\n(6-30)\\nWe discuss several uses of these equations in the following sections.\\nEXAMPLE 6.2 :  The HSI values corresponding to the image of the RGB color cube.\\nFigure 6.13 shows the hue, saturation, and intensity images for the RGB values in Fig. 6.8. Figure 6.13(a) \\nis the hue image. Its most distinguishing feature is the discontinuity in value along a 45° line in the front \\n(red) plane of the cube. To understand the reason for this discontinuity, refer to Fig. 6.8, draw a line from \\nthe red to the white vertices of the cube, and select a point in the middle of this line. Starting at that point, \\ndraw a path to the right, following the cube around until you return to the starting point. The major \\ncolors encountered in this path are yellow, green, cyan, blue, magenta, and back to red. According to \\nFig. 6.11, the values of hue along this path should increase from 0° to 360° (i.e., from the lowest to highest \\nb a\\nc\\nFIGURE 6.13\\n HSI components of the image in Fig. 6.8: (a) hue, (b) saturation, and (c) intensity images.\\nDIP4E_GLOBAL_Print_Ready.indb   416\\n6/16/2017   2:08:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 417}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n417\\npossible values of hue). This is precisely what Fig. 6.13(a) shows, because the lowest value is represented \\nas black and the highest value as white in the grayscale. In fact, the hue image was originally normalized \\nto the range [0, 1] and then scaled to 8 bits; that is, we converted it to the range [0, 255], for display.\\nThe saturation image in Fig. 6.13(b) shows progressively darker values toward the white vertex of the \\nRGB cube, indicating that colors become less and less saturated as they approach white. Finally, every \\npixel in the intensity image shown in Fig. 6.13(c) is the average of the RGB values at the corresponding \\npixel in Fig. 6.8.\\nManipulating HSI Component Images\\nIn the following discussion, we take a look at some simple techniques for manipulating \\nHSI component images. This will help you develop familiarity with these comonents, \\nand deepen your understanding of the HSI color model. Figure 6.14(a) shows an \\nimage composed of the primary and secondary RGB colors. Figures 6.14(b) through \\n(d) show the H, S, and I components of this image, generated using Eqs. (6-16) through \\n(6-19). Recall from the discussion earlier in this section that the gray-level values in \\nFig. 6.14(b) correspond to angles; thus, for example, because red corresponds to 0°, \\nthe red region in Fig. 6.14(a) is mapped to a black region in the hue image. Similarly, \\nthe gray levels in Fig. 6.14(c) correspond to saturation (they were scaled to [0, 255] for \\ndisplay), and the gray levels in Fig. 6.14(d) are average intensities.\\nTo change the individual color of any region in the RGB image, we change the \\nvalues of the corresponding region in the hue image of Fig. 6.14(b). Then we convert \\nb a\\nd c\\nFIGURE 6.14\\n(a) RGB image \\nand the  \\ncomponents of \\nits corresponding \\nHSI image:  \\n(b) hue,  \\n(c) saturation, and \\n(d) intensity.\\nDIP4E_GLOBAL_Print_Ready.indb   417\\n6/16/2017   2:08:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 418}),\n",
       " Document(page_content='418\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nthe new H image, along with the unchanged S and I images, back to RGB using the \\nprocedure explained in Eqs. (6-20) through (6-30). To change the saturation (purity) \\nof the color in any region, we follow the same procedure, except that we make the \\nchanges in the saturation image in HSI space. Similar comments apply to changing \\nthe average intensity of any region. Of course, these changes can be made simulta-\\nneously. For example, the image in Fig. 6.15(a) was obtained by changing to 0 the \\npixels corresponding to the blue and green regions in Fig. 6.14(b). In Fig. 6.15(b), \\nwe reduced by half the saturation of the cyan region in component image S from \\nFig. 6.14(c). In Fig. 6.15(c), we reduced by half the intensity of the central white \\nregion in the intensity image of Fig. 6.14(d). The result of converting this modiﬁed \\nHSI image back to RGB is shown in Fig. 6.15(d). As expected, we see in this ﬁgure \\nthat the outer portions of all circles are now red; the purity of the cyan region was \\ndiminished, and the central region became gray rather than white. Although these \\nresults are simple, they clearly illustrate the power of the HSI color model in allow-\\ning independent control over hue, saturation, and intensity. These are quantities with \\nwhich humans are quite familiar when describing colors.\\nA DEVICE INDEPENDENT COLOR MODEL\\nAs noted earlier, humans see a broad spectrum of colors and color shades. However, \\ncolor perception differs between individuals. Not only that, but color across devices \\nsuch as monitors and printers can vary significantly unless these devices are prop-\\nerly calibrated. \\nb a\\nd c\\n \\nFIGURE 6.15\\n(a)-(c) Modiﬁed \\nHSI component \\nimages.  \\n(d) Resulting RGB \\nimage. (See Fig. \\n6.14 for the original \\nHSI images.)\\nDIP4E_GLOBAL_Print_Ready.indb   418\\n6/16/2017   2:08:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 419}),\n",
       " Document(page_content='6.2\\n  \\nColor Models\\n    \\n419\\nColor transformations can be performed on most desktop computers. In conjunc-\\ntion with digital cameras, ﬂatbed scanners, and ink-jet printers, they turn a personal \\ncomputer into a \\ndigital darkroom\\n. Also, commercial devices exist that use a combi-\\nnation of spectrometer measurements and software to develop color proﬁles that \\ncan then be loaded on monitors and printers to calibrate their color responses. \\nThe effectiveness of the transformations examined in this section is judged ulti-\\nmately in print. Because these transformations are developed, reﬁned, and evaluated \\non monitors, it is necessary to maintain a high degree of color consistency between \\nthe monitors used and the eventual output devices. This is best accomplished with \\na device-independent color model that relates the color gamuts (see Section 6.1) \\nof the monitors and output devices, as well as any other devices being used, to one \\nanother. The success of this approach depends on the quality of the color proﬁles \\nused to map each device to the model, as well as the model itself. The model of \\nchoice for many color management systems (CMS) is the CIE \\nLab\\n**\\n*\\n model, also \\ncalled CIELAB (CIE [1978],\\n Robertson [1977]). \\nThe \\nLab\\n**\\n*\\n color components are given by the following equations:\\n \\nLh\\nY\\nY\\nW\\n*\\n=⋅\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n−\\n116\\n16  \\n(6-31)\\n \\nah\\nX\\nX\\nh\\nY\\nY\\nWW\\n*\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n−\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n500\\n \\n(6-32)\\nand\\n \\nbh\\nY\\nY\\nh\\nZ\\nZ\\nWW\\n*\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n−\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n200\\n \\n(6-33)\\nwhere\\n \\nhq\\nqq\\nqq\\n()\\n=\\n>\\n+≤\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n3\\n0 008856\\n7 787 16 116 0 008856\\n.\\n./ .\\n \\n(6-34)\\nand \\nXY\\nWW\\n,,\\n and \\nZ\\nW\\n are reference white tristimulus values—typically the white \\nof a perfectly reflecting diffuser under CIE standard D65 illumination (defined by \\nx\\n=\\n0\\n3127 .\\n and \\ny\\n=\\n0\\n3290 .\\n in the CIE chromaticity diagram of Fig. 6.5). The \\nLab\\n**\\n*\\n \\ncolor space is \\ncolorimetric\\n (i.e\\n., colors perceived as matching are encoded identically), \\nperceptually uniform\\n (i.e., color differences among various hues are perceived uni-\\nformly—see the classic paper by MacAdams [1942]), and \\ndevice independent\\n. While \\nLab\\n**\\n*\\n colors are not directly displayable (conversion to another color space is \\nrequired),\\n the \\nLab\\n**\\n*\\n gamut encompasses the entire visible spectrum and can \\nrepresent accurately the colors of any display\\n, print, or input device. Like the HSI \\nsystem, the \\nLab\\n**\\n*\\n system is an excellent decoupler of intensity (represented by \\nlightness \\nL\\n*)\\n and color (represented by \\na\\n*\\n for red minus green and \\nb\\n*\\n for green \\nminus blue),\\n making it useful in both image manipulation (tone and contrast edit-\\ning) and image compression applications. Studies indicate that the degree to which \\nDIP4E_GLOBAL_Print_Ready.indb   419\\n6/16/2017   2:08:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 420}),\n",
       " Document(page_content='420\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nthe lightness information is separated from the color information in the \\nLab\\n**\\n*\\n \\nsystem is greater than in any other color system (see Kasson and Plouffe [1972]).\\nT\\nhe principal benefit of calibrated imaging systems is that they allow tonal and color \\nimbalances to be corrected interactively and independently—that is, in two sequen-\\ntial operations. Before color irregularities, like over- and under-saturated colors, are \\nresolved, problems involving the image’s tonal range are corrected. The tonal range \\nof an image, also called its \\nkey type\\n, refers to its general distribution of color intensi-\\nties. Most of the information in high-key images is concentrated at high (or light) \\nintensities; the colors of low-key images are located predominantly at low intensi-\\nties; middle-key images lie in between. As in the monochrome case, it is often desir-\\nable to distribute the intensities of a color image equally between the highlights and \\nthe shadows.  In Section 6.4, we give  examples showing a variety of color transfor-\\nmations for the correction of tonal and color imbalances.\\n6.3 PSEUDOCOLOR IMAGE PROCESSING \\nPseudocolor\\n (sometimes called \\nfalse color\\n) image processing consists of assigning \\ncolors to gray values based on a specified criterion. The term pseudo or false color is \\nused to differentiate the process of assigning colors to achromatic images from the \\nprocesses associated with true color images, a topic discussed starting in Section 6.4. \\nThe principal use of pseudocolor is for human visualization and interpretation of \\ngrayscale events in an image or sequence of images. As noted at the beginning of this \\nchapter, one of the principal motivations for using color is the fact that humans can \\ndiscern thousands of color shades and intensities, compared to less than two dozen \\nshades of gray.\\nINTENSITY SLICING AND COLOR CODING\\nThe techniques of \\nintensity\\n (sometimes called \\ndensity\\n) \\nslicing\\n and color coding are \\nthe simplest and earliest examples of pseudocolor processing of digital images. If an \\nimage is interpreted as a 3-D function [see Fig. 2.18(a)], the method can be viewed \\nas one of placing planes parallel to the coordinate plane of the image; each plane \\nthen “slices” the function in the area of intersection. Figure 6.16 shows an example \\nof using a plane at \\nfx y l\\ni\\n,\\n()\\n=\\n to slice the image intensity function into two levels.\\nIf a different color is assigned to each side of the plane in F\\nig. 6.16, any pixel \\nwhose intensity level is above the plane will be coded with one color, and any pixel \\nbelow the plane will be coded with the other. Levels that lie on the plane itself may \\nbe arbitrarily assigned one of the two colors, or they could be given a third color to \\nhighlight all the pixels at that level. The result is a two- (or three-) color image whose \\nrelative appearance can be controlled by moving the slicing plane up and down the \\nintensity axis.\\nIn general, the technique for multiple colors may be summarized as follows. Let \\n[, ]\\n01\\nL\\n−\\n represent the grayscale, let level \\nl\\n0\\n represent black \\n[(,) ] ,\\nfx\\ny\\n=\\n0\\n and level \\nl\\nL\\n−\\n1\\n represent white \\n[(,) ] .\\nfx\\ny L\\n=−\\n1\\n Suppose that \\nP\\n planes perpendicular to the \\nintensity axis are deﬁned at levels \\nll l\\nP\\n12\\n,, , .\\n…\\n Then, assuming that \\n01\\n<<\\n−\\nPL\\n, \\nthe \\nP\\n planes partition the grayscale into \\nP\\n+\\n1\\n intervals, \\nII I\\nP\\n12 1\\n,,, .\\n…\\n+\\n Intensity to \\ncolor assignments at each pixel location \\n(,)\\nxy\\n are made according to the equation\\n6.3\\nDIP4E_GLOBAL_Print_Ready.indb   420\\n6/16/2017   2:08:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 421}),\n",
       " Document(page_content='6.3\\n  \\nPseudocolor Image Processing\\n    \\n421\\nif  , let  \\nfx y I fx y c\\nkk\\n(,)\\n,\\n∈\\n()\\n=\\n \\n(6-35)\\nwhere \\nc\\nk\\n is the color associated with the \\nk\\nth intensity interval \\nI\\nk\\n,\\n defined by the \\nplanes at \\nlk\\n=−\\n1 and \\nlk\\n=\\n.\\nFigure 6.16 is not the only way to visualize the method just described. Figure 6.17 \\nshows an equivalent approach.\\n According to the mapping in this ﬁgure, any image \\nintensity below level \\nl\\ni\\n is assigned one color, and any level above is assigned another. \\nWhen more partitioning levels are used, the mapping function takes on a staircase \\nform.\\nEXAMPLE 6.3:  Intensity slicing and color coding.\\nA simple but practical use of intensity slicing is shown in Fig. 6.18. Figure 6.18(a) is a grayscale image of \\nthe Picker Thyroid Phantom (a radiation test pattern), and Fig. 6.18(b) is the result of intensity slicing \\nthis image into eight colors. Regions that appear of constant intensity in the grayscale image are actually \\nquite variable, as shown by the various colors in the sliced image. For instance, the left lobe is a dull gray \\nin the grayscale image, and picking out variations in intensity is difﬁcult. By contrast, the color image \\nSlicing plane\\n(Black) 0\\n(White) \\nL\\n \\n/H11002\\n 1\\nIntensity\\nf\\n(\\nx\\n, \\ny\\n)\\nx\\ny\\nl\\ni\\nFIGURE 6.16\\nGraphical  \\ninterpretation of \\nthe intensity- \\nslicing technique.\\nFIGURE 6.17\\nAn alternative \\nrepresentation of \\nthe intensity- \\nslicing technique.\\nIntensity levels\\nColor\\nc\\n1\\nc\\n2\\nl\\ni\\nL\\n/H11002\\n1\\n0\\nDIP4E_GLOBAL_Print_Ready.indb   421\\n6/16/2017   2:08:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 422}),\n",
       " Document(page_content='422\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nclearly shows eight different regions of constant intensity, one for each of the colors used. By varying the \\nnumber of colors and the span of the intensity intervals, one can quickly determine the characteristics \\nof intensity variations in a grayscale image. This is particularly true in situations such as the one shown \\nhere, in which the object of interest has uniform texture with intensity variations that are difﬁcult to \\nanalyze visually. This example also illustrates the comments made in Section 6.1 about the eye’s superior \\ncapability for detecting different color shades.\\nIn the preceding simple example, the grayscale was divided into intervals and a different color was \\nassigned to each, with no regard for the meaning of the gray levels in the image. Interest in that case was \\nsimply to view the different gray levels constituting the image. Intensity slicing assumes a much more \\nmeaningful and useful role when subdivision of the grayscale is based on physical characteristics of the \\nimage. For instance, Fig. 6.19(a) shows an X-ray image of a weld (the broad, horizontal dark region) \\ncontaining several cracks and porosities (the bright streaks running horizontally through the middle of \\nthe image). When there is a porosity or crack in a weld, the full strength of the X-rays going through the \\nobject saturates the imaging sensor on the other side of the object. Thus, intensity values of 255 in an \\n8-bit image coming from such a system automatically imply a problem with the weld. If human visual \\nanalysis is used to inspect welds (still a common procedure today), a simple color coding that assigns \\nb a\\nFIGURE 6.19\\n(a) X-ray image \\nof a weld.  \\n(b) Result of color \\ncoding. (Original \\nimage courtesy of \\nX-TEK Systems, \\nLtd.)\\nb a\\nFIGURE 6.18\\n(a) Grayscale \\nimage of the  \\nPicker Thyroid \\nPhantom.  \\n(b) Result of  \\nintensity slicing \\nusing eight colors.  \\n(Courtesy of Dr. \\nJ. L. Blankenship, \\nOak Ridge  \\nNational  \\nLaboratory.)\\nDIP4E_GLOBAL_Print_Ready.indb   422\\n6/16/2017   2:08:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 423}),\n",
       " Document(page_content='6.3\\n  \\nPseudocolor Image Processing\\n    \\n423\\none color to level 255 and another to all other intensity levels can simplify the inspector’s job consider-\\nably. Figure 6.19(b) shows the result. No explanation is required to arrive at the conclusion that human \\nerror rates would be lower if images were displayed in the form of Fig. 6.19(b), instead of the form in \\nFig. 6.19(a). In other words, if an intensity value, or range of values, one is looking for is known, intensity \\nslicing is a simple but powerful aid in visualization, especially if numerous images have to be inspected \\non a routine basis. \\nEXAMPLE 6.4 :  Use of color to highlight rainfall levels.\\nMeasurement of rainfall levels, especially in the tropical regions of the Earth, is of interest in diverse \\napplications dealing with the environment. Accurate measurements using ground-based sensors are \\ndifﬁcult and expensive to acquire, and total rainfall ﬁgures are even more difﬁcult to obtain because a \\nsigniﬁcant portion of precipitation occurs over the ocean. One approach for obtaining rainfall ﬁgures \\nremotely is to use satellites. The TRMM (Tropical Rainfall Measuring Mission) satellite utilizes, among \\nothers, three sensors specially designed to detect rain: a precipitation radar, a microwave imager, and a \\nvisible and infrared scanner (see Sections 1.3 and 2.3 regarding image sensing modalities).\\nThe results from the various rain sensors are processed, resulting in estimates of average rainfall \\nover a given time period in the area monitored by the sensors. From these estimates, it is not difﬁcult to \\ngenerate grayscale images whose intensity values correspond directly to rainfall, with each pixel repre-\\nsenting a physical land area whose size depends on the resolution of the sensors. Such an intensity image \\nis shown in Fig. 6.20(a), where the area monitored by the satellite is the horizontal band highlighted in \\nthe middle of the picture (these are tropical regions). In this particular example, the rainfall values are \\nmonthly averages (in inches) over a three-year period.\\nVisual examination of this picture for rainfall patterns is difﬁcult and prone to error. However, sup-\\npose that we code intensity levels from 0 to 255 using the colors shown in Fig. 6.20(b). In this mode of \\nintensity slicing, each slice is one of the colors in the color band. Values toward the blues signify low val-\\nues of rainfall, with the opposite being true for red. Note that the scale tops out at pure red for values of \\nrainfall greater than 20 inches. Figure 6.20(c) shows the result of color coding the grayscale image with \\nthe color map just discussed. The results are much easier to interpret, as shown in this ﬁgure and in the \\nzoomed area of Fig. 6.20(d). In addition to providing global coverage, this type of data allows meteorolo-\\ngists to calibrate ground-based rain monitoring systems with greater precision than ever before.\\nINTENSITY TO COLOR TRANSFORMATIONS\\nOther types of transformations are more general, and thus are capable of achieving \\na wider range of pseudocolor enhancement results than the simple slicing technique \\ndiscussed in the preceding section. Figure 6.21 shows an approach that is particularly \\nattractive. Basically, the idea underlying this approach is to perform three indepen-\\ndent transformations on the intensity of input pixels. The three results are then fed \\nseparately into the red, green, and blue channels of a color monitor. This method \\nproduces a composite image whose color content is modulated by the nature of the \\ntransformation functions. \\nThe method for intensity slicing discussed in the previous section is a special case \\nof the technique just described. There, piecewise linear functions of the intensity \\nlevels (see Fig. 6.17) are used to generate colors. On the other hand, the method \\nDIP4E_GLOBAL_Print_Ready.indb   423\\n6/16/2017   2:08:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 424}),\n",
       " Document(page_content='424\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nRed\\ntransformation\\nGreen\\ntransformation\\nBlue\\ntransformation\\nf\\nR\\n(\\nx\\n,\\ny\\n)\\nf\\nG\\n(\\nx\\n,\\ny\\n)\\nf\\nB\\n(\\nx\\n,\\ny\\n)\\nf\\n(\\nx\\n,\\ny\\n)\\nFIGURE 6.21\\nFunctional block \\ndiagram for \\npseudocolor image \\nprocessing. Images \\nf\\nR\\n, \\nf\\nG\\n,and \\nf\\nB\\n are \\nfed into the  \\ncorresponding red, \\ngreen, and blue \\ninputs of an RGB \\ncolor monitor.\\nb a\\nd c\\nFIGURE 6.20\\n (a) Grayscale image in which intensity (in the horizontal band shown) corresponds to average monthly \\nrainfall. (b) Colors assigned to intensity values. (c) Color-coded image. (d) Zoom of the South American region. \\n(Courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   424\\n6/16/2017   2:08:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 425}),\n",
       " Document(page_content='6.3\\n  \\nPseudocolor Image Processing\\n    \\n425\\ndiscussed in this section can be based on smooth, nonlinear functions, which gives \\nthe technique considerable ﬂexibility.\\nEXAMPLE 6.5 :  Using pseudocolor to highlight explosives in X-ray images.\\nFigure 6.22(a) shows two monochrome images of luggage obtained from an airport X-ray scanning sys-\\ntem. The image on the left contains ordinary articles. The image on the right contains the same articles, \\nas well as a block of simulated plastic explosives. The purpose of this example is to illustrate the use of \\nintensity to color transformations to facilitate detection of the explosives.\\nFigure 6.23 shows the transformation functions used. These sinusoidal functions contain regions \\nof relatively constant value around the peaks as well as regions that change rapidly near the valleys. \\nChanging the phase and frequency of each sinusoid can emphasize (in color) ranges in the grayscale. For \\ninstance, if all three transformations have the same phase and frequency, the output will be a grayscale \\nimage. A small change in the phase between the three transformations produces little change in pixels \\nwhose intensities correspond to peaks in the sinusoids, especially if the sinusoids have broad proﬁles \\n(low frequencies). Pixels with intensity values in the steep section of the sinusoids are assigned a much \\nstronger color content as a result of signiﬁcant differences between the amplitudes of the three sinu-\\nsoids caused by the phase displacement between them.\\nThe image in Fig. 6.22(b) was obtained using the transformation functions in Fig. 6.23(a), which shows \\nthe gray-level bands corresponding to the explosive, garment bag, and background, respectively. Note \\nthat the explosive and background have quite different intensity levels, but they were both coded with \\napproximately the same color as a result of the periodicity of the sine waves. The image in Fig. 6.22(c) \\nwas obtained with the transformation functions in Fig. 6.23(b). In this case, the explosives and garment \\nbag intensity bands were mapped by similar transformations, and thus received essentially the same \\nb\\na\\nc\\nFIGURE 6.22\\nPseudocolor \\nenhancement by \\nusing the gray \\nlevel to color \\ntransformations in \\nFig. 6.23. (Original \\nimage courtesy of \\nDr. Mike Hurwitz, \\nWestinghouse.)\\nDIP4E_GLOBAL_Print_Ready.indb   425\\n6/16/2017   2:08:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 426}),\n",
       " Document(page_content='426\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\ncolor assignments. Note that this mapping allows an observer to “see” through the explosives. The back-\\nground mappings were about the same as those used for Fig. 6.22(b), producing almost identical color \\nassignments for the two pseudocolor images.\\nThe approach in Fig. 6.21 is based on a single grayscale image. Often, it is of \\ninterest to combine several grayscale images into a single color composite, as illus-\\ntrated in Fig. 6.24. A frequent use of this approach is in multispectral image process-\\ning, where different sensors produce individual grayscale images, each in a different \\nspectral band (see Example 6.6 below). The types of additional processing shown in \\nFig. 6.24 can be techniques such as color balancing and spatial ﬁltering, as discussed \\nlater in this chapter. When coupled with background knowledge about the physical \\ncharacteristics of each band, color-coding in the manner just explained is a powerful \\naid for human visual analysis of complex multispectral images.\\nEXAMPLE 6.6 :  Color coding of multispectral images.\\nFigures 6.25(a) through (d) show four satellite images of the Washington, D.C., area, including part of \\nthe Potomac River. The ﬁrst three images are in the visible red (R), green (G), and blue (B) bands, and \\nTransformation\\nT\\n1\\nAdditional\\nprocessing\\nTransformation\\nT\\n2\\nTransformation\\nT\\nK\\nf\\nK\\n(\\nx\\n,\\ny\\n)\\nf\\n1\\n(\\nx\\n,\\ny\\n)\\nf\\n2\\n(\\nx\\n,\\ny\\n)\\ng\\nK\\n(\\nx\\n,\\ny\\n)\\ng\\n1\\n(\\nx\\n,\\ny\\n)\\ng\\n2\\n(\\nx\\n,\\ny\\n)\\nh\\nR\\n(\\nx\\n,\\ny\\n)\\nh\\nG\\n(\\nx\\n,\\ny\\n)\\nh\\nB\\n(\\nx\\n,\\ny\\n)\\nFIGURE 6.24\\nA pseudocolor \\ncoding approach \\nusing multiple \\ngrayscale images. \\nThe inputs are \\ngrayscale images. \\nThe outputs are \\nthe three  \\ncomponents of an \\nRGB composite \\nimage.\\nb a\\nFIGURE 6.23\\nTransformation \\nfunctions used to \\nobtain the  \\npseudocolor \\nimages in  \\nFig. 6.22.\\nExplosive\\nBackground\\nBag\\nIntensity\\nRed\\nGreen\\nBlue\\nExplosive\\nBackground\\nBag\\nIntensity \\nColor\\nDIP4E_GLOBAL_Print_Ready.indb   426\\n6/16/2017   2:08:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 427}),\n",
       " Document(page_content='6.3\\n  \\nPseudocolor Image Processing\\n    \\n427\\nthe fourth is in the near infrared (IR) band (see Table 1.1 and Fig. 1.10). The latter band is responsive \\nto the biomass content of a scene, and we want to use this fact to create a composite RGB color image \\nin which vegetation is emphasized and the other components of the scene are displayed in more muted \\ntones. \\nFigure 6.25(e) is an RGB composite obtained by replacing the red image by infrared. As you see, veg-\\netation shows as a bright red, and the other components of the scene, which had a weaker response in \\nthe near-infrared band, show in pale shades of blue-green. Figure 6.25(f) is a similar image, but with the \\ngreen replaced by infrared. Here, vegetation shows in a bright green color, and the other components of \\nthe scene show in purplish color shades, indicating that their major components are in the red and blue \\nbands. Although the last two images do not introduce any new physical information, these images are \\nmuch easier to interpret visually once it is known that the dominant component of the images are pixels \\nof areas heavily populated by vegetation.\\nThe type of processing just illustrated uses the physical characteristics of a single band in a multi-\\nspectral image to emphasize areas of interest. The same approach can help visualize events of interest \\nb a\\nc\\ne\\nd\\nf\\n \\nFIGURE 6.25\\n (a)–(d) Red (R), green (G), blue (B), and near-infrared (IR) components of a LANDSAT multispectral \\nimage of the Washington, D.C. area. (e) RGB color composite image obtained using the IR, G, and B component \\nimages. (f) RGB color composite image obtained using the R, IR, and B component images. (Original multispectral \\nimages courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   427\\n6/16/2017   2:08:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 428}),\n",
       " Document(page_content='428\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nin complex images in which the events are beyond human visual sensing capabilities. Figure 6.26 is an \\nexcellent illustration of this. These are images of the Jupiter moon Io, shown in pseudocolor by combin-\\ning several of the sensor images from the Galileo spacecraft, some of which are in spectral regions not \\nvisible to the eye. However, by understanding the physical and chemical processes likely to affect sensor \\nresponse, it is possible to combine the sensed images into a meaningful pseudocolor map. One way to \\ncombine the sensed image data is by how they show either differences in surface chemical composition \\nor changes in the way the surface reﬂects sunlight. For example, in the pseudocolor image in Fig. 6.26(b), \\nbright red depicts material newly ejected from an active volcano on Io, and the surrounding yellow \\nmaterials are older sulfur deposits. This image conveys these characteristics much more readily than \\nwould be possible by analyzing the component images individually.\\nb\\na\\nFIGURE 6.26\\n(a) Pseudocolor \\nrendition of  \\nJupiter Moon Io.  \\n(b) A close-up. \\n(Courtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   428\\n6/16/2017   2:08:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 429}),\n",
       " Document(page_content='6.4\\n  \\nBasics of Full-Color Image Processing\\n    \\n429\\n6.4 BASICS OF FULL-COLOR IMAGE PROCESSING  \\nIn this section, we begin the study of processing methods for full-color images. The \\ntechniques developed in the sections that follow are illustrative of how full-color \\nimages are handled for a variety of image processing tasks. Full-color image process-\\ning approaches fall into two major categories. In the first category, we process each \\ngrayscale component image individually, then form a composite color image from \\nthe individually processed components. In the second category, we work with color \\npixels directly. Because full-color images have at least three components, color pix-\\nels are vectors. For example, in the RGB system, each color point can be interpreted \\nas a vector extending from the origin to that point in the RGB coordinate system \\n(see Fig. 6.7).\\nLet \\nc\\n represent an arbitrary vector in RGB color space:\\n \\nc\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nc\\nc\\nc\\nR\\nG\\nB\\nR\\nG\\nB\\n \\n(6-36)\\nThis equation indicates that the components of \\nc\\n are the RGB components of a \\ncolor image at a point.\\n We take into account the fact that the colors of the pixels in \\nan image are a function of spatial coordinates \\n(,)\\nxy\\n by using the notation\\n \\nc\\nxy\\ncx\\ny\\ncx y\\ncx y\\nRx y\\nGx y\\nBx y\\nR\\nG\\nB\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n()\\n=\\n()\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n()\\n()\\n()\\n⎡\\n⎣ ⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n \\n(6-37)\\nFor an image of size \\nMN\\n×\\n,\\n there are \\nMN\\n such vectors\\n, \\nc\\n(,\\n) ,\\nxy\\n for \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n \\nand \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\nEquation (6-37) depicts a vector whose components are \\nspatial\\n variables \\nx\\n and \\ny\\n.\\n This is a frequent source of confusion that can be avoided by focusing on the fact \\nthat our interest lies in spatial processes. That is, we are interested in image process-\\ning techniques formulated in \\nx\\n and \\ny\\n. The fact that the pixels are now color pixels \\nintroduces a factor that, in its easiest formulation, allows us to process a color image \\nby processing each of its component images separately, using standard grayscale \\nimage processing methods. However, the results of individual color component pro-\\ncessing are not always equivalent to direct processing in color vector space, in which \\ncase we must use approaches for processing the elements of color points directly. \\nWhen these points have more than two components, we call them \\nvoxels\\n. We use the \\nterms vectors, points, and voxels interchangeably when the meaning is clear that we \\nare referring to images composed of more than one 2-D image. \\nIn order for per-component-image and vector-based processing to be equivalent, \\ntwo conditions have to be satisﬁed: ﬁrst, the process has to be applicable to both \\nvectors and scalars; second, the operation on each component of a vector (i.e., each \\nvoxel) must be independent of the other components. As an illustration, Fig. 6.27 \\nshows spatial neighborhood processing of grayscale and full-color images. Suppose \\n6.4\\nAlthough an RGB image \\nis composed of three \\ngrayscale component \\nimages, pixels in all three \\nimages are registered \\nspatially. That is, a \\nsingle\\n \\npair of spatial  \\ncoordinates, (\\nx\\n,  \\ny\\n), \\naddresses the \\nsame\\n \\npixel location in all three \\nimages, as illustrated in \\nFig. 6.27(b) below.\\nDIP4E_GLOBAL_Print_Ready.indb   429\\n6/16/2017   2:08:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 430}),\n",
       " Document(page_content='430\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nthat the process is neighborhood averaging. In Fig. 6.27(a), averaging would be done \\nby summing the intensities of all the pixels in the 2-D neighborhood, then dividing \\nthe result by the total number of pixels in the neighborhood. In Fig. 6.27(b), averag-\\ning would be done by summing all the voxels in the 3-D neighborhood, then divid-\\ning the result by the total number of voxels in the neighborhood. Each of the three \\ncomponent of the average voxel is the sum of the pixels in the single image neigh-\\nborhood centered on that location. But the same result would be obtained if the \\naveraging were done on the pixels of each image, independently, and then the sum of \\nthe three values were added for each. Thus, spatial neighborhood averaging can be \\ncarried out on a per-component-image or directly on RGB image voxels. The results \\nwould be the same. In the following sections we develop methods for which the per-\\ncomponent-image approach is suitable, and methods for which it is not.\\n6.5 COLOR TRANSFORMATIONS  \\nThe techniques described in this section, collectively called \\ncolor transformations\\n, \\ndeal with processing the components of a color image within the context of a \\nsingle\\n \\ncolor model, as opposed to color transformations \\nbetween\\n color models, as in Sec-\\ntion 6.2.\\nFORMULATION\\nAs with the intensity transformation techniques of Chapter 3, we model color trans-\\nformations for multispectral images using the general expression\\n \\nsT r i n\\nii i\\n=\\n()\\n=\\n12\\n,, ,\\n…\\n \\n(6-38)\\nwhere \\nn\\n is the total number of component images\\n, \\nr\\ni\\n are the intensity values of the \\ninput component images, \\ns\\ni\\n are the spatially corresponding intensities in the output \\ncomponent images, and \\nT\\ni\\n are a set of \\ntransformation\\n or \\ncolor mapping\\n \\nfunctions \\nthat operate on \\nr\\ni\\n to produce \\ns\\ni\\n.\\n Equation (6-38) is applied individually to all pixels \\nin the input image\\n. For example, in the case of RGB color images, \\nn\\n=\\n3,\\n \\nrrr\\n123\\n,,\\n are \\nthe intensities values at a point in the input components images\\n, and \\nsss\\n123\\n,,\\n are \\n6.5\\n(\\nx\\n,\\ny\\n)\\n(\\nx\\n,\\ny\\n)\\nRGB color image\\nGrayscale image\\nPixel\\nVoxel\\n3-D neighborhood\\n2-D neighborhood\\nb a\\nFIGURE 6.27\\nSpatial  \\nneighborhoods \\nfor grayscale \\nand RGB color \\nimages. Observe \\nin (b) that a \\nsingle\\n \\npair of spatial \\ncoordinates, \\n(,) ,\\nxy\\n \\naddresses the \\nsame spatial  \\nlocation in all \\nthree images\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   430\\n6/16/2017   2:08:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 431}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n431\\nthe corresponding transformed pixels in the output image. The fact that \\ni\\n is also a \\nsubscript on \\nT\\n means that, in principle, we can implement a different transformation \\nfor each input component image. \\nAs an illustration, the ﬁrst row of Fig. 6.28 shows a full color CMYK image of a \\nsimple scene, and the second row shows its four component images, all normalized \\nto the range \\n[, ] .\\n01\\n We see that the strawberries are composed of large amounts of \\nmagenta and yellow because the images corresponding to these two CMYK compo-\\nnents are the brightest.\\n Black is used sparingly and is generally conﬁned to the cof-\\nfee and shadows within the bowl of strawberries. The fourth row shows the equiva-\\nlent RGB images obtained from the CMYK images using Eqs. (6-13)-(6-15). Here \\nwe see that the strawberries contain a large amount of red and very little (although \\nsome) green and blue. From the RGB images, we obtained the CMY images in \\nthe third row using Eq. (6-5). Note that these CMY images are slightly different \\nfrom the CMY images in the row above them. This is because the CMY images \\nin these two systems are different as a result of using K in one of them. The last \\nrow of Fig. 6.28 shows the HSI components, obtained from the RGB images using \\nEqs. (6-16)-(6-19). As expected, the intensity (I) component is a grayscale rendition \\nof the full-color original. The saturation image (S) is as expected also. The strawber-\\nries are relatively pure in color; as a result, they show the highest saturation (least \\ndilution by white light) values of any of the other elements of the image. Finally, \\nwe note some difﬁculty in interpreting the values of the hue (H) component image. \\nThe problem is that (1) there is a discontinuity in the HSI model where 0° and 360° \\nmeet [see Fig. 6.13(a)], and (2) hue is undeﬁned for a saturation of 0 (i.e., for white, \\nblack, and pure grays). The discontinuity of the model is most apparent around the \\nstrawberries, which are depicted in gray level values near both black (0) and white \\n(1). The result is an unexpected mixture of highly contrasting gray levels to represent \\na single color—red.\\nWe can apply Eq. (6-38) to any of the color-space component images in Fig. 6.28. \\nIn theory, any transformation can be performed in any color model. In practice, how-\\never, some operations are better suited to speciﬁc models. For a given transformation, \\nthe effects of converting between representations must be factored into the decision \\nregarding the color space in which to implement it. For example, suppose that we \\nwish to modify the intensity of the full-color image in the ﬁrst row of Fig. 6.28 by a \\nconstant value, \\nk\\n in the range \\n[, ] .\\n01\\n In the HSI color space we need to modify only \\nthe intensity component image:\\n \\nsk r\\n33\\n=\\n \\n(6-39)\\nand we let \\nsr\\n11\\n=\\n and \\nsr\\n22\\n=\\n.\\n In terms of our earlier discussion note that we are using \\ntwo different transformation functions:\\n \\nT\\n1\\n and \\nT\\n2\\n are identity transformations, and \\nT\\n3\\n is a constant transformation. \\nIn the RGB color space we need to modify all three components by the same \\nconstant transformation:\\n \\nsk r i\\nii\\n==\\n123\\n,,\\n \\n(6-40)\\nDIP4E_GLOBAL_Print_Ready.indb   431\\n6/16/2017   2:08:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 432}),\n",
       " Document(page_content='432\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nFull color image\\nRed\\nGreen\\nBlue\\nHue\\nSaturation\\nIntensity\\nCyan\\nMagenta\\nYellow\\nBlack\\nCyan\\nMagenta\\nYellow\\nFIGURE 6.28\\n A full-color image and its various color-space components. (Original image courtesy of MedData Interactive.)\\nDIP4E_GLOBAL_Print_Ready.indb   432\\n6/16/2017   2:08:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 433}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n433\\nThe CMY space requires a similar set of linear transformations (see Problem 6.16):\\n \\nsk r k i\\nii\\n=+ −\\n()\\n=\\n11 2 3\\n,,\\n \\n(6-41)\\nSimilarly, the transformations required to change the intensity of the CMYK image \\nis given by \\n \\ns\\nri\\nkr\\nk i\\ni\\ni\\ni\\n=\\n=\\n+− =\\n⎧\\n⎨\\n⎩\\n123\\n14\\n,,\\n()\\n \\n(6-42)\\nThis equation tells us that to change the intensity of a CMYK image, we only change \\nthe fourth (K) component.\\nF\\nigure 6.29(b) shows the result of applying the transformations in Eqs. (6-39)\\nthrough (6-42) to the full-color image of Fig. 6.28, using \\nk\\n=\\n07\\n..\\n The mapping func-\\ntions themselves are shown graphically in F\\nigs. 6.29(c) through (h). Note that the \\nmapping function for CMYK consist of two parts, as do the functions for HSI; one of \\nthe transformations handles one component, and the other does the rest. Although \\nC,M,Y\\n1\\n0\\n1\\n1\\n/H11002 \\nk\\nR,G,B\\n1\\n0\\n1\\nk\\nI\\n1\\n0\\n1\\nk\\nH,S\\n0\\n1\\n1\\n0\\n1\\nK\\nH,S\\n0\\n1\\nC,M,Y\\nb a\\nc\\ne\\nd\\nf\\ng\\nh\\nFIGURE 6.29\\n Adjusting the intensity of an image using color transformations. (a) Original image. (b) Result of decreas-\\ning its intensity by 30% (i.e., letting \\nk\\n=\\n07\\n.)\\n.\\n (c) The required RGB mapping function. (d)–(e) The required CMYK \\nmapping functions\\n. (f) The required CMY mapping function. (g)–(h) The required HSI mapping functions. (Origi-\\nnal image courtesy of MedData Interactive.)\\nDIP4E_GLOBAL_Print_Ready.indb   433\\n6/16/2017   2:08:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 434}),\n",
       " Document(page_content='434\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nwe used several different transformations, the net result of changing the intensity of \\nthe color by a constant value was the same for all.\\nIt is important to note that each transformation deﬁned in Eqs. (6-39) through \\n(6-42) depends only on one component within its color space. For example, the red \\noutput component, \\ns\\n1\\n,\\n in Eq. (6-40) is independent of the green \\n()\\nr\\n2\\n and blue \\n()\\nr\\n3\\n \\ninputs; it depends only on the red \\n()\\nr\\n1\\n input. Transformations of this type are among \\nthe simplest and most frequently used color processing tools. They can be carried \\nout on a per-color-component basis, as mentioned at the beginning of our discussion. \\nIn the remainder of this section, we will examine several such transformations and \\ndiscuss a case in which the component transformation functions are dependent on \\nall the color components of the input image and, therefore, cannot be done on an \\nindividual color-component basis.\\nCOLOR COMPLEMENTS\\nThe \\ncolor circle\\n (also called the \\ncolor wheel\\n) shown in Fig. 6.30 originated with Sir \\nIsaac Newton, who in the seventeenth century created its first form by joining the \\nends of the color spectrum. The color circle is a visual representation of colors that \\nare arranged according to the chromatic relationship between them. The circle is \\nformed by placing the primary colors equidistant from each other. Then, the sec-\\nondary colors are placed between the primaries, also in an equidistant arrangement. \\nThe net result is that hues directly opposite one another on the color circle are \\ncom-\\nplements\\n. Our interest in complements stems from the fact that they are analogous \\nto the grayscale negatives we studied in Section 3.2. As in the grayscale case, color \\ncomplements are useful for enhancing detail that is embedded in dark regions of \\na color image—particularly when the regions are dominant in size. The following \\nexample illustrates some of these concepts.\\nGreen\\nYellow\\nRed\\nMagenta\\nBlue\\nCyan\\nComplements\\nFIGURE 6.30\\nColor  \\ncomplements on \\nthe color circle.\\nDIP4E_GLOBAL_Print_Ready.indb   434\\n6/16/2017   2:08:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 435}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n435\\nEXAMPLE 6.7 :  Computing color image complements.\\nFigures 6.31(a) and (c) show the full-color image from Fig. 6.28 and its color complement. The RGB \\ntransformations used to compute the complement are plotted in Fig. 6.31(b). They are identical to the \\ngrayscale negative transformation deﬁned in Section 3.2. Note that the complement is reminiscent of \\nconventional photographic color ﬁlm negatives. Reds of the original image are replaced by cyans in the \\ncomplement. When the original image is black, the complement is white, and so on. Each of the hues in \\nthe complement image can be predicted from the original image using the color circle of Fig. 6.30, and \\neach of the RGB component transforms involved in the computation of the complement is a function \\nof only the corresponding input color component.\\nUnlike the intensity transformations of Fig. 6.29, the RGB complement transformation functions \\nused in this example do not have a straightforward HSI equivalent. It is left as an exercise (see Prob-\\nlem 6.19) to show that the saturation component of the complement cannot be computed from the satu-\\nration component of the input image alone. Figure 6.31(d) shows an approximation of the complement \\nusing the hue, saturation, and intensity transformations in Fig. 6.31(b). The saturation component of the \\ninput image is unaltered; it is responsible for the visual differences between Figs. 6.31(c) and (d).\\n0\\n1\\n1\\nI\\n0\\n1\\n1\\nS\\n0\\n1\\n1\\nR,G,B\\n0\\n1\\n1\\nH\\nb a\\nd c\\nFIGURE 6.31\\nColor  \\ncomplement \\ntransformations. \\n(a) Original \\nimage.  \\n(b) Complement \\ntransformation \\nfunctions.  \\n(c) Complement \\nof (a) based on \\nthe RGB mapping \\nfunctions. (d) An \\napproximation of \\nthe RGB  \\ncomplement using \\nHSI  \\ntransformations.\\nDIP4E_GLOBAL_Print_Ready.indb   435\\n6/16/2017   2:08:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 436}),\n",
       " Document(page_content='436\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nCOLOR SLICING\\nHighlighting a specific range of colors in an image is useful for separating objects \\nfrom their surroundings. The basic idea is either to: (1) display the colors of interest \\nso that they stand out from the background; or (2) use the region defined by the \\ncolors as a mask for further processing. The most straightforward approach is to \\nextend the intensity slicing techniques of Section 3.2. However, because a color pixel \\nis an \\nn\\n-dimensional quantity, the resulting color transformation functions are more \\ncomplicated than their grayscale counterparts in Fig. 3.11. In fact, the required trans-\\nformations are more complex than the color component transforms considered thus \\nfar. This is because all practical color-slicing approaches require each pixel’s trans-\\nformed color components to be a function of all \\nn\\n original pixel’s color components.\\nOne of the simplest ways to “slice” a color image is to map the colors outside \\nsome range of interest into a nonprominent neutral color. If the colors of interest \\nare enclosed by a cube (or \\nhypercube\\n for \\nn\\n>\\n3)\\n of width \\nW\\n and centered at a pro-\\ntotypical (e\\n.g., average) color with components \\naa a\\nn\\n12\\n,, , ,\\n…\\n()\\n the necessary set of \\ntransformations are given by\\n \\ns\\nra\\nW\\nr\\nin\\ni\\njj\\njn\\ni\\n=\\n−>\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n=\\n≤≤\\n05\\n2\\n12\\n1\\n.\\n,, ,\\nif\\notherwise\\nany\\n…\\n \\n(6-43)\\nThese transformations highlight the colors around the prototype by forcing all \\nother colors to the midpoint of the reference color space (this is an arbitrarily cho-\\nsen neutral point).\\n For the RGB color space, for example, a suitable neutral point \\nis middle gray or color (0.5, 0.5, 0.5).\\nIf a sphere is used to specify the colors of interest, Eq. (6-43) becomes\\n \\ns\\nra\\nR\\nr\\nin\\ni\\njj\\nj\\nn\\ni\\n=\\n−\\n()\\n>\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n=\\n=\\n∑\\n05\\n12\\n2\\n0\\n2\\n1\\n.\\n,, ,\\nif\\notherwise\\n…\\n \\n(6-44)\\nHere, \\nR\\n0\\n is the radius of the enclosing sphere (or hypersphere for \\nn\\n>\\n3)\\n and \\naa a\\nn\\n12\\n,,,\\n…\\n()\\n are the components of its center (i.e., the prototypical color). Other \\nuseful variations of Eqs. (6-43) and (6-44) include implementing multiple color pro-\\ntotypes and reducing the intensity of the colors outside the region of interest—rath-\\ner than setting them to a neutral constant.\\nEXAMPLE 6.8 :  Color slicing.\\nEquations (6-43) and (6-44) can be used to separate the strawberries in Fig. 6.29(a) from their sepals, cup, \\nbowl, and other background elements. Figures 6.32(a) and (b) show the results of using both transfor-\\nmations. In each case, a prototype red with RGB color coordinate (0.6863, 0.1608, 0.1922) was selected \\nfrom the most prominent strawberry. Parameters \\nW\\n and \\nR\\n0\\n were chosen so that the highlighted region \\nwould not expand to other portions of the image. The actual values used, \\nW\\n=\\n0\\n2549 .\\n and \\nR\\n0\\n0 1765\\n=\\n.,\\n \\nwere determined interactively\\n. Note that the sphere-based transformation of Eq. (6-44) performed \\nslightly better, in the sense that it includes more of the strawberries’ red areas. A sphere of radius 0.1765 \\nDIP4E_GLOBAL_Print_Ready.indb   436\\n6/16/2017   2:08:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 437}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n437\\ndoes not completely enclose a cube of width 0.2549, but it is not small enough to be completely enclosed \\nby the cube either. In Section 6.7, and later in Chapter 10, you will learn more advanced techniques for \\nusing color and other multispectral information to extract objects from their background.\\nTONE AND COLOR CORRECTIONS\\nProblems involving an image’s tonal range need to be corrected before color irregu-\\nlarities, such as over- and under-saturated colors, can be resolved, The tonal range of \\nan image, also called its \\nkey type\\n, refers to its general distribution of color intensities. \\nMost of the information in \\nhigh-key\\n images is concentrated at high (or light) intensi-\\nties; the colors of \\nlow-key\\n images are located predominantly at low intensities; and \\nmiddle-key\\n images lie in between. As in the grayscale case, it is often desirable to \\ndistribute the intensities of a color image equally between the highlights and the \\nshadows. The following examples illustrate a variety of color transformations for the \\ncorrection of tonal and color imbalances.\\nEXAMPLE 6.9 :  Tonal transformations.\\nTransformations for modifying image tones normally are selected interactively. The idea is to adjust \\nexperimentally the image’s brightness and contrast to provide maximum detail over a suitable range of \\nintensities. The colors themselves are not changed. In the RGB and CMY(K) spaces, this means map-\\nping all the color components, except \\nK\\n, with the same transformation function (see Fig. 6.29); in the \\nHSI color space, only the intensity component is modiﬁed, as noted in the previous section.\\nFigure 6.33 shows typical RGB transformations used for correcting three common tonal imbalances—\\nﬂat, light, and dark images. The S-shaped curve in the ﬁrst row of the ﬁgure is ideal for boosting contrast \\nb a\\nFIGURE 6.32\\n Color-slicing transformations that detect (a) reds within an RGB cube of width \\nW\\n=\\n0\\n2549 .\\n centered at (0.6863, 0.1608, 0.1922), and (b) reds within an RGB sphere of radius \\n0.1765 centered at the same point.\\n Pixels outside the cube and sphere were replaced by color \\n(0.5, 0.5, 0.5).\\nDIP4E_GLOBAL_Print_Ready.indb   437\\n6/16/2017   2:08:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 438}),\n",
       " Document(page_content='438\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nR,G,B\\n1\\n01\\nR,G,B\\n1\\n01\\nR,G,B\\n1\\n01\\nFlat\\nLight\\nDark\\nCorrected\\nCorrected\\nCorrected\\nFIGURE 6.33\\n  Tonal corrections for ﬂat, light (high key), and dark (low key) color images. Adjusting the red, green, and \\nblue components equally does not always alter the image hues signiﬁcantly.\\nDIP4E_GLOBAL_Print_Ready.indb   438\\n6/16/2017   2:08:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 439}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n439\\n[see Fig. 3.2(a)]. Its midpoint is anchored so that highlight and shadow areas can be lightened and dark-\\nened, respectively. (The inverse of this curve can be used to correct excessive contrast.) The transforma-\\ntions in the second and third rows of the ﬁgure correct light and dark images, and are reminiscent of \\nthe power-law transformations in Fig. 3.6. Although the color components are discrete, as are the actual \\ntransformation functions, the transformation functions themselves are displayed and manipulated as \\ncontinuous quantities—typically constructed from piecewise linear or higher order (for smoother map-\\npings) polynomials. Note that the keys of the images in Fig. 6.33 are visually evident; they could also be \\ndetermined using the histograms of the images’ color components.\\nEXAMPLE 6.10 :  Color balancing.\\nAny color imbalances are addressed after the tonal characteristics of an image have been corrected. \\nAlthough color imbalances can be determined directly by analyzing a known color in an image with \\na color spectrometer, accurate visual assessments are possible when white areas, where the RGB or \\nCMY(K) components should be equal, are present. As Fig. 6.34 shows, skin tones are excellent subjects \\nfor visual color assessments because humans are highly perceptive of proper skin color. Vivid colors, \\nsuch as bright red objects, are of little value when it comes to visual color assessment.\\nThere are a variety of ways to correct color imbalances. When adjusting the color components of an \\nimage, it is important to realize that every action affects its overall color balance. That is, the perception \\nof one color is affected by its surrounding colors. The color wheel of Fig. 6.30 can be used to predict \\nhow one color component will affect others. Based on the color wheel, for example, the proportion of \\nany color can be increased by decreasing the amount of the opposite (or complementary) color in the \\nimage. Similarly, it can be increased by raising the proportion of the two immediately adjacent colors \\nor decreasing the percentage of the two colors adjacent to the complement. Suppose, for instance, that \\nthere is too much magenta in an RGB image. It can be decreased: (1) by removing both red and blue, or \\n(2) by adding green.\\nFigure 6.34 shows the transformations used to correct simple CMYK output imbalances. Note that \\nthe transformations depicted are the functions required for correcting the images; the inverses of these \\nfunctions were used to generate the associated color imbalances. Together, the images are analogous to \\na color ring-around print of a darkroom environment and are useful as a reference tool for identifying \\ncolor printing problems. Note, for example, that too much red can be due to excessive magenta (per the \\nbottom left image) or too little cyan (as shown in the rightmost image of the second row).\\nHISTOGRAM PROCESSING OF COLOR IMAGES\\nUnlike the interactive enhancement approaches of the previous section, the gray-\\nlevel histogram processing transformations of Section 3.3 can be applied to color \\nimages in an automated way. Recall that histogram equalization automatically \\ndetermines a transformation that seeks to produce an image with a uniform histo-\\ngram of intensity values. We showed in Section 3.3 that histogram processing can be \\nquite successful at handling low-, high-, and middle-key images (for example, see \\nFig. 3.20). As you might suspect, it is generally unwise to histogram equalize the \\ncomponent images of a color image independently. This results in erroneous color. A \\nmore logical approach is to spread the color intensities uniformly, leaving the colors \\nthemselves (e.g., hues) unchanged. The following example shows that the HSI color \\nspace is ideally suited to this type of approach.\\nDIP4E_GLOBAL_Print_Ready.indb   439\\n6/16/2017   2:08:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 440}),\n",
       " Document(page_content='440\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nB\\n1\\n0\\n1\\nHeavy in\\nblack\\nOriginal/Corrected\\nB\\n1\\n0\\n1\\nWeak in\\nblack\\nC\\n1\\n0\\n1\\nHeavy in\\ncyan\\nC\\n1\\n0\\n1\\nWeak in\\ncyan\\nM\\n1\\n0\\n1\\nHeavy in\\nmagenta\\nM\\n1\\n0\\n1\\nWeak in\\nmagenta\\nY\\n1\\n0\\n1\\nHeavy in\\nyellow\\nY\\n1\\n0\\n1\\nWeak in\\nyellow\\nFIGURE 6.34\\n Color balancing a CMYK image.\\nDIP4E_GLOBAL_Print_Ready.indb   440\\n6/16/2017   2:08:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 441}),\n",
       " Document(page_content='6.5\\n  \\nColor Transformations\\n    \\n441\\nEXAMPLE 6.11 :  Histogram equalization in the HSI color space.\\nFigure 6.35(a) shows a color image of a caster stand containing cruets and shakers whose intensity com-\\nponent spans the entire (normalized) range of possible values, [0, 1]. As can be seen in the histogram of \\nits intensity component prior to processing [see Fig. 6.35(b)], the image contains a large number of dark \\ncolors that reduce the median intensity to 0.36. Histogram equalizing the intensity component, without \\naltering the hue and saturation, resulted in the image shown in Fig. 6.35(c). Note that the overall image \\nis signiﬁcantly brighter, and that several moldings and the grain of the wooden table on which the caster \\nis sitting are now visible. Figure 6.35(b) shows the intensity histogram of the new image, as well as the \\nintensity transformation used to equalize the intensity component [see Eq. (3-15)].\\nAlthough intensity equalization did not alter the values of hue and saturation of the image, it did \\nimpact the overall color perception. Note, in particular, the loss of vibrancy in the oil and vinegar in the \\ncruets. Figure 6.35(d) shows the result of partially correcting this by increasing the image’s saturation \\ncomponent, subsequent to histogram equalization, using the transformation in Fig. 6.35(b). This type of \\nH\\n1\\n0\\n1\\nI\\n1\\n0\\n0.36\\n0.5\\n1\\nS\\n1\\n0\\n1\\nHistogram after processing\\n(median\\n/H11005 \\n0.5)\\nHistogram before processing\\n(median\\n/H11005 \\n0.36)\\nb a\\nd c\\nFIGURE 6.35\\nHistogram  \\nequalization  \\n(followed by \\nsaturation  \\nadjustment) in the \\nHSI color space.\\nDIP4E_GLOBAL_Print_Ready.indb   441\\n6/16/2017   2:08:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 442}),\n",
       " Document(page_content='442\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nadjustment is common when working with the intensity component in HSI space because changes in \\nintensity usually affect the relative appearance of colors in an image.\\n6.6 COLOR IMAGE SMOOTHING AND SHARPENING  \\nThe next step beyond transforming each pixel of a color image without regard to its \\nneighbors (as in the previous section) is to modify its value based on the character-\\nistics of the surrounding pixels. In this section, the basics of this type of neighbor-\\nhood processing will be illustrated within the context of color image smoothing and \\nsharpening.\\nCOLOR IMAGE SMOOTHING\\nWith reference to Fig. 6.27(a) and the discussion in Sections 3.4 and 3.5, grayscale \\nimage smoothing can be viewed as a spatial filtering operation in which the coef-\\nficients of the filtering kernel have the same value. As the kernel is slid across the \\nimage to be smoothed, each pixel is replaced by the average of the pixels in the \\nneighborhood encompassed by the kernel. As Fig. 6.27(b) shows, this concept is eas-\\nily extended to the processing of full-color images. The principal difference is that \\ninstead of scalar intensity values, we must deal with component vectors of the form \\ngiven in Eq. (6-37).\\nLet \\nS\\nxy\\n denote the set of coordinates deﬁning a neighborhood centered at \\n(,)\\nxy\\n \\nin an RGB color image\\n. The average of the RGB component vectors in this neigh-\\nborhood is\\n \\nc\\nxy\\nK\\nst\\nst S\\nxy\\n,,\\n,\\n()\\n=\\n()\\n()\\n∈\\n∑\\n1\\nc\\n \\n(6-45)\\nIt follows from Eq. (6-37) and the properties of vector addition that\\n \\nc\\nxy\\nK\\nst\\nK\\nst\\nK\\ns\\nst S\\nst S\\nst S\\nxy\\nxy\\nxy\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n()\\n=\\n()\\n()\\n()\\n∈\\n()\\n∈\\n()\\n∈\\n∑\\n∑\\n∑\\n1\\n1\\n1\\nR\\nG\\nB\\nt t\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(6-46)\\nWe recognize the components of this vector as the scalar images that would be \\nobtained by independently smoothing each plane of the original RGB image using \\nconventional grayscale neighborhood processing\\n. Thus, we conclude that smoothing \\nby neighborhood averaging can be carried out on a per-color-plane basis. The result \\nis the same as when the averaging is performed using RGB color vectors.\\n6.6\\nDIP4E_GLOBAL_Print_Ready.indb   442\\n6/16/2017   2:08:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 443}),\n",
       " Document(page_content='6.6\\n  \\nColor Image Smoothing and Sharpening\\n    \\n443\\nEXAMPLE 6.12 :  Color image smoothing by neighborhood averaging.\\nConsider the RGB color image in Fig. 6.36(a). Its three component images are shown in Figs. 6.36(b) \\nthrough (d). Figures 6.37(a) through (c) show the HSI components of the image. Based on the discus-\\nsion in the previous paragraph, we smoothed each component image of the RGB image in Fig. 6.36 \\nindependently using a \\n55\\n×\\n averaging kernel. We then combined the individually smoothed images \\nto form the smoothed,\\n full-color RGB result in Fig. 6.38(a). Note that this image appears as we would \\nexpect from performing a spatial smoothing operation, as in the examples given in Section 3.5.\\nIn Section 6.2, we mentioned that an important advantage of the HSI color model is that it decouples \\nintensity and color information. This makes it suitable for many grayscale processing techniques and \\nsuggests that it might be more efﬁcient to smooth only the intensity component of the HSI repre-\\nsentation in Fig. 6.37. To illustrate the merits and/or consequences of this approach, we next smooth \\nonly the intensity component (leaving the hue and saturation components unmodiﬁed) and convert the \\nprocessed result to an RGB image for display. The smoothed color image is shown in Fig. 6.38(b). Note \\nb a\\nd c\\nFIGURE 6.36\\n(a) RGB image. \\n(b) Red  \\ncomponent image.  \\n(c)Green  \\ncomponent.  \\n(d) Blue  \\ncomponent.\\nDIP4E_GLOBAL_Print_Ready.indb   443\\n6/16/2017   2:08:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 444}),\n",
       " Document(page_content='444\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nb a\\nc\\nFIGURE 6.38\\n Image smoothing with a \\n55\\n×\\n averaging kernel. (a) Result of processing each RGB component image. \\n(b) Result of processing the intensity component of the HSI image and converting to RGB\\n. (c) Difference between \\nthe two results.  \\nb a\\nc\\nFIGURE 6.37\\n HSI components of the RGB color image in Fig. \\n6.36\\n(a). (a) Hue. (b) Saturation. (c) Intensity.\\nthat it is similar to Fig. 6.38(a), but, as you can see from the difference image in Fig. 6.38(c), the two \\nsmoothed images are not identical. This is because in Fig. 6.38(a) the color of each pixel is the average \\ncolor of the pixels in the neighborhood. On the other hand, by smoothing only the intensity component \\nimage in Fig. 6.38(b), the hue and saturation of each pixel was not affected and, therefore, the pixel \\ncolors did not change. It follows from this observation that the difference between the two smoothing \\napproaches would become more pronounced as a function of increasing kernel size.\\nCOLOR IMAGE SHARPENING\\nIn this section we consider image sharpening using the Laplacian (see Section 3.6). \\nFrom vector analysis, we know that the Laplacian of a vector is defined as a vector \\nDIP4E_GLOBAL_Print_Ready.indb   444\\n6/16/2017   2:08:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 445}),\n",
       " Document(page_content='6.7\\n  \\nUsing Color in Image Segmentation\\n    \\n445\\nwhose components are equal to the Laplacian of the individual scalar components \\nof the input vector. In the RGB color system, the Laplacian of vector \\nc\\n in Eq. (6-37) \\nis\\n \\n∇\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n∇\\n()\\n∇\\n()\\n∇\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n2\\n2\\n2\\n2\\nc\\nxy\\nxy\\nxy\\nxy\\n,\\n,\\n,\\n,\\nR\\nG\\nB\\n \\n(6-47)\\nwhich, as in the previous section, tells us that we can compute the Laplacian of a \\nfull-color image by computing the Laplacian of each component image separately\\n.\\nEXAMPLE 6.13 :  Image sharpening using the Laplacian.\\nFigure 6.39(a) was obtained using Eq. (3-54) and the kernel in Fig. 3.45(c) to compute the Laplacians of \\nthe RGB component images in Fig. 6.36. These results were combined to produce the sharpened full-\\ncolor result. Figure 6.39(b) shows a similarly sharpened image based on the HSI components in Fig. 6.37. \\nThis result was generated by combining the Laplacian of the intensity component with the unchanged \\nhue and saturation components. The difference between the RGB and HSI sharpened images is shown \\nin Fig. 6.39(c). The reason for the discrepancies between the two images is as in Example 6.12.\\n6.7 USING COLOR IN IMAGE SEGMENTATION  \\nSegmentation is a process that partitions an image into regions. Although segmenta-\\ntion is the topic of Chapters 10 and 11, we consider color segmentation briefly here \\nfor the sake of continuity. You will have no difficulty following the discussion.\\n6.7\\nb a\\nc\\nFIGURE 6.39\\n Image sharpening using the Laplacian. (a) Result of processing each RGB channel. (b) Result of process-\\ning the HSI intensity component and converting to RGB. (c) Difference between the two results.\\nDIP4E_GLOBAL_Print_Ready.indb   445\\n6/16/2017   2:08:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 446}),\n",
       " Document(page_content='446\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nSEGMENTATION IN HSI COLOR SPACE\\nIf we wish to segment an image based on color and, in addition, we want to carry out \\nthe process on individual planes, it is natural to think first of the HSI space because \\ncolor is conveniently represented in the hue image. Typically, saturation is used as a \\nmasking image in order to isolate further regions of interest in the hue image. The \\nintensity image is used less frequently for segmentation of color images because it \\ncarries no color information. The following example is typical of how segmentation \\nis performed in the HSI color space.\\nEXAMPLE 6.14 :  Segmenting a color image in HSI color space.\\nSuppose that it is of interest to segment the reddish region in the lower left of the image in Fig. 6.40(a). \\nFigures 6.40(b) through (d) are its HSI component images. Note by comparing Figs. 6.40(a) and (b) that \\nthe region in which we are interested has relatively high values of hue, indicating that the colors are on \\nthe blue-magenta side of red (see Fig. 6.11). Figure 6.40(e) shows a binary mask generated by threshold-\\ning the saturation image with a threshold equal to 10% of the maximum value in that image. Any pixel \\nvalue greater than the threshold was set to 1 (white). All others were set to 0 (black).\\nFigure 6.40(f) is the product of the mask with the hue image, and Fig. 6.40(g) is the histogram of the \\nproduct image (note that the grayscale is in the range [0, 1]). We see in the histogram that high values \\n(which are the values of interest) are grouped at the very high end of the grayscale, near 1.0. The result \\nof thresholding the product image with threshold value of 0.9 resulted in the binary image in Fig. 6.40(h). \\nThe spatial location of the white points in this image identiﬁes the points in the original image that \\nhave the reddish hue of interest. This was far from a perfect segmentation because there are points in \\nthe original image that we certainly would say have a reddish hue, but that were not identiﬁed by this \\nsegmentation method. However, it can be determined by experimentation that the regions shown in \\nwhite in Fig. 6.40(h) are about the best this method can do in identifying the reddish components of the \\noriginal image. The segmentation method discussed in the following section is capable of yielding better \\nresults.\\nSEGMENTATION IN RGB SPACE\\nAlthough working in HSI space is more intuitive in the sense of colors being repre-\\nsented in a more familiar format, segmentation is one area in which better results \\ngenerally are obtained by using RGB color vectors (see Fig. 6.7). The approach is \\nstraightforward. Suppose that the objective is to segment objects of a specified color \\nrange in an RGB image. Given a set of sample color points representative of the col-\\nors of interest, we obtain an estimate of the “average” color that we wish to segment. \\nLet this average color be denoted by the RGB vector \\na\\n. The objective of segmenta-\\ntion is to classify each RGB pixel in a given image as having a color in the specified \\nrange or not. In order to perform this comparison, it is necessary to have a measure \\nof similarity. One of the simplest measures is the Euclidean distance. Let \\nz\\n denote \\nan arbitrary point in RGB space. We say that \\nz\\n is similar to \\na\\n if the distance between \\nthem is less than a specified threshold, \\nD\\n0\\n.\\n The Euclidean distance between \\nz\\n and \\na\\n \\nis given by\\nDIP4E_GLOBAL_Print_Ready.indb   446\\n6/16/2017   2:08:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 447}),\n",
       " Document(page_content='6.7\\n  \\nUsing Color in Image Segmentation\\n    \\n447\\nFIGURE 6.40\\n Image segmentation in HSI space. (a) Original. (b) Hue. (c) Saturation.  \\n(d) Intensity. (e) Binary saturation mask (black = 0). (f) Product of (b) and (e). (g) His-\\ntogram of (f). (h) Segmentation of red components from (a).\\nb a\\nd c\\nf e\\nh\\ng\\nDIP4E_GLOBAL_Print_Ready.indb   447\\n6/16/2017   2:08:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 448}),\n",
       " Document(page_content='448\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\n \\nD\\nT\\nRR GG BB\\n(, )\\nza z a\\nza za\\nza za za\\n=−\\n=−\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n()\\n+−\\n()\\n+−\\n(\\n)\\n⎡\\n/H20648/H20648\\n1\\n2\\n2\\n2\\n2\\n⎣ ⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n2\\n \\n(6-48)\\nwhere the subscripts R, G, and B denote the RGB components of vectors \\na\\n and \\nz\\n. \\nT\\nhe locus of points such that \\nDD\\nza\\n,\\n()\\n≤\\n0\\n is a solid sphere of radius \\nD\\n0\\n,\\n as illustrated \\nin F\\nig. 6.41(a). Points contained within the sphere satisfy the specified color crite-\\nrion; points outside the sphere do not. Coding these two sets of points in the image \\nwith, say, black and white, produces a binary segmented image.\\nA useful generalization of Eq. (6-48) is a distance measure of the form\\n \\nD\\nT\\nza z a C z a\\n,\\n()\\n=−\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n1\\n1\\n2\\n \\n(6-49)\\nwhere \\nC\\n is the covariance matrix (see Section 11.5) of the samples chosen to be \\nrepresentative of the color range we wish to segment.\\n The locus of points such that \\nDD\\nza\\n,\\n()\\n≤\\n0\\n describes a solid 3-D elliptical body [Fig. 6.41(b)] with the important \\nproperty that its principal axes are oriented in the direction of maximum data spread. \\nWhen \\nCI\\n=\\n,\\n the \\n33\\n×\\n identity matrix, Eq. (6-49) reduces to Eq. (6-48). Segmenta-\\ntion is as described in the preceding paragraph.\\nBecause distances are positive and monotonic\\n, we can work with the distance \\nsquared instead, thus avoiding square root computations. However, implementing \\nEq. (6-48) or (6-49) is computationally expensive for images of practical size, even \\nif the square roots are not computed. A compromise is to use a bounding box, as \\nillustrated in Fig. 6.41(c). In this approach, the box is centered on \\na\\n, and its dimen-\\nsions along each of the color axes is chosen proportional to the standard deviation \\nof the samples along each of the axis. We use the sample data to compute the stan-\\ndard deviations, which are the parameters used for segmentation with this approach. \\nGiven an arbitrary color point, we segment it by determining whether or not it is on \\nthe surface or inside the box, as with the distance formulations. However, determin-\\ning whether a color point is inside or outside a box is much simpler computationally \\nThis equation is called \\nthe \\nMahalanobis dis-\\ntance\\n. You are seeing it \\nused here for \\nmultivariate \\nthresholding\\n (see  \\nSection 10.3 regarding \\nthresholding). \\nR\\nG\\nB\\nR\\nG\\nB\\nR\\nG\\nB\\nb a\\nc\\nFIGURE 6.41\\nThree approaches \\nfor enclosing data \\nregions for RGB \\nvector  \\nsegmentation.\\nDIP4E_GLOBAL_Print_Ready.indb   448\\n6/16/2017   2:08:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 449}),\n",
       " Document(page_content='6.7\\n  \\nUsing Color in Image Segmentation\\n    \\n449\\nwhen compared to a spherical or elliptical enclosure. Note that the preceding discus-\\nsion is a generalization of the color-slicing method introduced in Section 6.5.\\nEXAMPLE 6.15 :  Color segmentation in RGB color space.\\nThe rectangular region shown Fig. 6.42(a) contains samples of reddish colors we wish to segment out \\nof the color image. This is the same problem we considered in Example 6.14 using hue, but now we \\napproach the problem using RGB color vectors. The approach followed was to compute the mean vec-\\ntor \\na\\n using the color points contained within the rectangle in Fig. 6.42(a), and then to compute the \\nstandard deviation of the red, green, and blue values of those samples. A box was centered at \\na\\n, and its \\ndimensions along each of the RGB axes were selected as 1.25 times the standard deviation of the data \\nalong the corresponding axis. For example, let \\ns\\nR\\n denote the standard deviation of the red components \\nb\\na\\nFIGURE 6.42\\nSegmentation in \\nRGB space.  \\n(a) Original image \\nwith colors of \\ninterest shown \\nenclosed by a \\nrectangle.  \\n(b) Result of \\nsegmentation \\nin RGB vector \\nspace. Compare \\nwith Fig. 6.40(h).\\nDIP4E_GLOBAL_Print_Ready.indb   449\\n6/16/2017   2:08:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 450}),\n",
       " Document(page_content='450\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nof the sample points. Then the dimensions of the box along the R-axis extended from \\na\\nRR\\n−\\n()\\n12 5\\n.\\ns\\n to \\na\\nRR\\n+\\n()\\n12 5\\n.,\\ns\\n where \\na\\nR\\n is the red component of average vector \\na\\n. Figure 6.42(b) shows the result of \\ncoding each point in the color image as white if it was on the surface or inside the box, and as black \\notherwise. Note how the segmented region was generalized from the color samples enclosed by the \\nrectangle. In fact, by comparing Figs. 6.42(b) and 6.40(h), we see that segmentation in the RGB vector \\nspace yielded results that are much more accurate, in the sense that they correspond much more closely \\nwith what we would deﬁne as “reddish” points in the original color image. This result is not unexpected, \\nbecause in the RGB space we used three color variables, as opposed to just one in the HSI space. \\nCOLOR EDGE DETECTION\\nAs we will discuss in Section 10.2, edge detection is an important tool for image \\nsegmentation. In this section, we are interested in the issue of computing edges on \\nindividual component images, as opposed to computing edges directly in color vec-\\ntor space. \\nWe introduced edge detection by gradient operators in Section 3.6, when discuss-\\ning image sharpening. Unfortunately, the gradient discussed there is not deﬁned for \\nvector quantities. Thus, we know immediately that computing the gradient on indi-\\nvidual images and then using the results to form a color image will lead to erroneous \\nresults. A simple example will help illustrate the reason why.\\nConsider the two \\nMM\\n×\\n color images (\\nM\\n odd) in F\\nigs. 6.43(d) and (h), com-\\nposed of the three component images in Figs. 6.43(a) through (c) and (e) through (g),  \\nrespectively. If, for example, we compute the gradient image of each of the com-\\nponent images using Eq. (3-58), then add the results to form the two correspond-\\ning RGB gradient images, the value of the gradient at point \\n() , ()\\nMM\\n++\\n[]\\n12 12\\n \\nwould be the same in both cases\\n. Intuitively, we would expect the gradient at that \\npoint to be stronger for the image in Fig. 6.43(d) because the edges of the R, G, \\nand B images are in the same direction in that image, as opposed to the image \\nin Fig. 6.43(h), in which only two of the edges are in the same direction. Thus we \\nsee from this simple example that processing the three individual planes to form \\na composite gradient image can yield erroneous results. If the problem is one of \\njust detecting edges, then the individual-component approach can yield acceptable \\nresults. If accuracy is an issue, however, then obviously we need a new deﬁnition of \\nthe gradient applicable to vector quantities. We discuss next a method proposed by \\nDi Zenzo [1986] for doing this.\\nThe problem at hand is to deﬁne the gradient (magnitude and direction) of the \\nvector \\nc\\n in Eq. (6-37) at any point \\n(,) .\\nxy\\n As we just mentioned, the gradient we \\nstudied in Section 3.6 is applicable to a \\nscalar\\n function \\nfx y\\n(,\\n) ;\\n it is not applicable \\nto vector functions\\n. The following is one of the various ways in which we can extend \\nthe concept of a gradient to vector functions. Recall that for a scalar function \\nfx y\\n(,\\n) ,\\n \\nthe gradient is a vector pointing in the direction of maximum rate of change of \\nf\\n at \\ncoordinates \\n(,) .\\nxy\\nLet \\nr\\n, \\ng\\n,\\n and \\nb\\n be unit vectors along the R, G, and B axis of RGB color space (see \\nFig. 6.7), and deﬁne the vectors\\nDIP4E_GLOBAL_Print_Ready.indb   450\\n6/16/2017   2:08:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 451}),\n",
       " Document(page_content='6.7\\n  \\nUsing Color in Image Segmentation\\n    \\n451\\n \\nu\\n=\\n∂\\n∂\\n+\\n∂\\n∂\\n∂\\n∂\\nR\\nx\\nG\\nx\\nB\\nx\\nrg + b\\n \\n(6-50)\\nand\\n \\nv\\n=\\n∂\\n∂\\n+\\n∂\\n∂\\n∂\\n∂\\nR\\ny\\nG\\ny\\nB\\ny\\nrg + b\\n \\n(6-51)\\nLet the quantities \\ng\\nxx\\n, \\ng\\nyy\\n,\\n and \\ng\\nxy\\n be defined in terms of the dot product of these \\nvectors, as follows:\\n \\ng\\nR\\nx\\nG\\nx\\nB\\nx\\nxx\\nT\\n=⋅= =\\n∂\\n∂\\n+\\n∂\\n∂\\n+\\n∂\\n∂\\nuu uu\\n22 2\\n \\n(6-52)\\n \\ng\\nR\\ny\\nG\\ny\\nB\\ny\\nyy\\nT\\n=⋅= =\\n∂\\n∂\\n+\\n∂\\n∂\\n+\\n∂\\n∂\\nvv vv\\n22 2\\n \\n(6-53)\\nand\\n \\ng\\nR\\nx\\nR\\ny\\nG\\nx\\nG\\ny\\nB\\nx\\nB\\ny\\nxy\\nT\\n=⋅= =\\n∂\\n∂\\n∂\\n∂\\n+\\n∂\\n∂\\n∂\\n∂\\n+\\n∂\\n∂\\n∂\\n∂\\nuv uv\\n \\n(6-54)\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 6.43\\n (a)–(c) R, G, and B component images, and (d) resulting RGB color image. (e)–(g) R, G, and B compo-\\nnent images, and (h) resulting RGB color image.\\nDIP4E_GLOBAL_Print_Ready.indb   451\\n6/16/2017   2:08:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 452}),\n",
       " Document(page_content='452\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nKeep in mind that R, G, and B, and consequently the \\ng\\n’s, are functions of \\nx\\n and \\ny\\n. \\nUsing this notation, it can be shown (Di Zenzo [1986]) that the direction of maxi-\\nmum rate of change of \\nc\\n(,\\n)\\nxy\\n is given by the angle\\n \\nu\\nxy\\ng\\ngg\\nxy\\nxx yy\\n,t a n\\n()\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−\\n1\\n2\\n2\\n1\\n \\n(6-55)\\nand that the value of the rate of change at \\n(,)\\nxy\\n in the direction of \\nu\\n(,\\n)\\nxy\\n is given by\\n     \\nFx y g g g g x y g x y\\nxx yy xx yy\\nxy\\nu\\nuu\\n,c o s , s i n ,\\n()\\n=+\\n()\\n+−\\n()\\n()\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n1\\n2\\n22 2\\n⎧ ⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n1\\n2\\n \\n(6-56)\\nBecause \\ntan tan ,\\naa\\np\\n()\\n=±\\n()\\n \\nif\\nu\\n0\\n is a solution to Eq. (6-55), so is \\nup\\n0\\n2\\n±\\n.\\n Fur-\\nthermore\\n, \\nFF\\nuu p\\n=\\n+\\n,\\n so \\nF\\n has to be computed only for values of \\nu\\n in the half-open \\ninterval \\n[, ) .\\n0\\np\\n The fact that Eq. (6-55) gives two values 90° apart means that this \\nequation associates with each point \\n(,)\\nxy\\n a pair of orthogonal directions. Along one \\nof those directions \\nF\\n is maximum,\\n and it is minimum along the other. The deriva-\\ntion of these results is rather lengthy, and we would gain little in terms of the fun-\\ndamental objective of our current discussion by detailing it here. Consult the paper \\nby Di Zenzo [1986] for details. The Sobel operators discussed in Section 3.6 can \\nbe used to compute the partial derivatives required for implementing Eqs. (6-52)  \\nthrough (6-54).\\nEXAMPLE 6.16 :  Edge detection in RGB vector space.\\nFigure 6.44(b) is the gradient of the image in Fig. 6.44(a), obtained using the vector method just dis-\\ncussed. Figure 6.44(c) shows the image obtained by computing the gradient of each RGB component \\nimage and forming a composite gradient image by adding the corresponding values of the three com-\\nponent images at each coordinate \\n(,) .\\nxy\\n The edge detail of the vector gradient image is more complete \\nthan the detail in the individual-plane gradient image in F\\nig. 6.44(c); for example, see the detail around \\nthe subject’s right eye. The image in Fig. 6.44(d) shows the difference between the two gradient images \\nat each point \\n(,) .\\nxy\\n It is important to note that both approaches yielded reasonable results. Whether \\nthe extra detail in F\\nig. 6.44(b) is worth the added computational burden over the Sobel operator com-\\nputations can only be determined by the requirements of a given problem. Figure 6.45 shows the three \\ncomponent gradient images, which, when added and scaled, were used to obtain Fig. 6.44(c).\\n6.8 NOISE IN COLOR IMAGES  \\nThe noise models discussed in Section 5.2 are applicable to color images. Usually, the \\nnoise content of a color image has the same characteristics in each color channel, but \\nit is possible for color channels to be affected differently by noise. One possibility is \\nfor the electronics of a particular channel to malfunction. However, different noise \\nlevels are more likely caused by differences in the relative strength of illumination \\navailable to each of the color channels. For example, use of a red filter in a CCD \\ncamera will reduce the strength of illumination detected by the red sensing elements. \\nCCD sensors are noisier at lower levels of illumination, so the resulting red com-\\n6.8\\nDIP4E_GLOBAL_Print_Ready.indb   452\\n6/16/2017   2:08:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 453}),\n",
       " Document(page_content='6.8\\n  \\nNoise in Color Images\\n    \\n453\\nb a\\nd c\\nFIGURE 6.44\\n(a) RGB image. \\n(b) Gradient \\ncomputed in RGB \\ncolor vector space. \\n(c) Gradient \\nimage formed by \\nthe elementwise \\nsum of three \\nindividual  \\ngradient images, \\neach computed \\nusing the Sobel \\noperators.  \\n(d) Difference \\nbetween (b) and \\n(c).\\nb a\\nc\\nFIGURE 6.45\\n Component gradient images of the color image in Fig. 6.44. (a) Red component, (b) green component, \\nand (c) blue component. These three images were added and scaled to produce the image in Fig. 6.44(c).\\nDIP4E_GLOBAL_Print_Ready.indb   453\\n6/16/2017   2:08:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 454}),\n",
       " Document(page_content='454\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nponent of an RGB image would tend to be noisier than the other two component \\nimages in this situation.\\nEXAMPLE 6.17 :  Illustration of the effects of noise when converting noisy RGB images to HSI.\\nIn this example, we take a brief look at noise in color images and how noise carries over when convert-\\ning from one color model to another. Figures 6.46(a) through (c) show the three color planes of an RGB \\nimage corrupted by additive Gaussian noise, and Fig. 6.46(d) is the composite RGB image. Note that \\nﬁne grain noise such as this tends to be less visually noticeable in a color image than it is in a grayscale \\nimage. Figures 6.47(a) through (c) show the result of converting the RGB image in Fig. 6.46(d) to HSI. \\nCompare these results with the HSI components of the original image (see Fig. 6.37) and note how sig-\\nniﬁcantly degraded the hue and saturation components of the noisy image are. This was caused by the \\nnonlinearity of the cos and min operations in Eqs. (6-17) and (6-18), respectively. On the other hand, \\nthe intensity component in Fig. 6.47(c) is slightly smoother than any of the three noisy RGB component \\nimages. This is because the intensity image is the average of the RGB images, as indicated in Eq. (6-19). \\n(Recall the discussion in Section 2.6 regarding the fact that image averaging reduces random noise.)\\nb a\\nd c\\nFIGURE 6.46\\n(a)–(c) Red, \\ngreen, and blue \\n8-bit component  \\nimages  \\ncorrupted by \\nadditive  \\nGaussian noise of \\nmean 0 and stan-\\ndard deviation of \\n28 intensity levels.  \\n(d) Resulting \\nRGB image. \\n[Compare (d) \\nwith Fig. 6.44(a).]\\nDIP4E_GLOBAL_Print_Ready.indb   454\\n6/16/2017   2:08:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 455}),\n",
       " Document(page_content='6.9\\n  \\nColor Image Compression\\n    \\n455\\nIn cases when, say, only one RGB channel is affected by noise, conversion to HSI spreads the noise to \\nall HSI component images. Figure 6.48 shows an example. Figure 6.48(a) shows an RGB image whose \\ngreen component image is corrupted by salt-and-pepper noise, with a probability of either salt or pepper \\nequal to 0.05. The HSI component images in Figs. 6.48(b) through (d) show clearly how the noise spread \\nfrom the green RGB channel to all the HSI images. Of course, this is not unexpected because computa-\\ntion of the HSI components makes use of all RGB components, as discussed in Section 6.2.\\nAs is true of the processes we have discussed thus far, filtering of full-color images \\ncan be carried out on a per-image basis, or directly in color vector space, depending \\non the process. For example, noise reduction by using an averaging filter is the pro-\\ncess discussed in Section 6.6, which we know gives the same result in vector space as \\nit does if the component images are processed independently. However, other filters \\ncannot be formulated in this manner. Examples include the class of order statistics \\nfilters discussed in Section 5.3. For instance, to implement a median filter in color \\nvector space it is necessary to find a scheme for ordering vectors in a way that the \\nmedian makes sense. While this was a simple process when dealing with scalars, the \\nprocess is considerably more complex when dealing with vectors. A discussion of \\nvector ordering is beyond the scope of our discussion here, but the book by Platani-\\notis and Venetsanopoulos [2000] is a good reference on vector ordering and some of \\nthe filters based on the concept of ordering.\\n6.9 COLOR IMAGE COMPRESSION  \\nBecause the number of bits required to represent color is typically three to four \\ntimes greater than the number employed in the representation of gray levels, data \\ncompression plays a central role in the storage and transmission of color images. \\nWith respect to the RGB, CMY(K), and HSI images of the previous sections, the \\ndata that are the object of any compression are the components of each color pixel \\n(e.g., the red, green, and blue components of the pixels in an RGB image); they are \\n6.9\\nb a\\nc\\nFIGURE 6.47\\n HSI components of the noisy color image in Fig. 6.46(d). (a) Hue. (b) Saturation. (c) Intensity.\\nDIP4E_GLOBAL_Print_Ready.indb   455\\n6/16/2017   2:08:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 456}),\n",
       " Document(page_content='456\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nthe means by which the color information is conveyed. Compression is the process \\nof reducing or eliminating redundant and/or irrelevant data. Although compression \\nis the topic of Chapter 8, we illustrate the concept briefly in the following example \\nusing a color image.\\nEXAMPLE 6.18 :  An example of color image compression.\\nFigure 6.49(a) shows a 24-bit RGB full-color image of an iris, in which 8 bits each are used to represent \\nthe red, green, and blue components. Figure 6.49(b) was reconstructed from a compressed version of the \\nimage in (a) and is, in fact, a compressed and subsequently decompressed approximation of it. Although \\nthe compressed image is not directly displayable—it must be decompressed before input to a color \\nmonitor—the compressed image contains only 1 data bit (and thus 1 storage bit) for every 230 bits of \\ndata in the original image (you will learn about the origin of these numbers in Chapter 8). Suppose that \\nthe image is of size \\n2000\\n×\\n3000 = 6 10\\n6\\n⋅\\n pixels. The image is 24 bits/pixel, so it storage size is \\n144 10\\n6\\n⋅\\n bits. \\nb a\\nd c\\nFIGURE 6.48\\n(a) RGB image \\nwith green plane \\ncorrupted by salt-\\nand-pepper noise. \\n(b) Hue  \\ncomponent of \\nHSI image.  \\n(c) Saturation \\ncomponent.  \\n(d) Intensity  \\ncomponent.\\nDIP4E_GLOBAL_Print_Ready.indb   456\\n6/16/2017   2:08:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 457}),\n",
       " Document(page_content='6.9\\n  \\nColor Image Compression\\n    \\n457\\nb\\na\\nFIGURE 6.49\\nColor image  \\ncompression.  \\n(a) Original RGB \\nimage.  \\n(b) Result of  \\ncompressing, then  \\ndecompressing \\nthe image in (a).\\nSuppose that you are sitting at an airport waiting for your ﬂight, and want to upload 100 such images \\nusing the airport’s public WiFi connection. At a (relatively high) upload speed of \\n10 10\\n6\\n⋅\\n bits/sec, it \\nwould take you about 24 min to upload your images\\n. In contrast, the compressed images would take \\nabout 6 sec to upload. Of course, the transmitted data would have to be decompressed at the other end \\nfor viewing, but the decompression can be done in a matter of seconds. Note that the reconstructed \\napproximation image is slightly blurred. This is a characteristic of many lossy compression techniques; it \\ncan be reduced or eliminated by changing the level of compression. The JPEG 2000 compression algo-\\nrithm used to generate Fig. 6.49(b) is described in detail in Section 8.2.\\nDIP4E_GLOBAL_Print_Ready.indb   457\\n6/16/2017   2:08:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 458}),\n",
       " Document(page_content='458\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n6.1 \\nGive the percentages of red (X), green (Y), and \\nblue (Z) light required to generate the point labeled\\n \\n“warm white” in Fig. 6.5.\\n6.2 * \\nConsider any two valid colors \\nc\\n1\\n and \\nc\\n2\\n with coor-\\ndinates \\n(,)\\nxy\\n11\\n and \\n(,)\\nxy\\n22\\n in the chromaticity \\ndiagram of Fig. 6.5. Derive the necessary general \\nexpression(s) for computing the relative percent-\\nages of colors \\nc\\n1\\n and \\nc\\n2\\n composing any color that \\nis known to lie on the straight line joining these two \\ncolors.\\n6.3 \\nConsider any three valid colors \\nc\\n1\\n, \\nc\\n2\\n,\\n and \\nc\\n3\\n with \\ncoordinates \\n(,) ,\\nxy\\n11\\n \\n(,) ,\\nxy\\n22\\n and \\n(,) ,\\nxy\\n33\\n in the \\nchromaticity diagram of Fig. 6.5. Derive the nec-\\nessary general expression(s) for computing the \\nrelative percentages of \\nc\\n1\\n, \\nc\\n2\\n,\\n and \\nc\\n3\\n composing a \\ncolor that is known to lie within the triangle whose \\nvertices are at the coordinates of \\nc\\n1\\n, \\nc\\n2\\n,\\n and \\nc\\n3\\n.\\n6.4 * \\nIn an automated assembly application, three types \\nof parts are to be color\\n-coded to simplify detection. \\nHowever, only a monochrome TV camera is avail-\\nable to acquire digital images. Propose a technique \\nfor using this camera to detect the three different \\ncolors.\\nSummary, References, and Further Reading\\n  \\nThe material in this chapter is an introduction to color image processing and covers topics selected to provide a \\nsolid background in the techniques used in this branch of image processing. Our treatment of color fundamentals \\nand color models was prepared as foundation material for a ﬁeld that is wide in technical scope and areas of applica-\\ntion. In particular, we focused on color models that we felt are not only useful in digital image processing but pro-\\nvide also the tools necessary for further study in this area of image processing. The discussion of pseudocolor and \\nfull-color processing on an individual image basis provides a tie to techniques that were covered in some detail in \\nChapters 3 through 5. The material on color vector spaces is a departure from methods that we had studied before \\nand highlights some important differences between grayscale and full-color processing. Our treatment of noise in \\ncolor images also points out that the vector nature of the problem, along with the fact that color images are rou-\\ntinely transformed from one working space to another, has implications on the issue of how to reduce noise in these \\nimages. In some cases, noise ﬁltering can be done on a per-image basis, but others, such as median ﬁltering, require \\nspecial treatment to reﬂect the fact that color pixels are vector quantities, as mentioned earlier. Although segmenta-\\ntion is the topic of Chapters 10 and 11, and image data compression is the topic of Chapter 8, we introduced them \\nbrieﬂy in the context of color image processing. \\nFor a comprehensive reference on the science of color, see Malacara [2011]. Regarding the physiology of color, \\nsee Snowden et al. [2012]. These two references, together with the book by Kuehni [2012], provide ample supple-\\nmentary material for the discussion in Section 6.1. For further reading on color models (Section 6.2), see Fortner \\nand Meyer [1997], Poynton [1996], and Fairchild [1998]. For a detailed derivation of the equations for the HSI \\nmodel see the paper by Smith [1978] or consult the book website. The topic of pseudocolor (Section 6.3) is closely \\ntied to the general area of image data visualization. Wolff and Yaeger [1993] is a good basic reference on the use of \\npseudocolor. See also Telea [2008]. For additional reading on the material in Sections 6.4 and 6.5, see Plataniotis and \\nVenetsanopoulos [2000]. The material on color image ﬁltering (Section 6.6) is based on the vector formulation intro-\\nduced in Section 6.4 and on our discussion of spatial ﬁltering in Chapter 3. The area of color image segmentation \\n(Section 6.7) is of signiﬁcant current interest. For an overview of current trends in this ﬁeld see the survey by Van-\\ntaram and Saber [2012]. For more advanced color image processing techniques than those discussed in this chapter \\nsee Fernandez-Maloigne [2012]. The discussion in Section 6.8 is based on the noise models introduced in Section 5.2. \\nReferences on color image compression (Section 6.9) are listed at the end of Chapter 8. For details of software \\nimplementation of many of the techniques discussed in this chapter, see Gonzalez, Woods, and Eddins [2009].\\nDIP4E_GLOBAL_Print_Ready.indb   458\\n6/16/2017   2:08:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 459}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n459\\n6.5 \\nThe R, G, and B component images of an RGB \\nimage have the horizontal intensity proﬁles shown\\n \\nin the following diagram. What color would a per-\\nson see in the middle column of this image?\\n1.0\\n0.5\\n0\\nN\\n/\\n2\\nN \\n/H11002 \\n1\\nPosition\\nColor\\nBlue\\n1.0\\n0.5\\n0\\nN\\n/\\n2\\nN \\n/H11002 \\n1\\nPosition\\nColor\\nRed\\n1.0\\n0.5\\n0\\nN\\n/\\n2\\nN \\n/H11002 \\n1\\nPosition\\nColor\\nGreen\\n6.6 * \\nSketch the RGB components of the following \\nimage as they would appear on a monochrome \\nmonitor\\n. All colors are at maximum intensity and \\nsaturation. In working this problem, consider the \\ngray border as part of the image.\\nBlack\\nRed\\nYellow\\nGreen\\nCyan\\nBlue\\nMagenta\\nWhite\\nGray (50% Black)\\n6.7 \\nWhat is the maximum number of possible differ-\\nent shades of gray in an RGB image whose three \\ncomponent images are 8-bit images?\\n6.8 \\nConsider the RGB cube in Fig. 6.8 and answer \\neach of the following questions\\n.\\n(a) * \\nDescribe how the gray levels vary in each of \\nthe R,\\n G, and B primary images that make \\nup the front face of the color cube (this is the \\nface closer to you). Assume that each com-\\nponent image is an 8-bit image.\\n(b) \\nSuppose that we replace every color in the \\nRGB cube by its CMY color\\n. This new cube \\nis displayed on an RGB monitor. Label with \\na color name the eight vertices of the new \\ncube that you would see on the screen.\\n(c) \\nWhat can you say about the colors on the \\nedges of the RGB color cube regarding satu-\\nration?\\n6.9 \\nDo the following.\\n(a) * \\nSketch the CMY components of the image \\nin Problem 6.6 as they would appear on a \\nmonochrome monitor\\n.\\n(b) \\nIf the CMY components sketched in (a) are \\nfed into the red,\\n green, and blue inputs of \\na color monitor, respectively, describe the \\nappearance of the resulting image.\\n6.10 * \\nSketch the HSI components of the image in \\nProblem 6.6 as they would appear on a mono-\\nchrome monitor\\n.\\n6.11 \\nPropose a method for generating a color band \\nsimilar to the one shown in the zoomed section \\nentitled \\nV\\nisible Spectrum\\n in Fig. 6.2. Note that the \\nband starts at a dark purple on the left and pro-\\nceeds toward pure red on the right. (\\nHint:\\n Use \\nthe HSI color model.)\\n6.12 * \\nPropose a method for generating a color ver-\\nsion of the image shown diagrammatically in \\nF\\nig. 6.11(c). Give your answer in the form of a \\nﬂow chart. Assume that the intensity value is \\nﬁxed and given. (\\nHint:\\n Use the HSI color model.)\\n6.13 \\nConsider the following image composed of solid \\ncolor squares\\n. For discussing your answer, choose \\na gray scale consisting of eight shades of gray, 0 \\nthrough 7, where 0 is black and 7 is white. Sup-\\npose that the image is converted to HSI color \\nspace. In answering the following questions, use \\nspeciﬁc numbers for the gray shades if using \\nnumbers makes sense. Otherwise, the relation-\\nships “same as,” “lighter than,” or “darker than” \\nare sufﬁcient. If you cannot assign a speciﬁc gray \\nlevel or one of these relationships to the image \\nyou are discussing, give the reason.\\n(a) * \\nSketch the hue image.\\n(b) \\nSketch the saturation image.\\n(c) \\nSketch the intensity image.\\nDIP4E_GLOBAL_Print_Ready.indb   459\\n6/16/2017   2:08:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 460}),\n",
       " Document(page_content='460\\n    \\nChapter\\n \\n6\\n  \\nColor Image Processing\\nBlack\\nRed\\nGreen\\nBlue\\nMagenta\\nCyan\\nYellow\\nWhite\\n6.14 \\nThe following 8-bit images are the H, S, and I com-\\nponent images from F\\nig. 6.14. The numbers indi-\\ncate gray-level values. Answer the following ques-\\ntions, explaining the basis for your answer in each. \\nIf it is not possible to answer a question based on \\nthe given information, state why you cannot do so.\\n(a) * \\nGive the gray-level values of all regions in \\nthe hue image\\n.\\n(b) \\nGive the gray-level value of all regions in \\nthe saturation image\\n.\\n(c) \\nGive the gray-level values of all regions in \\nthe intensity image\\n.\\nHue\\nSaturation\\nIntensity\\n6.15 * \\nCompute the \\nLab\\n**\\n*\\n components of the image \\nin Problem 6.6 assuming:\\n \\nX\\nY\\nZ\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n0\\n588 0 179 0 183\\n0 29 0 606 0 105\\n0 0 068 1 021\\n...\\n.. .\\n..\\n⎢ ⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nR\\nG\\nB\\nThis matrix equation deﬁnes the tristimulus \\nvalues of the colors generated by the standard \\nNational \\nTelevision System Committee (NTSC) \\ncolor TV phosphors viewed under \\nD65\\n standard \\nillumination (Benson [1985]).\\n6.16 * \\nDerive the CMY intensity mapping function \\nof Eq.\\n (6-41) from the RGB counterpart in \\nEq. (6-40). [\\nHint:\\n Start with Eq. (6-5).]\\n6.17 \\nStart with Eqs. (6-6)-(6-12) and derive Eq. (6-42). \\n(\\nHint:\\n \\nThe intensity of the CMYK image is \\nchanged by changing the K component only.)\\n6.18 \\nRefer to Fig. 6.25 in answering the following:\\n(a) * \\nWhy does the image in Fig. 6.25(e) exhibit \\npredominantly red tones?\\n(b) * \\nSuggest an automated procedure for coding \\nthe water in F\\nig. 6.25 in a bright-blue color.\\n(c) \\nSuggest an automated procedure for coding \\nthe predominantly man-made components \\nin a bright yellow color\\n. [\\nHint: \\nWork with \\nFig. 6.25(e).]\\n6.19 * \\nShow that the saturation component of the com-\\nplement of a color image cannot be computed \\nfrom the saturation component of the input \\nimage alone\\n.\\n6.20 \\nExplain the shape of the hue transformation \\nfunction for the image complement approxima-\\ntion in F\\nig. 6.31(b) using the HSI color model.\\n6.21 * \\nDerive the CMY transformations to generate the \\ncomplement of a color image\\n.\\n6.22 \\nDraw the general shape of the transformation \\nfunctions used to correct excessive contrast in \\nthe RGB color space\\n.\\n6.23 * \\nAssume that the monitor and printer of an imag-\\ning system are imperfectly calibrated.\\n An image \\nthat looks balanced on the monitor appears yel-\\nlowish in print. Describe general transformations \\nthat might correct the imbalance. (\\nHints:\\n Refer \\nto the color wheel in Fig. 6.30 and the discussion \\nof the \\nLab\\n**\\n*\\n color system in Section 6.2.)\\nDIP4E_GLOBAL_Print_Ready.indb   460\\n6/16/2017   2:08:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 461}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n461\\n6.24 * \\nGiven an image in the RGB, CMY, or CMYK \\ncolor system,\\n how would you implement the col-\\nor equivalent of gray-scale histogram matching \\n(speciﬁcation) from Section 3.3?\\n6.25 \\nConsider the following \\n500 500\\n×\\n RGB image, in \\nwhich the squares are fully saturated red,\\n green, \\nand blue, and each of the colors is at maximum \\nintensity. An HSI image is generated from this \\nimage. Answer the following questions.\\nGreen\\nRed\\nBlue\\nGreen\\n(a) \\nDescribe the appearance of each HSI com-\\nponent image\\n.\\n(b) * \\nThe saturation component of the HSI image \\nis smoothed using an averaging kernel of \\nsize \\n125 125\\n×\\n.\\n Describe the appearance of \\nthe result.\\n (You may ignore image border \\neffects in the ﬁltering operation.)\\n(c) \\nRepeat (b) for the hue image.\\n6.26 \\nAnswer the following.\\n(a) * \\nRefer to the discussion in Section 6.7 about \\nsegmentation in the RGB color space\\n. Give \\na procedure (in ﬂow chart form) for deter-\\nmining whether a color vector (point) \\nz\\n is \\ninside a cube with sides \\nW\\n, centered at an \\naverage color vector \\na\\n. Distance computa-\\ntions are not allowed.\\n(b) \\nIf the box is aligned with the axes this pro-\\ncess also can be implemented on an image-\\nby-image basis\\n. Show how you would do it.\\n6.27 \\nShow that Eq. (6-49) reduces to Eq. (6-48) when \\nCI\\n=\\n, the identity matrix.\\n6.28 \\nSketch the surface in RGB space for the points \\nthat satisfy the equation\\n \\nDD\\nT\\nz,a z a C z a\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n−−\\n1\\n0\\n1\\n2\\nwhere \\nD\\n0\\n is a positive constant. Assume that \\na0\\n=\\n,\\n and that\\n \\nC\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n800\\n01\\n0\\n001\\n6.29 \\nRefer to the discussion on color edge detection \\nin Section 6.7.\\n One might think that a logical \\napproach for deﬁning the gradient of an RGB \\nimage at any point \\n(,)\\nxy\\n would be to compute \\nthe gradient vector (see Section 3.6) of each com-\\nponent image and then form a gradient vector for \\nthe color image by summing the three individual \\ngradient vectors\\n. Unfortunately, this method can \\nat times yield erroneous results. Speciﬁcally, it is \\npossible for a color image with clearly deﬁned \\nedges to have a zero gradient if this method were \\nused. Give an example of such an image. (\\nHint:\\n \\nTo simplify your analysis, set one of the color \\nplanes to a constant value.)\\nDIP4E_GLOBAL_Print_Ready.indb   461\\n6/16/2017   2:08:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 462}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 463}),\n",
       " Document(page_content='4637\\nWavelet and Other Image \\nTransforms\\nPreview\\nThe discrete Fourier transform of Chapter 4 is a member of an important class of linear transforms that \\ninclude the Hartley, sine, cosine, Walsh-Hadamard, Slant, Haar, and wavelet transforms. These trans-\\nforms, which are the subject of this chapter, decompose functions into weighted sums of orthogonal or \\nbiorthogonal basis functions, and can be studied using the tools of linear algebra and functional analysis. \\nWhen approached from this point of view, images are vectors in the vector space of all images. Basis \\nfunctions determine the nature and usefulness of image transforms. Transforms are the coefﬁcients of \\nlinear expansions. And for a given image and transform (or set of basis functions), both the orthogo-\\nnality of the basis functions and the coefﬁcients of the resulting transform are computed using inner \\nproducts. All of an image’s transforms are equivalent in the sense that they contain the same informa-\\ntion and total energy. They are reversible and differ only in the way that the information and energy is \\ndistributed among the transform’s coefﬁcients.\\nUpon competion of this chapter, readers should:\\n Understand image transforms in the context \\nof series expansions.\\n Be familiar with a variety of important image \\ntransforms and transform basis functions.\\n Know the difference between orthogonal and \\nbiorthogonal basis functions.\\n Be able to construct the transformation \\nmatrices of the discrete Fourier, Hartley, \\nsine, cosine, Walsh-Hadamard, Slant, and \\nHaar transforms.\\n Be able to compute traditional image trans-\\nforms, like the Fourier and Haar transforms, \\nusing elementary matrix operations.\\n Understand the time-frequency plane and its \\nrelationship to wavelet transforms.\\n Be able to compute 1-D and 2-D fast wavelet \\ntransforms (FWTs) using ﬁlter banks.\\n Understand wavelet packet representations.\\n Be familiar with the use of discrete orthogo-\\nnal transforms in image processing.\\nDo not conform any longer to the pattern of this world, but be  \\ntransformed by the renewing of your mind.\\nRomans 12:2 \\nDIP4E_GLOBAL_Print_Ready.indb   463\\n6/16/2017   2:08:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 464}),\n",
       " Document(page_content='464\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\n7 .1 PRELIMINARIES  \\nIn linear algebra and functional analysis, a \\nvector space\\n (or more formally an \\nabstract \\nvector space\\n) is a set of mathematical objects or entities, called \\nvectors\\n, that can be \\nadded together and multiplied by \\nscalars\\n. An \\ninner product space\\n is an abstract vec-\\ntor space over a ﬁeld of numbers, together with an \\ninner product function\\n that maps \\ntwo vectors of the vector space to a scalar of the number ﬁeld such that \\n(a) \\nuv vu\\n,,\\n*\\n=\\n(b) \\nuv w u w v w\\n++\\n,, ,\\n=\\n(c) \\naa\\nuv uv\\n,,\\n=\\n(d) \\nvv vv\\nv\\n,,\\nÚ\\n00 0\\n and  if and only if \\n==\\nwhere \\nu\\n, \\nv\\n,\\n and \\nw\\n are vectors, \\na\\n is a scalar, and \\np\\n denotes the inner product opera-\\ntion.\\n A simple example of a vector space is the set of directed line segments in two \\ndimensions, where the line segments are represented mathematically as \\n21\\n×\\n col-\\numn vectors\\n, and the addition of vectors is the arithmetic equivalent of combining \\nthe line segments in a head to tail manner. An example of an inner product space is \\nthe set of real numbers \\nR\\n combined with inner product function \\nuv u v\\n,,\\n=\\n where \\nthe \\n“vectors” are real numbers, the inner product function is multiplication, and axi-\\noms (a) through (d) above correspond to the commutative, distributive, associative, \\nand “positivity of even powers” properties of multiplication, respectively.\\nThree inner product spaces are of particular interest in this chapter: \\n1. \\nEuclidean\\n space \\nR\\nN\\n over real number ﬁeld \\nR\\n with \\ndot\\n or \\nscalar\\n inner product\\n \\nuv u v\\n,\\n== + =\\n−−\\n=\\n∑\\nT\\nNN\\nii\\ni\\nN\\nuv uv u v u v\\n00 11\\n1 1\\n0\\n1\\n++\\n−\\n…\\n \\n(7-1)\\nwhere \\nu\\n and \\nv\\n are \\nN\\n×\\n1 column vectors.\\n2. \\nUnitary\\n space \\nC\\nN\\n over complex number ﬁeld \\nC\\n with inner product function\\n \\nuv u v\\nvu\\n,,\\n**\\n*\\n== =\\n=\\n−\\n∑\\nT\\nii\\ni\\nN\\nuv\\n0\\n1\\n \\n(7-2)\\nwhere * denotes the complex conjugate operation, and \\nu\\n and \\nv\\n are complex-\\nvalued \\nN\\n×\\n1 column vectors.\\n3. \\nInner product space \\nC\\n([\\na\\n, \\nb\\n]),\\n where the vectors are continuous functions on the \\ninterval \\naxb\\n≤≤\\n and the inner product function is the \\nintegral inner product\\n \\nf x gx f xgxd x\\na\\nb\\n() ,() ()()\\n*\\n=\\n2\\n \\n(7-3)\\nIn all three inner product spaces, the \\nnorm\\n or \\nlength\\n of vector \\nz\\n,\\n denoted as \\nz\\n, is\\n \\nzz z\\n=\\n,  \\n(7-4)\\n7.1\\nConsult the Tutorials sec-\\ntion of the book website \\nfor a brief tutorial on \\nvectors and matrices.\\nIn Chapter 2, the inner \\nproduct of two column  \\nvectors, \\nu\\n and \\nv\\n, is \\ndenoted \\nu\\n \\ni\\n \\nv\\n [see \\nEq. (2-50)]. In this \\nchapter, \\nuv\\n,\\n is used to \\ndenote inner products \\nwithin any inner product \\nspace satisfying condi-\\ntions (a)–(d), including \\nthe Euclidean inner \\nproduct space and real-\\nvalued column vectors of \\nChapter 2. \\nEuclidean space \\nR\\nN\\n is an \\ninﬁnite set containing all \\nreal \\nN\\n-tuples.\\nA complex vector space \\nwith an inner product is \\ncalled a \\ncomplex inner \\nproduct space\\n or \\nunitary\\n \\nspace.\\nThe notation \\nC\\n[\\na\\n, \\nb\\n} \\nis also used in the \\nliterature.\\nEquations (7-4) through \\n(7-15) are valid for all \\ninner product spaces, \\nincluding those deﬁned \\nby Eqs. (7-1) to (7-3).\\nDIP4E_GLOBAL_Print_Ready.indb   464\\n6/16/2017   2:08:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 465}),\n",
       " Document(page_content=\"7.1\\n  \\nPreliminaries\\n    \\n465\\nand the \\nangle\\n between two nonzero vectors \\nz\\n and \\nw\\n is\\n \\nu\\n=\\ncos\\n,\\n−\\n1\\nzw\\nzw\\n \\n(7-5)\\nIf the norm of \\nz\\n is 1,\\n \\nz\\n is said to be \\nnormalized\\n. If \\nzw\\n,\\n=\\n0\\n in Eq. (7-5), \\nu\\n=\\n90\\n°\\n and \\nz\\n and \\nw \\nare said to be \\northogonal\\n.\\n A natural consequence of these definitions is that \\na set of nonzero vectors \\nw\\n0\\n, \\nw\\n1\\n, \\nw\\n2\\n, ... is mutually or pairwise orthogonal if and only if\\n \\nww k l\\nkl\\n,\\n=\\n0   for \\n≠\\n \\n(7-6)\\nThey are an \\northogonal basis\\n of the inner product space that they are said to \\nspan\\n.\\n If \\nthe \\nbasis vectors\\n are normalized, they are an \\northonormal basis\\n and\\n \\nww\\nkl\\nkl\\nkl k l\\n,\\n==\\n=\\n⎧\\n⎨\\n⎩\\nd\\n0\\n1\\n   for \\n   for \\n≠\\n \\n(7-7)\\nSimilarly, a set of vectors \\nw\\n0\\n, \\nw\\n1\\n, \\nw\\n2\\n, ...  and a complementary set of \\ndual vectors\\n \\nwww\\n012\\n'''\\np\\n,,,\\n are said to be \\nbiorthogonal\\n and a \\nbiorthogonal basis\\n of the vector \\nspace that they span if\\n \\nHI\\nww\\nk l\\nkl\\n'\\n,\\n=\\n0   for \\n≠\\n \\n(7-8)\\nThey are a \\nbiorthonormal basis\\n if and only if\\n \\nHI\\nww\\nkl\\nkl\\nkl k l\\n'\\n,\\n==\\n≠\\n=\\n⎧\\n⎨\\n⎩\\nd\\n0\\n1\\nfor \\nfor \\n \\n(7-9)\\nAs a mechanism for concisely describing an infinite set of vectors, the basis of \\nan inner product space is one of the most useful concepts in linear algebra.\\n The \\nfollowing derivation, which relies on the orthogonality of basis vectors, is founda-\\ntional to the matrix-based transforms of the next section. Let \\nWw w w\\n=\\n{}\\n012\\n,,,\\n…\\n \\nbe an orthogonal basis of inner product space \\nV\\n,\\n and let \\nzV\\n∈\\n.\\n Vector \\nz\\n can then be \\nexpressed as the following linear combination of basis vectors\\n \\nzw w w\\n=+\\n++\\naaa\\n00 11 22\\n…\\n \\n(7-10)\\nwhose inner product with basis vector \\nw\\ni\\n is\\n \\nwz w w w w\\nww ww ww\\nii\\nii i i i\\n,,\\n,, ,\\n=+ + +\\n=++ + +\\naaa\\naa a\\n00 11 22\\n00 11\\n…\\n……\\n \\n(7-11)\\nSince the \\nw\\ni\\n are mutually orthogonal, the inner products on the right side of \\nEq. (7-11) are 0 unless the subscripts of the vectors whose inner products are being \\nWhile you must always \\ntake the context into \\naccount, we generally \\nuse the word “vector” \\nfor vectors in an abstract \\nsense. A vector can be \\nan \\nN\\n*\\n1 matrix (i.e., \\ncolumn vector) or a \\ncontinuous function.\\nRecall from linear \\nalgebra that a \\nbasis\\n of a \\nvector space is a set of \\nlinearly independent vec-\\ntors for which any vector \\nin the space can be writ-\\nten uniquely as a linear \\ncombination of basis \\nvectors. The linear com-\\nbinations are the \\nspan\\n \\nof the basis vectors. A \\nset of vectors is \\nlinearly \\nindependent\\n if no vector \\nin the set can be written \\nas a linear combination \\nof the others.\\nWhile you must always \\ntake to the context into \\naccount, we often use \\nthe phrase “orthogonal \\nbasis” or “orthogonal \\ntransform” to refer to \\nany basis or transform \\nthat is orthogonal, ortho-\\nnormal, biorthogonal, or \\nbiorthonormal.\\nDIP4E_GLOBAL_Print_Ready.indb   465\\n6/16/2017   2:08:58 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 466}),\n",
       " Document(page_content=\"466\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\ncomputed match [see Eq. (7-7)]. Thus, the only nonzero term is \\na\\nii i\\nww\\n,.\\n Eliminat-\\ning the zero terms and dividing both sides of the equation by \\nww\\nii\\n,  gives\\n \\na\\ni\\ni\\nii\\nwz\\nww\\n=\\n,\\n,\\n \\n(7-12)\\nwhich reduces to\\n \\na\\nii\\nwz\\n=\\n,  \\n(7-13)\\nif the norms of the basis vectors are 1. A similar derivation, which is left as an exer-\\ncise for the reader\\n, yields\\n \\na\\ni\\ni\\nii\\nwz\\nww\\n=\\nHI\\nHI\\n'\\n'\\n,\\n,\\n \\n(7-14)\\nand\\n \\na\\nii\\nwz\\n=\\nHI\\n'\\n,  \\n(7-15)\\nfor biorthogonal and biorthonormal basis vectors, respectively. Note when a basis \\nand its dual are identical,\\n biorthogonality reduces to orthogonality.\\nEXAMPLE 7.1 :   Vector norms and angles.\\nThe norm of vector \\nfx x\\n()\\nc o s\\n=\\n of inner product space \\nC\\n02\\n,)\\np\\n[]\\n(\\n)\\n is\\n \\nfx fx fx\\nx d x x x\\n( ) ( ), ( ) cos\\nsin( )\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n0\\n2\\n2\\n1\\n2\\n0\\n2\\n1\\n2\\n1\\n4\\n2\\np\\np\\n2\\n⎦ ⎦\\n⎥\\n⎥\\n=\\n1\\n2\\np\\nThe angle between vectors \\nz\\n=\\n[]\\n11\\nT\\n and \\nw\\n=\\n[]\\n10\\nT\\n of Euclidean inner product space \\nR\\n2\\n is\\n \\nu\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n=\\ncos\\n,\\ncos\\n−−\\n°\\n11\\n1\\n2\\n45\\nzw\\nzw\\nThese results follow from Eqs. (7-1), (7-3), (7-4) and (7-5).\\n7 .2 MATRIX-BASED TRANSFORMS  \\nThe 1-D discrete Fourier transform of Chapter 4 is one of a class of important trans-\\nforms that can be expressed in terms of the general relation\\n \\nTu f xrxu\\nx\\nN\\n() () (,)\\n=\\n=\\n∑\\n0\\n1\\n-\\n \\n(7-16)\\n7.2\\nIn mathematics, the word \\ntransform\\n is used to \\ndenote a change in form \\nwithout an accompany-\\ning change in value.\\nDIP4E_GLOBAL_Print_Ready.indb   466\\n6/16/2017   2:08:59 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 467}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n467\\nwhere \\nx\\n is a \\nspatial variable\\n, \\nTu\\n()\\n is the transform of \\nfx\\n()\\n,\\n \\nrxu\\n(,\\n)\\n is a \\nforwar\\nd trans-\\nformation kernel\\n, and integer \\nu\\n is a \\ntransform variable\\n with values in the range \\n01 1\\n,,\\n, .\\np -\\nN\\n Similarly, the inverse transform of \\nTu\\n()\\n is\\n \\nfx T u s x u\\nu\\nN\\n() () (,)\\n=\\n=\\n∑\\n0\\n1\\n-\\n \\n(7-17)\\nwhere \\nsxu\\n(,\\n)\\n is an \\ninverse transformation kernel\\n and \\nx\\n takes on values in the range \\n01 1\\n,,\\n, .\\np -\\nN\\n Transformation kernels \\nrxu\\n(,\\n)\\n and \\nsxu\\n(,\\n)\\n in Eqs. (7-16) and (7-17), \\nwhich depend only on indices \\nx\\n and \\nu\\n and not on the values of \\nfx\\n()\\n and \\nTu\\n()\\n,\\n deter-\\nmine the nature and usefulness of the \\ntransform pair\\n that they define\\n.\\nEquation (7-17) is depicted graphically in Fig. 7.1. Note that \\nfx\\n()\\n is a weighted \\nsum of \\nN\\n inverse kernel functions (i.e\\n., \\nsxu\\n(,\\n)\\n for \\nuN\\n=\\n01\\n1\\n,, ,\\np -\\n) and that \\nTu\\n()\\n \\nfor \\nuN\\n=\\n01\\n1\\n,, ,\\np -\\n are the weights. All \\nN\\n \\nsxu\\n(,\\n)\\n contribute to the value of \\nfx\\n()\\n at \\nevery \\nx\\n.\\n If we expand the right side of Eq. (7-17) to obtain\\n \\nf x T sx T sx TN sxN\\n(\\n) () (,) ( ) (,) ( ) (, )\\n=\\n00 11 1 1\\n++ p + - -\\n \\n(7-18)\\nit is immediately apparent that the computation depicted in Fig. 7.1 is a \\nlinear expan-\\nsion\\n like that of Eq.\\n (7-10)—with the \\nsxu\\n(,\\n)\\n and \\nTu\\n()\\n in Eq. (7-18) taking the place \\nof the \\nw\\ni\\n (i.e., the basis vectors) and the \\na\\ni\\n in Eq. (7-10). If we assume the \\nsxu\\n(,\\n)\\n in \\nEq.\\n (7-18) are orthonormal basis vectors of an inner product space, Eq. (7-13) tells \\nus that\\n \\nTu sxu f x\\n() (,) , ()\\n=\\n \\n(7-19)\\nand transform \\nTu\\n()\\n for \\nuN\\n=\\n01\\n1\\n,, ,\\np -\\n can be computed via inner products.\\nFIGURE 7.1\\nA graphical  \\nillustration of \\nEq. (7-18).\\nf\\n(\\nx\\n)\\nx\\n0\\n=\\n+\\n+\\n+\\nx\\nx\\nx\\n0\\nE\\nf x T sx T sx TN sxN\\n() () (,) ( ) (,) ( ) (, )\\n=\\n00 11 1 1\\n++ + − −\\n…\\nsxN\\n(, )\\n−\\n1\\nN\\n−\\n1\\nN\\n−\\n1\\n×−\\nTN\\n()\\n1\\n×\\nT\\n()\\n1\\n×\\nT\\n()\\n0\\nsx\\n(,)\\n0\\nsx\\n(,)\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   467\\n6/16/2017   2:09:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 468}),\n",
       " Document(page_content='468\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nWe are now ready to express Eqs. (7-16) and (7-17) in matrix form. We begin by \\ndeﬁning functions \\nfx\\n()\\n,\\n \\nTu\\n()\\n,\\n and \\nsxu\\n(,\\n)\\n as column vectors\\n \\nf\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nf\\nf\\nfN\\nf\\nf\\nf\\nN\\n()\\n()\\n()\\n0\\n1\\n1\\n0\\n1\\n1\\no\\n-\\no\\n-\\n \\n(7-20)\\n \\nt\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nT\\nT\\nTN\\nt\\nt\\nt\\nN\\n()\\n()\\n()\\n0\\n1\\n1\\n0\\n1\\n1\\no\\n-\\no\\n-\\n \\n(7-21)\\nand\\n \\ns\\nu\\nu\\nu\\nuN\\nsu\\nsu\\nsN u\\ns\\ns\\ns\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n(, )\\n(, )\\n(, )\\n,\\n,\\n,\\n0\\n1\\n1\\n0\\n1\\n1\\no\\n-\\no\\n-\\n⎤ ⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n   for \\nuN\\n01 1\\n,, ,\\n…\\n−\\n \\n(7-22)\\nand using them to rewrite Eq. (7-19) as\\n \\nTu\\nu N\\nu\\n() ,\\n,, ,\\n==\\nsf\\n    for 0 1 1\\np -\\n \\n(7-23)\\nCombining the \\nN\\n basis vectors of the transform in an \\nNN\\n*\\n \\ntransformation matrix\\n \\nA\\ns\\ns\\ns\\nss\\ns\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n[]\\n0\\n1\\n1\\n01 1\\nT\\nT\\nN\\nT\\nN\\nT\\no\\np\\n-\\n-\\n \\n(7-24)\\nwe can then substitute Eq. (7-23) into Eq. (7-21) and use Eq. (7-1) to get\\n \\nt\\nsf\\nsf\\nsf\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n++ +\\n0\\n1\\n1\\n00 0 10 1\\n10 1\\n,\\n,\\n,\\n,, ,\\no\\np\\n-\\n--\\nN\\nNN\\nsf sf s f\\ns\\nsf sf s f\\nsf sf s f\\nNN\\nNN N N N\\n01 0 11 1 11 1\\n01 0 11 1 11\\n,, ,\\n,, ,\\n++ +\\n++ +\\np\\no\\np\\n--\\n-- - - -\\n-\\n-\\n--\\n--\\np\\no\\n1\\n00 10\\n10\\n01 11\\n12\\n01\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\nss s\\nss\\ns\\nss\\nN\\nNN\\nNN\\n,,\\n,\\n,,\\n,\\n,\\n/downslopeellipsis\\n2\\n21 11\\n0\\n1\\n1\\n,,\\nNN N\\nN\\ns\\nf\\nf\\nf\\n-- -\\n-\\no\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(7-25)\\nWe will often use \\nsubscripts to denote the \\nelements of a matrix or \\nvector. Thus, \\nf\\n0\\n denotes \\nthe ﬁrst element of \\ncolumn vector \\nf\\n, which is \\nf\\n(0), and \\ns\\n3,0\\n denotes the \\nﬁrst element of column \\nvector \\ns\\n3\\n, which is \\ns\\n(0, 3).\\nBy employing Eq. (7-1), \\nwe assume the most \\ncommon case of real-\\nvalued basis vectors. \\nEquation (7-2) must be \\nused for a complex inner \\nproduct space.\\nDIP4E_GLOBAL_Print_Ready.indb   468\\n6/16/2017   2:09:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 469}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n469\\nor\\n \\ntA f\\n=\\n \\n(7-26)\\nThe inverse of this equation follows from the observation that\\n \\nAA\\ns\\ns\\ns\\nss\\ns\\nss ss ss\\nT\\nT\\nT\\nN\\nT\\nN\\nTT T\\nN\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n[]\\n=\\n0\\n1\\n1\\n01 1\\n00 01 0\\no\\np\\np\\n-\\n-\\n- -\\n-- -\\no\\no\\np\\n1\\n10 11\\n10 1 1\\n00 01\\nss ss\\nss ss\\nss ss\\nTT\\nN\\nT\\nN\\nT\\nN\\n/downslopeellipsis\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n,,\\np p\\no\\no\\np\\np\\n-\\n--\\n-\\nss\\nss ss\\nss\\nss\\n01\\n10 11\\n10\\n11\\n10 0\\n,\\n,,\\n,,\\nN\\nNN\\nN\\n/downslopeellipsis\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n0\\n01\\n01\\no\\no\\np\\n/downslopeellipsis\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\nI\\n \\n(7-27)\\nwhere the last two steps are a consequence of Eqs. (7-1) and (7-7), respectively. \\nSince \\nAA\\nT\\n = \\nI\\n, premultiplying Eq. (7-26) by \\nA\\nT\\n and simplifying gives \\nf\\n = \\nA\\nT\\nt\\n. Thus, \\nEqs. (7-16) and (7-17) become the matrix-based transform pair\\n \\ntA f\\n=\\n \\n(7-28)\\nand\\n \\nfA t\\n=\\nT\\n \\n(7-29)\\nIt is important to remember that, in the derivation of Eqs. (7-28) and (7-29), we \\nassumed the \\nN\\n transform basis vectors (i.e\\n., the \\ns\\nu\\n for \\nuN\\n=\\n01\\n1\\n,, ,\\n…\\n−\\n) of transfor-\\nmation matrix \\nA\\n are real and orthonormal.\\n In accordance with Eq. (7-7),\\n \\nHI\\nss\\ns s\\nkl k\\nT\\nlk l\\nkl\\nkl\\n,\\n== =\\n≠\\n=\\n⎧\\n⎨\\n⎩\\nd\\n0\\n1\\n \\n(7-30)\\nDIP4E_GLOBAL_Print_Ready.indb   469\\n6/16/2017   2:09:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 470}),\n",
       " Document(page_content='470\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nThe assumed orthonormality allows forward transforms to be computed without \\nexplicit reference to a forward transformation kernel—that is, \\nt\\n = \\nAf\\n where \\nA\\n is a \\nfunction of the inverse transformation kernal \\nsxu\\n(,\\n)\\n alone. It is left as an exercise \\nfor the reader (see Problem 7.3) to show that for real orthonormal basis vectors\\n, \\nrxu sxu\\n(,\\n) (,) .\\n=\\nBecause the basis vectors of \\nA\\n are real and orthonormal,\\n the transform deﬁned \\nin Eq. (7-28) is called an \\northogonal transform\\n. It preserves inner products—i.e., \\nf f t t Af Af\\n12 12 1 2\\n,, ,\\n==\\n—and thus the distances and angles between vectors \\nbefore and after transformation.\\n Both the rows and the columns of \\nA\\n are ortho-\\nnormal bases and \\nAA A A I\\nTT\\n==\\n, so \\nAA\\n-\\n1\\n=\\nT\\n.\\n The result is that Eqs. (7-28) and \\n(7-29) are a \\nreversible transform pair\\n.\\n Substituting Eq. (7-29) into (7-28) yields \\nt\\nAf AA t t\\n== =\\nT\\n,\\n while substituting Eq. (7-28) into (7-29) gives \\nfA tA A Ff\\n==\\n=\\nTT\\n.\\nFor 2-D square arrays or images, Eqs. (7-16) and (7-17) become\\n \\nTuv f xyrxy uv\\ny\\nN\\nx\\nN\\n(,) (, ) (,,,)\\n=\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1\\n- -\\n \\n(7-31)\\nand\\n \\nfx y T u v s x y u v\\nv\\nN\\nu\\nN\\n(,) (,) (,,,)\\n=\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1\\n- -\\n \\n(7-32)\\nwhere \\nrxy uv\\n(,\\n,,)\\n and \\nsxyuv\\n(,\\n,,)\\n are forward and inverse transformation ker-\\nnels\\n, respectively. Transform \\nTuv\\n(,\\n)\\n and inverse transformation kernel \\nsxyuv\\n(,\\n,,)\\n \\nagain can be viewed as weighting coefficients and basis vectors\\n, respectively, with \\nEq. (7-32) defining a linear expansion of \\nfx y\\n(,\\n) .\\n As was noted in Chapter 2, forward \\ntransformation kernel \\nrxy uv\\n(,\\n,,)\\n is \\nseparable\\n if\\n \\nrxy uv r xur y v\\n(,\\n,,) (,) (,)\\n=\\n12\\n \\n(7-33)\\nand \\nsymmetric\\n if \\nr\\n1\\n is functionally equal to \\nr\\n2\\n so\\n \\nrxy uv r xur y v\\n(,\\n,,) (,)(,)\\n=\\n11\\n \\n(7-34)\\nIf the transformation kernels are real and orthonormal, and both \\nr\\n and \\ns\\n are sepa-\\nrable and symmetric\\n, the matrix equivalents of Eqs. (7-31) and (7-32) are\\n \\nTA F A\\n=\\nT\\n \\n(7-35)\\nand\\n \\nFA T A\\n=\\nT\\n \\n(7-36)\\nwhere \\nF\\n is an \\nNN\\n*\\n matrix containing the elements of \\nfx y\\n(,\\n) ,\\n \\nT\\n is its \\nNN\\n*\\n trans-\\nform,\\n and \\nA\\n is as previously defined in Eq. (7-24). The pre- and post-multiplications \\nof \\nF\\n by \\nA\\n and \\nA\\nT\\n in Eq. (7-35) compute the column and row transforms of \\nF\\n, respec-\\ntively. This, in effect, breaks the 2-D transform into two 1-D transforms, mirroring \\nthe process described in Section 4.11 for the 2-D DFT.\\nEquations (7-31) and \\n(7-32) are simpliﬁed ver-\\nsions of Eqs. (2-55) and \\n(2-56) with \\nM\\n = \\nN\\n.\\nSubstitute \\ns\\n for \\nr\\n in \\nEqs. (7-33) and (7-34) for \\nseparable and separable \\nsymmetric inverse ker-\\nnals, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   470\\n6/16/2017   2:09:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 471}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n471\\nEXAMPLE 7.2 :   A simple orthogonal transformation.\\nConsider the 2-element basis vectors\\n \\nss\\n01\\n1\\n2\\n1\\n1\\n1\\n2\\n1\\n1\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n   and   \\n-\\n and note they are orthonormal in accordance with Eq. (7-30):\\n \\nss s s\\nss s s\\n01 0 1\\n10 1 0\\n1\\n2\\n11\\n1\\n1\\n1\\n2\\n11 0\\n1\\n2\\n11\\n1\\n1\\n,(\\n)\\n,\\n==\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n==\\n==\\n[]\\nT\\nT\\n-\\n-\\n-\\n⎡ ⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n==\\n==\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n==\\n1\\n2\\n11 0\\n1\\n2\\n11\\n1\\n1\\n1\\n2\\n11 1\\n00 0 0\\n11\\n()\\n,(\\n)\\n,\\n-\\n+\\nss s s\\nss\\nT\\n=\\n==\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n==\\nss\\n11\\n1\\n2\\n11\\n1\\n1\\n1\\n2\\n11 1\\nT\\n-\\n-\\n+\\n()\\nSubstitution of \\ns\\n0\\n and \\ns\\n1\\n into Eq. (7-24) with \\nN\\n=\\n2 yields transformation matrix\\n \\nAss\\n=\\n[]\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n01\\n1\\n2\\n11\\n11\\nT\\n-\\n \\n(7-37)\\nand the transform of \\n22\\n*\\n matrix\\n \\nF\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n20\\n63\\n21 128\\nfollows from Eq. (7-35):\\n \\nT\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n1\\n2\\n11\\n11\\n20 63\\n21 128\\n11\\n11\\n1\\n2\\n41 191\\n2\\n--\\n-\\nT\\n1\\n16 5\\n11\\n11\\n1\\n2\\n232 150\\n66 64\\n116 75\\n33 32\\n--\\n-\\n-\\n-\\n-\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤ ⎤\\n⎦\\n⎥\\nIn accordance with Eq. (7-36), the inverse of transform \\nT \\nis\\n \\nF\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n1\\n2\\n11\\n11\\n116 75\\n33 32\\n11\\n11\\n1\\n2\\n83 4\\n2\\n-\\n-\\n--\\n-\\nT\\n3 3\\n149 107\\n11\\n11\\n20 63\\n21 128\\n--\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nDIP4E_GLOBAL_Print_Ready.indb   471\\n6/16/2017   2:09:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 472}),\n",
       " Document(page_content='472\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nFinally, we note \\nA\\n is an orthogonal transformation matrix for which\\n \\nAA\\nI\\nT\\nT\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n1\\n2\\n11\\n11\\n1\\n2\\n11\\n11\\n1\\n2\\n20\\n02\\n10\\n01\\n--\\nand \\nA\\n-1\\n = \\nA\\nT\\n. It is also interesting to note Eq. (7-37) is the transformation matrix of the discrete \\nFourier\\n, \\nHartley\\n, \\nCosine\\n, \\nSin\\n, \\nWalsh\\n-\\nHadamard\\n, \\nSlant\\n, and \\nHaar\\n transforms for 1- and 2-D inputs of size  \\n21\\n×\\n and \\n22\\n×\\n, respectively. These transforms are discussed in detail in Sections 7.6 through 7.9.\\nAlthough formulated for real orthonormal bases and square arrays\\n, Eqs. (7-35) \\nand (7-36) can be modiﬁed to accomodate a variety of situations, including rectan-\\ngular arrays, complex-valued basis vectors, and biorthonormal bases.\\nRECTANGULAR ARRAYS\\nWhen the arrays to be transformed are rectangular, as opposed to square, Eqs. (7-35) \\nand (7-36) become\\n \\nTA F A\\n=\\nMN\\nT\\n \\n(7-38)\\nand\\n \\nFA T A\\n=\\nM\\nT\\nN\\n \\n(7-39)\\nwhere \\nF\\n, \\nA\\nM\\n, and \\nA\\nN \\nare of size \\nMN\\n*\\n, \\nMM\\n*\\n,\\n and \\nNN\\n*\\n,\\n respectively. Both \\nA\\nM\\n \\nand \\nA\\nN\\n are defined in accordance with Eq. (7-24).\\nEXAMPLE 7.3 :   Computing the transform of a rectangular array.\\nA simple transformation in which \\nM\\n and \\nN\\n are 2 and 3, respectively, is \\n \\nTA F A\\n==\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n23\\n1\\n2\\n11\\n11\\n5 100 44\\n6 103 40\\n1\\n3\\n111\\n1 0 366 1 366\\n1\\nT\\n-\\n-\\n..\\n- -\\n--\\n-\\n1 366 0 366\\n1\\n6\\n11 203 84\\n13 4\\n111\\n1 0 366 1 36\\n..\\n..\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nT\\n6 6\\n1 1 366 0 366\\n121 6580 12 0201 96 1657\\n0 3 0873 1\\n-\\n--\\n-\\n..\\n...\\n.\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n. .8624\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nwhere matrices \\nF\\n, \\nA\\n2\\n, and \\nA\\n3\\n are as deﬁned in the ﬁrst step of the computation. As would be expected, \\n23\\n*\\n output transform \\nT\\n is the same size as \\nF\\n.\\n It is left as an exercise for the reader (see Problem 7.5) \\nto show that \\nA\\n3\\n is an orthogonal transformation matrix, and that the transformation is reversable using \\nEq. (7-39). The orthonormality of \\nA\\n2\\n was established in Example 7.2.\\nDIP4E_GLOBAL_Print_Ready.indb   472\\n6/16/2017   2:09:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 473}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n473\\nCOMPLEX ORTHONORMAL BASIS VECTORS\\nComplex-valued basis vectors are orthonormal if and only if\\n \\nss s s ss\\nkl lk k\\nT\\nlk l\\nkl\\nkl\\n,,\\n*\\n*\\n== = =\\n≠\\n=\\n⎧\\n⎨\\n⎩\\nd\\n0\\n1\\n \\n(7-40)\\nwhere * denotes the complex conjugate operation. When basis vectors are complex, \\nas opposed to real-valued,\\n Eqs. (7-35) and (7-36) become\\n \\nTA F A\\n=\\nT\\n \\n(7-41)\\nand\\n \\nFA T A\\n=\\n**\\nT\\n \\n(7-42)\\nrespectively. Transformation matrix \\nA\\n is then called a \\nunitary matrix\\n and Eqs\\n. (7-41) \\nand (7-42) are a \\nunitary transform\\n pair. An important and useful property of \\nA\\n is \\nthat \\nAA A A A A A A I\\n** * *\\n,\\nTT T T\\n====\\n so \\nAA\\n-\\n1\\n=\\n*\\n.\\nT\\n The 1-D counterparts of \\nEq. (7-41) and (7-42) are:\\n \\ntA f\\n=\\n  \\n(7-43)\\n  \\nfA t\\n=\\n*\\nT\\n \\n(7-44)\\nEXAMPLE 7.4 :   A transform with complex-valued basis vectors.\\nUnlike orthogonal transformation matrices, where the inverse of the transformation matrix is its trans-\\npose, the inverse of unitary transformation matrix\\n \\nA\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n1\\n3\\n111\\n1 0 5 0 866 0 5 0 866\\n1 0 5 0 866 0 5 0 866\\n-- -+\\n-+ --\\n.. ..\\n.. ..\\njj\\njj\\n⎤ ⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n \\n(7-45)\\nis its conjugate transpose. Thus,\\n \\nAA\\n*\\n.. ..\\n.. ..\\nT\\njj\\njj\\n=\\n⎡\\n⎣\\n1\\n3\\n111\\n1 0 5 0 866 0 5 0 866\\n1 0 5 0 866 0 5 0 866\\n-- -+\\n-+ --\\n⎢ ⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n*\\n.. ..\\n.. .\\nT\\njj\\njj\\n1\\n3\\n111\\n1 0 5 0 866 0 5 0 866\\n1 0 5 0 866 0 5 0\\n-- -+\\n-+ --\\n..\\n.. ..\\n..\\n866\\n1\\n3\\n111\\n1 0 5 0 866 0 5 0 866\\n1 0 5 0 866 0\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n-+ --\\n-- -\\njj\\nj\\n.. .\\n.. ..\\n..\\n5 0 866\\n111\\n1 0 5 0 866 0 5 0 866\\n1 0 5 0 866\\n+\\n-- -+\\n-+\\nj\\njj\\nj\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n-\\n--\\n05 08 6 6\\n1\\n3\\n300\\n030\\n003\\n..\\nj\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\nI\\nwhere \\nj\\n=\\n-\\n1\\n and matrix \\nA\\n is a unitary matrix that can be used in Eqs\\n. (7-41) through (7-44). It is easy \\nOrthogonal transforms\\n \\nare a special case of \\nunitary transforms\\n in \\nwhich the expansion \\nfunctions are real-valued. \\nBoth transforms preserve \\ninner products.\\nDIP4E_GLOBAL_Print_Ready.indb   473\\n6/16/2017   2:09:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 474}),\n",
       " Document(page_content=\"474\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nto show (see Problem 7.4) that when \\nA\\n*\\nT\\nA\\n = \\nI\\n, the basis vectors in \\nA\\n satisfy Eq. (7-40) and are thus \\northonormal.\\nBIORTHONORMAL BASIS VECTORS\\nExpansion functions \\nss s\\n01 1\\n,, ,\\np \\n-\\nN\\n in Eq. (7-24) are \\nbiorthonormal\\n if there exists a \\nset of \\ndual expansion functions\\n \\nss s\\n01 1\\n''\\np \\n'\\n-\\n,, ,\\nN\\n such that\\n \\nHI\\nss\\nkl k l\\nkl\\nkl\\n'\\n,\\n==\\n≠\\n=\\n⎧\\n⎨\\n⎩\\nd\\n0\\n1\\n \\n(7-46)\\nNeither the expansion functions nor their duals need be orthonormal themselves. \\nGiven a set of \\nbiorthonormal expansion functions\\n,\\n Eqs. (7-35) and (7-36) become\\n \\nTA F A\\n=\\n''\\nT\\n \\n(7-47)\\nand\\n \\nFA T A\\n=\\nT\\n \\n(7-48)\\nTransformation matrix \\nA\\n remains as defined in Eq.\\n (7-24); \\ndual transformation \\nmatrix\\n \\nAss s\\n'\\n'' '\\np\\n-\\n=\\n[]\\n01 1\\nN\\nT\\n is an \\nNN\\n*\\n matrix whose rows are transposed dual \\nexpansion functions\\n. When the expansion functions and their duals are identical—\\nthat is, when \\nss\\nuu\\n'\\n=\\n—Eqs. (7-47) and (7-48) reduce to Eqs. (7-35) and (7-36), respec-\\ntively\\n. The 1-D counterparts of Eqs. (7-47) and (7-48) are:\\n  \\ntA f\\n=\\n'\\n \\n(7-49)\\n \\nfA t\\n=\\nT\\n  \\n(7-50)\\nEXAMPLE 7.5 :   A biorthonormal transform.\\nConsider the real biorthonormal transformation matrices\\n \\nA\\n=\\n05\\n05 05 05\\n11 11\\n0 5303 0 5303 0 1768 0 1768\\n0 1768 0 176\\n.. ..\\n.. ..\\n..\\n--\\n--\\n-\\n8 8 0 5303 0 5303\\n05 05 05 05\\n02 5 0\\n-\\n--\\n'\\n..\\n....\\n.\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n   and   \\nA\\n.\\n...\\n....\\n...\\n25 0 25 0 25\\n1 0607 1 0607 0 3536 0 3536\\n0 3536 0 3536 1 0607\\n--\\n--\\n1 1 0607 .\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nIt is left as an exercise for the reader (see Problem 7.16) to show that \\nA\\n and \\nA\\n'\\n are biorthonormal. The \\ntransform of 1-D column vector \\nf\\n=\\n[]\\n30 11 210 6\\nT\\n is\\n \\ntA f\\n==\\n'\\n--\\n--\\n05 05 05 05\\n02 5 02 5 02 5 02 5\\n1 0607 1 0607 0 3536 0\\n....\\n....\\n.... 3\\n3536\\n0 3536 0 3536 1 0607 1 0607\\n30\\n11\\n210\\n6\\n....\\n--\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦ ⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n128 5\\n43 75\\n51 9723\\n209 6572\\n.\\n.\\n.\\n.\\n-\\nDIP4E_GLOBAL_Print_Ready.indb   474\\n6/16/2017   2:09:08 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 475}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n475\\nSince\\n \\nff f f\\n,,\\n==\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\nT\\n30 11 210 6\\n30\\n11\\n210\\n6\\n45 157\\nand \\ntt t t\\n,, ,\\n==\\nT\\n65 084\\n which is not equal to \\nff\\n,,\\n the transformation does \\nnot\\n preserve inner products\\n. \\nIt is, however, reversable:\\n \\nfA t\\n==\\nT\\n05 05 05 05\\n11 11\\n0 5303 0 5303 0 1768 0 1768\\n0 1768 0\\n.. ..\\n.. ..\\n.\\n--\\n--\\n-\\n.. . .\\n.\\n.\\n.\\n.\\n1768 0 5303 0 5303\\n128 5\\n43 75\\n51 9723\\n209 6572\\n--\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nT\\n⎡ ⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n30\\n11\\n210\\n6\\nHere, the forward and inverse transforms were computed using Eqs. (7-49) and (7-50), respectively.\\nF\\ninally, we note the bulk of the concepts presented in this section can be general-\\nized to continuous expansions of the form\\n \\nfx s x\\nuu\\nu\\n() ()\\n=\\n-\\na\\n=\\n∑\\n/H11009\\n/H11009\\n \\n(7-51)\\nwhere \\na\\nu\\n and the \\nsx\\nu\\n()\\n for \\nu\\n=±\\n±\\n012\\n,,,\\np\\n represent expansion coefficients and \\nbasis vectors of inner product space \\nC\\n([\\na\\n, \\nb\\n]),\\n respectively. For a given \\nfx\\n()\\n and basis \\nsx\\nu\\n()\\n for \\nu\\n=±\\n±\\n012\\n,,, ,\\np\\n the appropriate expansion coefficients can be computed \\nfrom the definition of the integral inner product of \\nC\\n([\\na\\n, \\nb\\n])—i.e\\n., Eq. (7-3)—and \\nthe general properties of all inner product spaces—i.e, Eqs. (7-10) through (7-15). \\nThus, for example, if \\nsx\\nu\\n()\\n for \\nu\\n=±\\n±\\n012\\n,,,\\np\\n \\nare orthonormal basis vectors of \\nC\\n([\\na\\n, \\nb\\n]),\\n \\na\\nuu\\nsxf x\\n=\\n() , ()\\n \\n(7-52)\\nHere, we have simply replaced \\ni\\n, \\nz\\n,\\n and \\nw\\ni\\n in Eq. (7-13) with \\nu\\n, \\nfx\\n()\\n,\\n and \\nsx\\nu\\n() .\\n In \\nthe next example\\n, Eq. (7-52) will be used in the derivation of the continuous Fourier \\nseries.\\nEXAMPLE 7.6 :   The Fourier series and discrete Fourier transform.\\nConsider the representation of a continuous periodic function of period \\nT\\n as a linear expansion of \\northonormal basis vectors of the form\\n \\nsx\\nT\\neu\\nu\\nju x T\\n()\\n, , ,\\n==\\n±\\n±\\n1\\n012\\n2\\np\\n   for \\np \\n \\n(7-53)\\nDIP4E_GLOBAL_Print_Ready.indb   475\\n6/16/2017   2:09:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 476}),\n",
       " Document(page_content='476\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nIn accordance with Eqs. (7-51) and (7-52),\\n \\nfx\\nT\\ne\\nT\\ne\\nu\\nju x T\\nu\\nu\\nju x T\\nu\\n()\\n=\\n=\\n-\\n-\\na\\na\\np\\np\\n1\\n1\\n2\\n2\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n=\\n∑\\n∑\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n \\n(7-54)\\nand\\n \\na\\np\\nuu\\nT\\nT\\nju x T\\nT\\nT\\nj\\nsxf x\\nT\\nef x d x\\nT\\nfx e\\n=\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n() , ()\\n()\\n()\\n*\\n-\\n-\\n-\\n2\\n2\\n2\\n2\\n2\\n1\\n1\\n2\\n2\\n2 2\\np\\nux T\\ndx\\n \\n(7-55)\\nWith the exception of the variable names and normalization (i.e., the use of \\n1\\nT\\n in the above two \\nequations as opposed to \\n1\\nT\\n in only one of them), Eqs. (7-54) and (7-55) are the familiar Fourier series \\nof Eqs\\n. (4-8) and (4-9) in Chapter 4. An almost identical derivation, which is left as an exercise for the \\nreader (see Problem 7.22), yields the following discrete counterparts of Eqs. (7-53) through (7-55):\\n \\nsxu\\nN\\neu N\\nju x N\\n(,)\\n,, ,\\n==\\n1\\n01 1\\n2\\np\\n   for \\np-\\n \\n(7-56)\\n \\nfx\\nN\\nTue\\nju x N\\nu\\nN\\n() ()\\n=\\n=\\n∑\\n1\\n2\\n0\\n1\\np\\n−\\n \\n(7-57)\\nand\\n \\nTu\\nN\\nfx e\\nju x N\\nx\\nN\\n() ()\\n=\\n=\\n∑\\n1\\n2\\n0\\n1\\n−\\n−\\np\\n \\n(7-58)\\nThe discrete complex basis vectors of Eq. (7-56) are an orthonormal basis of inner product space \\nC\\nN\\n. \\nEquations (7-58) and (7-57), except for the variable names and normalization, are the familiar discrete \\nFourier transform of Eqs. (4-44) and (4-45) in Chapter 4.\\nNow consider the use of Eqs. (7-55) and (7-58) in the computation of both the Fourier series and \\ndiscrete Fourier transform of \\nfx x\\n(\\n) sin( )\\n=\\n2\\np\\n of period \\nT\\n = 1.\\n In accordance with Eq. (7-55),\\n  \\nap\\np\\np\\np\\n1\\n12\\n12\\n21 1\\n12\\n12\\n2\\n1\\n1\\n2\\n2\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n-\\n-\\n-\\n2\\n2\\nex\\nd\\nx\\nex\\njx\\njx\\n()\\n*\\nsin( )\\nsin( )\\nd\\ndx\\nx j x x dx\\nxj\\nx\\n=\\n[]\\n=\\n-\\n-\\n--\\n12\\n12\\n2\\n22 2\\n1\\n4\\n2\\n2\\n2\\ncos( ) sin( ) sin( )\\nsin ( )\\npp p\\np\\np\\n1 1\\n8\\n40\\n5\\n12\\n12\\np\\np\\nsin( ) .\\nxj\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n-\\n-\\nDIP4E_GLOBAL_Print_Ready.indb   476\\n6/16/2017   2:09:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 477}),\n",
       " Document(page_content='7.2\\n  \\nMatrix-based Transforms\\n    \\n477\\nand, in the same way, \\na\\n-\\n1\\n05\\n=\\nj\\n..\\n Since all other coefﬁcients are zero, the resulting Fourier series is\\nf x je je\\njx jx\\n() . .\\n=\\n05 05\\n22\\n-\\n-\\npp\\n(7-59)\\nEquation (7-58) with \\nN\\n=\\n8 and \\nfx x\\n(\\n) sin( )\\n=\\n2\\np\\n for \\nx\\n=\\n01\\n7\\n,, ,,\\n…\\n on the other hand, yields\\nTu\\nju\\nju\\n()\\n.\\n.\\n=\\n=\\n=\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−\\n+\\n1\\n414 1\\n1 414 7\\n0 otherwise\\n \\n(7-60)\\nFigure 7.2 depicts both computations as “matrix multiplications” in which continuous or discrete basis \\nvectors (the rows of matrix \\nA\\n) are multiplied by a continuous or discrete function (column vector \\nf\\n) \\nand integrated or summed to produce a set of discrete expansion or transform coefﬁcients (column \\nvector \\nt\\n).\\n For the Fourier series, the expansion coefﬁcients are integral inner products of \\nsin( )\\n2\\np\\nx\\n and \\none of a potentially inﬁnite set of continuous basis vectors. For the DFT, each transform coefﬁcient is a \\ndiscrete inner product of \\nf\\n and one of eight discrete basis vectors using Eq. (7-2). Note since the DFT is \\nbased on complex orthonormal basis vectors, the transform can be computed as a matrix multiplication \\n[in accordance with Eq. (7-43)]. Thus, the inner products that generate the elements of transform \\nt\\n are \\nembedded in matrix multiplication \\nAf\\n. That is, each element of \\nt\\n is formed by multiplying one row of \\nA\\n—i.e., one discrete expansion function—element by element by \\nf\\n and summing the resulting products.\\nFIGURE 7.2\\n Depicting the continuous Fourier series and 8-point DFT of \\nfx x\\n(\\n) sin( )\\n=\\n2\\np\\n as “matrix multiplication\\ns.” \\nT\\nhe real and imaginary parts of all complex quantities are shown in blue and black, respectively. Continuous \\nand discrete functions are represented using lines and dots, respectively. Dashed lines are included to show that \\nss\\n53\\n=\\n*\\n, \\nss\\n62\\n=\\n*\\n, and \\nss\\n71\\n=\\n*\\n, effectively cutting the maximum frequency of the DFT in half. The negative indices to \\nthe left of \\nt\\n are for the F\\nourier series computation alone.\\n=\\n1234567\\nx\\n  =  0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nu\\n  =  0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nx\\n  =  0\\n1\\n2\\n3\\n4\\n-\\n3 or 5\\n-\\n2 or 6\\n-\\n1 or 7\\nu\\n  =  0\\na\\nu\\nTu\\n⇔\\n( ) or \\nt\\nfx fx\\n() ()\\n⇔\\n or \\nf\\nsx s x u s s s\\nu\\nT\\n() (,) [ ]\\n⇔=\\n or \\nA\\n01 7\\n…\\nDIP4E_GLOBAL_Print_Ready.indb   477\\n6/16/2017   2:09:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 478}),\n",
       " Document(page_content='478\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\n7 .3 CORRELATION  \\nExample 7.6 highlights the role of inner products in the computation of orthogo-\\nnal transform coefficients. In this section, we turn our attention to the relationship \\nbetween those coefficients and correlation.\\nGiven two continuous functions \\nfx\\n()\\n and \\ngx\\n()\\n,\\n the \\ncorrelation\\n of \\nf\\n and \\ng\\n,\\n denoted \\nfx\\ng\\n/H22845\\n() ,\\n/H9004\\n is deﬁned as\\n \\nfx f x g x x d x\\nfx g x x\\ng\\n/H22845\\n()\\n*\\n()( )\\n() ,(\\n/H9004/H9004\\n/H9004\\n/H11009\\n/H11009\\n=\\n=\\n−\\n2\\n+\\n+\\n \\n(7-61)\\nwhere the final step follows from Eq. (7-3) with \\na\\n=\\n-\\n/H11009\\n and \\nb\\n=\\n/H11009\\n.\\n Sometimes called \\nthe \\nsliding inner product\\n of \\nf\\n and \\ng\\n,\\n correlation measures the similarity of \\nfx\\n()\\n and \\ngx\\n()\\n as a function of their relative displacement \\n/H9004\\nx\\n. If \\n/H9004\\nx\\n=\\n0,\\n \\nff x g x\\ng\\n/H22845\\n()\\n() ,()\\n0\\n=\\n \\n(7-62)\\nand Eq. (7-52), which defines the coefficients of the continuous orthonormal expan-\\nsion in Eq.\\n (7-51), can be alternately written as\\n \\na\\nuu u\\nfs f s\\n==\\n,( )\\n/H22845\\n0  \\n(7-63)\\nThus, the expansion coefficients are \\nsingle-point correlations\\n in which the displace-\\nment \\n/H9004\\nx\\n is zero. Each \\na\\nu\\n measures the similarity of \\nfx\\n()\\n and one \\nsx\\nu\\n() .\\nThe discrete equivalents of Eqs. (7-61) through (7-63) are\\n \\nfg\\n/H22845\\n()\\n*\\nmf g\\nnn m\\nx\\n=\\n=\\n∑\\n+\\n-\\n/H11009\\n/H11009\\n \\n(7-64)\\n \\nfg f g\\n/H22845\\n() ,\\n0\\n=\\n \\n(7-65)\\nand\\n \\nTu\\nuu\\n() ,\\n()\\n==\\nsf s f\\n/H22845\\n0\\n \\n(7-66)\\nrespectively. Comments similar to those made in regard to Eq. (7-63) and contin-\\nuous series expansions also can be made with respect to Eq.\\n (7-66) and discrete \\northogonal transforms. Each element of an orthogonal transform [i.e., transform \\ncoefficient \\nTu\\n()\\n of Eq. (7-23)] is a single-point correlation that measures the similar-\\nity of \\nf\\n and vector \\ns\\nu\\n.\\n This powerful property of orthogonal transforms is the basis \\nupon which the sinusoidal interference in F\\nig. 2.45(a) of Example 2.11 in Chapter 2 \\nand Fig. 4.65(a) of Example 4.25 in Chapter 4 was identiﬁed and eliminated.\\n7.3\\nTo be precise, we should \\nuse the term \\ncross-corre-\\nlation\\n when \\nfx g x\\n() ()\\n≠\\nand \\nauto-correlation\\n \\nwhen \\nfx g x\\n() () .\\n=\\n Equa-\\ntion (7-61) is valid for \\nboth cases. \\nAs the name \\nsliding inner \\nproduct\\n suggests, visual-\\nize sliding one function \\nover another, multiplying \\nthem together, and \\ncomputing the area. \\nAs the area increases, \\nthe functions become \\nincreasingly similar.\\nThe equation for 2-D dis-\\ncrete correlation is given \\nin Table 4.3. In Eq. (7-64), \\nn\\n and \\nm\\n are integers, \\nf\\nn\\n \\ndenotes the \\nn\\nth element \\nof \\nf\\n, and \\ng\\nn\\n+\\nm\\n denotes the \\n()\\nnm\\n+\\nth  element of \\ng\\n. \\nEquation (7-66) follows \\nfrom Eqs. (7-65) \\nand (7-23).\\nDIP4E_GLOBAL_Print_Ready.indb   478\\n6/16/2017   2:09:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 479}),\n",
       " Document(page_content='7.4\\n  \\nBasis Functions in the Time-Frequency Plane\\n    \\n479\\nEXAMPLE 7.7 :   Correlation in the DFT of Example 7.6.\\nConsider again the 8-point DFT in Example 7.6 and note, in accordance with Eq. (7-56), the basis vec-\\ntors are complex exponentials of the following harmonically related angular frequencies: 0, 2\\np\\n, 4\\np\\n, 6\\np\\n, \\n8\\np\\n, 6\\np\\n, 4\\np\\n, and 2\\np\\n (aliasing reduces the last three frequencies from 10\\np\\n, 12\\np\\n, and 14\\np\\n, respectively). \\nSince discrete input \\nfx x\\n(\\n) sin( )\\n=\\n2\\np\\n is a single frequency sinusoid of angular frequency 2\\np\\n, \\nf\\n should be \\nhighly correlated with basis vectors \\ns\\n1\\n and \\ns\\n7\\n. As can be seen in Fig. 7.2, transform \\nt\\n does indeed reach its \\nmaximum at \\nu\\n = 1 and 7; it is nonzero at these two frequencies alone.\\n7 .4 BASIS FUNCTIONS IN THE TIME-FREQUENCY PLANE  \\nBecause transforms measure the degree to which a function resembles a selected \\nset of basis vectors, we now turn our attention to the basis vectors themselves. In \\nthe following discussions, the terms basis vector and basis function are synonomous.\\nAs can be seen in Fig. 7.3, where the basis vectors of some commonly encoun-\\ntered transforms are depicted, most orthogonal bases are mathematically related \\nsets of sinusoids, square waves, ramps, and other small waves called \\nwavelets\\n. If \\nht\\n()\\n \\nis a basis vector and \\ngt\\n()\\n is the function being transformed, transform coefﬁcient \\ng\\nh\\n/H22845\\n() ,\\n0\\n as noted in the previous section, is a measure of the similarity of \\ng\\n and \\nh\\n. \\nLarge values of \\ng\\nh\\n/H22845\\n()\\n0\\n indicate that \\ng\\n and \\nh\\n share important characteristics in time \\nand frequency (e.g., shape and bandwidth). Thus, if \\nh\\n is the ramp-shaped basis func-\\ntion at \\nu\\n = 1 in Fig. 7.3(d), transform coefﬁcient \\ng\\nh\\n/H22845\\n()\\n0\\n can be used to detect linear \\nbrightness gradients across a row of an image. If \\nh\\n is a sinusoidal basis function like \\nthose of Fig. 7.3(a), on the other hand, \\ng\\nh\\n/H22845\\n()\\n0\\n can be used to spot sinusoidal inter-\\nference patterns. Plots like those of Fig. 7.3, together with a similarity measure like \\ng\\nh\\n/H22845\\n() ,\\n0\\n can reveal a great deal about the time and frequency characteristics of the \\nfunction being transformed.\\nA purely objective descriptor of \\nh\\n, and thus of \\ng\\n for large values of \\ng\\nh\\n/H22845\\n() ,\\n0\\n is the \\nlocation of \\nh\\n on the \\ntime-frequency plane\\n of Fig. 7.4(a). Let \\np\\nth t h t\\nh\\n() () ()\\n=\\n22\\n be \\na probability density function with mean\\n \\nm\\nt\\nht\\nth\\ntd t\\n=\\n−\\n1\\n2\\n2\\n()\\n()\\n/H11009\\n/H11009\\n2\\n \\n(7-67)\\nand variance\\n \\nsm\\ntt\\nht\\nth t d t\\n2\\n2\\n2\\n2\\n1\\n=\\n−\\n()\\n() ( )\\n/H11009\\n/H11009\\n2\\n-\\n \\n(7-68)\\nand let \\np\\nfH f H f\\nH\\n() () ()\\n=\\n22\\n be a probability density function with mean\\n \\nm\\nf\\nHf\\nfH\\nfd f\\n=\\n−\\n1\\n2\\n2\\n()\\n()\\n/H11009\\n/H11009\\n2\\n \\n(7-69)\\nand variance\\n \\nsm\\nff\\nHf\\nfH f d f\\n2\\n2\\n2\\n2\\n1\\n=\\n−\\n()\\n() ( )\\n/H11009\\n/H11009\\n2\\n-\\n \\n(7-70)\\n7.4\\nIn our introduction to \\nthe \\ntime-frequency plane\\n, \\nindependent variables \\nt\\n \\nand \\nf\\n, rather than spatial \\nvariables \\nx\\n and \\nu\\n, are \\nemployed. Continuous \\nfunctions \\ngt\\n( ) and \\nhf\\n()  \\ntake the place of \\nfx\\n()  \\nand \\nsx\\nu\\n( ) in the previous \\nsections. Though the \\nconcepts are presented \\nusing continuous func-\\ntions and variables, they \\nare equally applicable \\nto discrete functions and \\nvariables. \\nIn Eq. (7-67), each value \\nof \\nt\\n is weighted by \\npt\\nh\\n()  \\nto compute a weighted \\nmean with respect to \\ncoordinate \\nt\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   479\\n6/16/2017   2:09:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 480}),\n",
       " Document(page_content='480\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nu\\n = 0\\nu\\n = 1\\nu\\n = 2\\nu\\n = 3\\nu\\n = 4\\nu\\n = 5\\nu\\n = 6\\nu\\n = 7\\nu\\n = 8\\nu\\n = 9\\nu\\n = 10\\nu\\n = 11\\nu\\n = 12\\nu\\n = 13\\nu\\n = 14\\nu\\n = 15\\nu\\n = 0\\nu\\n = 1\\nu\\n = 2\\nu\\n = 3\\nu\\n = 4\\nu\\n = 5\\nu\\n = 6\\nu\\n = 7\\nu\\n = 8\\nu\\n = 9\\nu\\n = 10\\nu\\n = 11\\nu\\n = 12\\nu\\n = 13\\nu\\n = 14\\nu\\n = 15\\nDFT\\nDCT\\nSLT\\nWHT\\nHAAR\\nDB4\\nSTD\\nBIOR3.1\\nb\\na\\nc\\ng\\nf\\nh\\nd\\ne\\nFIGURE 7.3\\nBasis vectors \\n(for \\nN\\n = 16) of \\nsome commonly \\nencountered \\ntransforms:  \\n(a) Fourier basis \\n(real and imagi-\\nnary parts),  \\n(b) discrete  \\nCosine basis,  \\n(c) Walsh-Had-\\namard basis,  \\n(d) Slant basis,  \\n(e) Haar basis,  \\n(f) Daubechies \\nbasis,  \\n(g) Biorthogonal \\nB-spline basis and \\nits dual, and  \\n(h) the standard \\nbasis, which is \\nincluded for refer-\\nence only (i.e., not \\nused as the basis \\nof a transform).\\nDIP4E_GLOBAL_Print_Ready.indb   480\\n6/16/2017   2:09:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 481}),\n",
       " Document(page_content='7.4\\n  \\nBasis Functions in the Time-Frequency Plane\\n    \\n481\\nwhere \\nf\\n denotes frequency and \\nHf\\n()\\n is the Fourier transform of \\nht\\n()\\n.\\n Then the \\nenergy\\n†\\n of basis function \\nh\\n, as illustrated in Fig. 7.4(a), is concentrated at \\n(, )\\nmm\\ntf\\non the time-frequency plane. The majority of the energy falls in a rectangular region, \\ncalled a \\nHeisenberg box\\n or \\ncell\\n, of area \\n4\\nss\\ntf\\n such that\\nss\\np\\ntf\\n22\\n2\\n1\\n16\\nÚ\\n(7-71)\\nSince the \\nsupport\\n of a function can be defined as the set of points where the func-\\ntion is nonzero\\n, \\nHeisenberg’s uncertainity principle\\n tells us that it is impossible for a \\nfunction to have finite support in both time and frequency. Equation (7-71), called \\nthe \\nHeisenberg-Gabor inequaltiy\\n, places a lower bound on the area of the Heisen-\\nberg cell in Fig. 7.4(a), revealing that \\ns\\nt\\n and \\ns\\nf\\n cannot both be arbitrarily small. \\nThus, while basis function \\nd\\n()\\ntt\\n-\\n0\\n in Fig. 7.4(b) is perfectly \\nlocalized\\n in time [that is, \\ns\\nt\\n=\\n0\\n since the width of \\nd\\n()\\ntt\\n-\\n0\\n is zero], its spectrum is nonzero on the entire \\nf\\n-axis. \\nThat is, since \\n/H5219\\ndp\\n(\\n) exp( )\\ntt j f t\\n--\\n00\\n2\\n{}\\n=\\n and \\nexp( )\\n−=\\njf t\\n21\\n0\\np\\n for all \\nf\\n,  \\ns\\nf\\n=\\n/H11009\\n.\\nThe result is an infinitesimally narrow, infinitely high Heisenberg cell on the time-\\nfrequenc\\ny plane. Basis function \\nexp( ) 2\\n0\\np\\nft\\n of Fig. 7.4(c), on the other hand, is essen-\\ntially nonzero on the entire time axis\\n, but is perfectly localized in frequency. Because \\n/H5219\\nexp\\n( ) ( ),\\n2\\n00\\npd\\nft f f\\n{}\\n=\\n-\\n spectrum \\nd\\n()\\nff\\n-\\n0\\n is zero at all frequencies other than \\nf\\n = \\nf\\n0\\n. The resulting Heisenberg cell is infinitely wide \\n()\\ns\\nt\\n=\\n/H11009\\n and infinitesimally \\nsmall in height \\n() .\\ns\\nf\\n=\\n0\\n As Figs. 7.4(b) and (c) illustrate, perfect localization in time \\nis accompanied by a loss of localization in frequenc\\ny and vice versa. \\nReturning again to Fig. 7.3, note the DFT basis in Fig. 7.3(a) and the standard \\nbasis in Fig. 7.3(h) are discrete examples (for \\nN\\n = 16) of the impulse and complex \\n† The energy of continuous function \\nht\\n()\\n is \\n-\\n/H11009\\n/H11009\\n2\\nht d t\\n()\\n.\\n2\\nThe constant on the right \\nside of Eq. (7-71) is ¼ if \\nstated in terms of angular \\nfrequency \\nv\\n. Equality is \\npossible, but only with a \\nGaussian basis function, \\nwhose transform is also a \\nGaussian function.\\nb\\na\\nc\\nFIGURE 7.4\\n (a) Basis function localization in the time-frequency plane. (b) A standard basis function, its spectrum, \\nand location in the time-frequency plane. (c) A complex sinusoidal basis function (with its real and imaginary parts \\nshown as solid and dashed lines, respectively), its spectrum, and location in the time-frequency plane.\\n+\\n1\\n0\\n1.2\\n0\\n1\\n-1\\n0\\n0\\nt\\nt\\nf\\nf\\nf\\nt\\nf\\nt\\nf\\nt\\nTime-frequency Plane\\nTime-frequency Plane\\n1\\n2\\ns\\nf\\n2\\ns\\nt\\nm\\nf\\nm\\nt\\npf\\nH\\n()\\npt\\nh\\n()\\nd\\n()\\ntt\\n−\\n0\\nd\\n()\\nff\\n−\\n0\\nexp( ) 2\\n0\\np\\nft\\nexp( ) 2\\n0\\np\\ntf\\nDIP4E_GLOBAL_Print_Ready.indb   481\\n6/16/2017   2:09:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 482}),\n",
       " Document(page_content='482\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nexponential functions in Figs. 7.4(c) and (b), respectively. Every other basis in the \\ntop half of Fig. 7.3 is both frequency ordered on index \\nu\\n and of width or support 16. \\nFor a given \\nu\\n, their locations in the time-frequency plane are similar. This is particu-\\nlarly evident when \\nu\\n is 8 and the basis functions are identical—as are their Heisen-\\nberg cells. For all other \\nu\\n, Heisenberg cell parameters \\nm\\nt\\n, \\ns\\nt\\n, \\nm\\nf\\n,\\n and \\ns\\nf\\n are close \\nin value, with small differences accounting for the distinctive shapes of the cosine, \\nramp, and square wave. In a similar manner, the basis functions in the bottom half of \\nFig. 7.3, with the exception of the standard basis already discussed, are also similar \\nfor a given \\nu\\n. These basis functions are scaled and shifted small waves, called \\nwave-\\nlets\\n, of the form\\n \\ncc t\\nt\\ns\\nss\\ntt\\n,\\n() ( )\\n=\\n22\\n2\\n-\\n \\n(7-72)\\n \\nwhere \\ns\\n and \\nt\\n are integers and \\nmother wavelet\\n \\nc\\n()\\nt\\n is a real, square-integrable func-\\ntion with a \\nbandpass-like spectrum\\n.\\n Parameter \\nt\\n determines the position of \\nc\\nt\\ns\\nt\\n,\\n()\\n on \\nthe \\nt\\n-axis\\n, \\ns\\n determines its width—that is, how broad or narrow it is along the \\nt\\n-axis, \\nand \\n2\\n2\\ns\\n controls its amplitude.\\nIn conjunction with a properly designed mother wavelet, Eq. (7-72) generates a \\nbasis that is characterized by the Heisenberg cells on the right side of Fig. 7.5. Let-\\nting \\n/H9023\\nf\\n()\\n be the Fourier transform of \\nc\\n()\\n,\\nt\\n the transform of time-scaled wavelet \\nc\\n()\\n2\\ns\\nt\\n is\\n \\n/H5219\\nc\\n()\\n2\\n1\\n2\\n2\\ns\\ns\\ns\\nt\\nf\\n{}\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n/H9023\\n \\n(7-73)\\nand for positive values of \\ns\\n,\\n the spectrum is stretched—shifting each frequency \\ncomponent higher by a factor of \\n2\\ns\\n. As was the case for the rectangular pulse in \\nExample 4.1, compressing time expands the spectrum. This is illustrated graphically \\nin Figs. 7.5(b)–(d). Note the width of the basis function in Fig. 7.5(c) is half of that \\nin (d), while the width of its spectrum is double that of (d). It is shifted higher in fre-\\nquency by a factor of two. The same can be said for the basis function and spectrum \\nin Fig. 7.5(b) when compared to (c). This halving of support in time and doubling of \\nsupport in frequency produces Heisenberg cells of differing widths and heights, but \\nof equal area. Moreover, each row of cells on the right of Fig. 7.5 represents a unique \\nscale \\ns\\n and range of frequencies. The cells within a row are shifted with respect to \\none another in time. In accordance with Eq. (4-71) and Table 4.4 of Chapter 4, if \\nc\\n()\\nt\\n \\nis shifted in time by \\nt\\n,\\n \\n/H5219\\nct\\npt\\n()\\nte f\\njf\\n-\\n-\\n{}\\n=\\n()\\n2\\n/H9023\\n \\n(7-74)\\nThus, \\n/H5219\\nct\\n()\\ntf\\n-\\n{}\\n=\\n()\\n/H9023\\n and the spectra of the time-shifted wavelets are identical. \\nT\\nhis is demonstrated by the basis functions in Figs. 7.5(a) and (b). Note their Heisen-\\nberg cells are identical in size and differ only in position.\\nA principle consequence of the preceding comments is that each wavelet basis \\nfunction is characterized by a unique spectrum and location in time. Thus, the \\ntransform coefﬁcients of a wavelet-based transform, as inner products measuring \\nThe DFT basis func-\\ntions do not appear to \\nbe frequency ordered \\nbecause of aliasing. See \\nExample. 7.6.\\nAs will be seen in  \\nSection 7.10, the func-\\ntions corresponding to \\nu\\n = 0 in Fig. 7.3 have \\nlowpass spectra and are \\ncalled \\nscaling functions\\n.\\nThe proof of Eq. (7-73) \\nis left as an exercise for \\nthe reader (see Prob-\\nlem 7.24).\\nDIP4E_GLOBAL_Print_Ready.indb   482\\n6/16/2017   2:09:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 483}),\n",
       " Document(page_content='7.5\\n  \\nBasis Images\\n    \\n483\\nthe similarity of the function being transformed and the associated wavelet basis \\nfunctions, provide both frequency and temporal information. They furnish the \\nequivalent of a musical score for the function being transformed, revealing not \\nonly what notes to play but also \\nwhen\\n to play them. This is true for all the wavelet \\nbases depicted in the bottom half of Fig. 7.3. The bases in the top half of the ﬁgure \\nprovide only the notes; temporal information is lost in the transformation process or \\nis difﬁcult to extract from the transform coefﬁcients (e.g., from the phase component \\nof a Fourier transform).\\n7 .5 BASIS IMAGES  \\nSince inverse transformation kernel \\nsxyuv\\n(,\\n,,)\\n in Eq. (7-32) of Section 7.2 depends \\nonly on indices \\nx\\n, \\ny\\n, \\nu\\n, \\nv\\n,\\n and not on the values of \\nfx y\\n(,\\n)\\n or \\nTuv\\n(,\\n) ,\\n Eq. (7-32) can be \\nalternately written as the matrix sum\\n \\nFS\\n=\\n= =\\n∑ ∑\\nTuv\\nuv\\nv\\nN\\nu\\nN\\n(,)\\n,\\n0\\n1\\n0\\n1\\n- -\\n \\n(7-75)\\nwhere \\nF\\n is an \\nNN\\n*\\n matrix containing the elements of \\nfx y\\n(,\\n)\\n and\\n \\nS\\nuv\\ns uv s uv s N uv\\nsu v\\n,\\n(,, ,) (,, ,) (, , ,)\\n(, , , )\\n=\\n00 01 0 1\\n10\\np-\\nop o\\noo p o\\noo p o\\no\\no\\n-- p - -\\nsN uv sN uv sN N uv\\n(, , , ) (, , , ) (,, , )\\n10\\n11\\n1 1\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n \\n(7-76)\\n7.5\\nb\\na\\nc\\nd\\nFIGURE 7.5\\nTime and  \\nfrequency  \\nlocalization \\nof 128-point \\nDaubechies basis \\nfunctions.\\nSpectrum\\nBasis Function\\nTime-Frequency Plane\\n0.6\\n-0.6\\n0.6\\n-0.6\\n0.4\\n-0.4\\n-0.4\\n0.6\\n0\\n2\\n0\\n0\\n3\\n4\\n0\\n2\\n0\\n0\\n0\\n0\\nt\\nf\\nt\\nt\\nt\\nf\\nf\\nf\\nDIP4E_GLOBAL_Print_Ready.indb   483\\n6/16/2017   2:09:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 484}),\n",
       " Document(page_content='484\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nfor \\nuv N\\n,,\\n, , .\\n=\\n01 1\\np -\\n \\nF\\n is then explicitly defined as a linear combination of \\nN\\n2\\n \\nmatrices of size \\nNN\\n*\\n—that is, the \\nS\\nuv\\n,\\n for \\nuv N\\n,,\\n, , .\\n=\\n01 1\\np -\\n If the underlying  \\nsxyuv\\n(,\\n,,)\\n are real-valued, separable, and symmetric,\\n \\nSs s\\nuv u v\\nT\\n,\\n=\\n \\n(7-77)\\nwhere \\ns\\nu\\n and \\ns\\nv\\n are as previously defined by Eq. (7-22). In the context of digital \\nimage processing, \\nF\\n is a 2-D image and the \\nS\\nuv\\n,\\n are called \\nbasis images\\n. They can be \\narranged in an \\nNN\\n*\\n array, as shown in Fig. 7.6(a), to provide a concise visual rep-\\nresentation of the 2-D basis functions they represent.\\nEXAMPLE 7.8 :   The basis images of the standard basis.\\nThe basis in Fig. 7.3(h) is a speciﬁc instance (for \\nN\\n = 16) of \\nstandard basis\\n \\nee e\\n01 1\\n,, , ,\\np \\n-\\nN\\n{}\\n where \\ne\\nn\\n \\nis an \\nN\\n*\\n1\\n column vector whose \\nn\\nth element is 1 and all other elements are 0.\\n Because it is real and \\northonormal, the corresponding orthogonal transformation matrix [see Eq. (7-24)] is \\nA\\n = \\nI\\n, while the \\ncorresponding 2-D transform [see Eq. (7-35)] is \\nTA F A I F I F\\n==\\n=\\nTT\\n.\\n That is, the transform of \\nF\\n with \\nrespect to the standard basis is \\nF\\n—a conﬁrmation of the fact that when a discrete function is written in \\nvector form,\\n it is represented implicitly with respect to the standard basis.\\nFigure 7.6(b) shows the basis images of a 2-D standard basis of size \\n88\\n*\\n.\\n Like the 1-D basis vectors \\nin F\\nig. 7.3(h), which are nonzero at only one instant of time (or value of \\nx\\n), the basis images in Fig. 7.6(b) \\nare nonzero at only one point on the \\nxy\\n-plane. This follows from Eq. (7-77), since \\nS\\nee E\\nuv u v\\nT\\nuv\\n,,\\n,\\n==\\n \\nwhere \\nE\\nu\\n,\\nv\\n is an \\nNN\\n*\\n matrix of zeros with a 1 in the \\nu\\nth row and \\nv\\nth column.\\n In the same way, the DFT \\nbasis images in Fig. 7.7 follow from Eq. (7-77), Eq. (7-22), and the defining equation of the 1-D DFT \\nexpansion functions [i.e., Eq. (7-56)]. Note the DFT basis image of maximum frequency occurs when \\nu\\n \\nand \\nv\\n are 4, just as the 1-D DFT basis function of maximum frequency occurred at \\nu\\n = 4 in Fig. 7.2.\\n7 .6 FOURIER-RELATED TRANSFORMS  \\nAs was noted in Chapter 4, the Fourier transform of a real function is complex-valued. \\nIn this section, we examine three Fourier-related transforms that are real rather \\n7.6\\nb a\\nFIGURE 7.6\\n(a) Basis image \\norganization and \\n(b) a standard \\nbasis of size \\n88\\n*\\n. \\nF\\nor clarity, a gray \\nborder has been \\nadded around \\neach basis image. \\nThe origin of each \\nbasis image (i.e., \\nx\\n = \\ny\\n = 0) is at its \\ntop left.\\nSS\\nS\\nS\\nSS\\n00 01\\n0 1\\n10\\n10\\n1 1\\n,,\\n,\\n,\\n,,\\npp\\no\\no\\no\\npp\\n-\\n--\\n-\\nN\\nNN\\nN\\n/downslopeellipsis\\n/downslopeellipsis\\nu\\nv\\n01234567\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nDIP4E_GLOBAL_Print_Ready.indb   484\\n6/16/2017   2:09:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 485}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n485\\nthan complex-valued—the \\ndiscrete Hartley transform\\n, \\ndiscrete cosine transform\\n, and \\ndiscrete sine transform\\n. All three transforms avoid the computational complexity of \\ncomplex numbers and can be implemented via fast FFT-like algorithms.\\nTHE DISCRETE HARTLEY TRANSFORM\\nThe transformation matrix of the \\ndiscrete Hartley transform\\n (DHT) is obtained by \\nsubstituting the inverse transformation kernel\\n \\nsxu\\nN\\nux\\nN\\nN\\nux\\nN\\nux\\nN\\n(,)\\ncos sin\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n+\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n12\\n12 2\\ncas\\np\\npp\\n⎤ ⎤\\n⎦\\n⎥\\n \\n(7-78)\\nwhose separable 2-D counterpart is\\n \\nsxyuv\\nN\\nux\\nN\\nN\\nvy\\nN\\n(,,,)\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n12 12\\nca\\nsc\\na\\ns\\npp\\n \\n(7-79)\\ninto Eqs. (7-22) and (7-24). Since the resulting DHT transformation matrix—denoted \\nA\\nHY\\n in Fig. 7.8—is real, orthogonal, and symmetric, \\nAAA\\nHY HY HY\\n==\\nT\\n-\\n1\\n and \\nA\\nHY\\n \\ncan \\nbe used in the computation of both forward and inverse transforms. For 1-D trans-\\nforms, \\nA\\nHY\\n is used in conjunction with Eqs. (7-28) and (7-29) of Section 7.2; for 2-D \\ntransforms, Eqs. (7-35) and (7-36) are used. Since \\nA\\nHY\\n is symmetric, the forward and \\ninverse transforms are identical. \\nFunction cas, an acronym \\nfor the \\ncosine-and-sin\\n \\nfunction, is deﬁned as\\ncas( ) cos( ) sin( ).\\nuuu\\n=\\n+\\nWe will not consider \\nthe non-separable form   \\nsxyuv\\nN\\nux vy\\nN\\n(,,,)\\n()\\n.\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n12\\ncas\\np\\n+\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\nv\\n-\\nj\\n-\\nj\\nv\\n-\\n1\\n-\\nv\\nj\\nj\\nv\\n1\\n-\\nj\\n-\\n1\\nj\\n1\\n-\\nj\\n-\\n1\\nj\\n1\\n-\\nj\\nv\\nj\\nv\\n-\\n1\\nj\\nv\\n-\\nj\\n-\\nv\\n1\\n-\\n1\\n1\\n-\\n1\\n1\\n-\\n1\\n1\\n-\\n1\\n1\\n-\\nv\\n-\\nj\\nj\\nv\\n-\\n1\\nv\\nj\\n-\\nj\\nv\\n1\\nj\\n-\\n1\\n-\\nj\\n1\\nj\\n-\\n1\\n-\\nj\\n1\\nj\\nv\\nj\\n-\\nv\\n-\\n1\\n-\\nj\\nv\\n-\\nj\\nv\\n\\x19 \\x90\\nb a\\nc\\nFIGURE 7.7\\n (a) Tranformation matrix \\nA\\nF\\n of the discrete Fourier transform for \\nN\\n = 8, where \\nv\\np\\n=\\ne\\nj\\n-\\n28\\n or \\n().\\n12\\n-\\nj\\n \\n(b) and (c) \\nThe real and imaginary parts of the DFT basis images of size \\n88\\n*\\n.\\n For clarity,  a black border has been \\nadded around each basis image. For 1-D transforms, matrix \\nA\\nF\\n is used in conjunction with Eqs. (7-43) and (7-44); \\nfor 2-D transforms, it is used with Eqs. (7-41) and (7-42).\\nDIP4E_GLOBAL_Print_Ready.indb   485\\n6/16/2017   2:09:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 486}),\n",
       " Document(page_content='486\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nNote the similarity of the harmonically related DHT basis functions in Fig. 7.8(a) \\nand the real part of the DFT basis functions in Fig. 7.2. It is easy to show that\\n \\nAA A\\nA\\nHY\\nF\\nF\\nF\\nal ag\\nal\\n=\\n{} {}\\n=\\n{}\\nRe Im\\nRe ( )\\n-\\n+\\n1\\nj\\n \\n(7-80)\\nwhere \\nA\\nF\\n denotes the unitary transformation matrix of the DFT. Furthermore, since \\nthe real part of the DFT kernel [see Eq. (7-56)] is\\n \\nRe ( , ) Re\\ncos\\nsx u\\nN\\ne\\nN\\nux\\nN\\nju x N\\nF\\n{}\\n=\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n11 2\\n2\\np\\np\\n \\n(7-81)\\nand triginometric identity \\ncas\\nuu p\\n()\\n=\\n()\\n24\\ncos\\n-\\n can be used to rewrite the dis-\\ncrete Hartley kernel [see Eq.\\n (7-78)] as\\n \\nsx u\\nN\\nux\\nN\\nH\\n(,) c o s\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n22\\n4\\npp\\n-\\n \\n(7-82)\\nthe basis functions of the discrete Fourier and Hartley transforms are scaled and \\nshifted versions of one another—i.e\\n., scaled by the \\n2\\n and shifted by \\np\\n4.\\n The \\nshift is clearly evident when comparing F\\nigs. 7.2 and 7.8(a). Additionally, for a \\ngiven value of \\nN\\n and sampling interal \\n/H9004\\nT\\n,\\n the Fourier and Hartley transforms \\nhave the same frequenc\\ny resolution \\n/H9004/H9004\\nuN T\\n=\\n1( ) ,\\n same range of frequencies \\n05 051 1 2\\n.. ( ) ( ) ,\\nRT T\\n==\\n/H9004/H9004\\n and are both undersampled when \\nuN\\n7\\n2.\\n Compare \\nF\\nigs. 7.2 and 7.8(a) for \\nu\\n=\\n56\\n7\\n,,.\\n Finally, we note the \\n88\\n*\\n basis images of the two \\ntransforms are also similar\\n. As can be seen in Figs. 7.8(c) and 7.7(b), for example, the \\nbasis images of maximum frequency occur when \\nu\\n and \\nv\\n are \\nN\\n2 or 4.\\nIn Eqs. (7-81) and (7-82), \\nsubscripts \\nHY\\n and \\nF\\n are \\nused to denote the  \\nHartley and Fourier \\nkernels, respectively. \\nAliasing reduces the \\nfrequency range to \\n05\\n.,\\nR\\n \\nwhere \\nR\\n \\nis as deﬁned by \\nEq. (4.51).\\nb a\\nc\\nFIGURE 7.8\\n The transformation matrix and basis images of the discrete Hartley transform for \\nN\\n = 8: (a) Graphical \\nrepresentation of orthogonal transformation matrix \\nA\\nHY\\n, (b) \\nA\\nHY\\n \\nrounded to two decimal places, and (c) 2-D basis \\nimages. For 1-D transforms, matrix \\nA\\nHY\\n is used in conjunction with Eqs. (7-28) and (7-29); for 2-D transforms, it is \\nused with Eqs. (7-35) and (7-36).\\n 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35\\n 0.35 0.50 0.35 0 \\n-\\n0.35 \\n-\\n0.50 \\n-\\n0.35 0\\n 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35\\n 0.35 0 \\n-\\n0.35 0.50 \\n-\\n0.35 0 0.35 \\n-\\n0.50\\n 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35\\n 0.35 \\n-\\n0.50 0.35 0 \\n-\\n0.35 0.50 \\n-\\n0.35 0\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35\\n 0.35 0 \\n-\\n0.35 \\n-\\n0.50 \\n-\\n0.35 0 0.35 0.50\\nDIP4E_GLOBAL_Print_Ready.indb   486\\n6/16/2017   2:09:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 487}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n487\\nEXAMPLE 7.9 :   DHT and DFT reconstruction.\\nConsider discrete function \\nf\\n=\\n[]\\n11000000\\nT\\n and its discrete Fourier transform\\n \\nt\\nF\\n=\\n07 1 06 02 5 03 5 03 5 01 02 5 0 01 02 5 03 5 03 5 06\\n.... ... ... ..\\n−− − ++\\njj j jj\\n+ +\\nj\\nT\\n02 5\\n.\\n[\\n]\\nwhere \\nt\\nAf\\nFF\\n=\\n and \\nAA A\\nFF r F j\\n=+\\nj\\n \\nis the \\n88\\n×\\n unitary transformation matrix of Fig. 7.7(a). The real \\nand imaginary parts of \\nt\\nF\\n, denoted \\nt\\nFr\\n and \\nt\\nFj\\n, are\\n \\nt\\nt\\nFr\\nFj\\n=\\n[]\\n=\\n0 71 0 60 0 35 0 10 0 0 10 0 35 0 60\\n0 0 25 0 35 0 25 0 0\\n.... ...\\n...\\nT\\n---\\n. ...\\n25 0 35 0 25\\n[]\\nT\\nand discrete Hartley transform \\nt\\nAf A AfA fA ft t\\nHY HY Fr Fj Fr Fj Fr Fj\\n== = =\\n()\\n−− −\\n is\\n \\nt\\nHY\\n=\\n[]\\n0 71 0 85 0 71 0 35 0 0 15 0 0 35\\n.... . .\\n-\\nT\\nIn accordance with Eq. (7-17), \\nf\\n can be written as\\n \\nfx T u s x u x\\nu\\n() () (,) ,, ,\\n==\\n=\\n∑\\nHY HY\\n   for \\n0\\n7\\n01 7\\n…\\nwhere \\nf\\n=\\n[]\\nff f\\nT\\n() () ()\\n01 7\\n…\\n and \\nt\\nHY HY HY\\nHY\\n=\\n[]\\nTT T\\nT\\n() () () .\\n01 7\\n…\\n Thus, \\nf\\n can be recon-\\nstructed \\nfrom \\nt\\nHY\\n as a sum of products involving the computed transform coefﬁcients and correspond-\\ning basis functions. In Fig. 6.9(a), such a reconstruction is done progressively, beginning with the average \\nor DC value of \\nf\\n (for \\nu\\n = 0) at the top of the ﬁgure and converging to \\nf\\n (for \\nu\\n = 0, 1, …, 7) at the bottom \\nof the ﬁgure. As higher frequency basis functions are included in the sum, the reconstructed function \\nbecomes a better approximation of \\nf\\n, with perfect reconstruction achieved when all eight weighted basis \\nfunctions are summed to generate the equivalent of inverse discrete Hartley transform \\nfAt\\n=\\nHY HY\\nT\\n.\\n A \\nsimilar progression is shown in F\\nig. 7.9(b) for the DFT.\\nTHE DISCRETE COSINE TRANSFORM\\nThe transformation matrix of the most commonly encountered form of the \\ndiscrete \\ncosine transform\\n (DCT) is obtained by substituting the inverse transformation \\nkernal\\n \\nsxu u\\nxu\\nN\\n(,) () c o s\\n()\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\na\\np\\n21\\n2\\n+\\n \\n(7-83)\\nwhere\\n \\na\\n()\\n,, ,\\nu\\nN\\nu\\nN\\nuN\\n=\\n=\\n=\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n1\\n0\\n2\\n12 1\\nfor \\nfor \\n…\\n-\\n \\n(7-84)\\nThere are eight standard \\nDCT variants and they \\nassume different sym-\\nmetry conditions. For \\nexample, the input could \\nbe assumed to be even \\nabout a sample or about \\na point halfway between \\ntwo samples.\\nDIP4E_GLOBAL_Print_Ready.indb   487\\n6/16/2017   2:09:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 488}),\n",
       " Document(page_content='488\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\ninto Eqs. (7-22) and (7-24). The resulting transformation matrix, denoted as \\nA\\nC\\n \\nin Fig. 7.10, is real and orthogonal, but not symmetric. The underlying basis func-\\ntions are \\nharmonically related cosines of frequency 0 to \\nRN N T\\n=\\n[] [ ]\\n() ( ) ;\\n-\\n11 2\\n/H9004\\n \\nthe spacing between adjacent frequencies (i.e\\n., the frequency resolution) is \\n/H9004/H9004\\nuN T\\n=\\n12\\n() .\\n A comparison of Fig. 7.10(a) to either Figs. 7.8(a) or 7.2 reveals \\nthat the spectrum of a discrete cosine transform has roughly the same frequenc\\ny \\nrange as that of the Fourier and Hartley transforms, but twice the frequency resolu-\\ntion. If \\nN\\n = 4 and \\n/H9004\\nT\\n=\\n1,\\n for example, the resulting DCT coefficients are at frequen-\\ncies \\n00 511 5\\n,.\\n,,.,\\n{}\\n while the DFT spectral components correspond to frequencies \\n0121\\n,,\\n, .\\n{}\\n Figures 7.10(c) and 7.8(c) further illustrate the point. Note that the \\nDCT basis image of maximum frequency occurs when \\nu\\n and \\nv\\n are 7, as opposed to \\n4 for the DFT. Since 2-D DCTs are based on the separable inverse transformation \\nkernel\\n \\nsxyuv u v\\nxu\\nN\\nyv\\nN\\n(,,,) ()() c o s\\n()\\ncos\\n()\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\naa\\npp\\n21\\n2\\n21\\n2\\n++\\n \\n(7-85)\\n012345678\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n-\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n012345678\\nb a\\nFIGURE 7.9\\nReconstructions \\nof a discrete \\nfunction by the \\naddition of pro-\\ngressively higher \\nfrequency com-\\nponents: (a) DHT \\nand (b) DFT.\\nDIP4E_GLOBAL_Print_Ready.indb   488\\n6/16/2017   2:09:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 489}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n489\\nwhere \\na\\n()\\nu\\n and \\na\\n()\\nv\\n are defined in accordance with Eq. (7-84), transformation \\nmatrix \\nA\\nC\\n can be used in the computation of both 1- and 2-D transforms (see the \\ncaption of Fig. 7.10 for the appropriate transform equations).\\nWhile sharing several attributes of the discrete Fourier transform, the discrete \\ncosine transform imposes a entirely different set of assumptions on the functions \\nbeing processed. Rather than \\nN\\n-point periodicity, the underlying assumption of the \\nDFT, the discrete cosine transform assumes 2\\nN\\n-point periodicity \\nand\\n even sym-\\nmetry. As can be seen in Fig. 7.11, while \\nN\\n-point periodicity can cause boundary \\ndiscontinuities that introduce “artiﬁcial” high-frequency components into a trans-\\nform, 2\\nN\\n-point periodicity and even symmetry minimize discontinuity, as well as the \\naccompanying high-frequency artifact. As will be seen in Chapter 8, this is an impor-\\ntant advantage of the DCT in image compression. In light of the above comments, it \\nshould come as no surprise that the DCT of \\nN\\n-point function \\nfx\\n()\\n can be obtained \\nfrom the DFT of a 2\\nN\\n-point symmetrically extended version of \\nfx\\n()\\n:\\n1. \\nSymmetrically extend \\nN\\n-point discrete function \\nfx\\n()\\n to obtain\\n \\ngx\\nfx\\nx N\\nfNx N x N\\n()\\n()\\n()\\n=\\n−−\\n⎧\\n⎨\\n⎩\\nfor \\nfor \\n0\\n21 2\\n…6\\n…6\\n  \\n(7-86)\\nwhere \\nf\\n=\\n[]\\nff f N\\nT\\n() () ( )\\n01 1\\np-\\n and \\ng\\n=\\n[]\\ngg g N\\nT\\n() () ( ) .\\n01 2 1\\np-\\n2. \\nCompute the 2\\nN\\n-point discrete F\\nourier transform of \\ng\\n:\\n \\ntA g\\nt\\nt\\nFF\\n==\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n2\\n(7-87)\\nb a\\nc\\nFIGURE 7.10\\n The transformation matrix and basis images of the discrete cosine transform for \\nN\\n = 8. (a) Graphical \\nrepresentation of orthogonal transformation matrix \\nA\\nC\\n, (b) \\nA\\nC\\n \\nrounded to two decimal places, and (c) basis images.  \\nFor 1-D transforms, matrix \\nA\\nC\\n is used in conjunction with Eqs. (7-28) and (7-29); for 2-D transforms, it is used with \\nEqs. (7-35) and (7-36).\\n 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35\\n 0.49 0.42 0.28 0.10 \\n-\\n0.10 \\n-\\n0.28 \\n-\\n0.42 \\n-\\n0.49\\n 0.46 0.19 \\n-\\n0.19 \\n-\\n0.46 \\n-\\n0.46 \\n-\\n0.19 0.19 0.46\\n 0.42 \\n-\\n0.10 \\n-\\n0.49 \\n-\\n0.28 0.28 0.49 0.10 \\n-\\n0.42\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35\\n 0.28 \\n-\\n0.49 0.10 0.42 \\n-\\n0.42 \\n-\\n0.10 0.49 \\n-\\n0.28\\n 0.19 \\n-\\n0.46 0.46 \\n-\\n0.19 \\n-\\n0.19 0.46 \\n-\\n0.46 0.19\\n 0.10 \\n-\\n0.28 0.42 \\n-\\n0.49 0.49 \\n-\\n0.42 0.28 \\n-\\n0.10\\nDIP4E_GLOBAL_Print_Ready.indb   489\\n6/16/2017   2:09:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 490}),\n",
       " Document(page_content='490\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nwhere \\nA\\nF\\n is the transformation matrix of the DFT and 2\\nN\\n-element transform \\nt\\nF\\n \\nis partitioned into two equal-length \\nN\\n-element column vectors, \\nt\\n1\\n and \\nt\\n2\\n.\\n3. \\nLet \\nN\\n-element column vector \\nh\\n=\\n[]\\nhh h N\\nT\\n() () ( )\\n01 1\\np-\\n where\\n  \\nhu e\\nu N\\nju N\\n()\\n,, ,\\n==\\n-\\np -\\np\\n2\\n01 1\\nfor \\n \\n(7-88)\\nand let \\ns\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n121 1 1\\np\\nT\\n. \\n4. \\nThe discrete cosine transform of \\nf\\n is then\\n \\nts h t\\nC\\n=\\n{}\\nRe\\n/H11568/H11568\\n1\\n \\n(7-89)\\nwhere \\n/H11568\\n denotes the \\nHadamar\\nd\\n \\nproduct\\n, a matrix multiplication in which the \\ncorresponding elements of two vectors or matrices are multiplied together—for \\nexample, \\n30 52 6 63\\n−\\n[] [ ]\\n=−\\n[]\\n..\\n°\\nEXAMPLE 7.10 :   Computing a 4-point DCT from a 8-point DFT.\\nIn this example, we use Eqs. (7-86) through (7-89) to compute the discrete cosine transform of 1-D func-\\ntion \\nfx x\\n()\\n=\\n2\\n for \\nx\\n=\\n01\\n23\\n,,,.\\n1. \\nLet \\nf\\n=\\n[]\\n0149\\nT\\n and use Eq. (7-86) to create an 8-point extension of \\nf\\n with even symmetry. \\nExtended function \\ng\\n=\\n[]\\n01499410\\nT\\n is one period of an even symmetric function like \\nthe one in Fig. 7.11(b).\\n2. \\nSubstituting the \\n88\\n*\\n unitary transformation matrix from Fig. 7.7(a) into Eq. (7-87), the discrete \\nF\\nourier transform of \\ng\\n is\\n \\ntA g\\nFF\\n==\\n-\\n--\\n+\\n--\\n-+\\n99\\n61 8 25 6\\n14 1 14 1\\n01 8 04 4\\n0\\n01 8 04 4\\n14\\n.\\n..\\n..\\n..\\n..\\n.\\nj\\nj\\nj\\nj\\n1\\n11 4 1\\n61 8 25 6\\n99\\n6\\n1\\n-\\n-+\\n-\\n-\\nj\\nj\\n.\\n..\\n.\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n so \\nt\\n.. .\\n..\\n..\\n.\\n18 2 56\\n14 1 14 1\\n01 8 04 4\\n0\\n01 8\\n2\\n-\\n+\\n--\\n-+\\nj\\nj\\nj\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n and \\nt\\nj j\\nj\\nj\\n04 4\\n14 1 14 1\\n61 8 25 6\\n.\\n..\\n..\\n-\\n-+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n3. \\nIn accordance with Eq. (7-88),\\n \\nh\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n1\\n1\\n09 2 03 8\\n07 1 07 1\\n03\\n4\\n2\\n34\\ne\\ne\\ne\\nj\\nj\\nj\\nj\\nj\\n-\\n-\\n-\\n-\\n-\\np\\np\\np\\n..\\n..\\n.8\\n80 9 2\\n-\\nj\\n.\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nDIP4E_GLOBAL_Print_Ready.indb   490\\n6/16/2017   2:09:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 491}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n491\\nand \\ns\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n[]\\n1 2 111 0 7 1111\\nT\\nT\\n..\\n4. \\nThe discrete cosine transform of \\nf\\n is then\\n \\nts h t\\nC\\n=\\n{}\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nRe\\nRe\\n.\\n..\\n..\\n/H11568/H11568\\n1\\n07 1\\n1\\n1\\n1\\n1\\n09 2 03 8\\n07 1 07 1\\n0\\n°\\n-\\n-\\nj\\nj\\n.. .\\n.\\n..\\n..\\n..\\n38 0 92\\n99\\n61 8 25 6\\n14 1 14 1\\n01 8 04\\n-\\n-\\n--\\n+\\n--\\nj\\nj\\nj\\nj\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n°\\n4 4\\n7\\n66 9\\n2\\n04 8\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n⎫\\n⎬\\n⎪\\n⎪\\n⎭\\n⎪\\n⎪\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n-\\n-\\n.\\n.\\nTo validate the result, we substitute Eq. (7-83) into Eqs. (7-22) and (7-24) with \\nN\\n = 4 and use the result-\\ning \\n44\\n*\\n DCT transformation matrix in Eq. (7-28) to obtain\\n \\ntA f\\nCC\\n0.5 0.5 0.5 0.5\\n0.65 0.27 0.27 0.65\\n0.5 0.5 0.5 0.5\\n0.27 0.\\n==\\n--\\n--\\n-\\n6 65 0.65 0.27\\n-\\n-\\n-\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n0\\n1\\n4\\n9\\n7\\n66 9\\n2\\n04 8\\n.\\n.\\n⎢ ⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nFigure 7.12 illustrates the reconstruction of \\nf\\n by the inverse discrete cosine transform.\\n Like the recon-\\nstructions in Fig. 7.9, the DC component at the top of the ﬁgure [i.e., Fig. 7.12(a)] is the average value of \\nthe discrete function—in this case, \\n() . .\\n0149 43 5\\n+++\\n=\\n It is an initial but crude approximation of \\nf\\n.\\n As \\nthree additional cosines of increasing frequency are added in the (b), (c), and (d) parts of the ﬁgure, the \\naccuracy of the approximation increases until a perfect reconstruction is achieved in (d). Note the \\nx\\n-axis \\nhas been extended to show that the resulting DCT expansion is indeed periodic with period 2\\nN\\n (in this \\ncase 8) and exhibits the even symmetry that is required of all discrete cosine transforms.\\nb\\na\\nFIGURE 7.11\\nThe periodicity \\nimplicit in the 1-D \\n(a) DFT and  \\n(b) DCT.\\nDiscontinuity\\nN\\n2\\nN\\nDiscontinuity\\nDIP4E_GLOBAL_Print_Ready.indb   491\\n6/16/2017   2:09:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 492}),\n",
       " Document(page_content='492\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nTHE DISCRETE SINE TRANSFORM\\nThe transformation matrix of the \\ndiscrete sine transform\\n (DST) is obtained by sub-\\nstituting the inverse transformation kernal\\n \\nsxu\\nN\\nxu\\nN\\n(,) s i n\\n()\\n()\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n2\\n1\\n11\\n1\\n++\\n++\\np\\n \\n(7-90)\\nwhose separable 2-D counterpart is\\n \\nsxyuv\\nN\\nxu\\nN\\nyv\\nN\\n(,,,) s i n\\n() ()\\nsin\\n()\\n()\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n⎛\\n⎝\\n⎜\\n2\\n1\\n11\\n1\\n11\\n1\\n++ +\\n++ ++\\npp\\n⎞ ⎞\\n⎠\\n⎟\\n \\n(7-91)\\ninto Eqs. (7-22) and (7-24). The resulting transformation matrix, denoted as \\nA\\nS\\n in \\nFig. 7.13, is real, orthogonal, and symmetric. As can be seen in the (a) part of the \\nfigure, the underlying basis functions are harmonically related sines of frequency \\n12 2\\n()\\nNT\\n+\\n/H9004\\n[]\\n to \\nNN T\\n22\\n() ;\\n+\\n/H9004\\n[]\\n the frequency resolution or the spacing \\nbetween adjacent frequencies is \\n/H9004/H9004\\nuN T\\n=\\n[]\\n12 2\\n() .\\n+\\n Like the DCT, the DST has \\nroughly the same frequenc\\ny range as the DFT, but twice the frequency resolution. \\nIf \\nN\\n = 4 and \\n/H9004\\nT\\n=\\n1,\\n for example, the resulting DST coefficients are at frequencies \\n04 08 12 16\\n.,\\n., ., . .\\n{}\\n Note unlike both the DCT and DFT, the DST has no DC (at \\nu\\n = 0) component. This results from an underlying assumption that the function \\nbeing transformed is \\n21\\n()\\nN\\n+\\n-point\\n periodic and odd symmetric, making its average \\nvalue zero\\n. In contrast to the DCT, where the function is assumed to be even, the \\nodd symmetry that is imposed by the DST does not reduce boundary discontinuity. \\nThis is clear in Fig. 6.14, where the result of computing the forward and inverse \\nDCT of  \\nfx x\\n()\\n=\\n2\\n for \\nx\\n=\\n01\\n23\\n,,,\\n is shown. Note that the underlying continuous \\nLike the DCT, there are \\neight variants and they \\nassume different sym-\\nmetry conditions—for \\ninstance, is the input odd \\nabout a sample or about \\na point halfway between \\ntwo samples?\\n02468 1 0 1 2 1 4 1 6\\n0\\n5\\n-\\n5\\n10\\n0\\n5\\n-\\n5\\n10\\n0\\n5\\n10\\n0\\n5\\n10\\nb\\na\\nc\\nd\\nFIGURE 7.12\\nDCT recon-\\nstruction of a \\ndiscrete function \\nby the addition \\nof progressively \\nhigher frequency \\ncomponents. Note \\nthe 2\\nN\\n-point \\nperiodicity and \\neven symmetry \\nimposed by the \\nDCT .\\nDIP4E_GLOBAL_Print_Ready.indb   492\\n6/16/2017   2:09:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 493}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n493\\nb a\\nc\\nFIGURE \\n7.13\\n The transformation matrix and basis images of the discrete sine transform for \\nN\\n = 8. (a) Graphical rep-\\nresentation of orthogonal transformation matrix \\nA\\nS\\n, (b) \\nA\\nS\\n \\nrounded to two decimal places, and (c) basis images.  \\nFor 1-D transforms, matrix \\nA\\nS\\n is used in conjunction with Eqs. (7-28) and (7-29); for 2-D transforms, it is used with \\nEqs. (7-35) and (7-36).\\n 0.16 0.30 0.41 0.46 0.46 0.41 0.30 0.16\\n 0.30 0.46 0.41 0.16 \\n-\\n0.16 \\n-\\n0.41 \\n-\\n0.46 \\n-\\n0.30\\n 0.41 0.41 0.00 \\n-\\n0.41 \\n-\\n0.41 0.00 0.41 0.41\\n 0.46 0.16 \\n-\\n0.41 \\n-\\n0.30 0.30 0.41 \\n-\\n0.16 \\n-\\n0.46\\n 0.46 \\n-\\n0.16 \\n-\\n0.41 0.30 0.30 \\n-\\n0.41 \\n-\\n0.16 0.46\\n 0.41 \\n-\\n0.41 0.00 0.41 \\n-\\n0.41 \\n-\\n0.00 0.41 \\n-\\n0.41\\n 0.30 \\n-\\n0.46 0.41 \\n-\\n0.16 \\n-\\n0.16 0.41 \\n-\\n0.46 0.30\\n 0.16 \\n-\\n0.30 0.41 \\n-\\n0.46 0.46 \\n-\\n0.41 0.30 \\n-\\n0.16\\nreconstruction, which was obtained by the same process that led to Fig. 6.12(d), \\nexhibits the aforementioned periodicity, odd symmetry, and boundary discontinuity.\\nThe discrete sine transform of an \\nN\\n-point function \\nfx\\n()\\n can be obtained from the \\nDFT of a \\n21\\n()\\nN\\n+\\n-point\\n symmetrically extended version of \\nfx\\n()\\n with odd symmetry:\\n1. \\nSymmetrically extend \\nN\\n-point function f(x) to obtain\\n \\ngx\\nx\\nfx\\nx N\\nxN\\nfNx N x\\n()\\n()\\n()\\n=\\n=\\n=\\n−\\n00\\n11\\n01\\n22\\nfor \\nfor \\nfor \\nfor \\n−≤\\n≤\\n+\\n−+ 1 +≤≤\\n2\\n22\\nN\\n+\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n \\n(7-92)\\nwhere \\nf\\n=\\n[]\\nff f N\\nT\\n() () ( )\\n01 1\\np-\\n and \\ng\\n=\\n[]\\ngg g N\\nT\\n() () ( ) .\\n01 2\\np\\n+2\\n2. \\nCompute the \\n21\\n()\\nN\\n+\\n-point\\n discrete Fourier transform of \\ng\\n:\\n \\ntA g\\nt\\nt\\nFF\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n0\\n0\\n1\\n2\\n \\n(7-93)\\n02468 1 0 1 2 1 4 1 6\\n0\\n-\\n10\\n10\\nFIGURE 7.14\\nA reconstruction \\nof the DST of the \\nfunction deﬁned \\nin Example 6.10.\\nDIP4E_GLOBAL_Print_Ready.indb   493\\n6/16/2017   2:09:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 494}),\n",
       " Document(page_content='494\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nwhere \\nA\\nF\\n is the transformation matrix of the DFT and \\n21\\n()\\nN\\n+\\n-eleme\\nnt\\n trans-\\nform \\nt\\nF\\n is partitioned into two single-element zero vectors, \\n0\\n=\\n[]\\n,\\n0\\n and two \\nN\\n-element column vectors \\nt\\n1\\n and \\nt\\n2\\n.\\n3. \\nThe discrete sine transform of \\nf\\n,\\n denoted \\nt\\nS\\n, is then\\n \\ntt\\nS\\nImag\\n=\\n{}\\n−\\n1\\n \\n(7-94)\\nEXAMPLE 7.11 :   Computing a 4-point DST from a 10-point DFT.\\nIn this example, we use Eqs. (7-92) through (7-94) to ﬁnd the DST of \\nf\\n=\\n[]\\n0149\\nT\\n from Example 7.10:\\n1. \\nCreate a \\n21\\n()\\nN\\n+\\n-point\\n extended version of \\nf\\n with odd symmetry\\n. In accordance with Eq. (7-92), \\ng\\n=\\n[]\\n001490 9 4 10\\n−−−\\nT\\n.\\n2. \\nCompute the discrete Fourier transform of \\ng\\n using Eq.\\n (7-93). Matrix \\nA\\nF\\n is a unitary DFT transfor-\\nmation matrix of size 10 10\\n×\\n and the resulting transform is\\n \\ntA g\\nFF\\n==\\n[]\\n0 63 5 65 3 35 6 15 4 0 63 5 65 3 35 6 15 4\\n−− −−\\njj jj j jj j\\nT\\n.. .. . .. .\\nNote the real part of \\nt\\nF\\n is zero and block \\nt\\n1\\n of \\nt\\nF\\n is \\n−−\\njj\\njj\\nT\\n63 5 65 3 35 6 15 4\\n.. .. .\\n[]\\n3. \\nIn accordance with Eq. (7-94), the DST of \\nf\\n  is then\\n \\ntt\\nS\\nImag\\n=−\\n{}\\n=\\n[]\\n1\\n63 5 65 3 35 6 15 4\\n.. ..\\n−−\\nT\\n \\nAlternately, the DST can be computed directly as\\n \\ntA f\\nSS\\n0.37 0.60 0.60 0.37\\n0.60 0.37 0.37 0.60\\n0.60 0.37 0.37 0.6\\n==\\n--\\n--\\n0 0\\n0.37 0.60 0.60 0.37\\n--\\n-\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n0\\n1\\n4\\n9\\n63 5\\n65 3\\n.\\n.\\n3\\n35 6\\n15 4\\n.\\n.\\n-\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nwhere \\nA\\nS\\n is obtained by substituting Eq. (7-90) into Eqs. (7-22) and (7-24) with \\nN\\n = 4.\\nEXAMPLE 7.12 :   Ideal lowpass ﬁltering with Fourier-related transforms.\\nFigure 7.15 shows the results of applying an ideal lowpass ﬁlter to the test image that was used in \\nExample 4.16 with all of the Fourier-related transforms that have been covered in this chapter. As in the \\nChapter 4 example, the test image shown in Fig. 7.15(a) is of size \\n688 688\\n×\\n and is padded to \\n1376 1376\\n×\\n \\nbefore computing any transforms\\n. For reference, the Fourier transform of the test image is shown in \\nFig. 7.15(b), where a blue overlay has been superimposed to show the lowpass ﬁlter function. Only the \\nfrequencies that are not shaded blue are passed by the ﬁlter. Since we are again using a cutoff frequency \\nwith a radius of 60, the ﬁltered result in Fig. 7.15(c) is similar to that of Fig. 4.41(d), with any differences \\ndue to the use of zero padding rather than mirror padding. Note once more the blurring and ringing that \\nwas discussed in Example 4.16.\\nFigures 7.15(d)–(i) provide comparable results using the three Fourier-related transforms covered \\nin this chapter. As was done for the Fourier transform in Fig. 6.15(b), Figs. 6.15(d)–(f) show the \\nDIP4E_GLOBAL_Print_Ready.indb   494\\n6/16/2017   2:09:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 495}),\n",
       " Document(page_content='7.6\\n  \\nFourier-Related Transforms\\n    \\n495\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 7.15\\n (a) Original image of the \\n688 688\\n×\\n test pattern from Fig. 4.41(a). (b) Discrete Fourier transform (DFT) \\nof the test pattern in (a) after padding to size \\n1376 1376\\n×\\n.\\n The blue overlay is an ideal lowpass ﬁlter (ILPF) with \\na radius of 60.\\n (c) Result of Fourier ﬁltering. (d)–(f) Discrete Hartley transform, discrete cosine transform (DCT), \\nand discrete sine transform (DST) of the test pattern in (a) after padding. The blue overlay is the same ILPF in (b), \\nbut appears bigger in (e) and (f) because of the higher frequency resolution of the DCT and DST. (g)–(i) Results of \\nﬁltering for the Hartley, cosine, and sine transforms, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   495\\n6/16/2017   2:09:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 496}),\n",
       " Document(page_content='496\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\ndiscrete Hartley, cosine, and sine transforms of the test image in Fig. 7.15(a) after zero-padding to size \\n1\\n376 1376\\n×\\n,\\n respectively. Although the filter functions for the cosine and sine transforms, which are \\nagain superimposed in blue\\n, appear to have twice the radii of the filters used with the Fourier and \\nHartley transforms, the same range of frequencies are passed by all filters. The apparent increase in size \\nis due to the greater frequency resolution of the sine and cosine transforms, which has already been \\ndiscussed. Note the spectra of these transforms do not need to be centered for easy interpretation, as \\nis the case for the Fourier and Hartley spectra. Finally, we note for all practical purposes the filtered \\nimages in Figs. 7.15(g)–(i) are equivalent to the Fourier filtered result in Fig. 7.15(c).\\nTo conclude the example, we note while Fourier-related transforms can be implemented in FFT-\\nlike algorithms or computed from the FFT itself, we used the matrix implementations that have been \\npresented in this section to compute both the forward and inverse transforms. Using MATLAB\\n®\\n, Win-\\ndows\\n®\\n 10, and a notebook PC with an Intel\\n®\\n i7-4600U processor at 2.1 GHz, the total times required to \\ncompute the Fourier-related transforms in this example were 2 to 5 times longer than the corresponding \\nFFT computations. All computations, however, took less than a second.\\n7 .7 WALSH-HADAMARD TRANSFORMS  \\nWalsh-Hadamard transforms\\n (WHTs) are non-sinusoidal transformations that \\ndecompose a function into a linear combination of rectangular basis functions, called \\nWalsh functions\\n, of value \\n+−\\n11\\n and \\n.\\n The ordering of the basis functions within a \\nW\\nalsh-Hadamard transformation matrix determines the variant of the transform \\nthat is being computed. For \\nHadamard ordering\\n (also called \\nnatural ordering\\n), the \\ntransformation matrix is obtained by substituting the inverse transformation kernal\\n \\nsxu\\nN\\nbx bu\\nii\\ni\\nn\\n(,) ( )\\n() ()\\n=\\n∑\\n=\\n1\\n1\\n0\\n1\\n−\\n−\\n \\n(7-95)\\ninto Eqs. (7-22) and (7-24), where the summation in the exponent of Eq. (7-95) is \\nperformed in modulo 2 arithmetic\\n,  \\nN\\nn\\n=\\n2,\\n and \\nbz\\nk\\n()\\n is the \\nk\\nth bit in the binary rep-\\nresentation of \\nz\\n.\\n For example, if \\nn\\n = 3 and \\nz\\n = 6 (110 in binary), \\nbz\\n0\\n0\\n() ,\\n=\\n \\nbz\\n1\\n1\\n() ,\\n=\\n \\nand \\nbz\\n2\\n1\\n() .\\n=\\n If \\nN\\n = 2,\\n the resulting Hadamard-ordered transformation matrix is\\n \\nA\\nW\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n2\\n11\\n11\\n−\\n \\n(7-96)\\nwhere the matrix on the right (without the scalar multiplier) is called a \\nHadamar\\nd \\nmatrix \\nof order 2. Letting \\nH\\nN\\n denote the Hadamard matrix of order \\nN\\n, a simple \\nrecursive relationship for generating Hadamard-ordered transfomation matrices is\\n \\nAH\\nW\\n=\\n1\\nN\\nN\\n \\n(7-97)\\nwhere\\n \\nH\\nHH\\nHH\\n2\\nN\\nNN\\nNN\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n \\n(7-98)\\n7.7\\nA\\nW\\n is used to denote the \\ntransformation matrix \\nof the Hadamard- or \\nnatural-ordered WHT. \\nAlthough of size \\n22\\n×\\n \\nhere, it is more generally \\nof size \\nNN\\n×\\n,  where \\nN\\n \\nis the dimension of the \\ndiscrete function being \\ntransformed.\\nDIP4E_GLOBAL_Print_Ready.indb   496\\n6/16/2017   2:09:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 497}),\n",
       " Document(page_content='7.7\\n  \\nWalsh-Hadamard Transforms\\n    \\n497\\nand\\n \\nH\\n2\\n11\\n11\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n \\n(7-99)\\nThus, Eq. (7-96) follows from Eqs. (7-97) and (7-99). In the same way,\\n \\nH\\nHH\\nHH\\n4\\n22\\n22\\n1111\\n1111\\n1111\\n1111\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n−\\n−−\\n−−\\n−−\\n \\n(7-100)\\nand\\n \\nH\\nHH\\nHH\\n8\\n44\\n44\\n11111111\\n11111111\\n11111111\\n11111\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n−−−−\\n−− −−\\n−− − −−\\n−−−−\\n−− −−\\n−−−−\\n−− − −\\n111\\n11111111\\n11111111\\n11111111\\n11111111\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢ ⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(7-101)\\nThe corresponding Hadamard-ordered transformation matrices are obtained by sub-\\nstituting \\nH\\n4\\n and \\nH\\n8\\n into Eq. (7-97).\\nThe number of sign changes along a row of a Hadamard matrix is known as the \\nsequency\\n of the row. Like frequency, sequency measures the rate of change of a \\nfunction, and like the sinusoidal basis functions of the Fourier transform, every \\nWalsh function has a unique sequency. Since the elements of a Hadamard matrix are \\nderived from inverse kernal values, the sequency concept applies to basis functions \\nsxu\\n(,\\n)\\n for \\nuN\\n=\\n01\\n1\\n,, ,\\n…\\n−\\n as well. For instance, the sequencies of the \\nH\\n4\\n basis vec-\\ntors in Eq. (7-100) are 0, 3, 1, 2; the sequencies of the \\nH\\n8\\n basis vectors in Eq. (7-101) \\nare 0, 7, 3, 4, 1, 6, 2, and 5. This arrangement of sequencies is the deﬁning character-\\nistic of a Hadamard-ordered Walsh-Hadamard transform.\\nArranging the basis vectors of a Hadamard matrix so the sequency increases \\nas a function of \\nu\\n is both desirable and common in signal and image processing \\nDIP4E_GLOBAL_Print_Ready.indb   497\\n6/16/2017   2:09:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 498}),\n",
       " Document(page_content='498\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\napplications. The transformation matrix of the resulting sequency-ordered Walsh-\\nHadamard transform is obtained by substituting the inverse transformation kernal\\n \\nsxu\\nN\\nbx pu\\nii\\ni\\nn\\n(,) ( )\\n() ()\\n=\\n∑\\n=\\n1\\n1\\n0\\n1\\n−\\n−\\n \\n(7-102)\\nwhere\\n \\npu b u\\npu\\nb u b u\\npu b u b u\\np\\nn\\nnn\\nnn\\nn\\n01\\n11 2\\n22 3\\n() ()\\n() () ()\\n() () ()\\n=\\n=\\n=\\n−\\n−−\\n−−\\n+\\n+\\n/vertellipsis\\n− −\\n+\\n11 0\\n() () ()\\nub u b u\\n=\\n \\n(7-103)\\ninto Eqs. (7-22) and (7-24). As before, the summations in Eqs. (7-102) and (7-103) \\nare performed in modulo 2 arithmetic\\n. Thus, for example,\\n \\n′\\n=\\nH\\n8\\n11111111\\n11111111\\n11111111\\n11111111\\n111111\\n−−−−\\n−−−−\\n−− −−\\n−− −−\\n1\\n11\\n11111111\\n11111111\\n11111111\\n−− − −\\n−− −−\\n−−−−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(7-104)\\nwhere the apostrophe \\n′\\n()\\n has been added to indicate sequency ordering as opposed \\nto Hadamard ordering\\n. Note the sequencies of the rows of \\n′\\nH\\n8\\n match their row num-\\nbers—i.e., 0, 1, 2, 3, 4, 5, 6, and 7. An alternate way to generate \\n′\\nH\\n8\\n is to rearrange the \\nrows of Hadamard-ordered \\nH\\n8\\n, noting that row \\ns\\n of \\n′\\nH\\n8\\n corresponds to the row of \\nH\\n8\\n that is the bit-reversed \\ngray code\\n of \\ns\\n. Since the \\nn\\n-bit gray code corresponding to \\n()\\nss\\ns s\\nn\\n−\\n12 1 0 2\\n…\\n can be computed as \\n \\ngs s i n\\ngs\\ni n\\nii i\\nnn\\n=\\n=\\n{\\n+\\n−−\\n≤≤ −\\n=−\\n1\\n11\\n02\\n1\\nfor \\nfor \\n \\n(7-105)\\nwhere \\n{\\n denotes the exclusive OR operation, row \\ns\\n of \\n′\\nH\\n8\\n is the same as row  \\n()\\ngg\\ng g\\nn\\n012 1 2\\n…\\n−\\n of \\nH\\n8\\n. For example, row 4 or (100)\\n2\\n of \\n′\\nH\\n8\\n,\\n whose gray code is (110)\\n2\\n, \\ncomes from row (011)\\n2 \\nor 3 of \\nH\\n8\\n. Note row 4 of \\n′\\nH\\n8\\n in Eq. (7-104) is indeed identical \\nto row 3 of \\nH\\n8 \\nin Eq. (7-101).\\nFigures 7.16(a) and (b) depict graphically and numerically the sequency-ordered \\nWHT transformation matrix for the case of \\nN\\n = 8. Note the sequency of the discrete \\nRecall that \\nN\\n = 2\\nn\\n, so \\nn\\n = log\\n2\\n \\nN\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   498\\n6/16/2017   2:09:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 499}),\n",
       " Document(page_content='7.7\\n  \\nWalsh-Hadamard Transforms\\n    \\n499\\nbasis functions in Fig. 7.16(a) increase as \\nu\\n goes from 0 to 7, as does the sequency \\nof the underlying square wave functions. Note also the transformation matrix in \\nFig. 7.16(b) is real, symmetric, and follows from Eqs. (7-105) and (7-97) as\\n \\nAH\\n′\\n=\\n′\\nW\\n1\\n8\\nN\\n \\n(7-106)\\nIt is left as an exercise for the reader to show that it is orthogonal and that \\nAA A\\n′′ ′\\n==\\nWW W\\nT\\n−\\n1\\n.\\n Finally, note the similarity of the sequency-ordered basis images \\nin F\\nig. 7.16(c), which are based on the separable 2-D inverse transformation kernal\\nsxyuv\\nN\\nbx pu by pv\\nii ii\\ni\\nn\\n(,,,) ( )\\n() () () ()\\n=\\n∑\\n[]\\n=\\n1\\n1\\n0\\n1\\n−\\n+\\n−\\n \\n(7-107)\\nto the basis images of the 2-D DCT in Fig. 7.10(c). Sequency increases as a function \\nof both \\nu\\n and \\nv\\n,\\n like frequency in the DCT basis images, but does not have as useful \\na physical interpretation.\\nEXAMPLE 7.13 :   A simple sequency-ordered Walsh-Hadamard transform.\\nTo compute the sequency-ordered Walsh-Hadamard transform of the 1-D function \\nf\\n=\\n[] ,\\n2345\\nT\\n we \\nbegin with the Hadamard-ordered Hadamard matrix \\nH\\n4\\n of Eq. (7-100) and use the procedure described \\nin conjunction with Eq. (7-105) to reorder the basis vectors. The mapping of the Hadamard-ordered \\nbasis vectors of \\nH\\n4\\n to the sequency-ordered basis vectors of \\n′\\nH\\n4\\n is computed as follows:\\n 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35\\n 0.35 0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35\\n 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35\\n 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35\\n 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35\\n 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 \\n-\\n0.35\\nb a\\nc\\nFIGURE 7.16\\n The transformation matrix and basis images of the sequency-ordered Walsh-Hadamard transform for \\nN\\n = 8. (a) Graphical representation of orthogonal transformation matrix \\nA\\n′\\nW\\n,\\n (b) \\nA\\n′\\nW\\n \\nrounded to two decimal \\nplaces, and (c) basis images.  For 1-D transforms, matrix \\nA\\n′\\nW\\n is used in conjunction with Eqs. (7-28) and (7-29); for \\n2-D transforms, it is used with Eqs. (7-35) and (7-36).\\nDIP4E_GLOBAL_Print_Ready.indb   499\\n6/16/2017   2:09:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 500}),\n",
       " Document(page_content='500\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nRow of \\n′\\nH\\n4\\nBinary Code Gray Code\\nBit-Reversed \\nGray Code\\nRow of H\\n4\\n0 00\\n00\\n00\\n0\\n10\\n1 0\\n11\\n0 2\\n21\\n0 1\\n11\\n1 3\\n3 11\\n10\\n01\\n1\\nThus, in accordance with Eqs. (7-106), the sequency-ordered Walsh-Hadamard transformation matrix \\nof size \\n44\\n×\\n is\\n \\nAH\\n′′\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nWW\\n1\\n4\\n1\\n2\\n1111\\n1111\\n1111\\n1111\\n−−\\n−−\\n−−\\nand the sequency-ordered transform is \\ntA f\\n′′\\n==\\nWW\\n[] .\\n72 01\\n−−\\nT\\n7 .8 SLANT TRANSFORM  \\nMany monochrome images have large areas of uniform intensity and areas of lin-\\nearly increasing or decreasing brightness. With the exception of the discrete sine \\ntransform, all of the transforms that we have presented to this point include a basis \\nvector (at frequency or sequency \\nu\\n = 0) for representing efficiently constant gray \\nlevel areas, but none has a basis function that is targeted specifically at the represen-\\ntation of linearly increasing or decreasing intensity values. The\\n \\ntransform considered \\nin this section, called the \\nslant transform\\n, includes such a basis function. The trans-\\nformation matrix of the slant transform of order \\nNN\\n×\\n where \\nN\\nn\\n=\\n2\\n is generated \\nrecursively using\\n \\nAS\\nSl\\n=\\n1\\nN\\nN\\n \\n(7-108)\\nwhere \\nslant matrix\\n \\nS\\n00\\n0I 0 I\\n00\\n0I\\nN\\nNN N N\\nNN\\nNN N N\\nN\\nab a b\\nba b a\\n=\\n10 10\\n01 0 1\\n22\\n22\\n22\\n−\\n−\\n−\\n−−\\n−\\n() ()\\n()\\n0\\n0I\\nS0\\n0S\\n−\\n−\\n()\\nN\\nN\\nN\\n22\\n2\\n2\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n \\n  \\n(7-109)\\n7.8\\nDIP4E_GLOBAL_Print_Ready.indb   500\\n6/16/2017   2:09:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 501}),\n",
       " Document(page_content='7.8\\n  \\nSlant Transform\\n    \\n501\\nHere, \\nI\\nN\\n is the identity matrix of order \\nNN\\n×\\n,\\n \\nS\\n2\\n11\\n11\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n \\n(7-110)\\nand coefficients \\na\\nN\\n and \\nb\\nN\\n are\\n \\na\\nN\\nN\\nN\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n3\\n41\\n2\\n2\\n12\\n()\\n−\\n \\n(7-111)\\nand\\n \\nb\\nN\\nN\\nN\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n2\\n2\\n12\\n4\\n41\\n−\\n−\\n()\\n \\n(7-112)\\nfor \\nN\\n > 1.\\n When \\nN\\n≥\\n8,\\n matrix \\nS\\nN\\n is not sequency ordered, but can be made so using \\nthe procedure demonstrated in Example 6.13 for the WHT. An example of the use \\nof Eqs. (7-108) through (7-112) is Slant transformation matrix\\n \\nAS\\nSl\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n1\\n4\\n1\\n2\\n1111\\n3\\n5\\n1\\n5\\n1\\n5\\n3\\n5\\n11\\n11\\n1\\n5\\n3\\n5\\n3\\n5\\n1\\n5\\n4\\n−−\\n−−\\n−−\\n⎥ ⎥\\n \\n(7-113)\\nSince \\nN\\n = 4,\\n the basis vectors of \\nA\\nSl\\n (and the rows of slant matrix of \\nS\\n4\\n) are sequency \\nordered.\\nEXAMPLE 7.14 :   A simple 1-D slant transform.\\nUsing Eqs.\\n (7-28) and\\n (7-113), the slant transform of function \\nf\\n=\\n[]\\n23\\n45\\nT\\n from Example 7.13 is \\ntA f\\nSl Sl\\n==\\n[. ] .\\n7 2 2 400\\n−\\nT\\n Note the transform contains only two nonzero terms, while the Walsh-Had-\\namard transform in the previous example had three nonzero terms. The slant transform represents \\nf\\n \\nmore efﬁciently because \\nf\\n is a linearly increasing function—that is, \\nf\\n is highly correlated with the slant \\nbasis vector of sequency one.  Thus, there are fewer terms in a linear expansion using slant basis func-\\ntions as opposed to Walsh basis functions.\\nFigures 7.17(a) and (b) depict graphically and numerically the sequency-ordered \\nslant transformation matrix for the case of \\nN\\n = 8. Just as apostrophes \\n′\\n()\\n were used \\nto denote sequenc\\ny ordering in Walsh-Hadamard transforms, \\n′\\nS\\n8\\n and \\nA\\nSl\\n′\\n are used \\nto denote sequency-ordered versions of Eqs. (7-108) and (7-109). Note the slant \\ntransformation matrix in Fig. 7.16(b) is real, but not symmetric. Thus, \\nAA\\nSl Sl\\n′′\\n=\\n−\\n1\\nT\\n \\nbut \\nAA\\nSl Sl\\n′′\\nT\\n≠\\n.\\n Matrix \\nA\\nSl\\n′\\n is also orthogonal and can be used in conjunction with \\nNote \\nI\\n1\\n is a 1 1\\n×\\n identity \\nmatrix \\n1\\n[]\\n and \\nI\\n0\\n is the \\nempty matrix of size \\n00\\n×\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   501\\n6/16/2017   2:09:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 502}),\n",
       " Document(page_content='502\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nEqs. (7-35) and (7-36) to implement 2-D separable slant transforms. Figure 6.17(c) \\nshows the 2-D slant basis images of size \\n88\\n×\\n.\\n Note for \\n45\\n≤≤\\nu\\n and \\n45\\n≤≤\\nv\\n,\\n they \\nare identical to the corresponding basis images of the \\nWHT in Fig. 7.16(c). This is \\nalso evident in Figs. 7.16(a) and 7.17(a) when \\n45\\n≤≤\\nu\\n.\\n In fact, all of the slant basis \\nvectors bear a striking resemblance to the basis vectors of the \\nWalsh-Hadamard \\ntransform. Finally, we note slant matrices have the necessary properties to allow \\nimplementation of a fast slant transform algorithm similar to the FFT.\\n7 .9 HAAR TRANSFORM  \\nDiscovered in 1910, the basis functions of the \\nHaar transform\\n (Haar [1910]) were \\nlater recognized to be the oldest and simplest orthonormal wavelets. We will look \\nat Haar’s functions in the context of wavelets in the next section. In this section, we \\napproach Haar’s transform as another matrix-based transformation that employs a \\nset of rectangular-shaped basis functions.\\nThe Haar transform is based on \\nHaar functions\\n, \\nhx\\nu\\n() ,\\n that are deﬁned over the \\ncontinuous\\n, half-open interval \\nx\\n∈\\n[\\n)\\n01\\n,.\\n Variable \\nu\\n is an integer that for \\nu\\n > 0 can be \\ndecomposed uniquely as\\n \\nuq\\np\\n=\\n2\\n+\\n \\n(7-114)\\nwhere \\np\\n is the largest power of 2 contained in \\nu\\n and \\nq\\n is the remainder—that is\\n, \\nqu\\np\\n=−\\n2.\\n The Haar basis functions are then\\nhx\\nux\\nuq x q\\nu\\nu\\npp p\\np\\n()\\n(. )\\n=\\n=\\n+\\n10 0 1\\n20 2 0 5 2\\n20\\n2\\n2\\n and \\n and \\n and \\n≤<\\n>≤ <\\n−>\\n(\\n(. ) ()\\nqx q\\npp\\n++\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n05\\n21 2\\n0\\n≤<\\notherwise\\n \\n(7-115)\\n7.9\\n 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35\\n 0.54 0.39 0.23 0.08 \\n-\\n0.08 \\n-\\n0.23 \\n-\\n0.39 \\n-\\n0.54\\n 0.47 0.16 \\n-\\n0.16 \\n-\\n0.47 \\n-\\n0.47 \\n-\\n0.16 0.16 0.47\\n 0.24 \\n-\\n0.04 \\n-\\n0.31 \\n-\\n0.59 0.59 0.31 0.04 \\n-\\n0.24\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35\\n 0.35 \\n-\\n0.35 \\n-\\n0.35 0.35 \\n-\\n0.35 0.35 0.35 \\n-\\n0.35\\n 0.16 \\n-\\n0.47 0.47 \\n-\\n0.16 \\n-\\n0.16 0.47 \\n-\\n0.47 0.16\\n 0.16 \\n-\\n0.47 0.47 \\n-\\n0.16 0.16 \\n-\\n0.47 0.47 \\n-\\n0.16\\nb a\\nc\\nFIGURE 7.17\\n The transformation matrix and basis images of the slant transform for \\nN\\n = 8. (a) Graphical representa-\\ntion of orthogonal transformation matrix \\nA\\nSl\\n′\\n,\\n (b) \\nA\\nSl\\n′\\n \\nrounded to two decimal places, and (c) basis images.  For \\n1-D transforms, matrix \\nA\\nSl\\n′\\n is used in conjunction with Eqs. (7-28) and (7-29); for 2-D transforms, it is used with \\nEqs. (7-35) and (7-36).\\nDIP4E_GLOBAL_Print_Ready.indb   502\\n6/16/2017   2:09:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 503}),\n",
       " Document(page_content='7.9\\n  \\nHaar Transform\\n    \\n503\\nWhen \\nu\\n is 0,  \\nhx\\n0\\n1\\n()\\n=\\n for all \\nx\\n;\\n the ﬁrst Haar function is independent of continu-\\nous variable \\nx\\n. For all other values of \\nu\\n, \\nhx\\nu\\n()\\n=\\n0 \\nexcept\\n in the half-open intervals \\nCB\\nqq\\npp\\n20 5 2\\n,( . )\\n+\\n and \\nCB\\n(. ) , (),\\nqq\\npp\\n++\\n05 2 1 2\\n where it is a rectangular wave \\nof magnitude \\n2\\n2\\np\\n and \\n−\\n2\\n2\\np\\n,\\n respectively. Parameter \\np\\n determines the amplitude \\nand width of both rectangular waves\\n, while \\nq\\n determines their position along \\nx\\n. As \\nu\\n increases, the rectangular waves become narrower and the number of functions \\nthat can be represented as linear combinations of the Haar functions increases. Fig-\\nure 7.18(a) shows the ﬁrst eight Haar functions (i.e., the curves depicted in blue).\\nThe transformation matrix of the \\ndiscrete Haar transform\\n can be obtained by sub-\\nstituting the inverse transformation kernal\\n \\nsxu\\nN\\nhx N x N\\nu\\n(,) ( ) ,, ,\\n==\\n1\\n01 1\\n    for \\n…\\n−\\n \\n(7-116)\\nfor \\nuN\\n=\\n01\\n1\\n,, , ,\\n…\\n−\\n where \\nN\\n = 2\\nn\\n, into Eqs. (7-22) and (7-24). The resulting trans-\\nformation matrix, denoted \\nA\\nH\\n, can be written as a function of the \\nNN\\n×\\n \\nHaar matrix\\n \\nH\\nN\\nNN\\nhN hN h N N\\nhN hN\\nhN h N N\\n=\\n00 0\\n11\\n11\\n01 1\\n01\\n01\\n() () ( )\\n() ()\\n() ( )\\np-\\no\\no\\np-\\n--\\n/downslopeellipsis\\n⎡ ⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(7-117)\\nas\\n \\nAH\\nH\\n=\\n1\\nN\\nN\\n \\n(7-118)\\nFor example, if \\nN\\n = 2,\\n \\nA\\nH\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n2\\n01 2\\n01 2\\n1\\n2\\n11\\n11\\n00\\n11\\nhh\\nhh\\n() ( )\\n() ( )\\n−\\n \\n(7-119)\\nIn the computation of \\nA\\nH\\n, \\nx\\n and \\nu\\n of Eq. (7-116) are 0 and 1, so Eqs. (7-114), \\n(7-115), and (7-116) give \\nsh\\n(,) () ,\\n00 0 2 1 2\\n0\\n==\\n \\nsh\\n(, ) (.) ,\\n10 05 2 1 2\\n0\\n==\\n \\nsh\\n(,) () ,\\n01 0 2 1 2\\n1\\n==\\nand \\nsh\\n(,) (.)\\n.\\n11 05 2 1 2\\n1\\n== −\\n For \\nN\\n = 4,\\n  \\nu\\n, \\nq\\n, and \\np\\n of \\nEq. (7-114) assume the values\\n \\nupq\\n100\\n210\\n311\\nVariables \\np\\n and \\nq\\n are \\nanalogous to \\ns\\n and \\nt\\n in \\nEq. (7-72).\\nDo not confuse the \\nHaar matrix with the \\nHadamard matrix of Sec-\\ntion 7.7. Since the same \\nvariable is used for both, \\nthe proper matrix must \\nbe determined from the \\ncontext of the discussion.\\nWhen \\nu\\n is 0, \\nhx\\nu\\n( )  is \\nindependent of \\np\\n and \\nq\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   503\\n6/16/2017   2:09:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 504}),\n",
       " Document(page_content='504\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nand the Haar transformation matrix of size \\n44\\n×\\n becomes\\n \\nA\\nH\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n1\\n2\\n1111\\n11\\n11\\n22 0 0\\n00 2 2\\n−−\\n−\\n−\\n \\n(7-120)\\nThe transformation matrix for \\nN\\n = 8 is shown in F\\nig. 7.18(b). \\nA\\nH\\n is real, orthogonal, \\nand sequency ordered. An important property of the Haar transformation matrix \\nis that it can be decomposed into products of matrices with fewer nonzero entries \\nthan the original matrix. This is true of all of the transforms we have discussed to this \\npoint. They can be implemented in FFT-like alogrithms of complexity \\nON N\\n(l\\no g ) .\\n2\\nThe Haar transformation matrix, however, has fewer nonzero entries before the \\ndecomposition process begins, making less complex algorithms on the order of \\nO\\n(\\nN\\n) \\npossible. As can be seen in Fig. 7.18(c), the basis images of the separable 2-D Haar \\ntransform for images of size \\n88\\n×\\n also have few nonzero entries.\\n7 .10 WAVELET TRANSFORMS  \\nIn 1987, \\nwavelets\\n were shown to be the foundation of a powerful new approach \\nto signal processing and analysis called \\nmultiresolution\\n theory (Mallat [1987]). \\nMultiresolution theory incorporates and unifies techniques from a variety of \\ndisciplines, including subband coding from signal processing, quadrature mirror \\nfiltering from digital speech recognition, and pyramidal image processing. As its \\nname implies, it is concerned with the representation and analysis of signals (or \\nimages) at more than one resolution. A \\nscaling function\\n is used to create a series of \\napproximations of a function or image, each differing by a factor of 2 in resolution \\n7.10\\nAs was noted in Sec-\\ntion 7.1, wavelets are \\nsmall waves with band-\\npass spectra as deﬁned in \\nEq. (7-72).\\n 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35\\n 0.35 0.35 0.35 0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35 \\n-\\n0.35\\n 0.50 0.50 \\n-\\n0.50 \\n-\\n0.50 0 0 0 0\\n 0 0 0 0 0.50 0.50 \\n-\\n0.50 \\n-\\n0.50\\n 0.71 \\n-\\n0.71 0 0 0 0 0 0\\n 0 0 0.71 \\n-\\n0.71 0 0 0 0\\n 0 0 0 0 0.71 \\n-\\n0.71 0 0\\n 0 0 0 0 0 0 0.71 \\n-\\n0.71\\nb a\\nc\\nFIGURE 7.18\\n The transformation matrix and basis images of the discrete Haar transform for \\nN\\n = 8. (a) Graphical rep-\\nresentation of orthogonal transformation matrix \\nA\\nH\\n, (b) \\nA\\nH\\n \\nrounded to two decimal places, and (c) basis images.  \\nFor 1-D transforms, matrix \\nA\\nH\\n is used in conjunction with Eqs. (7-28) and (7-29); for 2-D transforms, it is used with \\nEqs. (7-35) and (7-36).\\nDIP4E_GLOBAL_Print_Ready.indb   504\\n6/16/2017   2:09:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 505}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n505\\nfrom its nearest neighboring approximations, and complementary functions, called \\nwavelets\\n, are used to encode the differences between adjacent approximations. The \\ndiscrete wavelet transform\\n (DWT) uses those wavelets, together with a single scaling \\nfunction, to represent a function or image as a linear combination of the wavelets \\nand scaling function. Thus, the wavelets and scaling function serve as an othonormal \\nor biorthonormal basis of the DWT expansion. The Daubechies and Biorthogonal \\nB-splines of Figs. 7.3(f) and (g) and the Haar basis functions of the previous section \\nare but three of the many bases that can be used in DWTs.\\nIn this section, we present a mathematical framework for the interpretation and \\napplication of discrete wavelet transforms. We use the discrete wavelet transform \\nwith respect to Haar basis functions to illustrate the concepts introduced. As you \\nproceed through the material, remember that the discrete wavelet transform of a \\nfunction with respect to Haar basis functions is \\nnot\\n the Haar transform of the func-\\ntion (although the two are intimately related).\\nSCALING FUNCTIONS\\nConsider the set of basis functions composed of all integer translations and binary \\nscalings of the real, square-integrable \\nfather scaling function\\n \\nw\\n()\\nx\\n—that is, the set of \\nscaled and translated functions \\nEF\\nw\\njk\\nxj k\\n,\\n() |,\\nH\\nZ\\n where \\n \\nww\\njk\\njj\\nxx k\\n,\\n() ( )\\n=\\n22\\n2\\n-\\n \\n(7-121)\\nIn this equation, integer \\ntranslation\\n \\nk\\n determines the position of \\nw\\njk\\nx\\n,\\n()\\n along the \\nx\\n-axis and \\nscale\\n \\nj\\n determines its shape—i.e\\n., its width and amplitude. If we restrict \\nj\\n \\nto some value, say \\nj\\n = \\nj\\n0\\n, then \\nEF\\nw\\njk\\nk\\n0\\n,\\n|\\nH\\nZ\\n is the basis of the function space spanned \\nby the \\nw\\njk\\nx\\n,\\n()\\n for \\nj\\n = \\nj\\n0\\n and \\nk\\n \\n=\\n …, \\n−\\n1\\n, 0, 1, 2, …, denoted \\nV\\nj\\n0\\n.\\n Increasing \\nj\\n0\\n increases \\nthe number of representable functions in \\nV\\nj\\n0\\n,\\n \\nallowing functions with smaller varia-\\ntions and finer detail to be included in the space\\n. As is demonstrated in Fig. 6.19 with \\nHaar scaling functions, this is a consequence of the fact that as \\nj\\n0\\n increases, the scal-\\ning functions used to represent the functions in \\nV\\nj\\n0\\n become narrower and separated \\nby smaller changes in \\nx\\n.\\nEXAMPLE 7.15 :   The Haar scaling function.\\nConsider the unit-height, unit-width scaling function\\n \\nw\\n()\\nx\\nx\\n=\\n⎧\\n⎨\\n⎩\\n10\\n1\\n0\\n≤<\\notherwise\\n \\n(7-122)\\nand note it is the Haar basis function \\nhx\\n0\\n()\\n from Eq. (7-115). Figure 7.19 shows a few of the pulse-\\nshaped scaling functions that can be generated by substituting Eq.\\n (7-122) into Eq. (7-121). Note when \\nthe scale is 1 [i.e., when \\nj\\n = 1 as in Figs. 7.19(d) and (e)], the scaling functions are half as wide as when \\nthe scale is 0  (i.e., when \\nj\\n = 0 as in Figs. 7.19(a) and (b)]. Moreover, for a given interval on \\nx\\n, there are \\nThe discrete wavelet \\ntransform, like all \\ntransforms considered in \\nthis chapter, generates \\nlinear expansions of \\nfunctions with respect to \\nsets of orthonormal or \\nbiorthonormal expansion \\nfunctions.\\nThe coefﬁcients of a 1-D \\nfull-scale DWT with \\nrespect to Haar wavelets \\nand a 1-D Haar trans-\\nform are the same.\\nZ\\n is the set of integers.\\nRecall from Section 7.1 \\nthat the span of a basis is \\nthe set of functions that \\ncan be represented as \\nlinear combinations of \\nthe basis functions.\\nDIP4E_GLOBAL_Print_Ready.indb   505\\n6/16/2017   2:09:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 506}),\n",
       " Document(page_content='506\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\ntwice as many scale 1 as scale 0 scaling functions. For example, two \\nV\\n1\\n scaling functions, \\nw\\n10\\n,\\n and \\nw\\n11\\n,\\n,\\n are \\nlocated in interval \\n01\\n≤<\\nx\\n, while only one \\nV\\n0\\n scaling function, \\nw\\n00\\n,\\n, occupies the same interval.\\nF\\nigure 7.19(f) shows a member of scaling space \\nV\\n1\\n that does not belong in \\nV\\n0\\n.\\n The scaling func-\\ntions in F\\nigs. 7.19(a) and (b) are too coarse to represent it. Higher-resolution functions, like those in \\nFigs. 7.19(d) and (e), are required. They can be used, as is shown in Fig. 7.19(f), to represent the function \\nas the three-term expansion \\nfx x x x\\n()\\n=\\n()\\n+\\n() ()\\n05\\n02 5\\n10 11\\n14\\n.. .\\n,, ,\\nww w\\n−\\n In a similar manner, scaling func-\\ntion \\nw\\n00\\n,\\n,\\n which is both a basis function and member of \\nV\\n0\\n,\\n can be represented by a linear combination \\nof \\nV\\n1\\n scaling functions [see Fig. 7.19(c)] as follows:\\n \\nwww\\n01\\n2 1\\n2\\n1\\n1\\n2\\n1\\n2\\n,, ,\\nkk k\\nxx x\\n()\\n=\\n()\\n+\\n()\\n+\\nThe Haar scaling function of the preceding example, like the scaling functions of \\nall discrete wavelet transforms, obeys the four fundamental requirements of \\nmulti-\\nresolution analysis\\n (Mallat [1989a]):\\n1. \\nThe scaling function is orthogonal to its integer tranlates.\\n2. \\nThe function spaces spanned by the scaling function at low scales are nested \\nwithin those spanned at higher scales\\n. That is, \\n \\nVV V V V V\\n−−\\n/H11009/H11009\\n(( ( ( ( ((\\n……\\n1012\\n  \\n(7-123)\\nwhere \\n(\\n is used to denote “a subspace of.” The scaling functions satisfy the \\nintuitive condition that if \\nfx V\\nj\\n() ,\\nH\\n then \\nfxV\\nj\\n() .\\n2\\n1\\nH\\n+\\nx\\nx\\nx\\nx\\nx\\nx\\n0123\\n0\\n1\\n0123\\n0\\n1\\n0123\\n0\\n1\\n0123\\n0\\n1\\n0123\\n0\\n1\\n0123\\n0\\n1\\n05\\n10\\n.\\n,\\nw\\n−\\n02 5\\n14\\n.\\n,\\nw\\nww\\n00\\n,\\n() ()\\nxx\\n=\\nww\\n01\\n1\\n,\\n() ( )\\nxx\\n=−\\nw\\n00 1\\n,\\n()\\nxV\\nH\\nww\\n10\\n22\\n,\\n() ( )\\nxx\\n=\\nww\\n11\\n22 1\\n,\\n() ( )\\nxx\\n=−\\nfx V\\n()\\nH\\n1\\nw\\n10\\n2\\n,\\nw\\n11\\n2\\n,\\nw\\n11\\n,\\nb a\\nc\\ne d\\nf\\nFIGURE 7.19\\n The Haar scaling function.\\nDIP4E_GLOBAL_Print_Ready.indb   506\\n6/16/2017   2:09:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 507}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n507\\n3. \\nThe only function representable at every scale is \\nfx\\n()\\n.\\n=\\n0\\n4. \\nAll measureable, square-integrable functions can be represented as a linear \\ncombination of the scaling function as \\nj\\n→\\n/H11009\\n. In other words,\\n \\nVL\\n/H11009\\n=\\n2\\n()\\nR\\n \\n(7-124)\\nwhere \\nL\\n2\\n(\\nR\\n) is the set of measureable, square-integrable, 1-D functions.\\nUnder the above conditions,  \\nw\\n()\\nx\\n can be expressed as a linear combination of \\ndouble-resolution copies of itself:\\n \\nww\\nw\\n() () ( )\\nxh k x k\\nk\\n=\\n∑\\nH\\n-\\nZ\\n22\\n \\n(7-125)\\nCalled the \\nrefinement\\n or \\ndilation equation,\\n Eq.\\n (7-125) defines a series expan-\\nsion in which the expansion functions, in accordance with Eq. (7-121), are scal-\\ning functions from one scale higher than \\nw\\n()\\nx\\n and the \\nhk\\nw\\n()\\n are expansion coef-\\nficients\\n. The expansion coefficients, which can be collected into an ordered set \\nhk k\\nh h\\nww\\nw\\n() ,,, () , ( ) , ,\\n=\\n{}\\n=\\n{}\\n012 0 1\\n……\\n are commonly called \\nscaling function coef-\\nficients\\n.\\n For orthonormal scaling functions, it follows from Eqs. (7-51) and (7-52) that\\n \\nhk x x k\\nw\\nww\\n() () , ( )\\n=−\\n22\\n \\n(7-126)\\nEXAMPLE 7.16 :   Haar scaling function coefﬁcients.\\nThe coefﬁcients of the Haar scaling function [i.e., Eq. (7-122)] are \\nhn n\\nw\\n() , , ,\\n=\\n{}\\n=\\n{}\\n0 1 12 12\\n the \\nﬁrst row of Haar matrix \\nA\\nH\\n for \\nN\\n = 2 in Eq. (7-119). It is left as an exercise for the reader (see Prob-\\nlem 7.33) to compute these coefﬁcients using Eq. (7-126). Equation (7-125) then yields\\n \\nwww\\nww\\nxx x\\nxx\\n()\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=+\\n1\\n2\\n22\\n1\\n2\\n22 1\\n22 1\\n−\\n−\\n() ( )\\n \\nT\\nhis expansion is illustrated graphically in Fig. 7.19(c), where the bracketed terms of the preceding \\nexpression are seen to be \\nw\\n10\\n,\\n()\\nx\\n and \\nw\\n11\\n,\\n() .\\nx\\nWAVELET FUNCTIONS\\nGiven a father scaling function that meets the MRA requirements of the previous \\nsection, there exists a \\nmother wavelet function\\n \\nc\\n()\\nx\\n whose integer translations and \\nbinary scalings\\n,\\n \\ncc\\njk\\njj\\nxx k\\n,\\n() ( )\\n=\\n22\\n2\\n-\\n \\n(7-127)\\nfor all \\njk\\n,,\\nH\\nZ\\n span the difference between any two adjacent scaling spaces. If we \\nlet \\nW\\nj\\n0\\n denote the function space spanned by wavelet functions \\nc\\njk\\nk\\n0\\n,\\n|,\\nH\\nZ\\n{}\\n then\\n  \\nVV W\\njj j\\n00 0\\n1\\n+\\n=\\n{\\n \\n(7-128)\\nRecall that \\nR\\n is the set of \\nreal numbers.\\nScaling function coef-\\nﬁcients can also be \\ncombined in a \\nscaling \\nvector\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   507\\n6/16/2017   2:09:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 508}),\n",
       " Document(page_content='508\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nwhere \\n{\\n denotes the union of function spaces (like the union of sets). The \\northogo-\\nnal complement\\n of \\nV\\nj\\n0\\n in \\nV\\nj\\n0\\n1\\n+\\n is \\nW\\nj\\n0\\n,\\n and the scaling functions that are the basis of \\nV\\nj\\n0\\n are orthogonal to the wavelet functions that are the basis of \\nW\\nj\\n0\\n:\\n \\nwc\\njk jl\\nxx k l\\n00\\n0\\n,,\\n() , ()\\n=≠\\n   for \\n \\n(7-129)\\nFigure 7.20 illustrates graphically the relationship between scaling and wavelet \\nspaces\\n. Each oval in the ﬁgure is a scaling space that, in accordance with Eq. (7-123), \\nis nested or contained within the next higher resolution scaling space. The difference \\nbetween adjacent scaling spaces is a wavelet space. Since wavelet space \\nW\\nj\\n resides \\nwithin scaling space \\nV\\nj\\n+\\n1\\n and \\nc\\njk j j\\nxWV\\n,\\n() ,\\nH(\\n+\\n1\\n wavelet function \\nc\\n()\\nx\\n—like its scal-\\ning function counterpart in Eq.\\n (7-125)—can be written as a weighted sum of shifted, \\ndouble-resolution scaling functions. That is, we can write\\n \\ncw\\nc\\n() () ( )\\nxh k x k\\nk\\n=\\n∑\\n22\\n-\\n \\n(7-130)\\nwhere the \\nhk\\nc\\n()\\n coefficients, called \\nwavelet function coefficients\\n, \\ncan be combined \\ninto the ordered set \\nhk k\\nh h\\ncc\\nc\\n() ,,, () , ( ) , .\\n=\\n{}\\n=\\n{}\\n012 0 1\\n……\\n Since integer wavelet \\ntranslates are orthogonal to one another and to their complementary scaling func-\\ntions\\n, it can be shown (see, for example, Burrus, Gopinath, and Guo [1998]) that the \\nhk\\nc\\n()\\n of Eq. (7-130) are related to the \\nhk\\nw\\n()\\n of Eq. (7-125) by\\n \\nhk h k\\nk\\ncw\\n() ( ) ( )\\n=− −\\n11\\n \\n(7-131)\\nEXAMPLE 7.17 :   The Haar wavelet function and coefﬁcients.\\nIn the previous example, the Haar scaling coefﬁcients were deﬁned as \\nhn n\\nw\\n() , , .\\n=\\n{}\\n=\\n{}\\n0 1 12 12\\n \\nUsing Eq.\\n (7-131), the corresponding wavelet function coefﬁcients are\\n \\nhh\\nhh\\ncw\\ncw\\n() ( ) ( )\\n() ( ) ( )\\n01 1 0 1 2\\n11 1 1 1 2\\n0\\n1\\n==\\n==\\n−−\\n−− −\\nso \\nhn n\\nc\\n() , , .\\n=\\n{}\\n=\\n{}\\n0 1 1212\\n−\\nThese coefﬁcients correspond to the second row of matrix \\nA\\nH\\n for \\nThe \\northogonal comple-\\nment\\n of vector space \\nVW\\nH\\n is the set of \\nvectors in \\nV\\n that are \\northogonal to every  \\nvector in \\nW\\n.\\nWavelet function \\ncoefﬁcients can also be \\ncombined in a \\nwavelet \\nvector\\n.\\nFIGURE 7.20\\nThe relationship \\nbetween scaling \\nand wavelet func-\\ntion spaces.\\nV\\n0\\nVVW\\n10 0\\n=\\n{\\nVV WVWW\\n2110 0 1\\n==\\n{{ {\\nW\\n0\\nW\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   508\\n6/16/2017   2:09:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 509}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n509\\nN\\n = 2 in Eq. (7-119). Substituting these values into Eq. (7-130), we get \\ncw w\\n()\\n( ) ( ) ,\\nxx x\\n=\\n22 1\\n−−\\n which is \\nplotted in F\\nig. 7.21(a). Thus, the Haar \\nmother wavelet function\\n is\\n \\nc\\n()\\n.\\n.\\nx\\nx\\nx\\n=\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n10\\n05\\n10\\n5 1\\n0\\n≤<\\n−≤ <\\nelsewhere\\n \\n(7-132)\\nNote it is also the Haar basis function \\nhx\\n1\\n()\\n of Eq. (7-115). Using Eq. (7-127), we can now generate the \\nuniverse of scaled and translated Haar wavelets\\n. Two such wavelets, \\nc\\n02\\n,\\n()\\nx\\n and \\nc\\n10\\n,\\n() ,\\nx\\n are plotted in \\nF\\nigs. 7.21(b) and (c), respectively. Note wavelet \\nc\\n10 1\\n,\\n()\\nxW\\nH\\n is narrower than \\nc\\n02 0\\n,\\n()\\nxW\\nH\\n and as such can \\nbe used to represent functions of ﬁner detail.\\nF\\nigure 7.21(d) shows a member of function space \\nV\\n1\\n that is not in \\nV\\n0\\n.\\n This function was consid-\\nered in Example 7.15 [see F\\nig. 7.19(f)]. Although the function cannot be represented accurately in \\nV\\n0\\n, \\nEq.\\n (7-128) indicates that it can be written as a function of \\nV\\n0\\n and \\nW\\n0\\n scaling and wavelet functions. The \\nresulting expansion is\\n \\nfx f x f x\\nad\\n() () ()\\n=+\\nwhere\\n \\nfx x x\\na\\n() () ()\\n,,\\n=\\n32\\n4\\n2\\n8\\n00\\n02\\nww\\n−\\nand\\n \\nfx x x\\nd\\n() () ()\\n,,\\n=\\n−\\n−\\n2\\n4\\n2\\n8\\n00\\n02\\ncc\\n0123\\n0\\n1\\n−\\n1\\n0123\\n0\\n1\\n−\\n1\\n0123\\n0\\n1\\n−\\n1\\n0123\\n0\\n1\\n−\\n1\\n0123\\n0\\n1\\n−\\n1\\n0123\\n0\\n1\\n−\\n1\\nx\\nx\\nx\\nx\\nx\\nx\\nfx V V W\\n()\\nH{\\n10 0\\n=\\nfx V\\na\\n()\\nH\\n0\\nfx W\\nd\\n()\\nH\\n0\\n32 4\\n00\\nw\\n,\\n()\\nx\\n−\\n28\\n02\\nw\\n,\\n()\\nx\\ncc\\n() ()\\n,\\nxx\\n=\\n00\\ncc\\n02 00\\n2\\n,,\\n() ( )\\nxx\\n=−\\ncc\\n10\\n22\\n,\\n() ( )\\nxx\\n=\\n−\\n24\\n00\\nc\\n,\\n()\\nx\\n−\\n28\\n02\\nc\\n,\\n()\\nx\\nb a\\nc\\ne d\\nf\\nFIGURE 7.21\\n Haar wavelet functions.\\nDIP4E_GLOBAL_Print_Ready.indb   509\\n6/16/2017   2:09:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 510}),\n",
       " Document(page_content=\"510\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nHere, \\nfx\\na\\n()\\n is an approximation of \\nfx\\n()\\n using \\nV\\n0\\n scaling functions, while \\nfx\\nd\\n()\\n is difference \\nfx f x\\na\\n() ()\\n−\\n \\nas a sum of \\nW\\n0\\n wavelets. These approximations and differences, which are shown in Figs. 7.19(e)  \\nand (f), divide \\nfx\\n()\\n in a manner similar to lowpass and highpass ﬁltering. The low frequencies of \\nfx\\n()\\n \\nare captured in \\nfx\\na\\n()\\n—it assumes the average value of \\nfx\\n()\\n in each integer interval—while the higher-\\nfrequenc\\ny details are encoded in \\nfx\\nd\\n() .\\nWAVELET SERIES EXPANSION\\nCombining Eqs. (7-124) and (7-128), the space of all measureable, square-integra-\\nble functions can be defined as \\nLV W W\\njjj\\n2\\n1\\n000\\n()\\n,\\nR\\n=\\n+\\n{{ {\\n…\\n where \\nj\\n0\\n is an arbi-\\ntrary starting scale. We can then define the \\nwavelet series expansion\\n of function \\nfx L\\n()\\n( )\\nH\\n2\\nR\\n with respect to wavelet \\nc\\n()\\nx\\n and scaling function \\nw\\n()\\nx\\n as\\n \\nfx c k x d k x\\nj\\nk\\njk j j k\\nk\\njj\\n()\\n=\\n() ()\\n+\\n() ()\\n∑∑\\n∑\\n=\\n∞\\n00\\n0\\nwc\\n,,\\n \\n(7-133)\\nwhere \\nc\\nj\\n0\\n and \\nd\\nj\\n for \\njj\\n≥\\n0\\n are called \\napproximation\\n and \\ndetail\\n \\ncoefficients\\n, respec-\\ntively. Any \\nmeasureable, square-integrable, 1-D function can be expressed as a \\nweighted sum of \\nV\\nj\\n0\\n scaling functions and \\nW\\nj\\n wavelets for \\njj\\n≥\\n0\\n.\\n The first sum in \\nEq.\\n (7-133) produces an approximation of \\nfx\\n()\\n from scale \\nj\\n0\\n scaling functions; each \\nsuccessive scale of the second sum provides increasing detail as a sum of higher-\\nresolution wavelets. If the scaling and wavlet functions are orthonormal,\\n  \\ncf x x\\njj\\nk\\n00\\n=\\n() , ()\\n,\\nw\\n \\n(7-134)\\nand\\n  \\ndf x x\\njj\\nk\\n=\\n() , ()\\n,\\nc\\n \\n(7-135)\\nHere, we have used Eq. (7-13). If they are part of a biorthogonal basis, the \\nw\\n and \\nc\\n \\nterms must be replaced by their dual functions\\n, \\nw\\n'\\n and \\nc\\n'\\n,\\n respectively.\\nEXAMPLE 7.18 :   The Haar wavelet series expansion of \\ny\\n = \\nx\\n2\\n.\\nConsider the simple function\\n \\ny\\nxx\\n=\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n2\\n01\\n0\\n≤≤\\notherwise\\nshown in Fig. 7.22(a). Using Haar wavelets—see Eqs. (7-122) and (7-132)—and starting scale \\nj\\n0\\n = 0, \\nEqs. (7-134) and (7-135) can be used to compute the following expansion coefﬁcients:\\n \\ncx x d x x d x\\nx\\ndx x d x\\n0\\n2\\n00\\n2\\n3\\n0\\n1\\n0\\n2\\n00\\n0\\n3\\n1\\n3\\n0\\n0\\n1\\n0\\n1\\n0\\n1\\n()\\n=\\n()\\n== =\\n()\\n=\\n()\\n=\\n22\\n2\\nw\\nc\\n,\\n,\\n0 0\\n05\\n05\\n1\\n22\\n1\\n4\\n.\\n.\\n22\\nxd x xd x\\n−−\\n=\\nDIP4E_GLOBAL_Print_Ready.indb   510\\n6/16/2017   2:09:54 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 511}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n511\\n \\ndx x d x x d x x d x\\nd\\n1\\n2\\n10\\n22\\n1\\n02\\n2\\n2\\n32\\n1\\n0\\n1\\n0\\n02 5\\n02 5\\n05\\n0\\n()\\n=\\n()\\n=−=\\n()\\n=\\n22 2\\nc\\n,\\n.\\n.\\n.\\n−\\n1 1\\n05\\n07 5\\n07 5\\n1\\n2\\n11\\n22\\n22\\n32\\n32\\n22 2\\nx x dx x dx x dx\\nc\\n,\\n.\\n.\\n.\\n()\\n=−=\\n−\\nSubstituting these values into Eq. (7-133), we get the wavelet series expansion\\n \\nyx x\\nV\\nW\\nVVW\\n=\\n()\\n+\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=⊕\\n1\\n3\\n1\\n4\\n00\\n00\\n0\\n0\\n10 0\\nwc\\n,,\\n/dncurlybracketleft/dncurlybracketmid\\n/horizcurlybracketext/dncurlybracketright\\n/horizcurlybracketext\\n/dncurlybracketleft/dncurlybracketmid\\n/horizcurlybracketext\\n/horizcurlybracketext/dncurlybracketright\\n/horizcurlybracketext/horizcurlybracketext\\n/dncurlybracketleft/dncurlybracketmid\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n−\\n/dncurlybracketright /dncurlybracketright\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n/dncurlybracketleft/dncurlybracketmid\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n/horizcurlybracketext/dncurlybracketright\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n+\\n()\\n−\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=⊕\\n−\\n2\\n32\\n32\\n32\\n10\\n11\\n1\\n21\\ncc\\n,,\\nxx\\nW\\nVVW\\n1\\n10 0 1\\n=⊕ ⊕\\n+\\nVWW\\n/dncurlybracketleft/dncurlybracketmid\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n/horizcurlybracketext/dncurlybracketright\\n/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext/horizcurlybracketext\\n/midhorizellipsis\\nThe ﬁrst term in this expansion employs \\nc\\n0\\n0\\n()\\n to generate a \\nV\\n0\\n approximation of the function being \\nexpanded. This approximation is shown in Fig. 7.22(b) and is the average value of the original function. \\nThe second term uses \\nd\\n0\\n0\\n()\\n to reﬁne the approximation by adding a level of detail from wavelet space \\nW\\n0\\n.\\n The added detail and resulting \\nV\\n1\\n approximation are shown in Figs. 7.22(c) and (d), respectively. \\nAnother level of detail is formed from the products of \\nd\\n1\\n0\\n()\\n and \\nd\\n1\\n1\\n()\\n with the corresponding wavelets \\nof \\nW\\n1\\n.\\n This additional detail is shown in Fig. 7.22(e), and the resulting \\nV\\n2\\n approximation is depicted \\nin Fig. 7.22(f). Note the expansion is now beginning to resemble the original function. As higher scales \\n(greater levels of detail) are added, the approximation becomes a more precise representation of the \\nfunction, realizing it in the limit as \\nj\\n→\\n/H11009\\n.\\nb a\\nc\\ne d\\nf\\nFIGURE 7.22\\n A wavelet series expansion of \\ny\\n = \\nx\\n2\\n using Haar wavelets.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nx\\n0.25 0.5 0.75 1\\n0\\n0.5\\n1\\n0\\n−\\n05\\n.\\nyx\\n=\\n2\\n13\\n00\\nw\\n,\\n−\\n14\\n00\\nc\\n,\\n−\\n23 2\\n10\\nc\\n,\\n−3\\n23 2\\n11\\nc\\n,\\nW\\n0\\nW\\n1\\nV\\n0\\nV\\n1\\nV\\n2\\nDIP4E_GLOBAL_Print_Ready.indb   511\\n6/16/2017   2:09:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 512}),\n",
       " Document(page_content=\"512\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nDISCRETE WAVELET TRANSFORM IN ONE DIMENSION\\nLike a Fourier series expansion, the wavelet series expansion of the previous section \\nmaps a function of a single continuous variable into a sequence of discrete coef-\\nficients. If the function being expanded is discrete, the coefficients of the expansion \\nare its \\ndiscrete wavelet transform\\n (DWT) and the expansion itself is the function’s \\ninverse discrete wavelet transform\\n. Letting \\nj\\n0\\n = 0 in Eqs. (7-133) through (7-135) and \\nrestricting attention to \\nN\\n-point discrete functions in which \\nN\\n is a power of 2 (i.e., \\nN\\n = 2\\nJ\\n), we get\\n \\nfx\\nN\\nTx T j k x\\njk\\nk j\\nJ\\nj\\n() (,)() ( , ) ()\\n,\\n=+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n= =\\n∑ ∑\\n1\\n00\\n0\\n21\\n0\\n1\\nwc\\nwc\\n− −\\n \\n(7-136)\\nwhere\\n \\nTf x x f x x\\nN\\nfx x\\nx\\nN\\nw\\nww w\\n(,) () , () () ,() () ()\\n,\\n*\\n00\\n1\\n00\\n0\\n1\\n== =\\n=\\n−\\n∑\\n \\n(7-137)\\nand\\n \\nTj k f x x\\nN\\nfx x\\njk\\njk\\nx\\nN\\nc\\ncc\\n( , ) () , () () ()\\n,,\\n*\\n==\\n=\\n∑\\n1\\n0\\n1\\n−\\n \\n(7-138)\\nfor \\njJ\\n=\\n01\\n1\\n,, ,\\n…\\n−\\n and \\nk\\nj\\n=\\n01 2 1\\n,, , .\\n…\\n−\\n The transform coefficients defined by \\nEqs\\n. (7-137) and (7-138) are called \\napproximation\\n and \\ndetail coefficients\\n, respec-\\ntively. They correspond to the \\nck\\nj\\n0\\n()\\n and \\ndk\\nj\\n()\\n of the wavelet series expansion in the \\nprevious section.\\n Note the integrations of the series expansion have been replaced \\nby summations in Eqs. (7-137) through (7-138). In the discrete case, inner products \\nlike those of Eqs. (7-1) and (7-2), as opposed to Eq. (7-3), are used. In addition, a \\n1\\nN\\n normalizing factor, reminiscent of the DFT in Example 7.6, has been added \\nto both the forward and inverse transforms\\n. This factor alternately could be incor-\\nporated into the forward or inverse alone as \\n1\\nN\\n.\\n Finally, it should be remembered \\nthat Eqs\\n. (7-137) through (7-138) are valid for orthonormal bases. If the scaling and \\nwavelet functions are real-valued, the conjugations can be dropped. If the basis is \\nbiorthogonal, the \\nw\\n and \\nc\\n terms in Eqs. (7-137) and (7-138) must be replaced by \\ntheir duals\\n, \\nw\\n'\\n and \\nc\\n'\\n, respectively.\\nEXAMPLE 7.19 :   A 1-D discrete wavelet transform.\\nTo illustrate the use of Eqs. (7-137) through (7-138), consider a discrete function of four points in which \\nf\\n()\\n,\\n01\\n=\\n \\nf\\n()\\n,\\n14\\n=\\n \\nf\\n()\\n,\\n23\\n=−\\n and \\nf\\n()\\n.\\n30\\n=\\n Since \\nN\\n = 4,\\n  \\nJ\\n is 2 and the summations in Eqs. (7-136) through \\n(7-138) are performed for \\nx\\n = 0, 1, 2, 3. When \\nj\\n is 0, \\nk\\n is 0; when \\nj\\n is 1, \\nk\\n is 0 or 1. If we use Haar scaling \\nand wavelet functions and assume the four samples of \\nfx\\n()\\n are distributed over the support of the scal-\\ning function,\\n which is 1, Eq. (7-137) gives\\n \\nTf\\nx x\\nx\\nw\\nw\\n00\\n1\\n2\\n1\\n2\\n11 41 31 01\\n0\\n3\\n,( ) ( )\\n()\\n==\\n() ()\\n+\\n() ()\\n+\\n() ( )\\n+\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n∑\\n−=\\n=\\n1\\nRemember that for \\ndiscrete inputs, \\nx\\n is a \\ndiscrete variable that \\ntakes on integer values \\nbetween 0 and \\nN\\n−\\n1.\\nDIP4E_GLOBAL_Print_Ready.indb   512\\n6/16/2017   2:09:57 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 513}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n513\\nNote we have employed uniformly spaced samples of the Haar scaling function for \\nj\\n = \\nk\\n = 0—i.e.,  \\nw\\n()\\nx\\n=\\n1\\n for \\nx\\n = 0,\\n 1, 2, 3. The sampled values match the elements of the ﬁrst row of Haar transformation \\nmatrix \\nA\\nH\\n in Eq. (7-120) of Section 7.9. Using Eq. (7-138) and similarly spaced samples of \\nc\\njk\\nx\\n,\\n() ,\\n which \\nare the elements of rows 2,\\n 3, and 4 of \\nA\\nH\\n, we get\\n \\nT\\nT\\nc\\nc\\n00\\n1\\n2\\n11 41 3 1 0 1 4\\n10\\n1\\n2\\n1\\n,\\n,\\n()\\n=\\n() ()\\n+\\n() ()\\n+\\n() ()\\n+\\n() ( )\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n()\\n=\\n(\\n−− −\\n) )\\n(\\n)\\n+\\n()\\n(\\n)\\n() ( )\\n+\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n()\\n=\\n() ()\\n+\\n()\\n24 2 3 00 0 1 5 2\\n11\\n1\\n2\\n10 4\\n−+\\n−−\\n.\\n,\\nT\\nc\\n0\\n03 2 0 2 1 5 2\\n()\\n+\\n()\\n(\\n)\\n+\\n()\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−− −\\n.\\nThus, the discrete wavelet transform of our simple four-sample function relative to Haar scaling and \\nwavelet functions is \\n14 1 5 2 1 5 2\\n,, . , . .\\n−−\\n{}\\n Since the transform coeffcients are a function of two vari-\\nables—scale \\nj\\n and translation \\nk\\n—we combine them into an \\nordered set\\n. The elements of this set turn out \\nto be identical to the elements of the sequency-ordered Haar transform of the function:\\n \\ntA f\\nHH\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n11 11\\n11 11\\n22 00\\n00 22\\n1\\n4\\n3\\n0\\n−−\\n−\\n−\\n−\\n⎥ ⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n1\\n4\\n15 2\\n15 2\\n−\\n−\\n.\\n.\\nRecall from the previous section that Haar transforms are a function of a single transform domain vari-\\nable\\n, denoted \\nu\\n.\\nEquation (7-136) enables the reconstruction of the original function from its wavelet transform coef-\\nﬁcients. Expanding the summation, we get\\n \\nfx T x T x T x T\\n() (,)() (,) () ( ,) () ( ,)\\n,,\\n=+ + +\\n1\\n2\\n00 00\\n10\\n11\\n00\\n10\\n1\\nwc c c\\nwc c c\\n,,\\n()\\n1\\nx\\n⎡\\n⎣\\n⎤\\n⎦\\nfor \\nx\\n = 0,\\n 1, 2, 3. If \\nx\\n = 0, for instance,\\n \\nf\\n0\\n1\\n2\\n1 1 4 1 15 2 2 15 2 0 1\\n()\\n=\\n() ()\\n+\\n() ()\\n+\\n(\\n)\\n(\\n)\\n(\\n)\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−+ −\\n..\\nAs in the forward case, uniformly spaced samples of the scaling and wavelet functions are used in the \\ncomputation of the inverse\\n. \\nThe Fast Wavelet Transform\\nThe multiresolution reﬁnement equation and its wavelet counterpart, Eqs. (7-125) \\nand (7-130), make it possible to deﬁne the scaling and wavelet functions at any scale \\nas a function of shifted, double-resolution copies of the scaling functions at the next \\nhigher scale. In the same way, the expansion coefﬁcients of the wavelet series expan-\\nDIP4E_GLOBAL_Print_Ready.indb   513\\n6/16/2017   2:09:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 514}),\n",
       " Document(page_content='514\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nsion and discrete wavelet transform can be computed recursively (see Problem 7.35) \\nusing\\n \\nck hn k c n\\njj\\nn\\n() ( ) ()\\n=\\n+\\n∑\\nw\\n−\\n2\\n1\\n \\n(7-139)\\n \\n \\ndk hn k c n\\njj\\nn\\n() ( ) ()\\n=\\n+\\n∑\\nc\\n−\\n2\\n1\\n \\n(7-140)\\nand \\n \\nTj k hn k Tj n\\nn\\nww w\\n(, ) ( ) ( , )\\n=\\n∑\\n−+\\n21\\n \\n(7-141)\\n \\nTj k hn k Tj n\\nn\\ncc w\\n(, ) ( ) ( , )\\n=\\n∑\\n−+\\n21\\n \\n(7-142)\\nrespectively. In contrast to Eqs. (7-133) and (7-136), where the only scaling coef-\\nﬁcients that are needed in the computations are at scale \\nj\\n0\\n, Eqs. (7-139) through \\n(7-142) require the computation of all scaling coefﬁcients up to the highest scale of \\ninterest. Comparing these equations to the equation deﬁning discrete convolution \\n[i.e., Eq. (4-48)], we see that \\nn\\n is a dummy variable of convolution and the remaining \\nminus signs and 2\\nk\\n terms reverse the order of the \\nh\\nw\\n and \\nh\\nc\\n coefﬁcients and sample \\nthe convolution results at \\nn\\n = 0, 2, 4, ..., respectively. Thus, for the discrete wavelet \\ntransform, we can rewrite Eqs. (7-141) and (7-142) as\\n \\nTj k Tj n h n\\nww w\\n(, ) ( , ) ( )\\n=+ −\\n1\\n/H22841\\n \\n(7-143)\\n \\nTj k Tj n h n\\ncw c\\n(, ) ( , ) ( )\\n=+ −\\n1\\n/H22841\\n \\n(7-144)\\nwhere the convolutions are evaluated at instants \\nn\\nj\\n=\\n02 2 2\\n1\\n,, , .\\n…\\n+\\n−\\n As indicated \\nin F\\nig. 7.23, evaluating convolutions at nonnegative, even indices is equivalent to \\nﬁltering and \\ndownsampling\\n by 2 (i.e., discarding every other convolved value). For \\na 1-D sequence of samples \\nyn\\n()\\n for \\nn\\n = 0,\\n 1, 2, …, downsampled sequence \\nyn\\n2\\n↓\\n()\\n is \\ndeﬁned as\\n \\nyn y n n\\n2\\n20 1\\n↓\\n==\\n() ( ) ,,\\n    for \\n…\\n \\n(7-145)\\nEquations (7-143) and (7-144) are the deﬁning equations of a computationally \\nefﬁcient form of the D\\nWT called the \\nfast wavelet transform\\n (FWT). For an input \\nsequence of length \\nN\\n = 2\\nJ\\n, the number of mathematical operations involved is on \\nRecall from Section 3.4 \\nthat the use of  \\ncorrelation or  \\nconvolution in spatial \\nﬁltering is a matter of \\npersonal preference.\\nFIGURE 7.23\\nA FWT analysis \\nﬁlter bank for \\northonormal \\nﬁlters. The \\n/H22841\\n \\nand \\n2\\n↓\\n denote \\nconvolution and \\ndownsampling  \\nby 2,\\n respectively.\\n2\\n↓\\n2\\n↓\\nT\\njk\\nw\\n()\\n+,\\n1\\nT\\njk\\nc\\n()\\n,\\nT\\njk\\nw\\n()\\n,\\n/H22841\\nh\\nn\\nw\\n()\\n−\\n/H22841\\nh\\nn\\nc\\n()\\n−\\nDIP4E_GLOBAL_Print_Ready.indb   514\\n6/16/2017   2:09:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 515}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n515\\nthe order of \\nO\\n(\\nN\\n). That is, the number of multiplications and additions is linear with \\nrespect to the length of the input sequence—because the number of multiplications \\nand additions involved in the convolutions performed by the FWT ﬁlter bank in \\nFig. 7.23 is proportional to the length of the sequences being convolved. Thus, the \\nFWT compares favorably with the FFT algorithm, which requires on the order of \\nON N\\n(l\\no g )\\n2\\n operations.\\nFigure 7.24(a) shows a three-scale ﬁlter bank in which the FWT \\nanalysis ﬁlter \\nof \\nFig. 7.23 has been “iterated” three times to create a three-stage structure for com-\\nputing transform coefﬁcients at scales \\nJ\\n−\\n1,\\n \\nJ\\n−\\n2,\\n and \\nJ\\n−\\n3.\\n Note the highest scale \\ncoefﬁcients are assumed to be samples of the function itself\\n.\\n†\\n Otherwise, the approx-\\nimation and detail coefﬁcients at scale \\nj\\n are computed by convolving \\nTj k\\nw\\n(, ) ,\\n+\\n1  \\nthe scale \\nj\\n+\\n1\\n approximation coefﬁcients, with the order-reversed scaling and wave-\\nlet coefﬁcients\\n, \\nhn\\nw\\n()\\n−\\n and \\nhn\\nc\\n() ,\\n−\\n and subsampling the results. If there are \\nK\\n scal-\\ning and wavelet function coefﬁcients\\n, the order reversed scaling and wavelet coefﬁ-\\ncients are \\nEF\\nhK m m K\\nw\\n() | , , ,\\n−− −\\n10 1 1\\n=\\n…\\n and \\nEF\\nhK m m K\\nc\\n() | , , , ,\\n−−\\n−\\n10 1 1\\n=\\n…\\n \\nrespectively. For a discrete input of length \\nN\\n = 2\\nJ\\n, the filter bank in Fig. 7.23 can \\n† If function \\nfx\\n()\\n is sampled above the Nyquist rate, as is usually the case, its samples are good approximations \\nof the scaling coefﬁcients at the sampling resolution and can be used as the starting high-resolution scaling \\ncoefﬁcient inputs\\n. In other words, no wavelet or detail coefﬁcients are needed at the sampling scale. The highest-\\nresolution scaling functions act as unit discrete impulse functions in Eqs. (7-141) and (7-142), allowing \\nfx\\n()\\n to \\nbe used as the scaling (approximation) input to the ﬁrst two-band ﬁlter bank (Odegard,\\n Gopinath, and Burrus \\n[1992]).\\nA \\nP\\n-scale FWT employs \\nP\\n ﬁlter banks to generate \\na \\nP\\n-scale transform at \\nscales \\nJ\\n−\\n1,  \\nJ\\n−\\n2, …, \\nJP\\n−\\n,  where \\nPJ\\n≤\\n.\\nb\\na\\nFIGURE 7.24\\n(a) A three-stage \\nor three-scale \\nFWT analysis \\nﬁlter bank and  \\n(b) its frequency-\\nsplitting  \\ncharacteristics. \\nBecause of sym-\\nmetry in the DFT \\nof the ﬁlter’s \\nimpulse response, \\nit is common to \\ndisplay only the \\n0,\\np\\n[]\\n region.\\n/H22841\\nh\\nn\\nw\\n()\\n−\\n/H22841\\nh\\nn\\nc\\n()\\n−\\n2\\n↓\\n2\\n↓\\n/H22841\\nh\\nn\\nw\\n()\\n−\\n/H22841\\nh\\nn\\nc\\n()\\n−\\n2\\n↓\\n2\\n↓\\n/H22841\\nh\\nn\\nw\\n()\\n−\\n/H22841\\nh\\nn\\nc\\n()\\n−\\n2\\n↓\\n2\\n↓\\nT\\nJk\\nc\\n()\\n−3 ,\\nT\\nJk\\nc\\n()\\n−2 ,\\nT\\nJk\\nc\\n()\\n−1 ,\\nT\\nJk\\nw\\n()\\n−3 ,\\nT\\nJk\\nw\\n()\\n−2 ,\\nT\\nJk\\nw\\n()\\n−1 ,\\nv\\nH\\n()\\nv\\np\\np\\n2\\np\\n4\\np\\n8\\nW\\nJ\\n−\\n3\\nW\\nJ\\n−\\n2\\nV\\nJ\\n−\\n3\\nV\\nJ\\n−\\n2\\nV\\nJ\\n−\\n1\\nV\\nJ\\n0\\nW\\nJ\\n−\\n1\\nfx\\nT\\nJk\\n()\\n(, )\\n=\\nw\\nDIP4E_GLOBAL_Print_Ready.indb   515\\n6/16/2017   2:10:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 516}),\n",
       " Document(page_content='516\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nbe iterated up to \\nJ\\n times. In operation, the leftmost ﬁlter bank of Fig. 7.24(a) splits \\nthe input function into a lowpass \\napproximation\\n component that corresponds to \\nscaling coefﬁcients \\nTJ k\\nw\\n(, )\\n−\\n1\\n and a highpass \\ndetail\\n component corresponding to \\ncoefﬁcients \\nTJ k\\nc\\n(, ) .\\n−\\n1\\n This is illustrated graphically in Fig. 7.24(b), where scaling \\nspace \\nV\\nJ\\n is split into wavelet space \\nW\\nJ\\n−\\n1\\n and scaling space \\nV\\nJ\\n−\\n1\\n.\\n The spectrum of \\nthe original function is split into two half-band components\\n. The second ﬁlter bank \\nin Fig. 7.24(a) splits the spectrum of scaling space \\nV\\nJ\\n−\\n1\\n,\\n the lower half-band of the \\nﬁrst ﬁlter bank,\\n into quarter-band spaces \\nW\\nJ\\n−\\n2\\n and \\nV\\nJ\\n−\\n2\\n and corresponding FWT \\ncoefﬁcients \\nTJ k\\nc\\n(, )\\n−\\n2\\n and \\nTJ k\\nw\\n(, ) ,\\n−\\n2\\n respectively. Finally, the third ﬁlter bank \\ngenerates eigth-band spaces \\nW\\nJ\\n−\\n3\\n and \\nV\\nJ\\n−\\n3\\n with FWT coefﬁcients \\nTJ k\\nc\\n(, )\\n−\\n3\\n and \\nTJ k\\nw\\n(, ) .\\n−\\n3\\n As was noted in connection with Eq. (7-73) of Section 7.4 and demon-\\nstrated in F\\nig. 7.5, as the scale of the wavelet functions increases, the spectra of the \\nwavelets are stretched (i.e., their bandwidth is doubled and shifted higher by a fac-\\ntor of two). In Fig. 7.24(b), this is evidenced by the fact that the bandwidth of \\nW\\nJ\\n−\\n1\\n \\nis \\np\\n2,\\n while the bandwidths of \\nW\\nJ\\n−\\n2\\n and \\nW\\nJ\\n−\\n3\\n are \\np\\n4\\n and \\np\\n8,\\n respectively. For \\nhigher\\n-scale transforms, the spectra of the wavelets would continue to decrease in \\nbandwidth, but would never reach radian frequency \\nv\\n=\\n0.\\n A lowpass scaling func-\\ntion is always needed to capture the frequencies around DC\\n.\\nEXAMPLE 7.20 :   Computing a 1-D fast wavelet transform.\\nTo illustrate the preceding concepts, consider the discrete function \\nfx\\n()\\n, , ,\\n=\\n{}\\n14 30\\n−\\n from Exam-\\nple 7.19.\\n As in that example, we will compute its wavelet transform with respect to Haar scaling and \\nwavelet functions. Here, however, we will not use the Haar basis functions directly. Instead, we will use \\nthe corresponding scaling and wavelet coefﬁcients from Examples 7.16 and 7.17:\\n \\nhn n\\nw\\n() , ,\\n=\\n{}\\n=\\n{}\\n0 1 12 12\\n \\n(7-146)\\nand\\n \\nhn n\\nc\\n() , ,\\n=\\n{}\\n=\\n{}\\n0 1 12 12\\n−\\n \\n(7-147)\\nSince the transform computed in Example 7.19 was the ordered set \\nEF\\nTT\\nTT\\nwccc\\n( , ), ( , ), ( , ), ( , ) ,\\n00 00 10 11\\n \\nwe will compute the corresponding two-scale FWT for scales \\nj\\n=\\n{}\\n01\\n,.\\n Recall from the previous exam-\\nple that \\nk\\n = 0 when \\nj\\n = 0,\\n while \\nk\\n is 0 and 1 when \\nj\\n = 1. The transform will be computed using a two-stage \\nﬁlter bank that parallels the three-stage ﬁlter bank of Fig. 7.24(a). Figure 7.25 shows the resulting ﬁlter \\nbank and the sequences that follow from the required FWT convolutions and downsamplings. Note \\ninput function \\nfx\\n()\\n serves as the scaling (or approximation) input to the left most ﬁlter bank. To com-\\npute the \\nTn\\nc\\n(, )\\n1\\n coefﬁcients that appear at the end of the upper branch of Fig. 7.25, we ﬁrst convolve \\nfx\\n()\\n with \\nhn\\nc\\n() .\\n−\\n For Haar scaling and wavelet coefﬁcients, \\nK\\n = 2 and the order reversed wavelet coef-\\nﬁcients are \\nEF\\nE\\nF\\nE\\nF\\nh K mm K h mm\\ncc\\n( ) |, , , ( ) |, , .\\n−−\\n− −\\n−\\n10 1 1 10 1 1 2 1 2\\n=== =\\n…\\n As explained \\nin Section 3.4,\\n convolution requires ﬂipping one of the convolved functions about the origin, sliding it \\npast the other, and computing the sum of the point-wise product of the two functions. Flipping order-\\nreversed wavelet coefﬁcients \\nEF\\n−\\n12 12\\n,\\n to get \\nEF\\n1212\\n,\\n−\\n and sliding them from left-to-right \\nacross input sequence \\n14 30\\n,,\\n, ,\\n−\\n{}\\n we get\\n \\n−− −\\n12 32 72 32 0\\n,, ,,\\n{}\\nDIP4E_GLOBAL_Print_Ready.indb   516\\n6/16/2017   2:10:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 517}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n517\\nwhere the ﬁrst term corresponds to convolution index \\nn\\n=−\\n1.\\n In Fig. 7.25, convolution values that are \\nassociated with a negative dummy variable of convolution (i.e., \\nn\\n < 0) are denoted in blue. Since scale \\nj\\n = 1, the downsampled convolutions correspond to the even indices of \\nn\\n up to \\n22\\n1\\nj\\n+\\n−\\n.\\n Thus, \\nn\\n = 0 and \\n2 and \\nTn\\nc\\n13 2 3 2\\n,, .\\n()\\n=\\nEF\\n−−\\n The remaining convolutions and downsamplings are performed in a \\nsimilar manner\\n.\\nIn \\ndigital signal processing\\n (DSP), ﬁlters like those in Figs. 7.23 through 7.25 are \\nknown as \\nﬁnite impulse response\\n (FIR) ﬁlters. Their response to a unit impulse is \\na ﬁnite sequence of outputs that assumes the values of the ﬁlter coefﬁcients. Fig-\\nure 7.26(a) shows one well-known arrangement of real-coefﬁcient, FIR ﬁlters that \\nhas been studied extensively in the literature. Called a two-band \\nsubband coding\\n \\nand \\ndecoding\\n system, it is composed of two \\nanalysis\\n \\nfilters\\n, \\nhn\\n0\\n()\\n and \\nhn\\n1\\n() ,\\n and two \\nsynthesis filters\\n, \\ngn\\n0\\n()\\n and \\ngn\\n1\\n() .\\n The analysis filters decompose the input into two \\nhalf-length sequences \\nfn\\n0\\n()\\n and \\nfn\\n1\\n() .\\n As can be seen in Fig. 7.26(a), filter \\nhn\\n0\\n()\\n \\nis a lowpass filter whose output is an approximation of \\nfx\\n()\\n;\\n filter \\nhn\\n1\\n()\\n is a high-\\npass filter whose output is the difference between the lowpass approximation and \\nfx\\n()\\n.\\n As Fig. 7.26(b) shows, the spectrum of the input sequence is split into two \\nhalf-bands\\n, \\nH\\n0\\n()\\nv\\n and \\nH\\n1\\n() .\\nv\\n Synthesis bank filters \\ngn\\n0\\n()\\n and \\ngn\\n1\\n()\\n are then used \\nto reconstruct \\nˆ\\n()\\nfx\\n from \\nupsampled\\n versions of \\nfn\\n0\\n()\\n and \\nfn\\n1\\n() .\\n For a 1-D sequence \\nof samples \\nyn\\n()\\n,\\n upsampled sequence \\nyn\\n2\\n↑\\n()\\n can be defined as\\n \\nyn\\nyn n\\n2\\n2\\n0\\n↑\\n=\\n⎧\\n⎨\\n⎩\\n()\\n( ) if  is even\\notherwise\\n \\n(7-148)\\nwhere the upsampling is by a factor of 2. Upsampling by a factor of 2  can be thought \\nof as inserting a 0 after every sample of \\nyn\\n()\\n.\\nThe blocks containing  \\na \\n/H22841\\n in Figs. 7.23  \\nthrough 7.25 are FIR \\nﬁlters. FIR ﬁlters are also \\ndiscussed in Section 4.7. \\nNote we use \\nhn\\n( )  for \\nanalysis or decomposi-\\ntion ﬁlters, which include \\none scaling ﬁlter and one \\nwavelet ﬁlter, and \\ngn\\n()  \\nfor synthesis or recon-\\nstruction ﬁlters, which \\nalso include a scaling and \\nwavelet ﬁlter. The scaling \\nﬁlters are sometimes \\ncalled approximation or \\nlowpass ﬁlters and have a \\nsubscript of 0 in Fig. 7.26, \\nwhile the wavelet ﬁlters \\nare called detail or high-\\npass ﬁlters and have a \\nsubscript of 1.\\nT\\nw\\n(,)\\n00 1\\n=\\n{}\\nT\\nc\\n(,)\\n00 4\\n=\\n{}\\nTn\\nc\\n(, ) , ,\\n13 2 3 2\\n=\\n{}\\n−−\\nfx T n\\n() (,)\\n,, ,\\n=\\n=\\n{}\\nw\\n2\\n14 30\\n−\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n2\\n↓\\nTn\\nw\\n(, ) ,\\n15 2 3 2\\n=\\n{}\\n−\\n/H22841\\n12 12\\n,\\n{}\\n/H22841\\n12 12\\n,\\n{}\\n/H22841\\n−\\n12 12\\n,\\n{}\\n/H22841\\n−\\n12 12\\n,\\n{}\\n−\\n−−\\n12\\n32 72 32 0\\n,, ,,\\n{}\\n12\\n52 12 32 0\\n,,, ,\\n−\\n{}\\n−\\n−\\n25\\n41 5\\n.\\n,, .\\n{}\\n25\\n11 5\\n.\\n,, .\\n−\\n{}\\nFIGURE 7.25\\n Computing a two-scale fast wavelet transform of sequence \\n14 30\\n,,\\n,\\n−\\n{}\\n using Haar scaling and wavelet \\ncoefﬁcients.\\nDIP4E_GLOBAL_Print_Ready.indb   517\\n6/16/2017   2:10:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 518}),\n",
       " Document(page_content='518\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nThe goal in subband coding is to choose the analysis and synthesis ﬁlters so \\nˆ\\n()\\n() .\\nfx fx\\n=\\n When this is accomplished, the system is said to employ \\nperfect recon-\\nstruction ﬁlters\\n and the ﬁlters are\\n, up to some constant factors, related as follows:\\n \\ngn hn\\nn\\n01\\n() ( ) ()\\n=−\\n1  \\n(7-149)\\nand\\n \\ngn hn\\nn\\n10\\n() ( ) ()\\n=−\\n1  \\n(7-150)\\nIn these equations, \\n()\\n−\\n1\\nn\\n changes the signs of the odd-indexed analysis filter coef-\\nficients and is called \\nmodulation\\n. Each synthesis filter is a modulated version of the \\nanalysis filter that opposes it diagonally in Fig. 7.26(a). Thus, the analysis and synthe-\\nsis filters are said to be \\ncross-modulated\\n. Their impulse responses are biorthogonal. \\nIf they are also orthonormal and of length \\nK\\n, where \\nK\\n is divisible by 2, they satisfy \\nthe additional constraints that\\n \\ngn gK n\\nhn\\ngK n\\nhn gK n\\nn\\n10\\n00\\n11\\n()\\n=\\n() ( )\\n()\\n=\\n()\\n()\\n=\\n()\\n−− −\\n−−\\n−−\\n11\\n1\\n1\\n \\n(7-151)\\nNoting the similarity between the FWT analysis ﬁlter bank in Fig. 7.23 and the \\nsubband analysis ﬁlter bank of F\\nig. 7.26(a), we can postulate the inverse FWT \\nsynthe-\\nsis ﬁlter bank\\n of Fig. 7.27. For the case of orthonormal ﬁlters, Eq. (7-151) constrains \\nthe synthesis ﬁlters to be order-reversed versions of the analysis ﬁlters. Comparing \\nthe ﬁlters in Figs. 7.23 and 7.27, we see this is indeed the case. It must be remembered, \\nhowever, that perfect reconstruction is also possible with biorthogonal analysis and \\nsynthesis ﬁlters, which are not order-reversed versions of one another. Biorthogonal \\nanalysis and synthesis ﬁlters are cross-modulated in accordance with Eqs. (7-149) \\nand (7-150). Finally, we note the inverse ﬁlter bank of Fig. 7.27, like the forward FWT \\nEquations (7-149) and \\n(7-151) are described \\nin detail in the ﬁlter \\nbank literature (see, for \\nexample, Vetterli and \\nKovacevic [1995]).\\nFor many biorthogonal \\nﬁlters, \\ng\\n0\\n and \\ng\\n1\\n are dif-\\nferent in length, requir-\\ning the shorter ﬁlter to \\nbe zero-padded. In \\ncausal\\n \\nﬁlters, \\nn\\n≥\\n0  and the \\nouput depends only on \\ncurrent and past inputs.\\nb a\\nFIGURE 7.26\\n(a) A two-band \\ndigital ﬁltering \\nsystem for sub-\\nband coding and \\ndecoding and  \\n(b) its spectrum-\\nsplitting  \\nproperties.\\n2\\n↓\\n2\\n↓\\n2\\n↑\\n2\\n↑\\nfx\\n()\\n+\\nAnalysis\\nfilter bank\\nSynthesis\\nfilter bank\\n/H22841\\nhn\\n0\\n()\\n/H22841\\nhn\\n1\\n()\\n/H22841\\ngn\\n0\\n()\\n/H22841\\ngn\\n1\\n()\\nLow band High band\\n0\\nH\\n1\\n()\\nv\\nH\\n0\\n()\\nv\\nv\\np\\np\\n2\\nˆ\\n()\\nfx\\nfn\\n1\\n()\\nfn\\n0\\n()\\nDIP4E_GLOBAL_Print_Ready.indb   518\\n6/16/2017   2:10:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 519}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n519\\nﬁlter bank of Fig. 6.23, can be iterated for the computation of multiscale inverse \\nFWTs. In the next example, a two-scale inverse FWT structure is considered. The \\ncoefﬁcient combining process demonstrated there can be extended to any number \\nof scales.\\nEXAMPLE 7.21 :   Computing a 1-D inverse fast wavelet transform.\\nComputation of the inverse fast wavelet transform mirrors its forward counterpart. Figure 7.28 illustrates \\nthe process for the sequence considered in Example 7.20. To begin the calculation, the level 0 approxi-\\nmation and detail coefﬁcients are upsampled to yield \\n10\\n,\\n{}\\n and \\n40\\n,,\\n{}\\n respectively. Convolution with \\nﬁlters \\nhn\\nw\\n() ,\\n=\\nEF\\n12 12\\n and \\nhn\\nc\\n() ,\\n=\\nEF\\n1212\\n−\\n produces \\nEF\\n12 12 0\\n,,\\n and \\nEF\\n4242 0\\n,, ,\\n−\\n \\nwhich when added give \\nTn\\nw\\n(, ) , .\\n15 2 3 2\\n=\\nEF\\n−\\n Thus, the level 1 approximation of Fig. 7.28, which \\nmatches the computed approximation in Fig. 7.25, is reconstructed. Continuing in this manner, \\nfx\\n()\\n is \\nformed at the right of the second synthesis ﬁlter bank.\\n2\\n↑\\n2\\n↑\\n+\\n/H22841\\nh\\nn\\nw\\n()\\n/H22841\\nh\\nn\\nc\\n()\\nT\\njk\\nw\\n()\\n,\\nT\\njk\\nc\\n()\\n,\\nT\\njk\\nw\\n()\\n+,\\n1\\nFIGURE 7.27\\nAn inverse FWT \\nsynthesis ﬁlter \\nbank for ortho-\\nnormal ﬁlters.\\nT\\nc\\n(,)\\n00 4\\n=\\n{}\\nT\\nw\\n(,)\\n00 1\\n=\\n{}\\nTn\\nc\\n(, ) , ,\\n13 2 3 2\\n=\\n{}\\n−−\\nfx T n\\n() (,)\\n,, ,\\n=\\n=\\n{}\\nw\\n2\\n14 30\\n−\\n2\\n↑\\n2\\n↑\\n2\\n↑\\n2\\n↑\\nTn\\nw\\n(, ) ,\\n15 2 3 2\\n=\\n{}\\n−\\n/H22841\\n12 12\\n,\\n{}\\n/H22841\\n12 12\\n,\\n{}\\n/H22841\\n1212\\n,\\n−\\n{}\\n/H22841\\n1212\\n,\\n−\\n{}\\n+\\n+\\n10\\n,\\n{}\\n−−\\n32 0 32 0\\n,, ,\\n{}\\n52 0 32 0\\n,, ,\\n−\\n{}\\n40\\n,\\n{}\\n4242 0\\n,,\\n−\\n{}\\n25 25 15 15 0\\n., ., ., .,\\n−−\\n{}\\n12 12 0\\n,,\\n{}\\n−−\\n15 15 15 15 0\\n., ., ., .,\\n{}\\nFIGURE 7.28\\n Computing a two-scale inverse fast wavelet transform of sequence \\n14 1 5 2 1 5 2\\n,, . , .\\n−−\\n{}\\n with Haar \\nscaling and wavelet functions.\\nDIP4E_GLOBAL_Print_Ready.indb   519\\n6/16/2017   2:10:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 520}),\n",
       " Document(page_content='520\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nWAVELET TRANSFORMS IN TWO DIMENSIONS\\nThe 1-D wavelet transform of the previous section is easily extended to 2-D func-\\ntions such as images. In two dimensions, a two-dimensional scaling function, \\nw\\n(,\\n) ,\\nxy\\n \\nand three 2-D wavelets\\n, \\nc\\nH\\n(, ) ,\\nxy\\n \\nc\\nV\\n(, ) ,\\nxy\\n and \\nc\\nD\\n(, ) ,\\nxy\\n are required. Each is the \\nproduct of two 1-D functions\\n. Excluding products that produce 1-D results, like \\nwc\\n()\\n() ,\\nxx\\n the four remaining products produce the \\nseparable\\n scaling function\\n \\nf\\n1\\n/H11009\\n2\\n \\n(7-152)\\nand separable, “directionally sensitive” wavelets\\n \\ncc w\\nH\\n(, ) ()()\\nxy x y\\n=\\n \\n(7-153)\\n \\ncw c\\nV\\n(, ) ()()\\nxy x y\\n=\\n \\n(7-154)\\n \\ncc c\\nD\\n(, ) ()()\\nxy x y\\n=\\n \\n(7-155)\\nThese wavelets measure functional variations—intensity changes in images—along \\ndifferent directions:\\n \\nc\\nH\\n measures variations along columns (for example, horizontal \\nedges), \\nc\\nV\\n responds to variations along rows (like vertical edges), and \\nc\\nD\\n corre-\\nsponds to variations along diagonals. The directional sensitivity is a natural conse-\\nquence of the separability in Eqs. (7-153) to (7-155); it does not increase the compu-\\ntational complexity of the 2-D transform discussed in this section.\\nLike the 1-D discrete wavelet transform, the 2-D DWT can be implemented using \\ndigital ﬁlters and downsamplers. With separable 2-D scaling and wavelet functions, \\nwe simply take the 1-D FWT of the rows of \\nfx y\\n(,\\n) ,\\n followed by the 1-D FWT of \\nthe resulting columns\\n. Figure 7.29(a) shows the process in block diagram form. Note, \\nlike its 1-D counterpart in Fig. 7.23, the 2-D FWT “ﬁlters” the scale \\nj\\n+\\n1\\n approxima-\\ntion coefﬁcients\\n, denoted \\nTj k l\\nw\\n(, , )\\n+\\n1\\n in the ﬁgure, to construct the scale \\nj\\n approxi-\\nmation and detail coefﬁcients\\n. In the 2-D case, however, we get three sets of detail \\ncoefﬁcients—\\nhorizontal details\\n \\nTj k l\\nc\\nH\\n(, ,) ,\\n \\nvertical details\\n \\nTj k l\\nc\\nV\\n(, ,) ,\\n and \\ndiagonal \\ndetails\\n \\nTj k l\\nc\\nD\\n(, ,) .\\nThe single-scale ﬁlter bank of Fig. 7.29(a) can be “iterated” (by tying the approxi-\\nmation output to the input of another ﬁlter bank) to produce a \\nPJ\\n≤\\n scale trans-\\nform in which scale \\nj\\n is equal to \\nJJ J P\\n−−\\n−\\n12\\n,, , .\\n…\\n As in the 1-D case, image \\nfx y\\n(,\\n)\\n is used as the \\nTJ k l\\nw\\n(,, )\\n input. Convolving its rows with \\nhn\\nw\\n()\\n−\\n and \\nhn\\nc\\n()\\n−\\n \\nand downsampling its columns\\n, we get two subimages whose horizontal resolutions \\nare reduced by a factor of 2. The highpass or detail component characterizes the \\nimage’s high-frequency information with vertical orientation; the lowpass, approxi-\\nmation component contains its low-frequency, vertical information. Both subimages \\nare then ﬁltered columnwise and downsampled to yield four quarter-size output \\nsubimages—\\nT\\nw\\n, \\nT\\nc\\nH\\n, \\nT\\nc\\nV\\n,\\n and \\nT\\nc\\nD\\n.\\n These subimages, which are normally arranged as \\nin F\\nig. 7.29(b), are the inner products of \\nfx y\\n(,\\n)\\n and the two-dimensional scaling and \\nDIP4E_GLOBAL_Print_Ready.indb   520\\n6/16/2017   2:10:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 521}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n521\\nTj k l\\nw\\n(, , )\\n+\\n1\\nTj k l\\nw\\n(, ,)\\nTj k l\\nc\\nD\\n(, ,)\\nTj k l\\nc\\nH\\n(, ,)\\nTj k l\\nc\\nV\\n(, ,)\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n/H22841\\nh\\nn\\nc\\n()\\n−\\n/H22841\\nh\\nn\\nw\\n()\\n−\\n/H22841\\nh\\nm\\nc\\n()\\n−\\n/H22841\\nh\\nm\\nc\\n()\\n−\\n/H22841\\nh\\nm\\nw\\n()\\n−\\n/H22841\\nh\\nm\\nw\\n()\\n−\\nColumns\\n(along \\nn\\n)\\nColumns\\nRows\\n(along \\nm\\n)\\nRows\\nRows\\nRows\\nTj k l\\nw\\n(, ,)\\nTj k l\\nc\\nD\\n(, ,)\\nTj k l\\nc\\nH\\n(, ,)\\nTj k l\\nc\\nV\\n(, ,)\\nTj k l\\nw\\n(, , )\\n+\\n1\\n2\\n↑\\n2\\n↑\\n2\\n↑\\n2\\n↑\\n2\\n↑\\n2\\n↑\\n/H22841\\nh\\nn\\nw\\n()\\n/H22841\\nh\\nn\\nc\\n()\\n/H22841\\nh\\nm\\nc\\n()\\n/H22841\\nh\\nm\\nc\\n()\\n/H22841\\nh\\nm\\nw\\n()\\n/H22841\\nh\\nm\\nw\\n()\\n+\\n+\\n+\\nColumns\\n(along \\nn\\n)\\nColumns\\nRows\\n(along \\nm\\n)\\nRows\\nRows\\nRows\\nTj k l\\nw\\n(, , )\\n+\\n1\\nTj k l\\nw\\n(, ,)\\nTj k l\\nc\\nD\\n(, ,)\\nTj k l\\nc\\nV\\n(, ,)\\nTj k l\\nc\\nH\\n(, ,)\\nb\\na\\nc\\nFIGURE 7.29\\nThe 2-D fast \\nwavelet trans-\\nform: (a) the \\nanalysis ﬁlter \\nbank; (b) the \\nresulting decom-\\nposition; and  \\n(c) the synthesis \\nﬁlter bank. \\n \\nNote \\nm\\n and \\nn\\n are \\ndummy variables \\nof convolution, \\nwhile \\nj\\n, like in the \\n1-D case, is scale, \\nand \\nk\\n and \\nl\\n are \\ntranslations.\\nDIP4E_GLOBAL_Print_Ready.indb   521\\n6/16/2017   2:10:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 522}),\n",
       " Document(page_content='522\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nwavelet functions in Eqs. (7-152) through (7-155), followed by downsampling by two \\nin each dimension.\\nFigure 7.29(c) shows the synthesis ﬁlter bank that reverses the process just \\ndescribed. As would be expected, the reconstruction algorithm is similar to the 1-D \\ncase. At each iteration, four-scale \\nj\\n approximation and detail subimages are upsam-\\npled and convolved with two 1-D ﬁlters—one operating on the subimages’ columns \\nand the other on its rows. Addition of the results yields the scale \\nj\\n+\\n1\\n approximation, \\nand the process is repeated until the original image is reconstructed.\\nEXAMPLE 7.22 :   Computing 2-D fast wavelet transforms.\\nIn this example, we compute a 2-D, multiscale FWT with respect to Haar basis functions and compare \\nit to the traditional Haar transform of Section 7.9. Figures 7.30(a)–(d) show a \\n512 512\\n×\\n monochrome \\nimage of a vase on a windowsill,\\n its one- and two-scale discrete wavelet transforms with respect to Haar \\nbasis functions, and its Haar transform, respectively. The computation of the wavelet transforms will be \\ndiscussed shortly. The Haar transform in Fig. 7.30(d) is computed using a \\n512 512\\n×\\n Haar transformation \\nmatrix [see Eqs\\n. (7-114) through (7-118)] and the matrix-based operations deﬁned in Eq. (7-35). The \\ndetail coefﬁcients in Figs. 7.30(b) and (c), as well as the Haar transform coefﬁcients in Fig. 7.30(d), are \\nscaled to make their underlying structure more visible. When the same area of any two transforms is \\nshaded in blue, the corresponding pixels within those areas are identical in value.\\nTo compute the one-scale FWT of Fig. 7.30(b), the image in Fig. 7.30(a) is used as the input to a ﬁlter \\nbank like that of Fig. 7.29(a). Since \\nJ\\n==\\nlog\\n2\\n512 9\\n and \\nP\\n = 1,\\n \\nTk lf x y\\nw\\n(, ,) (, )\\n9\\n=\\n and the four resulting \\nquarter\\n-size decomposition outputs [i.e., approximation \\nTk l\\nw\\n(, ,)\\n8\\n and horizontal, vertical, and diagonal \\ndetails \\nTk l\\nc\\nH\\n(, ,) ,\\n8  \\nTk l\\nc\\nV\\n(, ,) ,\\n8\\n and \\nTk l\\nc\\nD\\n(, ,)\\n8\\n] are then arranged in accordance with Fig. 7.29(b) to pro-\\nduce F\\nig. 7.30(b). A similar process is used to generate the two-scale transform in Fig. 7.30(c), but the \\ninput to the ﬁlter bank is a quarter-size approximation subimage \\nTk l\\nw\\n(, ,) ,\\n8\\n from the upper left-hand \\ncorner of F\\nig. 7.30(b). As can be seen in Fig. 7.30(c), the quarter-size approximation subimage is then \\nreplaced by the four quarter-size (now \\n11 6 t h\\n of the size of the original image) decomposition results \\nthat were generated by the second ﬁltering pass\\n. Each pass through the ﬁlter bank produces four quar-\\nter-size output images which are substituted for the input from which they were derived. The process is \\nrepeatable until \\nP\\n = \\nJ\\n = 9, which produces a nine-scale transform.\\nNote the directional nature of the subimages associated with \\nT\\nc\\nH\\n, \\nT\\nc\\nV\\n,\\n and \\nT\\nc\\nD\\n in Figs. 7.30(b) and \\n(c). The diagonal details in these images (i.e., the \\nT\\nc\\nD\\n areas shaded in blue) are identical to the corre-\\nspondingly shaded areas of the Haar transform in Fig. 7.30(d). In the 1-D case, as was demonstrated \\nin Example 7.19, a \\nJ\\n-scale 1-D FWT with respect to Haar basis functions is the same as its 1-D Haar \\ntransform counterpart. This is due to the fact that the basis functions of the two transforms are identical; \\nboth contain one scaling function and a series of scaled and translated wavelet functions. In the 2-D case, \\nhowever, the basis images differ. The 2-D separable scaling and wavelet functions deﬁned in Eqs. (7-153) \\nthrough (7-155) introduce horizontal and vertical directionality that is not present in a traditional Haar \\ntransform. Figures 7.31(a) and (b), for example, are the basis images of an \\n88\\n×\\n Haar transform and \\nthree-scale FWT with respect to Haar basis functions\\n. Note the blue highlighted regions along the main \\ndiagonals in which the basis images match. The same pattern occurs in Fig. 7.30(b) through (d). If a nine-\\nscale wavelet transform of the vase were computed, it would match the Haar transform in Fig. 7.30(d) \\nin all of its shaded areas.\\nDIP4E_GLOBAL_Print_Ready.indb   522\\n6/16/2017   2:10:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 523}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n523\\nb a\\nd c\\nFIGURE 7.30\\n(a) A \\n512 512\\n×\\n \\nimage of a vase;\\n \\n(b) a one-scale \\nFWT; (c) a two-\\nscale FWT; and \\n(d) the Haar \\ntransform of the \\noriginal image. \\nAll transforms \\nhave been scaled \\nto highlight their \\nunderlying struc-\\nture. When cor-\\nresponding areas \\nof two transforms \\nare shaded in \\nblue, the corre-\\nspondent pixels \\nare identical.\\nb a\\nFIGURE 7.31\\n(a) Haar basis \\nimages of size \\n88\\n×\\n [from \\nF\\nig. 7.18(c)] and \\n(b) the basis \\nimages of a \\nthree-scale \\n88\\n×\\n \\ndiscrete wavelet \\ntransform with \\nrespect to Haar \\nbasis functions\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   523\\n6/16/2017   2:10:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 524}),\n",
       " Document(page_content='524\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nWe conclude the section with a simple example that demonstrates the use of \\nwavelets in image processing. As in the Fourier domain, the basic approach is to:\\n1. \\nCompute the 2-D wavelet transform of an image with respect to a selected \\nwavelet basis\\n. Table 7.1 shows some representative bases, including their scal-\\ning and wavelet functions and corrresponding ﬁlter coefﬁcients. The ﬁlter coef-\\nﬁcients are given in the context of Fig. 7.26. For orthonormal wavelets, lowpass \\nsynthesis coefﬁcients are speciﬁed; the remaining ﬁlters must be computed \\nusing Eq. (7-151). For biorthonormal wavelets, two analysis ﬁlters are given and \\nthe synthesis ﬁlters must be computed using Eqs. (7-149) and (7-150).\\n2. \\nAlter the computed transform to take advantage of the DWT’s ability to  \\n(1) decorrelate image pixels\\n, (2) reveal important frequency and temporal char-\\nacteristics, and/or (3) measure the image’s similarity to the transform’s basis \\nimages. Modiﬁcations designed for image smoothing, sharpening, noise reduc-\\ntion, edge detection, and compression (to name only a few) are possible.\\n3. \\nCompute the inverse wavelet transform.\\nSince the discrete wavelet transform decomposes an image into a weighted sum of \\nspatially limited,\\n bandlimited basis images, most Fourier-based imaging techniques \\nhave an equivalent “wavelet domain” counterpart.\\nEXAMPLE 7.23 :   Wavelet-based edge detection.\\nFigure 7.32 provides a simple illustration of the preceding three steps. Figure 7.32(a) shows a \\n128 128\\n×\\n \\ncomputer\\n-generated image of 2-D sine-shaped pulses on a black background. Figure 7.32(b) is the two-\\nscale discrete wavelet transform of the image with respect to 4th-order \\nsymlets\\n, short for “symmetrical \\nwavelets.” Although they are not perfectly symmetrical, they are designed to have the least asymmetry \\nand highest number of \\nvanishing moments\\n†\\n for a given compact support (Daubechies [1992]). Row 4 of \\nTable 7.1 shows the wavelet and scaling functions of the symlets, as well as the coefﬁcients of the cor-\\nresponding lowpass synthesis ﬁlter. The remaining ﬁlter coefﬁcients are obtained using Eq. (7-151) with \\nK\\n, the number of ﬁlter coefﬁcients, set to 8:\\n \\ngn g n\\nn\\n10\\n1 7 0 0758 0 0296 0 4976 0 8037 0 2979 0\\n( ) ( ) () ., ., ., ., ., .\\n=− =\\n−− −\\n0 0992 0 0126 0 0322\\n7 0 0758 0 0296 0 497\\n00\\n,. ,.\\n( )() ., ., .\\n−−\\n−−\\n{}\\n=− =\\nhn g n\\n6 6 0 8037 0 2979 0 0992 0 0126 0 0322\\n70\\n11\\n,. ,. , . , . ,.\\n() ( ) .\\n−−\\n−\\n{}\\n=− =\\nhn g n\\n0 0322 0 0126 0 0992 0 2979 0 8037 0 4976 0 0296 0 0758\\n, . ,. ,. , . ,. ,. , .\\n−−−\\n{}\\n}\\nIn Fig. 7.32(c), the approximation component of the discrete wavelet transform has been eliminated \\nby setting its values to zero. As Fig. 7.32(d) shows, the net effect of computing the inverse transform \\nusing these modiﬁed coefﬁcients is edge enhancement, reminiscent of the Fourier-based image sharpen-\\ning results discussed in Section 4.9. Note how well the transitions between signal and background are \\ndelineated, despite the fact that they are relatively soft, sinusoidal transitions. By zeroing the horizontal \\ndetails as well—see Figs. 7.32(e) and (f)—we can isolate vertical edges.\\n† The \\nk\\nth moment of wavelet \\nc\\n()\\nx\\n is \\nmk x xd x\\nk\\n() () .\\n=\\n∫\\nc\\n Vanishing moments impact the smoothness of the scal-\\ning and wavelet functions and our ability to represent them as polynomials\\n. An order-\\nN\\n symlet has \\nN\\n vanishing \\nmoments.\\nDIP4E_GLOBAL_Print_Ready.indb   524\\n6/16/2017   2:10:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 525}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n525\\nWavelet Name or Family Scaling Function Wavelet Function Filter Coefﬁcients\\nHaar\\nThe oldest and simplest \\nwavelets. Orthogonal and \\ndiscontinuous\\n.\\n1\\n0 1.2\\n1\\n1.2\\n0\\n1\\n0\\n1.2\\n1\\n0\\n1.5\\n−\\n1\\n−\\n15\\n.\\ngn\\n0\\n12 12\\n() ,\\n=\\n{}\\nDaubechies family\\nOrthogonal with the most \\nvanishing moments for a \\ngiven support. Denoted \\ndb\\nN\\n, where \\nN\\n is the num-\\nber of vanishing moments; \\ndb2 and db4 shown; db1 is \\nthe Haar of the previous \\nrow.\\n1\\n1.4\\n0\\n−\\n04\\n.\\n1\\n03\\n2\\n2\\n06\\n4 3\\n17\\n5\\n1\\n1.2\\n0\\n−\\n04\\n.\\n1\\n03\\n2\\n1\\n0\\n2\\n−\\n1\\n−\\n15\\n.\\n1\\n0\\n1.5\\n−\\n1\\n2\\n06\\n4 3\\n17\\n5\\ngn\\n0\\n0 482963\\n0 836516 0 224144\\n0 129410\\n() . ,\\n., .,\\n.\\n=\\n{\\n}\\n−\\ngn\\n0\\n0 230372\\n0 714847 0 630881\\n0 027984 0 187035\\n00 3\\n() . ,\\n., .,\\n., .,\\n.\\n=\\n{\\n−−\\n0 0841 0 032883\\n0 010597\\n,. ,\\n.\\n−\\n}\\nSymlet family\\nOrthogonal with the least \\nasymmetry and most \\nvanishing moments for a \\ngiven support (sym4 or \\n4th order shown)\\n.\\n1\\n0\\n1.4\\n−\\n02\\n.\\n2\\n06\\n4 3\\n17\\n5\\n2\\n06\\n4 3\\n17\\n5\\n1\\n0\\n2\\n−\\n1\\n−\\n15\\n.\\ngn\\n0\\n0 032231\\n0 012604 0 099220\\n0 297858 0 803739\\n04 9\\n() . ,\\n., .,\\n., .,\\n.\\n=\\n{\\n−−\\n7 7619 0 029636\\n0 075766\\n,. ,\\n.\\n−\\n−\\n}\\nCohen-Daubechies- \\nFeauveau 9/7\\nBiorthogonal B-spline \\nused in the irreversable \\nJPEG2000 compression \\nstandard (see Chapter 8)\\n.\\n1\\n1.4\\n0\\n−\\n04\\n.\\n1\\n0\\n2\\n−\\n1\\n−\\n15\\n.\\n2\\n06\\n4 3\\n17\\n58 9\\n1\\n0\\n1.4\\n−\\n02\\n.\\n1\\n0\\n2\\n−\\n1\\n2\\n06\\n4 3\\n17\\n58 9\\nhn\\n0\\n0 026749\\n0 016864 0 078223\\n0 266864 0 602949\\n02 6\\n() . ,\\n., .,\\n., .,\\n.\\n=\\n{\\n−−\\n6 6864 0 078223\\n0 016864 0 026749\\n0 091271\\n0 057\\n1\\n,. ,\\n., .\\n() . ,\\n.\\n−\\n−\\n−\\n−\\n}\\n=\\n{\\nhn\\n5 544 0 591272\\n1 115087 0 591272\\n0 057544 0 091271 0\\n,. ,\\n., .,\\n., .,\\n−\\n−\\n}\\nTABLE \\n7.1\\nSome representative wavelets.\\nDIP4E_GLOBAL_Print_Ready.indb   525\\n6/16/2017   2:10:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 526}),\n",
       " Document(page_content='526\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nWAVELET PACKETS\\nA fast wavelet transform decomposes a function into a sum of scaling and wavelet \\nfunctions whose bandwidths are logarithmically related. That is, the low-frequency \\ncontent of the function is represented using scaling and wavelet functions with nar-\\nrow bandwidths, while the high-frequency content is represented using functions \\nwith wider bandwidths. This is apparent in Fig. 6.5. Each horizontal strip of constant \\nheight tiles, which are the basis functions of a single FWT scale, increases logarithmi-\\ncally in height as you move up the frequency axis. To obtain greater control over the \\npartitioning of the time-frequency plane (e.g., to get smaller bandwidths for higher \\nfrequencies), the FWT must be generalized to yield a more flexible decomposition \\nb a\\nd c\\nf e\\nFIGURE 7.32\\nModifying a DWT \\nfor edge  \\ndetection:  \\n(a) orginal image; \\n(b) two-scale \\nDWT with respect \\nto 4th-order sym-\\nlets; (c) modiﬁed \\nDWT with the \\napproximation set \\nto zero;  (d) the \\ninverse DWT  \\nof (c); (e) modi-\\nﬁed DWT with \\nthe approximation \\nand horizontal \\ndetails set to zero; \\nand (f) the inverse \\nDWT of (e). \\n(Note when the \\ndetail coefﬁcients \\nare zero, they \\nare displayed as \\nmiddle gray; when \\nthe approxima-\\ntion coefﬁcients \\nare zeroed, they \\ndisplay as black.)\\nDIP4E_GLOBAL_Print_Ready.indb   526\\n6/16/2017   2:10:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 527}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n527\\ncalled a \\nwavelet packet\\n (Coifman and Wickerhauser [1992]). The cost of this gen-\\neralization is an increase in computational complexity from \\nO\\n(\\nN\\n) for the FWT to \\nO\\n(\\nN \\nlog\\n2 \\nN\\n) for a wavelet packet.\\nConsider again the three-scale ﬁlter bank of Fig. 7.24(a), but imagine the decom-\\nposition as a \\nbinary tree\\n. Figure 7.33(a) details the structure of that tree, and links \\nthe appropriate FWT scaling and wavelet coefﬁcients from Fig. 7.24(a) to the tree’s \\nnodes\\n. The \\nroot node\\n is assigned the highest-scale approximation coefﬁcients, which \\nare samples of the function itself, while the \\nleaves\\n inherit the transform’s approxi-\\nmation and detail coefﬁcient outputs. Two intermediate nodes, \\nTJ k\\nw\\n(, )\\n−\\n1\\n and \\nTJ k\\nw\\n(, ) ,\\n−\\n2\\n are ﬁlter-bank approximations that are subsequently ﬁltered to become \\nfour additional leaf nodes\\n. Note the coefﬁcients of each node are the weights of a \\nlinear expansion that produces a bandlimited “piece” of root node \\nfx\\n()\\n.\\n Because \\nany such piece is an element of a known scaling or wavelet subspace\\n, we can replace \\nthe generating coefﬁcients in Fig. 7.33(a) by the corresponding subspace. The result \\nis the \\nsubspace analysis tree\\n of Fig. 7.33(b).\\nAnalysis trees provide a compact and informative way of representing multiscale \\nwavelet transforms. They are simple to draw, take less space than their correspond-\\ning ﬁlter and subsampler-based block diagrams, and make it relatively easy to detect \\nvalid decompositions. The three-scale analysis tree of Fig. 7.33(b), for example, sug-\\ngests three possible expansion options:\\n \\nVV W\\nJJ J\\n=\\n−−\\n11\\n{\\n \\n(7-156)\\n \\nVV W W\\nJ JJJ\\n=\\n−−−\\n221\\n{{\\n \\n(7-157)\\n \\nVV W W W\\nJ JJJJ\\n=\\n−−−−\\n3321\\n{{{\\n \\n(7-158)\\nThey correspond to the one-, two-, and three-scale FWT decompositions of a 1-D \\nfunction.\\n A valid decomposition requires one approximation term (or scaling sub-\\nspace) and enough detail components (or wavelet subspaces) to cover the spectrum \\nof Fig. 7.24(b). In general, a \\nP\\n-scale FWT analysis tree supports \\nP\\n unique decompo-\\nsitions.\\nAnalysis trees are also an efﬁcient mechanism for representing wavelet packets, \\nwhich are nothing more than conventional wavelet transforms with the details ﬁl-\\ntered iteratively. Thus, the three-scale FWT analysis tree of Fig. 7.33(b) becomes the \\nRecall that \\n{\\n denotes \\nthe union of spaces (like \\nthe union of sets). Equa-\\ntions (7-156) through \\n(7-158) can be derived by \\nthe repeated application \\nof Eq. (7-128).\\nb a\\nFIGURE 7.33\\nAn (a) coef-\\nﬁcient tree and \\n(b) analysis tree \\nfor the two-scale \\nFWT analysis \\nbank of Fig. 7.24.\\nV\\nJ\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nV\\nJ\\n−\\n2\\nW\\nJ\\n−\\n2\\nV\\nJ\\n−\\n3\\nW\\nJ\\n−\\n3\\nTJ k f x\\nw\\n(, ) ()\\n=\\nTJ k\\nw\\n(, )\\n−\\n1\\nTJ k\\nc\\n(, )\\n−\\n1\\nTJ k\\nw\\n(, )\\n−\\n2\\nTJ k\\nc\\n(, )\\n−\\n2\\nTJ k\\nw\\n(, )\\n−\\n3\\nTJ k\\nc\\n(, )\\n−\\n3\\nDIP4E_GLOBAL_Print_Ready.indb   527\\n6/16/2017   2:10:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 528}),\n",
       " Document(page_content='528\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nthree-scale wavelet packet tree of Fig. 7.34. Note the additional subscripting that \\nmust be introduced. The ﬁrst subscript of each double-subscripted node identiﬁes \\nthe scale of the FWT parent node from which it is descended. The second, a variable \\nlength string of “A”s and “D”s, encodes the path from the parent node to the node \\nbeing examined. An “A” designates approximation ﬁltering, while a “D” indicates \\ndetail ﬁltering. Subspace node \\nW\\nJ\\n−\\n1,\\n,\\nDA\\n for example, is obtained by “ﬁltering” the \\nscale \\nJ\\n−\\n1\\n FWT coefﬁcients (i.e., parent \\nW\\nJ\\n−\\n1\\n in Fig. 7.34) through an additional \\ndetail ﬁlter (yielding \\nW\\nJ\\n−\\n1,D\\n), followed by an approximation ﬁlter (giving \\nW\\nJ\\n−\\n1,DA\\n).  \\nFigures 7.35(a) and (b) are the ﬁlter-bank and spectrum-splitting characteristics of \\nthe analysis tree in Fig. 7.34, respectively. Note the “naturally ordered” outputs of \\nthe ﬁlter bank in Fig. 7.35(a) have been reordered based on frequency content in \\nFig. 7.35(b) (see Problem 7.46 for more on “frequency ordered” wavelets).\\nThe three-scale packet tree in Fig. 7.34 almost triples the number of decomposi-\\ntions (and associated time-frequency tilings) that are possible with the three-scale \\nFWT tree. Recall that in a normal FWT, we split, ﬁlter, and downsample the lowpass \\nbands alone. This creates a ﬁxed logarithmic (base 2) relationship between the band-\\nwidths of the scaling and wavelet spaces used in the representation of a function [see \\nFigure 7.24(b)]. Thus, while the three-scale FWT analysis tree of Fig. 7.24(a) offers \\nthree possible decompositions—deﬁned by Eqs. (7-156) to (7-158)—the wavelet \\npacket tree of Fig. 7.34 supports 26 different decompositions. For instance, \\nV\\nJ\\n and \\ntherefore function \\nfx\\n()\\n can be expanded as\\n \\nVV W W W W\\nWW\\nW\\nJ JJJ J J\\nJJJ\\n=\\n−−− − −\\n−−−\\n332 2 1\\n111\\n{{ { {\\n{{{\\n,, ,\\n,,,\\nADA A\\nAD DA DD\\n \\n(7-159)\\nwhose spectrum is shown in Fig. 7.35(b), or as\\n \\nVV W W W\\nJJ J J J\\n=\\n−− − −\\n11 1 1\\n{{ {\\n,, ,\\nAD\\nAD\\nD\\n \\n(7-160)\\nwhose spectrum is depicted in Fig. 7.36. Note the difference between this last spec-\\ntrum and the full packet spectrum of F\\nig. 7.35(b), or the three-scale FWT spectrum \\nRecall that \\n{\\n denotes \\nthe union of spaces (like \\nthe union of sets). The 26 \\ndecompositions associ-\\nated with Fig. 7.34 are \\ndetermined by various \\ncombinations of nodes \\n(spaces) that can be \\ncombined to represent \\nthe root node (space) \\nat the top of the tree. \\nEqs. (7-159) and (7-160) \\ndeﬁne two of them.\\nFIGURE 7.34\\nA three-scale \\nwavelet packet \\nanalysis tree.\\nV\\nJ\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nV\\nJ\\n−\\n2\\nW\\nJ\\n−\\n2\\nV\\nJ\\n−\\n3\\nW\\nJ\\n−\\n3\\nW\\nJ\\n−\\n1,A\\nW\\nJ\\n−\\n1,D\\nW\\nJ\\n−\\n2,A\\nW\\nJ\\n−\\n2,D\\nW\\nJ\\n−\\n1,AA\\nW\\nJ\\n−\\n1,AD\\nW\\nJ\\n−\\n1,DA\\nW\\nJ\\n−\\n1,DD\\nDIP4E_GLOBAL_Print_Ready.indb   528\\n6/16/2017   2:10:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 529}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n529\\nof Fig. 7.24(b). In general, \\nP\\n-scale, 1-D wavelet packet transforms (and associated \\nP\\n+\\n1-level analysis trees) support\\n \\nDP DP\\n()\\n( )\\n+=\\n[]\\n+\\n11\\n2\\n \\n(7-161)\\nunique decompositions, where \\nD\\n(1) = 1.\\n With such a large number of valid expan-\\nsions, packet-based transforms provide improved control over the partitioning of the \\nb\\na\\nFIGURE 7.35\\nThe (a) ﬁlter  \\nbank and  \\n(b) spectrum-\\nsplitting char-\\nacteristics of a \\nthree-scale full \\nwavelet packet \\nanalysis tree.\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n2\\n↓\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1,D\\nW\\nJ\\n−\\n1,A\\nW\\nJ\\n−\\n2\\nV\\nJ\\n−\\n2\\nW\\nJ\\n−\\n1,DD\\nW\\nJ\\n−\\n1,DA\\nW\\nJ\\n−\\n1,AD\\nW\\nJ\\n−\\n1,AA\\nW\\nJ\\n−\\n2,D\\nW\\nJ\\n−\\n2,A\\nW\\nJ\\n−\\n3\\nV\\nJ\\n−\\n3\\nH\\n()\\nv\\nv\\np\\np\\n2\\np\\n4\\np\\n8\\n0\\nV\\nJ\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nV\\nJ\\n−\\n2\\nW\\nJ\\n−\\n2\\nW\\nJ\\n−\\n1,D\\nW\\nJ\\n−\\n1,A\\nV\\nJ\\n−\\n3\\nW\\nJ\\n−\\n3\\nW\\nJ\\n−\\n2,D\\nW\\nJ\\n−\\n2,A\\nW\\nJ\\n−\\n1,DA\\nW\\nJ\\n−\\n1,DD\\nW\\nJ\\n−\\n1,AD\\nW\\nJ\\n−\\n1,AA\\nfx V\\nJ\\n()\\nH\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n/H22841\\nhn\\n1\\n()\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n/H22841\\nhn\\n1\\n()\\n2\\n↓\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\n2\\n↓\\n/H22841\\nhn\\n0\\n()\\nFIGURE 7.36\\nThe spectrum of \\nthe decomposition \\nin Eq. (7-160).\\n0\\nH\\n()\\nv\\nv\\np\\np\\n2\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1,A\\nW\\nJ\\n−\\n1,DA\\nW\\nJ\\n−\\n1,DD\\n34\\np\\n58\\np\\nDIP4E_GLOBAL_Print_Ready.indb   529\\n6/16/2017   2:10:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 530}),\n",
       " Document(page_content='530\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nspectrum of the decomposed function. The cost of this control is an increase in com-\\nputational complexity. Compare the filter bank in Fig. 7.35(a) to that of Fig. 7.24(a).\\nNow consider the 2-D, four-band ﬁlter bank of Fig. 7.29(a). As was noted earlier, it \\nsplits approximation \\nTj k l\\nw\\n(, , )\\n+\\n1\\n into outputs \\nTj k l\\nw\\n(, ,) ,\\n \\nTj k l\\nc\\nH\\n(, ,) ,\\n \\nTj k l\\nc\\nV\\n(, ,) ,\\n and \\nTj k l\\nc\\nD\\n(, ,) .\\n As in the 1-D case, it can be “iterated” to generate \\nP\\n-scale transforms \\nat scales \\njJ J J P\\n=−\\n− −\\n12\\n,, , ,\\n…\\n with \\nTJ k l f x y\\nw\\n(,, ) (,) .\\n=\\n The spectrum resulting \\nfrom the ﬁrst iteration,\\n with \\njJ\\n+=\\n1\\n in Fig. 7.29(a), is shown \\nin F\\nig. 7.37(a). Note it \\ndivides the frequency plane into four equal areas. The low-frequency quarter-band \\nin the center of the plane coincides with transform coefﬁcients \\nTJ k l\\nw\\n(, , )\\n−\\n1\\n and \\nscaling space \\nV\\nJ\\n−\\n1\\n.\\n This nomenclature is consistent with the 1-D case. To accom-\\nmodate the 2-D nature of the input,\\n however, we now have three (rather than one) \\nwavelet subspaces. They are denoted \\nW\\nJ\\n−\\n1\\nH\\n, \\nW\\nJ\\n−1\\nV\\n,\\n and \\nW\\nJ\\n−\\n1\\nD\\n and correspond to coef-\\nﬁcients \\nTJ k l\\nc\\nH\\n(, , ) ,\\n−\\n1  \\nTJ k l\\nc\\nV\\n(, , ) ,\\n−\\n1\\n and \\nTJ k l\\nc\\nD\\n(, , ) ,\\n−\\n1\\n respectively. Figure 7.37(b) \\nshows the resulting four\\n-band, single-scale \\nquaternary FWT analysis tree\\n. Note the \\nsuperscripts that link the wavelet subspace designations to their transform coef-\\nﬁcient counterparts.\\nFigure 7.38 shows a portion of a three-scale, 2-D wavelet packet analysis tree. Like \\nits 1-D counterpart in Fig. 6.34, the ﬁrst subscript of every node that is a descendant \\nof a conventional FWT detail node is the scale of the parent detail node. The second \\nsubscript, a variable length string of “A”s, “H”s, “V”s, and “D”s, encodes the path \\nfrom the parent node to the node under consideration. The node labeled \\nW\\nJ\\n−\\n1,\\n,\\nVD\\nH\\n \\nfor example, is obtained by “row/column ﬁltering” the scale \\nJ\\n−\\n1\\n FWT horizontal \\ndetail coefﬁcients (i.e\\n., parent \\nW\\nJ\\n−\\n1\\nH\\n in Fig. 7.38) through an additional detail/approx-\\nimation ﬁlter (yielding \\nW\\nJ\\n−\\n1,V\\nH\\n), followed by a detail/detail ﬁlter (giving \\nW\\nJ\\n−\\n1,VD\\nH\\n). A \\nP\\n-scale, 2-D wavelet packet tree supports\\n \\nDP DP\\n()\\n( )\\n+=\\n[]\\n+\\n11\\n4\\n \\n(7-162)\\nunique expansions, where \\nD\\n(1) = 1.\\n Thus, the three-scale tree of Fig. 7.38 offers \\n83,522 possible decompositions. The problem of selecting among them is the subject \\nof the next example.\\n−\\np\\n−\\np\\np\\np\\nv\\nhorizontal\\nv\\nvertical\\nV\\nJ\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nD\\nW\\nJ\\n−\\n1\\nH\\nW\\nJ\\n−\\n1\\nV\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nH\\nW\\nJ\\n−\\n1\\nH\\nW\\nJ\\n−\\n1\\nV\\nW\\nJ\\n−\\n1\\nV\\nW\\nJ\\n−\\n1\\nD\\nW\\nJ\\n−\\n1\\nD\\nW\\nJ\\n−\\n1\\nD\\nW\\nJ\\n−\\n1\\nD\\nb a\\nFIGURE 7.37\\nThe ﬁrst decom-\\nposition of a 2-D \\nFWT: (a) the \\nspectrum and \\n(b) the subspace \\nanalysis tree.\\nDIP4E_GLOBAL_Print_Ready.indb   530\\n6/16/2017   2:10:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 531}),\n",
       " Document(page_content='7.10\\n  \\nWavelet Transforms\\n    \\n531\\nEXAMPLE 7.24 :   Two-dimensional wavelet packet decompositions.\\nAs noted in the above discussion, a single wavelet packet tree presents numerous decomposition options. \\nIn fact, the number of possible decompositions is often so large that it is impractical, if not impossible, \\nto enumerate or examine them individually. An efﬁcient algorithm for ﬁnding optimal decompositions \\nwith respect to application speciﬁc criteria is highly desirable. As will be seen, classical \\nentropy-\\n and \\nenergy-based cost functions\\n are applicable in many situations and are well-suited for use in binary and \\nquaternary tree searching algorithms.\\nConsider the problem of reducing the amount of data needed to represent the \\n400 480\\n×\\n ﬁngerprint \\nimage in F\\nig. 7.39(a). Image compression is discussed in detail in Chapter 8. In this example, we want to \\nselect the “best” three-scale wavelet packet decomposition as a starting point for the compression pro-\\ncess. Using three-scale wavelet packet trees, there are 83,522 [per Eq. (7-162)] potential decompositions. \\nFigure 7.39(b) shows one of them—a full wavelet packet, 64-leaf decomposition like the analysis tree \\nof Fig. 7.38. Note the leaves of the tree correspond to the subbands of the \\n88\\n×\\n array of decomposed \\nsubimages in F\\nig. 7.39(b). The probability that this particular 64-leaf decomposition is in some way opti-\\nmal for the purpose of compression, however, is relatively low. In the absence of a suitable optimality \\ncriterion, we can neither conﬁrm nor deny it.\\nOne reasonable criterion for selecting a decomposition for the compression of the image of Fig. 7.39(a) \\nis the additive cost function\\n \\nEf fxy\\nxy\\n()\\n=\\n()\\n∑\\n,\\n,\\n \\n(7-163)\\nThis function provides one possible measure\\n†\\n of the energy content of 2-D function \\nf\\n. Under this mea-\\nsure, the energy of function \\nfx y\\n(,\\n)\\n=\\n0\\n for all \\nx\\n and \\ny\\n is 0.\\n High values of \\nE\\n on the other hand, are indica-\\ntive of functions with many nonzero values. Since most transform-based compression schemes work \\nby truncating or thresholding the small coefﬁcients to zero, a cost function that maximizes the number \\nof near-zero values is a reasonable criterion for selecting a “good” decomposition from a compression \\npoint of view.\\n† Other possible energy measures include the sum of the squares of \\nfx y\\n(,\\n) ,\\n the sum of the log of the squares, etc. \\nProblem 7.48 deﬁnes one possible entropy-based cost function.\\n \\nFIGURE 7.38\\n A three-scale, full wavelet packet decomposition tree. Only a portion of the tree is provided.\\nV\\nJ\\nV\\nJ\\n−\\n1\\nW\\nJ\\n−\\n1\\nD\\nW\\nJ\\n−\\n1\\nH\\nW\\nJ\\n−\\n1\\nV\\nV\\nJ\\n−\\n2\\nV\\nJ\\n−\\n3\\nW\\nJ\\n−\\n2\\nH\\nW\\nJ\\n−\\n3\\nH\\nW\\nJ\\n−\\n3\\nV\\nW\\nJ\\n−\\n3\\nD\\nW\\nJ\\n−\\n1,V\\nH\\nW\\nJ\\n−\\n2,A\\nH\\nW\\nJ\\n−\\n2,H\\nH\\nW\\nJ\\n−\\n1,VA\\nH\\nW\\nJ\\n−\\n1,VD\\nH\\nW\\nJ\\n−\\n1,VH\\nH\\nW\\nJ\\n−\\n1,VV\\nH\\nW\\nJ\\n−\\n2,V\\nH\\nW\\nJ\\n−\\n2,D\\nH\\nDIP4E_GLOBAL_Print_Ready.indb   531\\n6/16/2017   2:10:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 532}),\n",
       " Document(page_content='532\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nThe cost function just described is both computationally simple and easily adapted to tree optimi-\\nzation routines. The optimization algorithm must use the function to minimize the “cost” of the leaf \\nnodes in the decomposition tree. Minimal energy leaf nodes should be favored because they have more \\nnear-zero values, which leads to greater compression. Because the cost function of Eq. (7-163) is a local \\nmeasure that uses only the information available at the node under consideration, an efﬁcient algorithm \\nfor ﬁnding minimal energy solutions is easily constructed as follows:\\nFor each node of the analysis tree, beginning with the root and proceeding level by level to the leaves:\\n1. \\nCompute both the energy of the node, denoted \\nE\\nP\\n (for parent energy), and the energy of its four \\noffspring—denoted as \\nE\\nA\\n, \\nE\\nH\\n, \\nE\\nV\\n, and \\nE\\nD\\n. For two-dimensional wavelet packet decompositions, \\nthe parent is a two-dimensional array of approximation or detail coefﬁcients; the offspring are the \\nﬁltered approximation, horizontal, vertical, and diagonal details.\\n2. \\nIf the combined energy of the offspring is less than the energy of the parent (that is, \\nE\\nA\\n + E\\nH\\n + E\\nV\\n \\n+ E\\nD\\n < E\\nP\\n), include the offspring in the analysis tree. If the combined energy of the offspring is \\ngreater than or equal to that of the parent, prune the offspring, keeping only the parent. It is a leaf \\nof the optimized analysis tree.\\nThe preceding algorithm can be used to (1) prune wavelet packet trees or (2) design procedures for com-\\nputing optimal trees from scratch. In the latter case, nonessential siblings—descendants of nodes that \\nb a\\nFIGURE 7.39\\n (a) A scanned ﬁngerprint and (b) its three-scale, full wavelet packet decomposition. Although the 64 \\nsubimages of the packet decomposition appear to be square (e.g., note the approximation subimage), this is merely \\nan aberration of the program used to produce the result. (Original image courtesy of the National Institute of Stan-\\ndards and Technology.)\\nDIP4E_GLOBAL_Print_Ready.indb   532\\n6/16/2017   2:10:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 533}),\n",
       " Document(page_content='  \\n  \\nSummary, References, and Further Reading\\n    \\n533\\nwould be eliminated in Step 2 of the algorithm—would not be computed. Figure\\n \\n7.40 shows the opti-\\nmized decomposition that results from applying the algorithm just described to the image of Fig.\\n \\n7.39(a) \\nwith the cost function of Eq. (7-163). Note many of the original full packet decomposition’s 64 subbands \\nin Fig.\\n \\n7.39(b) have been eliminated. In addition, the subimages that are not split (further decomposed) \\nin Fig. 7.40 are relatively smooth and composed of pixels that are middle gray in value. Because all \\nbut the approximation subimage of this ﬁgure have been scaled so that gray level 128 indicates a zero-\\nvalued coefﬁcient, these subimages contain little energy. There would be no overall decrease in energy \\nrealized by splitting them.\\nThe preceding example is based on a real-world problem that was solved through the use of wave-\\nlets. The Federal Bureau of Investigation (FBI) currently maintains a large database of ﬁngerprints, \\nand has established a wavelet-based national standard for the digitization and compression of ﬁnger-\\nprint images (FBI [1993]). Using Cohen-Daubechies-Feauveau (CDF) biorthogonal wavelets (Cohen, \\nDaubechies, and Feauveau [1992]), the standard achieves a typical compression ratio of 15:1. Table 7.2 \\ndetails the required analysis ﬁlter coefﬁcients. Because the scaling and wavelet functions of the CDF \\nfamily are symmetrical and have similar lengths, they are among the most widely used biorthogonal \\nwavelets. The advantages of wavelet-based compression over the more traditional JPEG approach are \\nexamined in Chapter 8.\\nSummary, References, and Further Reading\\n  \\nThe material in this chapter establishes a solid mathematical foundation for understanding and accessing the role \\nof image transforms, including the discrete wavelet transform, in image processing. We approach transforms as \\nseries expansions in which the transform coefﬁcients are inner products of a set of orthonormal or biorthonormal \\nbasis functions and the images being transformed. For many transforms, these inner products can be implemented \\nFIGURE 7.40\\nAn optimal \\nwavelet packet \\ndecomposition for \\nthe ﬁngerprint of \\nFig. 7.39(a).\\nDIP4E_GLOBAL_Print_Ready.indb   533\\n6/16/2017   2:10:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 534}),\n",
       " Document(page_content='534\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nas straightforward matrix operations. Further reading on the matrix formulation of image transforms is available in \\nbooks like those of \\nAndrews [1970] and Wang [2012], and in the original papers on the transforms themselves. See, \\nfor example, the original papers on the Haar transform (Haar [1910]), Walsh transform (Walsh [1923]), Hadamard \\ntransform (Hadamard [1893]), and the slant transform (Pratt, Chen, and Welch [1974]).\\nThere are many good texts on wavelets and their application. Several complement our treatment and were relied \\nupon during the development of the wavelet transform section of the chapter. Included among them are the books \\nby Vetterli and Kovacevic [1995] and Burrus, Gopinath, and Guo [1998]. A partial listing of the imaging applica-\\ntions that have been approached from a wavelet point of view includes image matching, registration, segmentation, \\ndenoising, restoration, enhancement, compression (see Chapter 8), morphological ﬁltering, and computed tomog-\\nraphy. The history of wavelet analysis is recorded in a book by Hubbard [1998]. The early predecessors of wavelets \\nwere developed simultaneously in different ﬁelds and uniﬁed in a paper by Mallat [1987]. It brought a mathematical \\nframework to the ﬁeld. Much of the history of wavelets can be traced through the works of Meyer [1987] [1990] \\n[1992a, 1992b] [1993], Mallat [1987] [1989a–c] [1998], and Daubechies [1988] [1990] [1992] [1993] [1996]. Finally, \\nthere have been a number of special issues devoted to wavelets, including a special issue on wavelet transforms and \\nmultiresolution signal anaysis in the \\nIEEE Transactions on Information Theory\\n [1992], a special issue on wavelets \\nand signal processing in the \\nIEEE Transactions on Signal Processing\\n [1993], and a special section on multiresolution \\nrepresentation in the \\nIEEE Transactions on Pattern Analysis and Machine Intelligence\\n [1989]. All of the examples in \\nthe chapter were done using MATLAB (see Gonzalez et al. [2004]).\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n7.1 \\nGiven column vectors\\n \\nss s\\n01 2\\n1\\n2\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n-\\n-\\n(a) \\nProve that \\ns\\n0\\n, \\ns\\n1\\n, and \\ns\\n2\\n are orthogonal.\\n(b) * \\nAre they orthonormal? If not, normalize \\nthem to create a transformation matrix of \\northonormal vectors\\n.\\n(c) \\nUsing the result of (b), write an orthogonal \\ntransformation matrix for \\ns\\n0\\n, \\ns\\n1\\n, and \\ns\\n2\\n.\\n(d) \\nCompute the transform of column vector \\nf\\n=\\n[]\\n36 5\\n-\\n.\\n(e) \\nCompute the inverse transform of the result \\nin (d).\\nnh\\n0\\n(\\nn\\n)\\nh\\n1\\n(\\nn\\n)\\nnh\\n0\\n(\\nn\\n)\\nh\\n1\\n(\\nn\\n)\\n0\\n0\\n0 9 0.825923\\n0.417849\\n1\\n0.001909\\n0 10 0.420796\\n0.040368\\n2\\n−\\n0\\n001914 .\\n01 1\\n−\\n0\\n094059 .\\n−\\n0 078722 .\\n3\\n−\\n0\\n016991 .\\n0.014427 12\\n−\\n0\\n077263 .\\n−\\n0 014468 .\\n4\\n0.011935\\n−\\n0\\n014468 .\\n13 0.049733\\n0.0144263\\n5\\n0.049733\\n−\\n0\\n078722 .\\n14 0.011935\\n0\\n6\\n−\\n0\\n077263 .\\n0.040368 15\\n−\\n0\\n016991 .\\n0\\n7\\n−\\n0\\n094059 .\\n0.417849 16\\n−\\n0\\n0019 .\\n0\\n8\\n0.420796\\n−\\n0\\n758908 .\\n17 0.0019\\n0\\nTABLE \\n7.2\\nBiorthogonal  \\nCohen-\\nDaubechies-\\nFeauveau recon-\\nstruction and \\ndecomposition \\nﬁlter coefﬁcients \\nwith 6 and 8 van-\\nishing moments, \\nrespectively. \\n(Cohen, \\nDaubechies,  \\nand Feauveau \\n[1992]).\\nDIP4E_GLOBAL_Print_Ready.indb   534\\n6/16/2017   2:10:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 535}),\n",
       " Document(page_content=\"  \\n  \\nProblems\\n    \\n535\\n7.2 \\nProve Eq. (7-23).\\n7.3 * \\nProve that \\nrx u sx u\\n(,\\n) (,)\\n=\\n in Eqs. (7-16) and \\n(7-17) for real,\\n orthonormal basis vectors.\\n7.4 \\nProve that if \\nA\\n*\\nT\\nA\\n = \\nI\\n, the associated expansion \\nfunctions are orthonormal.\\n7.5 \\nProve that matrix \\nA\\n3\\n in Example 7.3 is an orthog-\\nonal transformation matrix.\\n7.6 \\nProve that orthogonal transformations preserve \\ninner products\\n.\\n7.7 \\nUsing Eqs. (7-4) and (7-5),\\n(a) \\nFind the norm of \\nf\\n=\\n[]\\n32 1\\n+-\\njj\\nT\\n.\\n(b) \\nFind the norm of \\ng\\n=\\n[]\\n0 707 0 707\\n.. .\\n-\\nT\\n(c) \\nFind the angle between \\nh\\n=\\n[]\\n0 707 0 707\\n..\\nT\\n \\nand \\ng\\n.\\n(d) * \\nFind the norm of \\nfx x\\n()\\nc o s.\\n=\\n(e) \\nFind the angle between \\nf\\n from (d) and \\ngx x\\n()\\ns i n.\\n=\\n(f) \\nAre \\nf\\n and \\ng\\n orthogonal to one another?\\n(g) \\nAre \\nf\\n and \\ng\\n orthonormal?\\n7.8 \\nUsing the results from Problem 7.1(c)–(e) and \\ncolumn vector \\ng\\n=\\n[]\\n271\\n:\\n(a) \\nCompute the angle between \\nf\\n and \\ng\\n.\\n(b) \\nCompute the distance between \\nf\\n and \\ng\\n. \\nHint:\\n \\nThe distance between vectors \\nf\\n and \\ng\\n is\\n \\nd\\n=\\nfg fg\\n--\\n,\\n(c) * \\nShow that angles and distances are preserved \\nby this orthogonal transform.\\n7.9 \\nCompute the inverse transform of \\nT\\n in Exam-\\nple 7.3.\\n7.10 \\nProve that the set of sinusoidal expansion func-\\ntions \\n12 2\\n,c\\no s ,s i n ,c o s ,s i n ,\\nxx x x\\n…\\n{}\\n are orthog-\\nonal on the interval \\n−\\n[]\\npp\\n,.\\n7.11 \\nCompute the expansion coefﬁcients of 2-tuple \\n71\\n[]\\nT\\n and write the corresponding expansions \\nfor the following bases:\\n(a) * \\nss\\n01\\n0 707 0 707 0 707 0 707\\n=\\n[]\\n=\\n[]\\n.. ,. .\\nTT\\n-\\n \\non the set of real 2-tuples\\n.\\n(b) \\nss\\n01\\n10 11\\n=\\n[]\\n=\\n[]\\nTT\\n,\\n and the dual vectors \\nss\\n01\\n11 0 1\\n''\\n-\\n=\\n[]\\n=\\n[]\\nTT\\n,.\\n7.12 \\nAre expansion functions\\nss s\\n01 2\\n05\\n05\\n05\\n05\\n05\\n05\\n05\\n05\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n-\\n-\\n= =\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n05\\n05\\n05\\n05\\n05\\n05\\n05\\n05\\n3\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n-\\n-\\n-\\n-\\ns\\northonormal? If so, write the corresponding \\northogonal transformation matrix.\\n7.13 \\nIf \\nf\\n=\\n[]\\n4 321\\n-\\nT\\n,\\n ﬁnd the transform of \\nf\\n using \\nthe transformation matrix of Problem 7.12.\\n Then \\ncompute the inverse and show that the transform \\nis reversable.\\n7.14 \\nGiven the 2-D matrix\\n \\nF\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n44\\n40\\n5\\n31\\n50 5\\n24\\n80 5\\n13\\n31 5\\n-\\n--\\n--\\n--\\n.\\n.\\n.\\n.\\n(a) \\nCompute the transform of \\nF\\n with respect to \\nthe transformation matrix of Problem 7.12.\\n(b) * \\nUsing the 1-D transform computed in Prob-\\nlem 7.13,\\n explain how a 2-D transform is \\ncomputed as two 1-D transforms.\\n(c) \\nCompute the 2-D inverse transform of the \\nresult from (a).\\n7.15 \\nProve that expansion functions\\n \\nuu\\nuu\\n01\\n01\\n22\\n22\\n1\\n05\\n23\\n22 3\\n23\\n23\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n-\\n-\\n''\\n.\\n⎤ ⎤\\n⎦\\n⎥\\nare biorthonormal. Then show by example \\nwhether inner products\\n, angles, and distances are \\npreserved by the transform.\\n7.16 \\nProve that \\nA\\n and \\nA\\n'\\n in Example 7.5 are biortho-\\nnormal.\\n(a) \\nUsing biorthonormal matrices \\nA\\n and \\nA\\n'\\n of \\nExample 7.5, compute the transform of \\n44\\n*\\n \\narray\\n \\nF\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n16\\n2 3 13\\n51 1 1\\n08\\n976 1 2\\n41 4 1\\n51\\nDIP4E_GLOBAL_Print_Ready.indb   535\\n6/16/2017   2:10:23 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 536}),\n",
       " Document(page_content='536\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\n(b) \\nCompute the inverse transform of the result \\nin (a).\\n7.17 * \\nWrite a pair of 2-D transform matrix equations \\nfor rectangular arrays and complex orthonormal \\nexpansion functions\\n.\\n7.18 \\nWrite a pair of 2-D transform matrix equations \\nfor complex biorthonormal expansion functions\\n.\\n7.19 \\nShow that Eq. (7-59) of Example 7.6 is equivalent \\nto \\nsin( ).\\n2\\np\\nx\\n7.20 * \\nProve that the DFT expansion functions of \\nEq.\\n (7-56) are orthonormal.\\n7.21 \\nProve Eq. (7-52).\\n7.22 \\nBeginning with a series expansion of the expan-\\nsion functions deﬁned in Eq.\\n (7-56), derive an \\nexpression for the discrete Fourier transform.\\n7.23 \\nGiven standard basis vectors \\ne\\n0\\n10\\n=\\n[]\\nT\\n and \\ne\\n1\\n01\\n=\\n[]\\nT\\n of inner product space \\nR\\n2\\n and an \\narbitrary vector \\nr\\n of length \\nr\\n and angle \\nu\\n, compute \\nthe single-point cross-correlation of \\nr\\n with both \\ne\\n0\\n and \\ne\\n1\\n. When does \\nr\\n resemble \\ne\\n0\\n more than \\ne\\n1\\n \\nand vice versa?\\n7.24 * \\nProve that the Fourier transform of time-scaled \\nwavelet \\nc\\n()\\n2\\ns\\nt\\n is given by Eq. (7-73).\\n7.25 \\nProve Eq. (7-80).\\n7.26 \\nObtain the Hartley transfomation matrix for \\nN\\n = 4.\\n7.27 \\nWrite a pair of discrete cosine transform equa-\\ntions of the form given in Eqs\\n. (7-57) and (7-58) \\nfor the discrete Fouier transform.\\n7.28 \\nBecause the 2-D discrete cosine transform is \\nseparable\\n, the 2-D DCT of an image can be com-\\nputed by row and column passes with a 1-D DCT \\nalgorithm. In fact, an interesting property of the \\n1-D DCT is that it can be computed by using the \\nFFT algorithm. Show in detail how this computa-\\ntion can be made.\\n7.29 \\nDo the following:\\n(a) \\nCompute the Fourier, sine, cosine and Hart-\\nley transformation matrices of size \\nN\\n = 6.\\n(b) \\nCompute the Hartley transform of the  dis-\\ncrete function \\nfx\\n(\\n) { ,,, , , }\\n=− − −\\n253 1 0 3\\n \\nusing Eq.\\n (7-28).\\n(c) \\nCompute the Hartley transform of the func-\\ntion in (b) from its discrete F\\nourier transform. \\nIs it equal to the result in (b)?\\n(d) \\nUse Eqs. (7-86) through (7-89) to compute \\nthe DCT of \\nf\\n(\\nx\\n) = [3,\\n -6, 1].\\n(e) \\nUse Eq. (7-28) to compute the DST of the \\nfunction in (b).\\n7.30 \\nCompute the basis images of the Haar transform \\nfor \\nN\\n = 2.\\n7.31 \\nCreate a table mapping the rows of Hadamard-\\nordered transformation matrix \\nH\\n16\\n to sequency-\\nordered Hadamard transformation matrix \\n′\\nH\\n16\\n.\\n7.32 \\nObtain the slant transformation matrix for \\nN\\n = 8.\\n7.33 \\nDerive the Haar scaling coefﬁcients from \\nEqs\\n. (7-122) and (7-126).\\n7.34 \\nShow that scaling function\\n \\nw\\n()\\n..\\nx\\nx\\n=\\n⎧\\n⎨\\n⎩\\n1\\n0 25 0 75\\n0\\n≤<\\nelsewhere\\ndoes not satisfy the second requirement of a mul-\\ntiresolution analysis\\n.\\n7.35 * \\nDerive Eq. (7-140).\\n7.36 \\nWrite an expression for scaling space \\nV\\n3\\n as a \\nfunction of scaling function \\nw\\n()\\n.\\nx\\n Use the Haar \\nscaling function deﬁnition of Eq.\\n (7-122) to draw \\nthe Haar \\nV\\n3\\n scaling functions at translations \\nk\\n=\\n{}\\n012\\n,, .\\n7.37 * \\nDraw wavelet \\nc\\n33\\n,\\n()\\nx\\n for the Haar wavelet func-\\ntion.\\n Write an expression for \\nc\\n33\\n,\\n()\\nx\\n in terms of \\nHaar scaling functions\\n.\\n7.38 \\nSuppose function \\nfx\\n()\\n is a member of Haar scal-\\ning space \\nV\\n3\\n—that is, \\nfx V\\n()\\n.\\nH\\n3\\n Use Eq. (7-128) to \\nexpress \\nV\\n3\\n as a function of scaling space \\nV\\n0\\n and \\nany required wavelet spaces. If \\nfx\\n()\\n is 0 outside \\nthe interval [0,\\n 1), sketch the scaling and wavelet \\nfunctions required for a linear expansion of \\nfx\\n()\\n \\nbased on your expression.\\n7.39 \\nCompute the ﬁrst four terms of the wavelet series \\nexpansion of the function used in Example 7.18 \\nwith starting scale \\nj\\n0\\n1\\n=\\n.\\n Write the resulting \\nexpansion in terms of the scaling and wavelet \\nfunctions involved.\\n How does your result com-\\npare to the example, where the starting scale was \\nj\\n0\\n0\\n=\\n?\\n7.40 \\nThe DWT in Eqs. (7-137) and (7-138) is for a \\nstarting scale \\nj\\n0\\n0\\n=\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   536\\n6/16/2017   2:10:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 537}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n537\\n(a) * \\nRewrite these equations for any starting \\nscale \\njJ\\n0\\n≤\\n.\\n(b) \\nRecompute the 1-D DWT of function \\nfx\\n()\\n, , ,\\n=\\n{}\\n14 30\\n−\\n for \\n03\\n≤≤\\nx\\n in Exam-\\nple 7.19 with \\nj\\n0\\n1\\n=\\n (rather than 0).\\n(c) \\nUse the result from (b) to compute \\nf\\n()\\n1\\n from \\nthe transform values\\n.\\n7.41 * \\nDraw the FWT ﬁlter bank required to compute \\nthe transform in Problem 7.40.\\n Label all inputs \\nand outputs with the appropriate sequences.\\n7.42 \\nThe computational complexity of an \\nN\\n-point fast \\nwavelet transform is \\nO\\n(\\nN\\n).\\n That is, the number of \\noperations is proportional \\nN\\n. What determines \\nthe constant of proportionality?\\n7.43 \\nAnswer the following:\\n(a) * \\nIf the input to the three-scale FWT ﬁlter \\nbank of F\\nig. 7.24(a) is the Haar scaling func-\\ntion \\nw\\n()\\nx\\n=\\n1\\n for \\nn\\n = 0,\\n 1, …, 7 and 0 else-\\nwhere, what is the resulting transform with \\nrespect to Haar wavelets?\\n(b) \\nWhat is the transform if the input is the corre-\\nsponding Haar wavelet function \\nc\\n()\\n,,\\nx\\n=\\n{\\n11\\n \\n1 11111\\n,\\n,,,,\\n−−−−\\n}\\n for \\nn\\n = 0, 1, …, 7?\\n(c) \\nWhat input sequence produces transform  \\n100000 0\\n,,,,,,\\n,\\nB\\n{}\\n with nonzero coefﬁcient \\nTB\\nc\\n(,)\\n22\\n=\\n?\\n7.44 \\nCompute the 2-D wavelet transform with respect \\nto Haar wavelets of the \\n22\\n×\\n image\\n31\\n62\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nDraw the required ﬁlter bank and label all inputs \\nand outputs with the proper arrays\\n.\\n7.45 * \\nIn the Fourier domain\\nfx x y y Fu v e\\nux M vy N\\n(,) ( , )\\n()\\n−−\\n−\\n00\\n2\\n00\\n⇔\\n+\\np\\nand translation does not affect the display of \\nFu v\\n(,) .\\n Using the following sequence of images, \\nexplain the translation property of wavelet trans-\\nforms\\n. The top left image contains two \\n32 32\\n×\\n \\nwhite squares centered on a \\n128 28\\n×1\\n gray back-\\nground.\\n The top right image is its single-scale \\nwavelet transform with respect to Haar wavelets. \\nThe bottom left image is the wavelet transform of \\nthe original image after shifting its 32 pixels to the \\nright and downward, and the ﬁnal (bottom right) \\nimage is the wavelet transform of the original \\nimage after it has been shifted one pixel to the \\nright and downward.\\n7.46 \\nThe following table shows the Haar wavelet and \\nscaling functions for a four\\n-scale fast wavelet \\ntransform. Sketch the additional basis functions \\nneeded for a full three-scale packet decomposi-\\ntion. Give the mathematical expression or expres-\\nsions for determining them. Then order the basis \\nfunctions according to frequency content and \\nexplain the results.\\nV\\n0\\nW\\n0\\nV\\n1\\nV\\n2\\nW\\n1\\nV\\n3\\nW\\n2\\nW\\n2,D\\nW\\n2,A\\nW\\n1,A\\nW\\n2,AA\\nW\\n2,AD\\nW\\n2,DA\\nW\\n2,DD\\nW\\n1,D\\n7.47 \\nA wavelet packet decomposition of the vase from \\nF\\nig. 7.30(a) is shown below.\\n(a) \\nDraw the corresponding decomposition anal-\\nDIP4E_GLOBAL_Print_Ready.indb   537\\n6/16/2017   2:10:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 538}),\n",
       " Document(page_content='538\\n    \\nChapter\\n \\n7\\n  \\nWavelet and Other Image Transforms\\nysis tree, labeling all nodes with the names of \\nthe proper scaling and wavelet spaces\\n.\\n(b) \\nDraw and label the decomposition’s fre-\\nquenc\\ny spectrum.\\n7.48 \\nUsing the Haar wavelet, determine the minimum \\nentropy packet decomposition for the function \\n  \\nfor \\nfx\\n()\\n.\\n=\\n02 5\\n for \\nn\\n = 0,\\n 1, …, 15. Employ the \\nnonnormalized Shannon entropy\\n \\nEfx f x f x\\nx\\n() () l n ()\\n[]\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n∑\\n22\\nas the minimization criterion. Draw the opti-\\nmal tree, labeling the nodes with the computed \\nentropy values.\\nDIP4E_GLOBAL_Print_Ready.indb   538\\n6/16/2017   2:10:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 539}),\n",
       " Document(page_content='5398\\nImage Compression and \\nWatermarking\\nPreview\\nImage compression, the art and science of reducing the amount of data required to represent an image, \\nis one of the most useful and commercially successful technologies in the ﬁeld of digital image process-\\ning. The number of images that are compressed and decompressed daily is staggering, and the compres-\\nsions and decompressions themselves are virtually invisible to the user. Everyone who owns a digital \\ncamera, surfs the web, or streams the latest Hollywood movies over the Internet beneﬁts from the algo-\\nrithms and standards that will be discussed in this chapter. The material, which is largely introductory in \\nnature, is applicable to both still-image and video applications. We will introduce both theory and prac-\\ntice, examining the most frequently used compression techniques, and describing the industry standards \\nthat make them useful. The chapter concludes with an introduction to \\ndigital image watermarking\\n, the \\nprocess of inserting visible and invisible data (such as copyright information) into images.\\nUpon competion of this chapter, students should:\\n Be able to measure the amount of informa-\\ntion in a digital image.\\n Understand the main sources of data redun-\\ndancy in digital images.\\n Know the difference between lossy and error-\\nfree compression, and the amount of com-\\npression that is possible with each.\\n Be familiar with the popular image compres-\\nsion standards, such as JPEG and JPEG-2000, \\nthat are in use today.\\n Understand the principal image compression \\nmethods, and how and why they work.\\n Be able to compress and decompress grayscale, \\ncolor, and video imagery.\\n Know the difference between visible, invisible, \\nrobust, fragile, public, private, restricted-key,  \\nand unrestricted-key watermarks.\\n Understand the basics of watermark insertion \\nand extraction in both the spatial and trans-\\nform domain.\\nBut life is short and information endless ... Abbreviation is a  \\nnecessary evil and the abbreviator’s business is to make the best of \\na job which, although bad, is still better than nothing.\\nAldous Huxley\\nThe Titanic will protect itself.\\nRobert Ballard\\nDIP4E_GLOBAL_Print_Ready.indb   539\\n6/16/2017   2:10:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 540}),\n",
       " Document(page_content='540\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\n8.1 FUNDAMENTALS  \\nThe term \\ndata compression\\n refers to the process of reducing the amount of data \\nrequired to represent a given quantity of information. In this definition, \\ndata\\n and \\ninformation\\n are not the same; data are the means by which information is conveyed. \\nBecause various amounts of data can be used to represent the same amount of infor-\\nmation, representations that contain irrelevant or repeated information are said to \\ncontain \\nredundant data\\n. If we let \\nb\\n and \\n′\\nb\\n denote the number of bits (or information- \\ncarrying units) in two representations of the same information,\\n the \\nrelative data \\nredundancy\\n, \\nR\\n, of the representation with \\nb\\n bits is\\n \\nR\\nC\\n=\\n1\\n1\\n-\\n \\n(8-1)\\nwhere \\nC\\n,\\n commonly called the \\ncompression ratio\\n, is defined as\\n \\nC\\nb\\nb\\n=\\n′\\n \\n(8-2)\\nIf \\nC\\n = 10 (sometimes written 10:1),\\n for instance, the larger representation has 10 \\nbits of data for every 1 bit of data in the smaller representation. The corresponding \\nrelative data redundancy of the larger representation is 0.9 (\\nR\\n = 0.9), indicating that \\n90% of its data is redundant.\\nIn the context of digital image compression, \\nb\\n in Eq. (8-2) usually is the number of \\nbits needed to represent an image as a 2-D array of intensity values. The 2-D inten-\\nsity arrays introduced in Section 2.4 are the preferred formats for human viewing \\nand interpretation—and the standard by which all other representations are judged. \\nWhen it comes to compact image representation, however, these formats are far \\nfrom optimal. Two-dimensional intensity arrays suffer from three principal types of \\ndata redundancies that can be identiﬁed and exploited:\\n1. \\nCoding redundancy\\n.\\n A code is a system of symbols (letters, numbers, bits, and \\nthe like) used to represent a body of information or set of events. Each piece of \\ninformation or event is assigned a sequence of \\ncode symbols\\n, called a \\ncode word\\n. \\nThe number of symbols in each code word is its \\nlength\\n. The 8-bit codes that are \\nused to represent the intensities in most 2-D intensity arrays contain more bits \\nthan are needed to represent the intensities.\\n2. \\nSpatial\\n and \\ntemporal redundancy\\n.\\n Because the pixels of most 2-D intensity \\narrays are correlated spatially (i.e., each pixel is similar to or dependent upon \\nneighboring pixels), information is unnecessarily replicated in the representa-\\ntions of the correlated pixels. In a video sequence, temporally correlated pixels \\n(i.e., those similar to or dependent upon pixels in nearby frames) also duplicate \\ninformation.\\n3. \\nIrrelevant information\\n.\\n Most 2-D intensity arrays contain information that is \\nignored by the human visual system and/or extraneous to the intended use of \\nthe image. It is redundant in the sense that it is not used.\\n8.1\\nDIP4E_GLOBAL_Print_Ready.indb   540\\n6/16/2017   2:10:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 541}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n541\\nThe computer-generated images in Figs. 8.1(a) through (c) exhibit each of these fun-\\ndamental redundancies. As will be seen in the next three sections, compression is \\nachieved when one or more redundancy is reduced or eliminated.\\nCODING REDUNDANCY\\nIn Chapter 3, we developed techniques for image enhancement by histogram pro-\\ncessing, assuming that the intensity values of an image are random quantities. In this \\nsection, we will use a similar formulation to introduce optimal information coding.\\nAssume that a discrete random variable \\nr\\nk\\n in the interval \\n01\\n,\\nL\\n-\\n[]\\n is used to rep-\\nresent the intensities of an \\nMN\\n*\\n image, and that each \\nr\\nk\\n occurs with probability \\npr\\nrk\\n() .\\n As in Section 3.3,\\n \\npr\\nn\\nMN\\nkL\\nrk\\nk\\n()\\n, , , ,\\n==\\n012 1\\np-\\n \\n(8-3)\\nwhere \\nL\\n is the number of intensity values\\n, and \\nn\\nk\\n is the number of times that the \\nk\\nth \\nintensity appears in the image. If the number of bits used to represent each value of \\nr\\nk \\n \\nis \\nlr\\nk\\n() ,\\n then the average number of bits required to represent each pixel is\\n \\nLl r p r\\nkr k\\nk\\nL\\navg\\n=\\n=\\n∑\\n() ()\\n0\\n1\\n-\\n \\n(8-4)\\nThat is, the average length of the code words assigned to the various intensity val-\\nues is found by summing the products of the number of bits used to represent each \\nintensity and the probability that the intensity occurs\\n. The total number of bits \\nrequired to represent an \\nMN\\n*\\n image is \\nMNL\\navg\\n. If the intensities are represented \\nb a\\nc\\nFIGURE 8.1\\n Computer generated \\n256 256 8\\n**\\n bit images with (a) coding redundancy, (b) spatial redundancy, and \\n(c) irrelevant information. (Each was designed to demonstrate one principal redundancy, but may exhibit others \\nas well.)\\nDIP4E_GLOBAL_Print_Ready.indb   541\\n6/16/2017   2:10:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 542}),\n",
       " Document(page_content='542\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nusing a \\nnatural\\n \\nm\\n-bit fixed-length code,\\n†\\n the right-hand side of Eq. (8-4) reduces to \\nm\\n bits. That is, \\nL\\navg\\n = \\nm\\n when \\nm\\n is substituted for \\nlr\\nk\\n() .\\n The constant \\nm\\n can be taken \\noutside the summation,\\n leaving only the sum of the \\npr\\nrk\\n()\\n for \\n01\\n……\\n-\\nkL\\n,\\n which, \\nof course\\n, equals 1.\\nEXAMPLE 8.1 :   A simple illustration of variable-length coding.\\nThe computer-generated image in Fig. 8.1(a) has the intensity distribution shown in the second column \\nof Table 8.1. If a natural 8-bit binary code (denoted as code 1 in Table 8.1) is used to represent its four \\npossible intensities, \\nL\\navg\\n (the average number of bits for code 1) is 8 bits, because \\nlr\\nk\\n1\\n8\\n()\\n=\\n bits for all \\nr\\nk\\n. On the other hand, if the scheme designated as code 2 in Table 8.1 is used, the average length of the \\nencoded pixels is, in accordance with Eq. (8-4),\\n \\nL\\navg\\n bits \\n==\\n02 52 04 71 00 33 18 1\\n.( ) .( ) .( ) .\\n++\\nThe total number of bits needed to represent the entire image is \\nMNL\\navg\\n=\\n256 56 1 81\\n**\\n.,\\n or 118,621. \\nF\\nrom Eqs. (8-2) and (8-1), the resulting compression and corresponding relative redundancy are\\n \\nC\\n== ≈\\n256 256 8\\n118 621\\n8\\n18 1\\n44 2\\n**\\n,.\\n.\\nand\\n \\nR\\n==\\n1\\n1\\n44 2\\n0 774\\n-\\n.\\n.\\nrespectively. Thus, 77.4% of the data in the original 8-bit 2-D intensity array is redundant.\\nT\\nhe compression achieved by code 2 results from assigning fewer bits to the more probable inten-\\nsity values than to the less probable ones. In the resulting \\nvariable-length code\\n, \\nr\\n128\\n (the image’s most \\nprobable intensity) is assigned the 1-bit code word 1 [of length \\nl\\n2\\n128 1\\n()\\n=\\n],while \\nr\\n255\\n (its least probable \\noccurring intensity) is assigned the 3-bit code word 001 [of length \\nl\\n2\\n255 3\\n()\\n=\\n]. Note that the best \\nﬁx\\ned-\\nlength code\\n that can be assigned to the intensities of the image in Fig. 8.1(a) is the natural 2-bit count-\\ning sequence \\n00 01 10 11\\n,,\\n, ,\\n{}\\n but the resulting compression is only \\n82\\n or 4:1—about 10% less than the \\n4.42:1 compression of the variable-length code\\n.\\nAs the preceding example shows, \\ncoding redundancy\\n is present when the codes \\nassigned to a set of events (such as intensity values) do not take full advantage of \\nthe probabilities of the events. Coding redundancy is almost always present when \\nthe intensities of an image are represented using a natural binary code. The reason \\nis that most images are composed of objects that have a regular and somewhat pre-\\ndictable morphology (shape) and reﬂectance, and are sampled so the objects being \\ndepicted are much larger than the picture elements. The natural consequence is that, \\n† A \\nnatural\\n binary code is one in which each event or piece of information to be encoded (such as intensity value) \\nis assigned one of 2\\nm\\n codes from an \\nm\\n-bit binary counting sequence.\\nDIP4E_GLOBAL_Print_Ready.indb   542\\n6/16/2017   2:10:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 543}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n543\\nfor most images, certain intensities are more probable than others (that is, the his-\\ntograms of most images are not uniform). A natural binary encoding assigns the \\nsame number of bits to both the most and least probable values, failing to minimize \\nEq. (8-4), and resulting in coding redundancy.\\nSPATIAL AND TEMPORAL REDUNDANCY\\nConsider the computer-generated collection of constant intensity lines in Fig. 8.1(b). \\nIn the corresponding 2-D intensity array:\\n1. \\nAll 256 intensities are equally probable. As Fig. 8.2 shows, the histogram of the \\nimage is uniform.\\n2. \\nBecause the intensity of each line was selected randomly, its pixels are indepen-\\ndent of one another in the vertical direction.\\n3. \\nBecause the pixels along each line are identical, they are maximally correlated \\n(completely dependent on one another) in the horizontal direction.\\nT\\nhe first observation tells us that the image in Fig. 8.1(b) (when represented as a \\nconventional 8-bit intensity array) cannot be compressed by variable-length coding \\nalone. Unlike the image of Fig. 8.1(a) and Example 8.1, whose histogram was \\nnot\\n \\nuniform, a fixed-length 8-bit code in this case minimizes Eq. (8-4). Observations 2 \\nand 3 reveal a significant spatial redundancy that can be eliminated by representing \\nthe image in Fig. 8.1(b) as a sequence of \\nrun-length pairs\\n, where each run-length pair \\nspecifies the start of a new intensity and the number of consecutive pixels that have \\nthat intensity. A run-length based representation compresses the original 2-D, 8-bit \\nr\\nk\\np\\nr\\n(\\nr\\nk\\n)\\nCode 1\\nl\\n1\\n(\\nr\\nk\\n)\\nCode 2\\nl\\n2\\n(\\nr\\nk\\n)\\nr\\n87\\n = 87\\n0.25 01010111 8 01 2\\nr\\n128\\n = 128\\n0.47 01010111 8 1 1\\nr\\n186\\n = 186\\n0.25 01010111 8 000 3\\nr\\n255\\n = 255\\n0.03 01010111 8 001 3\\nr\\nk\\n for \\nk\\n = 87, 128, 186, 255\\n0— 8 — 0\\nTABLE \\n8.1\\nExample of \\nvariable-length \\ncoding.\\nFIGURE 8.2\\nThe intensity  \\nhistogram of the  \\nimage in \\nFig. 8.1(b).\\n0\\n0\\n100 250\\n150\\n256\\nn\\nk\\np\\nr\\n(\\nr\\nk\\n)\\nk\\n200\\n1\\n256\\n50\\nDIP4E_GLOBAL_Print_Ready.indb   543\\n6/16/2017   2:10:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 544}),\n",
       " Document(page_content='544\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nintensity array by \\n()\\n[\\n( ) ]\\n256 256 8 256 256 8\\n** + *\\n or 128:1. Each 256-pixel line of \\nthe original representation is replaced by a single 8-bit intensity value and length \\n256 in the run-length representation.\\nIn most images\\n, pixels are correlated spatially (in both \\nx\\n and \\ny\\n) and in time (when \\nthe image is part of a video sequence). Because most pixel intensities can be pre-\\ndicted reasonably well from neighboring intensities, the information carried by a sin-\\ngle pixel is small. Much of its visual contribution is redundant in the sense that it can \\nbe inferred from its neighbors. To reduce the redundancy associated with spatially \\nand temporally correlated pixels, a 2-D intensity array must be transformed into a \\nmore efﬁcient but usually “non-visual” representation. For example, run-lengths or \\nthe differences between adjacent pixels can be used. Transformations of this type \\nare called \\nmappings\\n. A mapping is said to be \\nreversible\\n if the pixels of the original \\n2-D intensity array can be reconstructed without error from the transformed data \\nset; otherwise, the mapping is said to be \\nirreversible\\n.\\nIRRELEVANT INFORMATION\\nOne of the simplest ways to compress a set of data is to remove superfluous data \\nfrom the set. In the context of digital image compression, information that is ignored \\nby the human visual system, or is extraneous to the intended use of an image, are \\nobvious candidates for omission. Thus, the computer-generated image in Fig. 8.1(c), \\nbecause it appears to be a homogeneous field of gray, can be represented by its \\naverage intensity alone—a single 8-bit value. The original \\n256 256 8\\n**\\n bit intensity \\narray is reduced to a single byte\\n, and the resulting compression is \\n()\\n256 256 8 8\\n**\\n \\nor 65,536:1.\\n Of course, the original \\n256 256 8\\n**\\n bit image must be recreated to view \\nand/or analyze it,\\n but there would be little or no perceived decrease in reconstructed \\nimage quality.\\nFigure 8.3(a) shows the histogram of the image in Fig. 8.1(c). Note that there \\nare several intensity values (125 through 131) actually present. The human visual \\nsystem averages these intensities, perceives only the average value, then ignores the \\nsmall changes in intensity that are present in this case. Figure 8.3(b), a histogram-\\nequalized version of the image in Fig. 8.1(c), makes the intensity changes visible \\nand\\n \\nreveals two previously undetected regions of constant intensity—one oriented verti-\\ncally, and the other horizontally. If the image in Fig. 8.1(c) is represented by its aver-\\nage value alone, this “invisible” structure (i.e., the constant intensity regions) and the \\nrandom intensity variations surrounding them (real information) is lost. Whether or \\nnot this information should be preserved is application dependent. If the informa-\\ntion is important, as it might be in a medical application like digital X-ray archival, it \\nshould not be omitted; otherwise, the information is redundant and can be excluded \\nfor the sake of compression performance.\\nWe conclude this section by noting that the redundancy examined here is fun-\\ndamentally different from the redundancies discussed in the previous two sections. \\nIts elimination is possible because the information itself is not essential for nor-\\nmal visual processing and/or the intended use of the image. Because its omission \\nresults in a loss of quantitative information, its removal is commonly referred to as  \\nDIP4E_GLOBAL_Print_Ready.indb   544\\n6/16/2017   2:10:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 545}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n545\\nquantization\\n. This terminology is consistent with normal use of the word, which gen-\\nerally means the mapping of a broad range of input values to a limited number of \\noutput values (see Section 2.4). Because information is lost, quantization is an irre-\\nversible operation.\\nMEASURING IMAGE INFORMATION\\nIn the previous sections, we introduced several ways to reduce the amount of data \\nused to represent an image. The question that naturally arises is: How few bits are \\nactually needed to represent the information in an image? That is, is there a mini-\\nmum amount of data that is sufficient to describe an image without losing infor-\\nmation? \\nInformation theory\\n provides the mathematical framework to answer this \\nand related questions. Its fundamental premise is that the generation of information \\ncan be modeled as a probabilistic process which can be measured in a manner that \\nagrees with intuition. In accordance with this supposition, a random event \\nE\\n with \\nprobability \\nP\\n(\\nE\\n) is said to contain\\n \\nIE\\nPE\\nPE\\n() l o g\\n()\\nlog ( )\\n==\\n1\\n-\\n \\n(8-5)\\nunits of information. If \\nP\\n(\\nE\\n) = 1 (that is \\n, the event always occurs), \\nI\\n(\\nE\\n) = 0 and no \\ninformation is attributed to it. Because no uncertainty is associated with the event, \\nno information would be transferred by communicating that the event has occurred \\n[it \\nalways\\n occurs if \\nP\\n(\\nE\\n) = 1].\\nThe base of the logarithm in Eq. (8-5) determines the unit used to measure infor-\\nmation. If the base \\nm\\n logarithm is used, the measurement is said to be in \\nm\\n-ary units. \\nIf the base 2 is selected, the unit of information is the \\nbit\\n. Note that if \\nP\\n(\\nE\\n) = ½, \\nIE\\n()\\nl o g\\n=\\n-\\n2\\n½\\n or 1 bit. That is, 1 bit is the amount of information conveyed when \\none of two possible equally likely events occurs\\n. A simple example is ﬂipping a coin \\nand communicating the result.\\nConsult the book web-\\nsite for a brief review of \\ninformation and prob-\\nability theory.\\n.\\nb a\\nFIGURE 8.3\\n(a) Histogram \\nof the image in \\nFig. 8.1(c) and \\n(b) a histogram \\nequalized version \\nof the image.\\nIntensity\\nNumber of pixels\\n0\\n0\\n100 250\\n150\\n1000\\n200\\n50\\n2000\\n3000\\n4000\\n5000\\n6000\\n7000\\nDIP4E_GLOBAL_Print_Ready.indb   545\\n6/16/2017   2:10:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 546}),\n",
       " Document(page_content='546\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nGiven a source of statistically independent random events from a discrete set of \\npossible events \\naa a\\nJ\\n11\\n,, ,\\np\\n{}\\n with associated probabilities \\nPa Pa Pa\\nJ\\n() ,() , ,( ) ,\\n11\\np\\n{}\\n \\nthe average information per source output, called the \\nentropy\\n of the source, is\\n \\nHP a P a\\njj\\nj\\nJ\\n=\\n=\\n∑\\n-\\n() l o g()\\n1\\n \\n(8-6)\\nThe \\na\\nj\\n in this equation are called \\nsource symbols\\n. Because they are statistically inde-\\npendent, the source itself is called a \\nzero-memory source\\n.\\nIf an image is considered to be the output of an imaginary zero-memory “inten-\\nsity source,” we can use the histogram of the observed image to estimate the symbol \\nprobabilities of the source. Then, the intensity source’s entropy becomes\\n \\n/tildenosp\\nHp\\nrp\\nr\\nrk rk\\nk\\nL\\n=\\n=\\n∑\\n-\\n-\\n() l o g ()\\n2\\n0\\n1\\n \\n(8-7)\\nwhere variables \\nL\\n, \\nr\\nk\\n, and \\npr\\nrk\\n()\\n are as defined earlier and in Section 3.3. Because \\nthe base 2 logarithm is used,\\n Eq. (8-7) is the average information per intensity out-\\nput of the imaginary intensity source in bits. It is not possible to code the \\nintensity \\nvalues\\n of the imaginary source (and thus the sample image) with fewer than \\n/tildenosp\\nH\\n bits/\\npixel.\\nEXAMPLE 8.2 :   Image entropy estimates.\\nThe entropy of the image in Fig. 8.1(a) can be estimated by substituting the intensity probabilities from \\nTable 8.1 into Eq. (8-7):\\n \\n/tildenosp\\nH\\n=\\n[]\\n-+ + +\\n=\\n02 5 02 5 04 7 04 7 02 5 02 5 00 3 00 3\\n2222\\n.l o g. .l o g. .l o g. .l o g.\\n-\\n- - +-+- +-\\n02 5 2 04 7 10 9 02 5 2 00 3 50 6\\n1 6614\\n. ( ). ( . ). ( ). ( . )\\n.\\n[]\\n≈\\n bits/p pixel\\nIn a similar manner, the entropies of the images in Fig. 8.1(b) and (c) can be shown to be 8 bits/pixel and \\n1.566 bits/pixel,\\n respectively. Note that the image in Fig. 8.1(a) appears to have the most visual informa-\\ntion, but has almost the lowest computed entropy—1.66 bits/pixel. The image in Fig. 8.1(b) has almost \\nﬁve times the entropy of the image in (a), but appears to have about the same (or less) visual informa-\\ntion. The image in Fig. 8.1(c), which seems to have little or no information, has almost the same entropy \\nas the image in (a). The obvious conclusion is that the amount of entropy, and thus information in an \\nimage, is far from intuitive.\\nShannon’s First Theorem\\nRecall that the variable-length code in Example 8.1 was able to represent the inten-\\nsities of the image in Fig. 8.1(a) using only 1.81 bits/pixel. Although this is higher \\nthan the 1.6614 bits/pixel entropy estimate from Example 8.2, Shannon’s first theo-\\nrem, also called the \\nnoiseless coding theorem\\n (Shannon [1948]), assures us that the \\nEquation (8-6) is for \\nzero-memory sources \\nwith \\nJ\\n source symbols. \\nEquation (8-7) uses \\nprobablitiy estimates \\nfor the \\nL\\n-\\n1  intensity \\nvalues in an image.\\nDIP4E_GLOBAL_Print_Ready.indb   546\\n6/16/2017   2:10:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 547}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n547\\nimage in Fig. 8.1(a) can be represented with as few as 1.6614 bits/pixel. To prove \\nit in a general way, Shannon looked at representing groups of consecutive source \\nsymbols with a single code word (rather than one code word per source symbol), \\nand showed that\\n \\nlim\\n,\\nn\\nn\\nL\\nn\\nH\\n→\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n/H11009\\navg\\n \\n(8-8)\\nwhere \\nL\\navg, \\nn\\n is the average number of code symbols required to represent all \\nn\\n-sym-\\nbol groups. In the proof, he defined the \\nnth extension\\n of a zero-memory source to \\nbe the hypothetical source that produces \\nn\\n-symbol blocks\\n†\\n using the symbols of the \\noriginal source, and computed \\nL\\navg, \\nn\\n by applying Eq. (8-4) to the code words used \\nto represent the \\nn\\n-symbol blocks. Equation (8-8) tells us that \\nLn\\nn\\navg,\\n can be made \\narbitrarily close to \\nH\\n by encoding infinitely long extensions of the single-symbol \\nsource. That is, it is possible to represent the output of a zero-memory source with \\nan average of \\nH\\n information units per source symbol.\\nIf we now return to the idea that an image is a “sample” of the intensity source \\nthat produced it, a block of \\nn\\n source symbols corresponds to a group of \\nn\\n adjacent \\npixels. To construct a variable-length code for \\nn\\n-pixel blocks, the relative frequencies \\nof the blocks must be computed. But the \\nn\\nth extension of a hypothetical intensity \\nsource with 256 intensity values has 256\\nn\\n possible \\nn\\n-pixel blocks. Even in the simple \\ncase of \\nn\\n = 2, a 65,536 element histogram and up to 65,536 variable-length code \\nwords must be generated. For \\nn\\n = 3, as many as 16,777,216 code words are needed. \\nSo even for small values of \\nn\\n, computational complexity limits the usefulness of the \\nextension coding approach in practice.\\nFinally, we note that although Eq. (8-7) provides a lower bound on the compres-\\nsion that can be achieved when directly coding statistically independent pixels, it \\nbreaks down when the pixels of an image are correlated. Blocks of correlated pixels \\ncan be coded with fewer average bits per pixel than the equation predicts. Rather \\nthan using source extensions, less correlated descriptors (such as intensity run-\\nlengths) are normally selected and coded without extension. This was the approach \\nused to compress Fig. 8.1(b) in the section on spatial and temporal redundancy. \\nWhen the output of a source of information depends on a ﬁnite number of preced-\\ning outputs, the source is called a \\nMarkov source\\n or \\nﬁnite memory source\\n.\\nFIDELITY CRITERIA\\nIt was noted earlier that the removal of “irrelevant visual” information involves a \\nloss of real or quantitative image information. Because information is lost, a means \\nof quantifying the nature of the loss is needed. Two types of criteria can be used for \\nsuch an assessment: (1) objective fidelity criteria, and (2) subjective fidelity criteria.\\n† The output of the \\nn\\nth extension is an \\nn\\n-tuple of symbols from the underlying \\nsingle-symbol source\\n. It was con-\\nsidered a \\nblock random variable\\n in which the probability of each \\nn\\n-tuple is the product of the probabilities of \\nits individual symbols. The entropy of the \\nn\\nth extension is then \\nn\\n times the entropy of the single-symbol source \\nfrom which it is derived.\\nDIP4E_GLOBAL_Print_Ready.indb   547\\n6/16/2017   2:10:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 548}),\n",
       " Document(page_content='548\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nWhen information loss can be expressed as a mathematical function of the input \\nand output of a compression process, it is said to be based on an \\nobjective ﬁdelity \\ncriterion\\n. An example is the root-mean-squared (rms) error between two images. \\nLet \\nfx y\\n(,\\n)\\n be an input image, and \\nˆ\\n(,\\n)\\nfx y\\n be an approximation of \\nfx y\\n(,\\n)\\n that results \\nfrom compressing and subsequently decompressing the input.\\n For any value of \\nx\\n  \\nand \\ny\\n, the error \\nexy\\n(,\\n)\\n between \\nfx y\\n(,\\n)\\n and \\nˆ\\n(,\\n)\\nfx y\\n is\\n \\nexy f xy f xy\\n(,) (,) (,)\\n=\\nˆ\\n-\\n \\n(8-9)\\nso that the total error between the two images is\\n \\nˆ\\n(,\\n) (,)\\nfx y fx y\\ny\\nN\\nx\\nM\\n-\\n-\\n--\\n⎡\\n⎣\\n⎤\\n⎦\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1\\n \\nwhere the images are of size \\nMN\\n*\\n.\\n The \\nroot-mean-squared error\\n, \\ne\\nrms\\n, between \\nfx y\\n(,\\n)\\n and \\nˆ\\n(,\\n)\\nfx y\\n is then the square root of the squared error averaged over the \\nMN\\n*\\n array, or\\n \\ne\\nMN\\nfx y fx y\\ny\\nN\\nx\\nM\\nrms\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n= =\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\n12\\nˆ\\n(,) (,)\\n-\\n-\\n--\\n \\n(8-10)\\nIf \\nˆ\\n(,\\n)\\nfx y\\n is considered [by a simple rearrangement of the terms in Eq. (8-9)] to be \\nthe sum of the original image \\nfx y\\n(,\\n)\\n and an error or “noise” signal \\nexy\\n(,\\n) ,\\n the \\nmean-\\nsquared signal-to-noise ratio\\n of the output image\\n, denoted SNR\\nms\\n, can be defined as \\nin Section 5.8:\\n \\nSNR\\nms\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n= =\\n= =\\n∑ ∑\\n∑\\nˆ\\n(,)\\nˆ\\n(,) (,)\\nfx y\\nfx y fx y\\ny\\nN\\nx\\nM\\ny\\nN\\nx\\nM\\n2\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n− −\\n−\\n−\\n− −\\n1\\n∑\\n \\n(8-11)\\nThe rms value of the signal-to-noise ratio, denoted SNR\\nrms\\n, is obtained by taking the \\nsquare root of Eq. (8-11).\\nWhile objective ﬁdelity criteria offer a simple and convenient way to evaluate \\ninformation loss, decompressed images are often ultimately viewed by humans. \\nSo, measuring image quality by the subjective evaluations of people is often more \\nappropriate. This can be done by presenting a decompressed image to a cross section \\nof viewers and averaging their evaluations. The evaluations may be made using an \\nabsolute rating scale, or by means of side-by-side comparisons of \\nfx y\\n(,\\n)\\n and \\nˆ\\n(,\\n) .\\nfx y\\n \\nT\\nable 8.2 shows one possible absolute rating scale. Side-by-side comparisons can be \\ndone with a scale such as \\n---\\n32\\n1 0 1 2 3\\n,,, , , ,\\n{}\\n to represent the subjective evaluations \\nmuch\\n{\\n worse\\n, \\nworse\\n, \\nslightly worse\\n, \\nthe same\\n, \\nslightly better\\n, \\nbetter\\n, \\nmuch \\nbetter\\n}\\n, \\nrespectively\\n. In either case, the evaluations are based on \\nsubjective ﬁdelity criteria\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   548\\n6/16/2017   2:10:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 549}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n549\\nEXAMPLE 8.3 :   Image quality comparisons.\\nFigure 8.4 shows three different approximations of the image in Fig. 8.1(a). Using Eq. (8-10) with \\nFig. 8.1(a) as \\nfx y\\n(,\\n)\\n and Figs. 8.4(a) through (c) as \\nˆ\\n(,\\n) ,\\nfx y\\n the computed rms errors are 5.17, 15.67, \\nand 14.17 intensity levels\\n, respectively. In terms of rms error (an objective ﬁdelity criterion), the images \\nare ranked in order of decreasing quality as \\n() , () , ().\\nac\\nb\\n{}\\n A subjective evaluation of the images using \\nTable 8.2, however, might yield an \\nexcellent\\n rating for (a), a \\nmarginal\\n rating for (b), and an \\ninferior\\n or \\nunusable\\n rating for (c). Thus, using a subjective ﬁdelity criteria, (b) is ranked ahead of (c).\\nIMAGE COMPRESSION MODELS\\nAs Fig. 8.5 shows, an image compression system is composed of two distinct func-\\ntional components: an \\nencoder\\n and a \\ndecoder\\n. The encoder performs compression, \\nand the decoder performs the complementary operation of decompression. Both \\noperations can be performed in software, as is the case in Web browsers and many \\ncommercial image-editing applications, or in a combination of hardware and firm-\\nware, as in commercial DVD players. A \\ncodec\\n is a device or program that is capable \\nof both encoding and decoding.\\nValue Rating\\nDescription\\n1 Excellent An image of extremely high quality, as good as you could desire.\\n2 Fine An image of high quality, providing enjoyable viewing. Interfer-\\nence is not objectionable.\\n3 Passable An image of acceptable quality. Interference is not objectionable.\\n4 Marginal An image of poor quality; you wish you could improve it. Interfer-\\nence is somewhat objectionable.\\n5 Inferior A very poor image, but you could watch it. Objectionable interfer-\\nence is deﬁnitely present.\\n6 Unusable An image so bad that you could not watch it.\\nTABLE \\n8.2\\nRating scale of \\nthe Television \\nAllocations Study \\nOrganization. \\n(Frendendall and \\nBehrend.)\\nb a\\nc\\nFIGURE 8.4\\n Three approximations of the image in Fig. 8.1(a).\\nDIP4E_GLOBAL_Print_Ready.indb   549\\n6/16/2017   2:10:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 550}),\n",
       " Document(page_content='550\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nInput image \\nfx\\n(,\\n)\\np\\n is fed into the encoder, which creates a compressed repre-\\nsentation of the input.\\n This representation is stored for later use, or transmitted for \\nstorage and use at a remote location. When the compressed representation is pre-\\nsented to its complementary decoder, a reconstructed output image \\nˆ\\n(,\\n)\\nfx\\np\\n is gen-\\nerated.\\n In still-image applications, the encoded input and decoder output are \\nfx y\\n(,\\n)\\n \\nand \\nˆ\\n(,\\n),\\nfx\\ny\\n respectively. In video applications, they are \\nfx y t\\n(,\\n, )\\n and \\nˆ\\n(,\\n,)\\n,\\nfx y t\\n where \\nthe discrete parameter \\nt\\n speciﬁes time\\n. In general, \\nˆ\\n(,\\n)\\nfx\\np\\n may or may not be an \\nexact replica of \\nfx\\n(,\\n) .\\np\\n If it is, the compression system is called \\nerror free\\n, \\nlossless\\n, \\nor \\ninformation preserving\\n.\\n If not, the reconstructed output image is distorted, and \\nthe compression system is referred to as \\nlossy\\n.\\nThe Encoding or Compression Process\\nThe encoder of Fig. 8.5 is designed to remove the redundancies described in the \\nprevious sections through a series of three independent operations. In the first stage \\nof the encoding process, a \\nmapper\\n transforms \\nfx\\n(,\\n)\\np\\n into a (usually nonvisual) for-\\nmat designed to reduce spatial and temporal redundanc\\ny. This operation generally is \\nreversible, and may or may not directly reduce the amount of data required to repre-\\nsent the image. Run-length coding is an example of a mapping that normally yields \\ncompression in the first step of the encoding process. The mapping of an image into \\na set of less correlated transform coefficients (see Section 8.9) is an example of the \\nopposite case (the coefficients must be further processed to achieve compression). \\nIn video applications, the mapper uses previous (and, in some cases, future) video \\nframes to facilitate the removal of temporal redundancy.\\nThe \\nquantizer\\n in Fig. 8.5 reduces the accuracy of the mapper’s output in accor-\\ndance with a pre-established ﬁdelity criterion. The goal is to keep irrelevant infor-\\nmation out of the compressed representation. As noted earlier, this operation is \\nirreversible. It must be omitted when error-free compression is desired. In video \\napplications, the \\nbit rate\\n of the encoded output is often measured (in bits/second), \\nand is used to adjust the operation of the quantizer so a predetermined average \\noutput rate is maintained. Thus, the visual quality of the output can vary from frame \\nto frame as a function of image content.\\nHere, the notation \\nfx\\n(, )\\np\\n is used to denote \\nboth \\nfx y\\n( , ) and \\nfx y t\\n(,, ) .\\nFIGURE 8.5\\nFunctional block \\ndiagram of a \\ngeneral image \\ncompression \\nsystem.\\nCompressed data\\nfor storage\\nand transmission\\nEncoder\\nDecoder\\nSymbol\\ncoder\\nQuantizer\\nMapper\\nSymbol\\ndecoder\\nInverse\\nmapper\\nor\\nor\\nf\\n(\\nx\\n, \\ny\\n)\\nf\\n(\\nx\\n, \\ny, t\\n)\\nˆ\\n(,)\\nfx y\\nˆ\\n(,, )\\nfx y t\\nDIP4E_GLOBAL_Print_Ready.indb   550\\n6/16/2017   2:10:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 551}),\n",
       " Document(page_content='8.1\\n  \\nFundamentals\\n    \\n551\\nIn the third and ﬁnal stage of the encoding process, the \\nsymbol coder\\n of Fig. 8.5 \\ngenerates a ﬁxed-length or variable-length code to represent the quantizer output, \\nand maps the output in accordance with the code. In many cases, a variable-length \\ncode is used. The shortest code words are assigned to the most frequently occur-\\nring quantizer output values, thus minimizing coding redundancy. This operation is \\nreversible. Upon its completion, the input image has been processed for the removal \\nof each of the three redundancies described in the previous sections.\\nThe Decoding or Decompression Process\\nThe decoder of Fig. 8.5 contains only two components: a \\nsymbol decoder\\n and an \\ninverse mapper\\n. They perform, in reverse order, the inverse operations of the encod-\\ner’s symbol encoder and mapper. Because quantization results in irreversible infor-\\nmation loss, an inverse quantizer block is not included in the general decoder model. \\nIn video applications, decoded output frames are maintained in an internal frame \\nstore (not shown) and used to reinsert the temporal redundancy that was removed \\nat the encoder.\\nIMAGE FORMATS, CONTAINERS, AND COMPRESSION STANDARDS\\nIn the context of digital imaging, an \\nimage file format\\n is a standard way to organize \\nand store image data. It defines how the data is arranged and the type of compres-\\nsion (if any) that is used. An \\nimage container\\n is similar to a file format, but han-\\ndles multiple types of image data. Image \\ncompression standards\\n, on the other hand, \\ndefine procedures for compressing and decompressing images—that is, for reducing \\nthe amount of data needed to represent an image. These standards are the underpin-\\nning of the widespread acceptance of image compression technology.\\nFigure 8.6 lists the most important image compression standards, ﬁle formats, and \\ncontainers in use today, grouped by the type of image handled. The entries in blue \\nare international standards sanctioned by the \\nInternational Standards Organization\\n \\n(ISO), the \\nInternational Electrotechnical Commission\\n (IEC), and/or the \\nInternational \\nTelecommunications Union\\n (ITU-T)—a \\nUnited Nations\\n (UN) organization that was \\nonce called the \\nConsultative Committee of the International Telephone and Telegraph\\n \\n(CCITT). Two video compression standards, VC-1 by the \\nSociety of Motion Pictures \\nand Television Engineers\\n (SMPTE) and A VS by the \\nChinese Ministry of Information \\nIndustry\\n (MII), are also included. Note that they are shown in black, which is used \\nin Fig. 8.6 to denote entries that are not sanctioned by an international standards \\norganization.\\nTables 8.3 through 8.5 summarize the standards, formats, and containers listed \\nin Fig. 8.6. Responsible organizations, targeted applications, and key compression \\nmethods are identiﬁed. The compression methods themselves are the subject of Sec-\\ntions 8.2 through 8.11, where we will describe the principal lossy and error-free com-\\npression methods in use today. The focus of these sections is on methods that have \\nproven useful in mainstream binary, continuous-tone still-image, and video com-\\npression standards. The standards themselves are used to demonstrate the methods \\npresented. In Tables 8.3 through 8.5, forward references to the relevant sections in \\nwhich the compression methods are described are enclosed in square brackets.\\nDIP4E_GLOBAL_Print_Ready.indb   551\\n6/16/2017   2:10:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 552}),\n",
       " Document(page_content='552\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nName Organization\\nDescription\\nBi-Level Still Images\\nCCITT \\nGroup 3\\nITU-T\\nDesigned as a facsimile (FAX) method for transmitting binary documents over \\ntelephone lines. Supports 1-D and 2-D run-length [8.6] and Huffman [8.2] coding.\\nCCITT \\nGroup 4\\nITU-T A simpliﬁed and streamlined version of the CCITT Group 3 standard supporting \\n2-D run-length coding only.\\nJBIG or \\nJBIG1\\nISO/IEC/\\nITU-T\\nA \\nJoint Bi-level Image Experts Group\\n standard for progressive, lossless compres-\\nsion of bi-level images. Continuous-tone images of up to 6 bits/pixel can be coded \\non a bit-plane basis [8.8]. Context-sensitive arithmetic coding [8.4] is used and an \\ninitial low-resolution version of the image can be gradually enhanced with addi-\\ntional compressed data.\\nJBIG2 ISO/IEC/\\nITU-T\\nA follow-on to JBIG1 for bi-level images in desktop, Internet, and FAX applica-\\ntions. The compression method used is content based, with dictionary-based meth-\\nods [8.7] for text and halftone regions, and Huffman [8.2] or arithmetic coding [8.4] \\nfor other image content. It can be lossy or lossless.\\nContinuous-Tone Still Images\\nJPEG ISO/IEC/\\nITU-T\\nA \\nJ\\noint Photographic Experts Group\\n standard for images of photographic quality. \\nIts lossy baseline coding system (most commonly implemented) uses quantized \\ndiscrete cosine transforms (DCT) on image blocks [8.9], Huffman [8.2], and run-\\nlength [8.6] coding. It is one of the most popular methods for compressing images \\non the Internet.\\nJPEG-LS ISO/IEC/\\nITU-T\\nA lossless to near-lossless standard for continuous-tone images based on adaptive \\nprediction [8.10], context modeling [8.4], and Golomb coding [8.3].\\nJPEG-\\n2000\\nISO/IEC/\\nITU-T\\nA follow-on to JPEG for increased compression of photographic quality images. \\nArithmetic coding [8.4] and quantized discrete wavelet transforms (DWT) [8.11] \\nare used. The compression can be lossy or lossless.\\nTABLE \\n8.3\\nInternationally sanctioned image compression standards. The numbers in brackets refer to sections in this chapter.\\nFIGURE 8.6\\nSome popular \\nimage compres-\\nsion standards, \\nﬁle formats, \\nand containers. \\nInternationally \\nsanctioned entries \\nare shown in blue; \\nall others are in \\nblack.\\n                   Binary\\nCCITT Group 3 \\nTIFF\\nCCITT Group 4\\nJBIG (or JBIG1)\\nJBIG2\\nStill Image\\nImage Compression\\nStandards, Formats, and Containers\\n                         Video\\nDV \\nAV S\\nH.261 \\nHDV\\nH.262 \\nM-JPEG\\nH.263 \\nQuickTime\\nH.264 \\nVC-1 (or WMV9)\\nHVEC/H.265 \\nWebP VP8\\n \\nMPEG-1\\nMPEG-2\\nMPEG-4\\nMPEG-4 A VC\\n    Continuous Tone\\nJPEG \\nBMP\\nJPEG-LS \\nGIF\\nJPEG-2000 \\nPDF\\n \\nPNG\\n \\nTIFF\\n \\nWebP\\nDIP4E_GLOBAL_Print_Ready.indb   552\\n6/16/2017   2:10:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 553}),\n",
       " Document(page_content='8.2\\n  \\nHuffman Coding\\n    \\n553\\n8.2 HUFFMAN CODING  \\nOne of the most popular techniques for removing coding redundancy is due to Huff-\\nman (Huffman [1952]). When coding the symbols of an information source individu-\\nally, \\nHuffman coding\\n yields the smallest possible number of code symbols per source \\nsymbol. In terms of Shannon’s first theorem (see Section 8.1), the resulting code is \\noptimal for a fixed value of \\nn\\n, subject to the constraint that the source symbols be \\ncoded \\none at a time\\n. In practice, the source symbols may be either the intensities of \\nan image or the output of an intensity mapping operation (pixel differences, run \\nlengths, and so on).\\n8.2\\nWith reference to \\nTables 8.3–8.5, Huffman \\ncodes are used in\\n• CCITT \\n• JBIG2 \\n• JPEG \\n• MPEG-1, 2, 4 \\n• H.261, H.262, \\n• H.263, H.264\\nand other compression \\nstandards.\\nName Organization\\nDescription\\nDV IEC\\nDigital Video\\n. A video standard tailored to home and semiprofessional video pro-\\nduction applications and equipment, such as electronic news gathering and cam-\\ncorders. Frames are compressed independently for uncomplicated editing using a \\nDCT-based approach [8.9] similar to JPEG.\\nH.261 ITU-T\\nA two-way videoconferencing standard for ISDN (\\nintegrated services digital net-\\nwork\\n) lines. It supports non-interlaced \\n352 288\\n*\\n and 176 144\\n*\\n resolution images, \\ncalled CIF (\\nCommon Intermediate F\\normat\\n) and QCIF (\\nQuarter CIF\\n), respectively. \\nA DCT-based compression approach [8.9] similar to JPEG is used, with frame-to-\\nframe prediction differencing [8.10] to reduce temporal redundancy. A block-based \\ntechnique is used to compensate for motion between frames.\\nH.262 ITU-T See MPEG-2 below.\\nH.263 ITU-T An enhanced version of H.261 designed for ordinary telephone modems (i.e., \\n28.8 Kb/s) with additional resolutions: SQCIF (\\nSub-Quarter\\n CIF \\n128 96\\n*\\n), 4CIF \\n()\\n704\\n576\\n*\\n and 16CIF \\n() .\\n1408\\n512\\n*\\nH.264 ITU-T An extension of H.261–H.263 for videoconferencing, streaming, and television. It \\nsupports prediction differences within frames [8.10],\\n variable block size integer \\ntransforms (rather than the DCT), and context adaptive arithmetic coding [8.4].\\nH.265 \\nMPEG-H \\nHEVC\\nISO/IEC \\nITU-T\\nHigh Efﬁciency Video Coding\\n (HVEC). An extension of H.264 that includes \\nsupport for macroblock sizes up to \\n64 64\\n*\\n and additional intraframe prediction \\nmodes\\n, both useful in 4K video applications.\\nMPEG-1 ISO/IEC\\nA \\nMotion Pictures Expert Group\\n standard for CD-ROM applications with non-\\ninterlaced video at up to 1.5 Mb/s. It is similar to H.261 but frame predictions can \\nbe based on the previous frame, next frame, or an interpolation of both. It is sup-\\nported by almost all computers and DVD players.\\nMPEG-2 ISO/IEC An extension of MPEG-1 designed for DVDs with transfer rates at up to 15 Mb/s. \\nSupports interlaced video and HDTV . It is the most successful video standard to \\ndate.\\nMPEG-4 ISO/IEC An extension of MPEG-2 that supports variable block sizes and prediction differ-\\nencing [8.10] within frames.\\nMPEG-4 \\nAV C\\nISO/IEC\\nMPEG-4 Part 10 \\nAdvanced Video Coding\\n (A VC). Identical to H.264.\\nTABLE \\n8.4\\nInternationally sanctioned video compresssion standards. The numbers in brackets refer to sections in this chapter.\\nDIP4E_GLOBAL_Print_Ready.indb   553\\n6/16/2017   2:10:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 554}),\n",
       " Document(page_content='554\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nName Organization\\nDescription\\nContinuous-Tone Still Images\\nBMP Microsoft\\nW\\nindows Bitmap\\n. A ﬁle format used mainly for simple uncompressed images. \\nGIF CompuServe\\nGraphic Interchange Format\\n. A ﬁle format that uses lossless LZW coding [8.5] for \\n1- through 8-bit images. It is frequently used to make small animations and short \\nlow-resolution ﬁlms for the Internet.\\nPDF Adobe \\nSystems\\nPortable Document Format\\n. A format for representing 2-D documents in a device \\nand resolution independent way. It can function as a container for JPEG, JPEG-\\n2000, CCITT, and other compressed images. Some PDF versions have become ISO \\nstandards.\\nPNG\\nWorld Wide \\nWeb Consor-\\ntium\\n (W3C)\\nPortable Network Graphics\\n. A ﬁle format that losslessly compresses full color \\nimages with transparency (up to 48 bits/pixel) by coding the difference between \\neach pixel’s value and a predicted value based on past pixels [8.10].\\nTIFF Aldus\\nTagged Image File Format\\n. A ﬂexible ﬁle format supporting a variety of image \\ncompression standards, including JPEG, JPEG-LS, JPEG-2000, JBIG2, and others.\\nWebP Google\\nWebP\\n supports lossy compression via WebP VP8 intraframe video compression \\n(see below) and lossless compression using spatial prediction [8.10] and a variant \\nof LZW backward referencing [8.5] and Huffman entropy coding [8.2]. Transpar-\\nency is also supported. \\nVideo\\nA VS MII\\nA\\nudio-Video Standard\\n. Similar to H.264 but uses exponential Golomb coding [8.3]. \\nDeveloped in China.\\nHDV Company \\nconsortium\\nHigh Deﬁnition Video\\n. An extension of DV for HD television that uses compres-\\nsion similar to MPEG-2, including temporal redundancy removal by prediction \\ndifferencing [8.10].\\nM-JPEG Various \\ncompanies\\nMotion JPEG\\n. A compression format in which each frame is compressed indepen-\\ndently using JPEG.\\nQuick-\\nTime\\nApple \\nComputer\\nA media container supporting DV , H.261, H.262, H.264, MPEG-1, MPEG-2, \\nMPEG-4, and other video compression formats.\\nVC-1 \\nWMV9\\nSMPTE \\nMicrosoft\\nThe most used video format on the Internet. Adopted for HD and \\nBlu-ray\\n high-\\ndeﬁnition DVDs. It is similar to H.264/A VC, using an integer DCT with varying \\nblock sizes [8.9 and 8.10] and context-dependent variable-length code tables [8.2], \\nbut no predictions within frames.\\nWebP \\nVP8\\nGoogle A ﬁle format based on block transform coding [8.9] prediction differences within \\nframes and between frames [8.10]. The differences are entropy encoded using an \\nadaptive arithmetic coder [8.4].\\nTABLE \\n8.5\\nPopular image and video compression standards, ﬁle formats, and containers not included in Tables 8.3 and 8.4. The \\nnumbers in brackets refer to sections in this chapter.\\nDIP4E_GLOBAL_Print_Ready.indb   554\\n6/16/2017   2:10:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 555}),\n",
       " Document(page_content='8.2\\n  \\nHuffman Coding\\n    \\n555\\nThe ﬁrst step in Huffman’s approach is to create a series of source reductions \\nby ordering the probabilities of the symbols under consideration, then combining \\nthe lowest probability symbols into a single symbol that replaces them in the next \\nsource reduction. Figure 8.7 illustrates this process for binary coding (\\nK\\n-ary Huff-\\nman codes also can be constructed). At the far left, a hypothetical set of source sym-\\nbols and their probabilities are ordered from top to bottom in terms of decreasing \\nprobability values. To form the ﬁrst source reduction, the bottom two probabilities, \\n0.06 and 0.04, are combined to form a “compound symbol” with probability 0.1. This \\ncompound symbol and its associated probability are placed in the ﬁrst source reduc-\\ntion column so that the probabilities of the reduced source also are ordered from the \\nmost to the least probable. This process is then repeated until a reduced source with \\ntwo symbols (at the far right) is reached.\\nThe second step in Huffman’s procedure is to code each reduced source, start-\\ning with the smallest source and working back to the original source. The minimal \\nlength binary code for a two-symbol source, of course, are the symbols 0 and 1. As \\nFig. 8.8 shows, these symbols are assigned to the two symbols on the right. (The \\nassignment is arbitrary; reversing the order of the 0 and 1 would work just as well.) \\nAs the reduced source symbol with probability 0.6 was generated by combining two \\nsymbols in the reduced source to its left, the 0 used to code it is now assigned to both \\nof these symbols, and a 0 and 1 are arbitrarily appended to each to distinguish them \\nfrom each other. This operation is then repeated for each reduced source until the \\noriginal source is reached. The ﬁnal code appears at the far left in Fig. 8.8. The aver-\\nage length of this code is\\n \\nL\\navg\\n=\\n(.) () (.) () (.) () (.) () (. ) () (. ) (\\n04 1 03 2 01 3 01 4 00 6 5 00 4\\n++++ +\\n5 5\\n22\\n)\\n.\\n=\\n bits/pixel\\nand the entropy of the source is 2.14 bits/symbol.\\nHuffman’\\ns procedure creates the optimal code for a set of symbols and probabili-\\nties \\nsubject to the constraint\\n that the symbols be coded one at a time. After the code \\nhas been created, coding and/or error-free decoding is accomplished in a simple \\nlookup table manner. The code itself is an instantaneous uniquely decodable block \\ncode. It is called a \\nblock code\\n because each source symbol is mapped into a ﬁxed \\nsequence of code symbols. It is \\ninstantaneous\\n because each code word in a string of \\nFIGURE 8.7\\nHuffman source \\nreductions.\\na\\n2\\na\\n6\\na\\n1\\na\\n4\\na\\n3\\na\\n5\\nSymbol\\n0.4\\n0.3\\n0.1\\n0.1\\n0.06\\n0.04\\nProbability\\n0.4\\n0.3\\n0.1\\n0.1\\n0.1\\n1\\n0.4\\n0.3\\n0.2\\n0.1\\n2\\n0.4\\n0.3\\n0.3\\n3\\n0.6\\n0.4\\n4\\nOriginal source\\nSource reduction\\nDIP4E_GLOBAL_Print_Ready.indb   555\\n6/16/2017   2:10:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 556}),\n",
       " Document(page_content='556\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\ncode symbols can be decoded without referencing succeeding symbols. It is \\nuniquely \\ndecodable\\n because any string of code symbols can be decoded in only one way. Thus, \\nany string of Huffman encoded symbols can be decoded by examining the individual \\nsymbols of the string in a left-to-right manner. For the binary code of Fig. 8.8, a left-\\nto-right scan of the encoded string 010100111100 reveals that the ﬁrst valid code \\nword is 01010, which is the code for symbol \\na\\n3\\n. The next valid code is 011, which cor-\\nresponds to symbol \\na\\n1\\n. Continuing in this manner reveals the completely decoded \\nmessage to be \\na\\n3\\n \\na\\n1\\n \\na\\n2\\n \\na\\n2\\n \\na\\n6\\n.\\nEXAMPLE 8.4 :   Huffman Coding.\\nThe \\n512 512\\n*\\n 8-bit monochrome image in Fig. 8.9(a) has the intensity histogram shown in Fig. 8.9(b). \\nBecause the intensities are not equally probable\\n, a MATLAB implementation of Huffman’s procedure \\nwas used to encode them with 7.428 bits/pixel, including the Huffman code table that is required to \\nreconstruct the original 8-bit image intensities. The compressed representation exceeds the estimated \\nentropy of the image [7.3838 bits/pixel from Eq. (8-7)] by \\n512 7 428 7 3838\\n2\\n×−\\n(. . )\\n or 11,587 bits—about \\n0.6%.\\n The resulting compression ratio and corresponding relative redundancy are \\nC\\n==\\n8 7 428 1 077\\n.. ,\\n \\nand \\nR\\n==\\n1 1 1 077 0 0715\\n-\\n(. ) . ,\\n respectively. Thus 7.15% of the original 8-bit ﬁxed-length intensity  \\nrepresentation was removed as coding redundanc\\ny.\\nWhen a large number of symbols is to be coded, the construction of an optimal \\nHuffman code is a nontrivial task. For the general case of \\nJ\\n source symbols, \\nJ\\n symbol \\nprobabilities, \\nJ\\n−\\n2\\n source reductions, and \\nJ\\n−\\n2\\n code assignments are required. When \\nsource symbol probabilities can be estimated in advance\\n, “near optimal” coding can \\nbe achieved with pre-computed Huffman codes. Several popular image compression \\nstandards, including the JPEG and MPEG standards discussed in Sections 8.9 and \\n8.10, specify default Huffman coding tables that have been pre-computed based on \\nexperimental data.\\n8.3 GOLOMB CODING  \\nIn this section, we consider the coding of nonnegative integer inputs with exponen-\\ntially decaying probability distributions. Inputs of this type can be optimally encoded \\n(in the sense of Shannon’s ﬁrst theorem) using a family of codes that are computa-\\ntionally simpler than Huffman codes. The codes themselves were ﬁrst proposed for \\nthe representation of nonnegative run lengths (Golomb [1966]). In the discussion \\n8.3\\nWith reference to \\nTables 8.3–8.5, Golomb \\ncodes are used in\\n• JPEG-LS \\n• A VS\\ncompression.\\nFIGURE 8.8\\nHuffman code  \\nassignment  \\nprocedure.\\na\\n2\\na\\n6\\na\\n1\\na\\n4\\na\\n3\\na\\n5\\nSymbol\\n0.4\\n0.3\\n0.1\\n0.1\\n0.06\\n0.04\\nProbability\\n1\\n00\\n011\\n0100\\n01010\\n01011\\nCode\\n0.4\\n0.3\\n0.1\\n0.1\\n0.1\\n1\\n00\\n011\\n0100\\n0101\\n1\\n0.4\\n0.3\\n0.2\\n0.1\\n1\\n00\\n010\\n011\\n2\\n0.4\\n0.3\\n0.3\\n1\\n00\\n01\\n3\\n0.6\\n0.4\\n0\\n1\\n4\\nOriginal source\\nSource reduction\\nDIP4E_GLOBAL_Print_Ready.indb   556\\n6/16/2017   2:10:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 557}),\n",
       " Document(page_content='8.3\\n  \\nGolomb Coding\\n    \\n557\\nthat follows, the notation \\nx\\n⎢\\n⎣\\n⎥\\n⎦\\n denotes the largest integer less than or equal to \\nx\\n, \\nx\\n⎡\\n⎢\\n⎤\\n⎥\\n \\nmeans the smallest integer greater than or equal to \\nx\\n,\\n and \\nxy\\nmo\\nd\\n is the remainder \\nof \\nx\\n divided by \\ny\\n.\\nGiven a nonnegative integer \\nn\\n and a positive integer \\ndivisor\\n \\nm\\n7\\n0,\\n the \\nGolomb \\ncode\\n of \\nn\\n with respect to \\nm\\n,\\n denoted \\nGn\\nm\\n() ,\\n is a combination of the unary code of \\nquotient\\n \\nnm\\n⎢\\n⎣\\n⎥\\n⎦\\n and the binary representation of \\nremainder\\n \\nnm\\nmo\\nd .\\n \\nGn\\nm\\n()\\n is con-\\nstructed as follows:\\n1. \\nForm the unary code of quotient \\nnm\\n⎢\\n⎣\\n⎥\\n⎦\\n.\\n (The \\nunary code\\n of an integer \\nq\\n is \\ndeﬁned as \\nq\\n 1’\\ns followed by a 0.)\\n2. \\nLet \\nkm\\n=\\n⎡\\n⎢\\n⎤\\n⎥\\nlog\\n,\\n2\\n \\ncm\\nk\\n=\\n2\\n-\\n, \\nrn m\\n=\\nmo\\nd ,\\n and compute truncated remainder \\n′\\nr\\n such that\\n \\n′\\n=\\nr\\nrk\\nrc\\nrc k\\n truncated to  bits\\n truncated to  bits othe\\n-… 6\\n+\\n10\\nr\\nrwise\\n⎧\\n⎨\\n⎩\\n \\n(8-12)\\n3. \\nConcatenate the results of Steps 1 and 2.\\nT\\no compute \\nG\\n4\\n9\\n() ,\\n for example, begin by determining the unary code of the quo-\\ntient \\n94 2 2 5 2\\n⎢\\n⎣\\n⎥\\n⎦\\n=\\n⎢\\n⎣\\n⎥\\n⎦\\n=\\n.,\\n which is 110 (the result of Step 1). Then let \\nk\\n=\\n⎡\\n⎢\\n⎤\\n⎥\\n=\\nlog\\n,\\n2\\n42\\n \\nc\\n==\\n24\\n0\\n2\\n-\\n,\\n and \\nr\\n=\\n94\\nmo\\nd ,\\n which in binary is \\n1001 0100\\nmo\\nd\\n or 0001. In accor-\\ndance with Eq.\\n (8-12), \\n′\\nr\\n is then \\nr\\n (i.e\\n., 0001) truncated to 2 bits, which is 01 (the result \\nof Step 2). Finally, concatenate 110 from Step 1 and 01 from Step 2 to get 11001, \\nwhich is \\nG\\n4\\n9\\n() .\\nFor the special case of \\nm\\nk\\n=\\n2,\\n \\nc\\n=\\n0\\n and \\n′\\n==\\nrr\\nn m\\nmod\\n truncated to \\nk\\n bits in \\nEq.\\n (8-12) for all \\nn\\n. The divisions required to generate the resulting Golomb codes \\nbecome binary shift operations, and the computationally simpler codes are called \\nGolomb-Rice\\n or \\nRice codes\\n (Rice [1975]). Columns 2, 3, and 4 of Table 8.6 list the \\nG\\n1\\n, \\nG\\n2\\n, and \\nG\\n4\\n codes of the ﬁrst ten nonnegative integers. Because \\nm\\n is a power  \\nof 2 in each case (i.e., \\n12\\n0\\n=\\n, \\n22\\n1\\n=\\n,\\n and \\n42\\n2\\n=\\n), they are the ﬁrst three Golomb-Rice \\ncodes as well.\\n Moreover, \\nG\\n1 \\nis the unary code of the nonnegative integers because \\nnn\\n1\\n⎢\\n⎣\\n⎥\\n⎦\\n=\\n and \\nn\\nmo\\nd1 0\\n=\\n for all \\nn\\n.\\nb a\\nFIGURE 8.9\\n(a) A \\n512 512\\n*\\n \\n8-bit image and \\n(b) its histogram.\\nIntensity\\nNumber of pixels\\n0\\n0\\n100 250\\n150 200\\n50\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nDIP4E_GLOBAL_Print_Ready.indb   557\\n6/16/2017   2:10:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 558}),\n",
       " Document(page_content='558\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nKeeping in mind that Golomb codes only can be used to represent nonnegative \\nintegers, and that there are many Golomb codes to choose from, a key step in their \\neffective application is the selection of divisor \\nm\\n. When the integers to be repre-\\nsented are \\ngeometrically\\n distributed with a \\nprobability mass function\\n (PMF)\\n†\\n \\nPn\\nn\\n() ( )\\n=\\n1\\n-\\nrr\\n \\n(8-13)\\nfor some \\n01\\n66\\nr\\n,\\n Golomb codes can be shown to be optimal in the sense that \\nGn\\nm\\n()\\n provides the shortest average code length of all uniquely decipherable codes \\n(Gallager and \\nVoorhis [1975]) when\\n \\nm\\n=\\n⎡\\n⎢\\n⎢\\n⎤\\n⎥\\n⎥\\nlog ( )\\nlog ( )\\n2\\n2\\n1\\n1\\n+\\nr\\nr\\n \\n(8-14)\\nFigure 8.10(a) plots Eq. (8-13) for three values of \\nr\\n and graphically illustrates the \\nsymbol probabilities that Golomb codes handle well (that is\\n, code efficiently). As is \\nshown in the figure, small integers are much more probable than large ones.\\nBecause the probabilities of the intensities in an image [see, for example, the his-\\ntogram of Fig. 8.9(b)] are unlikely to match the probabilities speciﬁed in Eq. (8-13) \\nand shown in Fig. 8.10(a), Golomb codes are seldom used for the coding of intensi-\\nties. When intensity differences are to be coded, however, the probabilities of the \\nresulting “difference values” (see Section 8.10) (with the notable exception of the \\nnegative differences) often resemble those of Eq. (8-13) and Fig. 8.10(a). To handle \\nnegative differences in Golomb coding, which can only represent nonnegative inte-\\ngers, a mapping like\\n†A \\nprobability mass function\\n (PMF) is a function that deﬁnes the probability that a discrete random variable is \\nexactly equal to some value. A PMF differs from a PDF in that a PDF’s values are not probabilities; rather, the \\nintegral of a PDF over a speciﬁed interval is a probability.\\nn\\nGn\\n1\\n()\\nGn\\n2\\n()\\nGn\\n4\\n()\\nGn\\nexp\\n0\\n()\\n0 0\\n00\\n000\\n0\\n1\\n10\\n01\\n001\\n100\\n2 110\\n100\\n010\\n101\\n3 1110\\n101\\n011\\n11000\\n4 11110\\n1100\\n1000\\n11001\\n5 111110\\n1101\\n1001\\n11010\\n6 1111110\\n11100\\n1010\\n11011\\n7 11111110\\n11101\\n1011\\n1110000\\n8 111111110 111100\\n11000\\n1110001\\n9 1111111110 111101\\n11001\\n1110010\\nTABLE \\n8.6\\nSeveral Golomb \\ncodes for the \\nintegers 0–9.\\nDIP4E_GLOBAL_Print_Ready.indb   558\\n6/16/2017   2:10:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 559}),\n",
       " Document(page_content='8.3\\n  \\nGolomb Coding\\n    \\n559\\n \\nMn\\nnn\\nnn\\n()\\n=\\n⎧\\n⎨\\n⎩\\n20\\n21 0\\nÚ\\n-6\\n \\n(8-15)\\nis typically used. Using this mapping, for example, the two-sided PMF shown in \\nF\\nig. 8.10(b) can be transformed into the one-sided PMF in Fig. 8.10(c). Its integers \\nare reordered, alternating the negative and positive integers so the negative integers \\nare mapped into the odd positive integer positions. If \\nP\\nn\\n()\\n is two-sided and centered \\nat zero\\n, \\nPMn\\n()\\n()\\n will be one-sided. The mapped integers, \\nMn\\n()\\n,\\n can then be effi-\\nciently encoded using an appropriate Golomb-Rice code (W\\neinberger et al. [1996]).\\nEXAMPLE 8.5 :   Golomb-Rice coding.\\nConsider again the image from Fig. 8.1(c) and note that its histogram [see Fig. 8.3(a)] is similar to the \\ntwo-sided distribution in Fig. 8.10(b) above. If we let \\nn\\n be some nonnegative integer intensity in the \\nimage, where \\n0 225\\n≤≤\\nn\\n,\\n and \\nm\\n be the mean intensity, \\nPn\\n−\\nm\\n()\\n is the two-sided distribution shown in \\nFig. 8.11(a). This plot was generated by normalizing the histogram in Fig. 8.3(a) by the total number of \\npixels in the image and shifting the normalized values to the left by 128 (which in effect subtracts the \\nmean intensity from the image). In accordance with Eq. (8-15), \\nPMn\\n−\\nm\\n()\\n(\\n)\\n is then the one-sided dis-\\ntribution shown in Fig. 8.11(b). If the reordered intensity values are Golomb coded using a MATLAB \\nimplementation of code \\nG\\n1\\n in column 2 of Table 8.6, the encoded representation is 4.5 times smaller \\nthan the original image (i.e., \\nC\\n=\\n45\\n.\\n). The \\nG\\n1\\n code realizes \\n45 51\\n.. ,\\n or 88% of the theoretical com-\\npression possible with variable-length coding\\n. [Based on the entropy calculated in Example (8-2), the \\nmaximum possible compression ratio through variable-length coding is \\nC\\n=≈\\n81 5 6 6 5 1\\n.. .\\n] Moreover, \\nGolomb coding achieves 96% of the compression provided by a MA\\nTLAB implementation of Huff-\\nman’s approach, and doesn’t require the computation of a custom Huffman coding table.\\nNow consider the image in Fig. 8.9(a). If its intensities are Golomb coded using the same \\nG\\n1\\n code as \\nabove, \\nC\\n=\\n0\\n0922\\n..\\n That is, there is \\ndata expansion\\n.\\n This is due to the fact that the probabilities of the \\nintensities of the image in Fig. 8.9(a) are much different than the probabilities deﬁned in Eq. (8-13). In \\na similar manner, Huffman codes can produce data expansion when used to encode symbols whose \\nprobabilities are different from those for which the code was computed. In practice, the further you \\ndepart from the input probability assumptions for which a code is designed, the greater the risk of poor \\ncompression performance and data expansion.\\nb a\\nc\\nFIGURE 8.10\\n(a) Three one-\\nsided geometric \\ndistributions from \\nEq. (8-13); (b) a \\ntwo-sided expo-\\nnentially decaying \\ndistribution; and \\n(c) a reordered \\nversion of (b) \\nusing Eq. (8-15).\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n02468\\n0 2 4\\n-2\\n- 4\\n02468\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nProbability\\nȡ\\n = 0.25\\nȡ\\n = 0.5\\nȡ\\n = 0.75\\nnn M\\n(\\nn\\n)\\nDIP4E_GLOBAL_Print_Ready.indb   559\\n6/16/2017   2:10:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 560}),\n",
       " Document(page_content='560\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nTo conclude our coverage of Golomb codes, we note that Column 5 of Table 8.6 \\ncontains the ﬁrst 10 codes of the zeroth-order \\nexponential Golomb code\\n, denoted \\nGn\\nexp\\n() .\\n0\\n Exponential-Golomb codes are useful for the encoding of run lengths, \\nbecause both short and long runs are encoded efﬁciently. An order-\\nk\\n exponential-\\nGolomb code \\nGn\\nk\\nexp\\n()\\n is computed as follows:\\n1. \\nFind an integer \\ni\\n≥\\n0 such that \\n \\n22\\n0\\n0\\n1\\njk\\njk\\nj\\ni\\nj\\ni\\nn\\n++\\n=\\n=\\n≤<\\n∑ ∑\\n−\\n \\n(8-16)\\nand form the unary code of \\ni\\n.\\n If \\nk\\n=\\n0,\\n \\nin\\n=+\\n()\\n⎢\\n⎣\\n⎥\\n⎦\\nlog\\n2\\n1\\n and the code is also \\nknown as the \\nElias gamma code\\n.\\n2. \\nTruncate the binary representation of\\n \\nn\\njk\\nj\\ni\\n−\\n−\\n2\\n0\\n1\\n+\\n=\\n∑\\n \\n(8-17)\\nto \\nki\\n+\\n least signiﬁcant bits.\\n3. \\nConcatenate the results of Steps 1 and 2.\\nT\\no find \\nG\\nexp\\n() ,\\n0\\n8\\n for example, we let \\ni\\n=\\n⎢\\n⎣\\n⎥\\n⎦\\nlog\\n2\\n9\\n or 3 in Step 1 because \\nk\\n=\\n0.\\n Equa-\\ntion (8-16) is then satisfied because\\n \\n282\\n0\\n0\\n31\\n0\\n0\\n3\\nj\\nj\\nj\\nj\\n+\\n−\\n+\\n≤<\\n==\\n∑∑\\n \\n28 2\\n0\\n2\\n0\\n3\\nj\\nj\\nj\\nj\\n≤<\\n==\\n∑∑\\nb a\\nFIGURE 8.11\\n(a) The probabil-\\nity distribution \\nof the image in \\nFig. 8.1(c) after \\nsubtracting the \\nmean intensity \\nfrom each pixel. \\n(b) A mapped  \\nversion of (a)  \\nusing Eq. (8-15).\\n012345678\\n- 4 - 3 - 2 - 1 01234\\nn\\n\\x03\\x10\\x03\\nμM\\n(\\nn\\n\\x03\\x10\\x03\\nμ\\n)\\nProbability\\n0\\n0.15\\n0.30\\n0.45\\n0.60\\n0.75\\n0\\n0.15\\n0.30\\n0.45\\n0.60\\n0.75\\nDIP4E_GLOBAL_Print_Ready.indb   560\\n6/16/2017   2:10:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 561}),\n",
       " Document(page_content='8.4\\n  \\nArithmetic Coding\\n    \\n561\\n \\n2228 2222\\n781 5\\n012 0123\\n++ +++\\n≤<\\n≤<\\nThe unary code of 3 is 1110 and Eq. (8-17) of Step 2 yields\\n \\n8 2 8 2 8 2 2 2 8 7 1 0001\\n00\\n1\\n2\\n0\\n2\\n0\\n31\\n−− − + + −\\n+\\n−\\njj\\nj\\nj\\n==\\n(\\n)\\n== =\\n=\\n=\\n∑ ∑\\nwhich when truncated to its \\n30\\n+\\n least significant bits becomes 001. The concatena-\\ntion of the results from Steps 1 and 2 then yields 1110001.\\n Note that this is the entry \\nin column 4 of Table 8.6 for \\nn\\n=\\n8.\\n Finally, we note that like the Huffman codes of the \\nlast section,\\n the Golomb codes of Table 8.6 are variable-length, instantaneous, and \\nuniquely decodable block codes.\\n8.4 ARITHMETIC CODING  \\nUnlike the variable-length codes of the previous two sections, \\narithmetic coding\\n gen-\\nerates nonblock codes. In arithmetic coding, which can be traced to the work of Elias \\n(Abramson [1963]), a one-to-one correspondence between source symbols and code \\nwords does not exist. Instead, an entire sequence of source symbols (or message) is \\nassigned a single arithmetic code word. The code word itself defines an interval of \\nreal numbers between 0 and 1. As the number of symbols in the message increases, \\nthe interval used to represent it becomes smaller, and the number of information \\nunits (say, bits) required to represent the interval becomes larger. Each symbol of \\nthe message reduces the size of the interval in accordance with its probability of \\noccurrence. Because the technique does not require, as does Huffman’s approach, \\nthat each source symbol translate into an integral number of code symbols (that is, \\nthat the symbols be coded one at a time), it achieves (but only in theory) the bound \\nestablished by Shannon’s first theorem of Section 8.1.\\nFigure 8.12 illustrates the basic arithmetic coding process. Here, a ﬁve-symbol \\nsequence or message, \\na\\n1\\na\\n2\\na\\n3\\na\\n3\\na\\n4\\n, from a four-symbol source is coded. At the start of \\nthe coding process, the message is assumed to occupy the entire half-open interval \\n[0, 1). As Table 8.7 shows, this interval is subdivided initially into four regions based \\non the probabilities of each source symbol. Symbol \\na\\n1\\n, for example, is associated with \\nsubinterval [0, 0.2). Because it is the first symbol of the message being coded, the \\nmessage interval is initially narrowed to [0, 0.2). Thus, in Fig. 8.12, [0, 0.2) is expanded \\nto the full height of the figure, and its end points labeled by the values of the nar-\\nrowed range. The narrowed range is then subdivided in accordance with the original \\n8.4\\nWith reference to \\nTables 8.3–8.5, arithmetic \\ncoding is used in\\n• JBIG1 \\n• JBIG2 \\n• JPEG-2000 \\n• H.264 \\n• MPEG-4 A VC\\nand other compression \\nstandards.\\nSource Symbol Probability Initial Subinterval\\na\\n1\\n0.2\\n[0.0, 0.2)\\na\\n2\\n0.2\\n[0.2, 0.4)\\na\\n3\\n0.4\\n[0.4, 0.8)\\na\\n4\\n0.2\\n[0.8, 1.0)\\nTABLE \\n8.7\\nArithmetic coding \\nexample.\\nDIP4E_GLOBAL_Print_Ready.indb   561\\n6/16/2017   2:10:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 562}),\n",
       " Document(page_content='562\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nsource symbol probabilities, and the process continues with the next message symbol. \\nIn this manner, symbol \\na\\n2\\n narrows the subinterval to [0.04, 0.08), \\na\\n3 \\nfurther narrows \\nit to [0.056, 0.072), and so on. The final message symbol, which must be reserved as a \\nspecial end-of-message indicator, narrows the range to [0.06752, 0.0688). Of course, \\nany number within this subinterval, for example, 0.068, can be used to represent the \\nmessage. In the arithmetically coded message of Fig. 8.12, three decimal digits are \\nused to represent the five-symbol message. This translates into 0.6 decimal digits per \\nsource symbol and compares favorably with the entropy of the source, which, from \\nEq. 8.6, is 0.58 decimal digits per source symbol. As the length of the sequence being \\ncoded increases, the resulting arithmetic code approaches the bound established by \\nShannon’s first theorem. In practice, two factors cause coding performance to fall \\nshort of the bound: (1) the addition of the end-of-message indicator that is needed \\nto separate one message from another, and (2) the use of finite precision arithmetic. \\nPractical implementations of arithmetic coding address the latter problem by intro-\\nducing a scaling strategy and a rounding strategy (Langdon and Rissanen [1981]). \\nThe scaling strategy renormalizes each subinterval to the [0, 1) range before subdi-\\nviding it in accordance with the symbol probabilities. The rounding strategy guaran-\\ntees that the truncations associated with finite precision arithmetic do not prevent \\nthe coding subintervals from being accurately represented.\\nADAPTIVE CONTEXT DEPENDENT PROBABILITY ESTIMATES\\nWith accurate input symbol \\nprobability models\\n, that is, models that provide the true \\nprobabilities of the symbols being coded, arithmetic coders are near optimal in the \\nsense of minimizing the average number of code symbols required to represent the \\nsymbols being coded. As in both Huffman and Golomb coding, however, inaccu-\\nrate probability models can lead to non-optimal results. A simple way to improve \\nthe accuracy of the probabilities employed is to use an adaptive, context depen-\\ndent probability model. \\nAdaptive\\n probability models update symbol probabilities as  \\nsymbols are coded or become known. Thus, the probabilities adapt to the local sta-\\ntistics of the symbols being coded. \\nContext-dependent\\n models provide probabilities \\nFIGURE 8.12\\nArithmetic coding \\nprocedure.\\nEncoding sequence\\na\\n4\\na\\n4\\na\\n4\\na\\n4\\na\\n4\\na\\n3\\na\\n3\\na\\n3\\na\\n3\\na\\n3\\na\\n2\\na\\n2\\na\\n2\\na\\n2\\na\\n2\\na\\n1\\na\\n1\\na\\n1\\na\\n1\\na\\n1\\n0.08\\n0.04\\n0.072\\n0.056\\n0.0688\\n0.06752\\n0.0624\\n0.2\\n0\\n1\\n0\\na\\n1\\na\\n2\\na\\n3\\na\\n3\\na\\n4\\nDIP4E_GLOBAL_Print_Ready.indb   562\\n6/16/2017   2:10:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 563}),\n",
       " Document(page_content='8.4\\n  \\nArithmetic Coding\\n    \\n563\\nthat are based on a predefined neighborhood of pixels, called the \\ncontext\\n, around \\nthe symbols being coded. Normally, a \\ncausal context\\n (one limited to symbols that \\nhave already been coded) is used. Both the Q-coder (Pennebaker et al. [1988]) and \\nMQ-coder (ISO/IEC [2000]), two well-known arithmetic coding techniques that \\nhave been incorporated into the JBIG, JPEG-2000, and other important image \\ncompression standards, use probability models that are both adaptive and context \\ndependent.The Q-coder dynamically updates symbol probabilities during the inter-\\nval renormalizations that are part of the arithmetic coding process. Adaptive con-\\ntext dependent models also have been used in Golomb coding, for example, in the \\nJPEG-LS compression standard.\\nFigure 8.13(a) diagrams the steps involved in adaptive, context-dependent arith-\\nmetic coding of \\nbinary\\n source symbols. Arithmetic coding often is used when binary \\nsymbols are to be coded. As each symbol (or bit) begins the coding process, its con-\\ntext is formed in the \\nContext determination\\n block of Fig. 8.13(a). Figures 8.13(b) \\nthrough (d) show three possible contexts that can be used: (1) the immediately pre-\\nceding symbol, (2) a group of preceding symbols, and (3) some number of preceding \\nsymbols plus symbols on the previous scan line. For the three cases shown, the \\nProb-\\nability estimation\\n block must manage 2\\n1\\n (or 2), 2\\n8\\n (or 256), and 2\\n5\\n (or 32) contexts \\nand their associated probabilities. For instance, if the context in Fig. 8.13(b) is used, \\nconditional probabilities \\nP\\na\\n00\\n=\\n(\\n)\\n (the probability that the symbol being coded is a \\n0 given that the preceding symbol is a 0), \\nPa\\n10\\n=\\n(\\n)\\n,\\n \\nPa\\n01\\n=\\n(\\n)\\n,\\n and \\nPa\\n11\\n=\\n(\\n)\\n must \\nbe tracked. The appropriate probabilities are then passed to the \\nArithmetic coding\\n \\nblock as a function of the current context, and drive the generation of the arithmeti-\\ncally coded output sequence in accordance with the process illustrated in Fig. 8.12. \\nThe probabilities associated with the context involved in the current coding step are \\nthen updated to reflect the fact that another symbol within that context has been \\nprocessed.\\nFinally, we note that a variety of arithmetic coding techniques are protected by \\nUnited States patents (and may be protected in other jurisdictions as well). Because \\na\\nb\\ncd\\ne\\nSymbol being coded\\nContext\\na\\nSymbol being coded\\nContext\\nd\\na\\nec\\nb\\nSymbol being coded\\nContext\\ng h\\nf\\nContext\\ndetermination\\nProbability\\nestimation\\nArithmetic\\ncoding\\nSymbol\\nand\\ncontext\\nSymbol\\nprobablity\\nInput\\nsymbols\\nCode\\nbits\\nUpdate probability\\nfor current context\\nb\\na\\nd c\\nFIGURE 8.13\\n(a) An adaptive, \\ncontext-based \\narithmetic coding \\napproach (often \\nused for binary \\nsource symbols). \\n(b)–(d) Three \\npossible context \\nmodels.\\nDIP4E_GLOBAL_Print_Ready.indb   563\\n6/16/2017   2:10:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 564}),\n",
       " Document(page_content='564\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nof these patents, and the possibility of unfavorable monetary judgments for their \\ninfringement, most implementations of the JPEG compression standard, which con-\\ntains options for both Huffman and arithmetic coding, typically support Huffman \\ncoding alone.\\n8.5 LZW CODING  \\nThe techniques covered in Sections 8.2 through 8.4 are focused on the removal of cod-\\ning redundancy. In this section, we consider an error-free compression approach that \\nalso addresses spatial redundancies in an image. The technique, called \\nLempel-Ziv- \\nWelch\\n (LZW) \\ncoding\\n, assigns ﬁxed-length code words to variable length sequences \\nof source symbols. Recall from the earlier section on measuring image information \\nthat Shannon used the idea of coding sequences of source symbols, rather than indi-\\nvidual source symbols, in the proof of his ﬁrst theorem. A key feature of LZW cod-\\ning is that it requires no a priori knowledge of the probability of occurrence of the \\nsymbols to be encoded. Despite the fact that until recently it was protected under a \\nUnited States patent, LZW compression has been integrated into a variety of main-\\nstream imaging ﬁle formats, including GIF, TIFF, and PDF. The PNG format was \\ncreated to get around LZW licensing requirements.\\nEXAMPLE 8.6 :   LZW coding Fig. 8.9(a).\\nConsider again the \\n512 512\\n×\\n,\\n 8-bit image from Fig. 8.9(a). Using Adobe Photoshop, an uncompressed \\nTIFF version of this image requires 286,740 bytes of disk space—262,144 bytes for the \\n512 512\\n×\\n 8-bit \\npixels plus 24,596 bytes of overhead.\\n Using TIFF’s LZW compression option, however, the resulting ﬁle \\nis 224,420 bytes. The compression ratio is \\nC\\n=\\n12\\n8\\n..\\n Recall that for the Huffman encoded representation \\nof F\\nig. 8.9(a) in Example 8.4, \\nC\\n=\\n1\\n077\\n..\\n The additional compression realized by the LZW approach is \\ndue the removal of some of the image’\\ns spatial redundancy.\\nLZW coding is conceptually very simple (Welch [1984]). At the onset of the cod-\\ning process, a codebook or \\ndictionary\\n containing the source symbols to be coded is \\nconstructed. For 8-bit monochrome images, the ﬁrst 256 words of the dictionary are \\nassigned to intensities 0, 1, 2, …, 255. As the encoder sequentially examines image \\npixels, intensity sequences that are not in the dictionary are placed in algorithmi-\\ncally determined (e.g., the next unused) locations. If the ﬁrst two pixels of the image \\nare white, for instance, sequence “255–255” might be assigned to location 256, the \\naddress following the locations reserved for intensity levels 0 through 255. The next \\ntime two consecutive white pixels are encountered, code word 256, the address of \\nthe location containing sequence 255–255, is used to represent them. If a 9-bit, 512-\\nword dictionary is employed in the coding process, the original \\n()\\n88\\n+\\n bits that were \\nused to represent the two pixels are replaced by a single 9-bit code word.\\n Clearly, the \\nsize of the dictionary is an important system parameter. If it is too small, the detec-\\ntion of matching intensity-level sequences will be less likely; if it is too large, the size \\nof the code words will adversely affect compression performance.\\n8.5\\nWith reference to \\nTables 8.3–8.5, LZW cod-\\ning is used in the\\n• GIF \\n• TIFF \\n• PDF\\nformats, but not in any \\nof the internationally \\nsanctioned compression \\nstandards.\\nDIP4E_GLOBAL_Print_Ready.indb   564\\n6/16/2017   2:10:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 565}),\n",
       " Document(page_content='8.5\\n  \\nLZW Coding\\n    \\n565\\nEXAMPLE 8.7 :   LZW coding.\\nConsider the following \\n44\\n*\\n, 8-bit image of a vertical edge:\\n \\n39 39 126 126\\n39\\n39 126 126\\n39 39 126 126\\n39 39 126 126\\nTable 8.8 details the steps involved in coding its 16 pixels. A 512-word dictionary with the following start-\\ning content is assumed:\\nDictionary Location\\nEntry\\n00\\n11\\noo\\n255\\n255\\n256\\n—\\noo\\n511\\n—\\nLocations 256 through 511 initially are unused.\\nThe image is encoded by processing its pixels in a left-to-right, top-to-bottom manner. Each succes-\\nsive intensity value is concatenated with a variable, column 1 of Table 8.8, called the “currently recog-\\nnized sequence.” As can be seen, this variable is initially null or empty. The dictionary is searched for \\neach concatenated sequence and if found, as was the case in the ﬁrst row of the table, is replaced by the \\nnewly concatenated and recognized (i.e., located in the dictionary) sequence. This was done in column \\n1 of row 2. No output codes are generated, nor is the dictionary altered. If the concatenated sequence \\nis not found, however, the address of the currently recognized sequence is output as the next encoded \\nvalue, the concatenated but unrecognized sequence is added to the dictionary, and the currently recog-\\nnized sequence is initialized to the current pixel value. This occurred in row 2 of the table. The last two \\ncolumns detail the intensity sequences that are added to the dictionary when scanning the entire 128-bit \\nimage. Nine additional code words are deﬁned. At the conclusion of coding, the dictionary contains 265 \\ncode words and the LZW algorithm has successfully identiﬁed several repeating intensity sequences—\\nleveraging them to reduce the original 128-bit image to 90 bits (i.e., 10 9-bit codes). The encoded output \\nis obtained by reading the third column from top to bottom. The resulting compression ratio is 1.42:1.\\nA unique feature of the LZW coding just demonstrated is that the coding dic-\\ntionary or code book is created while the data are being encoded. Remarkably, an \\nLZW decoder builds an identical decompression dictionary as it simultaneously \\ndecodes the encoded data stream. It is left as an exercise to the reader (see Prob-\\nlem 8.20) to decode the output of the preceding example and reconstruct the code \\nbook. Although not needed in this example, most practical applications require a \\nDIP4E_GLOBAL_Print_Ready.indb   565\\n6/16/2017   2:10:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 566}),\n",
       " Document(page_content='566\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nstrategy for handling dictionary overﬂow. A simple solution is to ﬂush or reinitialize \\nthe dictionary when it becomes full and continue coding with a new initialized dic-\\ntionary. A more complex option is to monitor compression performance and ﬂush \\nthe dictionary when it becomes poor or unacceptable. Alternatively, the least used \\ndictionary entries can be tracked and replaced when necessary.\\n8.6 RUN-LENGTH CODING  \\nAs was noted earlier, images with repeating intensities along their rows (or columns) \\ncan often be compressed by representing runs of identical intensities as \\nrun-length \\npairs\\n, where each run-length pair specifies the start of a new intensity and the num-\\nber of consecutive pixels that have that intensity. The technique, referred to as \\nrun-\\nlength encoding\\n (RLE), was developed in the 1950s and became, along with its 2-D \\nextensions, the standard compression approach in facsimile (FAX) coding. Com-\\npression is achieved by eliminating a simple form of spatial redundancy—groups of \\nidentical intensities. When there are few (or no) runs of identical pixels, run-length \\nencoding results in data expansion.\\nEXAMPLE 8.8 :   RLE in the BMP ﬁle format.\\nThe BMP ﬁle format uses a form of run-length encoding in which image data is represented in two dif-\\nferent modes: encoded and absolute. Either mode can occur anywhere in the image. In \\nencoded\\n mode, a \\n8.6\\nWith reference to \\nTables 8.3–8.5, the coding \\nof run-lengths is used in\\n• CCITT \\n• JBIG2 \\n• JPEG \\n• M-JPEG \\n• MPEG-1,2,4 \\n• BMP\\nand other compres-\\nsion standards and ﬁle \\nformats.\\nCurrently \\nRecognized \\nSequence\\nPixel Being \\nProcessed\\nEncoded \\nOutput\\nDictionary  \\nLocation \\n(Code Word)\\nDictionary Entry\\n39\\n39 39 39 256 39–39\\n39 126 39 257 39–126\\n126 126 126 258 126–126\\n126 39 126 259 126–39\\n39 39\\n39–39\\n126\\n256\\n260\\n39–39–126\\n126\\n126\\n126–126\\n39\\n258\\n261\\n126–126–39\\n39\\n39\\n39-39\\n126\\n39–39–126 126\\n260\\n262 39–39–126–126\\n126\\n39\\n126-39\\n39\\n259\\n263\\n126–39–39\\n39\\n126\\n39-126\\n126\\n257\\n264\\n39–126–126\\n126\\n126\\nTABLE \\n8.8\\nLZW Coding \\nexample.\\nDIP4E_GLOBAL_Print_Ready.indb   566\\n6/16/2017   2:10:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 567}),\n",
       " Document(page_content='8.6\\n  \\nRun-length Coding\\n    \\n567\\ntwo byte RLE representation is used. The ﬁrst byte speciﬁes the number of consecutive pixels that have \\nthe color index contained in the second byte. The 8-bit color index selects the run’s intensity (color or \\ngray value) from a table of 256 possible intensities.\\nIn \\nabsolute\\n mode, the ﬁrst byte is 0, and the second byte signals one of four possible conditions, as \\nshown in Table 8.9. When the second byte is 0 or 1, the end of a line or the end of the image has been \\nreached. If it is 2, the next two bytes contain unsigned horizontal and vertical offsets to a new spatial \\nposition (and pixel) in the image. If the second byte is between 3 and 255, it speciﬁes the number of \\nuncompressed pixels that follow with each subsequent byte containing the color index of one pixel. The \\ntotal number of bytes must be aligned on a 16-bit word boundary.\\nAn uncompressed BMP ﬁle (saved using Photoshop) of the \\n512 512 8\\n××\\n bit image shown in Fig. 8.9(a) \\nrequires 263,244 bytes of memory\\n. Compressed using BMP’s RLE option, the ﬁle expands to 267,706 \\nbytes, and the compression ratio is \\nC\\n=\\n09\\n8\\n..\\n There are not enough equal intensity runs to make run-\\nlength compression effective;\\n a small amount of expansion occurs. For the image in Fig. 8.1(c), however, \\nthe BMP RLE option results in a compression ratio \\nC\\n=\\n13\\n5\\n..\\n (Note that due to differences in overhead, \\nthe uncompressed BMP ﬁle is smaller than the uncompressed \\nTIFF ﬁle in Example 8.6.)\\nRun-length encoding is particularly effective when compressing binary images. \\nBecause there are only two possible intensities (black and white), adjacent pixels \\nare more likely to be identical. In addition, each image row can be represented by \\na sequence of lengths only, rather than length-intensity pairs as was used in Exam-\\nple 8.8. The basic idea is to code each contiguous group (i.e., run) of 0’s or 1’s encoun-\\ntered in a left-to-right scan of a row by its length \\nand\\n to establish a convention for \\ndetermining the value of the run. The most common conventions are (1) to specify \\nthe value of the ﬁrst run of each row, or (2) to assume that each row begins with a \\nwhite run, whose run length may in fact be zero.\\nAlthough run-length encoding is in itself an effective method of compressing \\nbinary images, additional compression can be achieved by variable-length coding \\nthe run lengths themselves. The black and white run lengths can be coded separately \\nusing variable-length codes that are speciﬁcally tailored to their own statistics. For \\nexample, letting symbol \\na\\nj\\n represent a black run of length \\nj\\n, we can estimate the \\nprobability that symbol \\na\\nj\\n was emitted by an imaginary black run-length source by \\ndividing the number of black run lengths of length \\nj\\n in the entire image by the total \\nnumber of black runs. An estimate of the entropy of this black run-length source, \\ndenoted as \\nH\\n0\\n,\\n follows by substituting these probabilities into Eq. (8-6). A similar \\nargument holds for the entropy of the white runs\\n, denoted as \\nH\\n1\\n.\\n The approximate \\nrun-length entropy of the image is then\\n \\nH\\nHH\\nLL\\nRL\\n=\\n01\\n01\\n+\\n+\\n \\n(8-18)\\nwhere the variables \\nL\\n0\\n and \\nL\\n1\\n denote the average values of black and white run \\nlengths, respectively. Equation (8-18) provides an estimate of the average number \\nof bits per pixel required to code the run lengths in a binary image using a variable-\\nlength code.\\nDIP4E_GLOBAL_Print_Ready.indb   567\\n6/16/2017   2:10:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 568}),\n",
       " Document(page_content='568\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nTwo of the oldest and most widely used image compression standards are the \\nCCITT Group 3 and 4 standards for binary image compression. Although they have \\nbeen used in a variety of computer applications, they were originally designed as \\nfacsimile (FAX) coding methods for transmitting documents over telephone net-\\nworks. The Group 3 standard uses a 1-D run-length coding technique in which the \\nlast \\nK\\n-\\n1\\n lines of each group of \\nK\\n lines (for \\nK\\n=\\n2\\n or 4) can be optionally coded in \\na 2-D manner\\n. The Group 4 standard is a simpliﬁed or streamlined version of the \\nGroup 3 standard in which only 2-D coding is allowed. Both standards use the same \\n2-D coding approach, which is two-dimensional in the sense that information from \\nthe previous line is used to encode the current line. Both 1-D and 2-D coding will \\nbe discussed next.\\nONE-DIMENSIONAL CCITT COMPRESSION\\nIn the 1-D CCITT Group 3 compression standard, each line of an image\\n†\\n is encoded \\nas a series of variable-length Huffman code words that represent the run lengths of \\nalternating white and black runs in a left-to-right scan of the line. The compression \\nmethod employed is commonly referred to as \\nModified Huffman\\n (MH) coding. The \\ncode words themselves are of two types, which the standard refers to as \\nterminating \\ncodes\\n and \\nmakeup codes\\n. If run length \\nr\\n is less than or equal to 63, a terminating code \\nis used to represent it. The standard specifies different terminating codes for black \\nand white runs. If \\nr\\n>\\n63\\n,\\n two codes are used; a makeup code for quotient \\nr\\n64 64\\n⎢\\n⎣\\n⎥\\n⎦\\n*\\n, \\nand terminating code for remainder \\nr\\nmod64.\\n Makeup codes may or may not depend \\non the intensity (black or white) of the run being coded. If \\nr\\n64 64 1728\\n⎢\\n⎣\\n⎥\\n⎦\\n*…\\n,\\n sepa-\\nrate black and white run makeup codes are specified;\\n otherwise, makeup codes are \\nindependent of run intensity. The standard requires that each line begin with a white \\nrun-length code word, which may in fact be 00110101, the code for a white run of \\nlength zero. Finally, a unique end-of-line (EOL) code word 000000000001 is used to \\nterminate each line, as well as to signal the first line of each new image. The end of a \\nsequence of images is indicated by six consecutive EOLs.\\nTWO-DIMENSIONAL CCITT COMPRESSION\\nThe 2-D compression approach adopted for both the CCITT Group 3 and 4 stan-\\ndards is a line-by-line method in which the position of each black-to-white or \\nwhite-to-black run transition is coded with respect to the position of a \\nreference \\nelement\\n \\na\\n0\\n that is situated on the current \\ncoding line\\n. The previously coded line is \\ncalled the \\nreference line\\n; the reference line for the first line of each new image is an  \\n† In the standard, images are referred to as \\npages\\n and sequences of images are called \\ndocuments\\n.\\nConsult the book web-\\nsite for tables of the MH \\nterminating and makeup \\ncodes.\\nRecall that the notation \\nx\\n⎢\\n⎣\\n⎥\\n⎦\\n denotes the largest \\ninterger less than or \\nequal to \\nx\\n.\\nSecond Byte Value\\nCondition\\n0\\nEnd of line\\n1\\nEnd of image\\n2\\nMove to a new position\\n3-255\\nSpecify pixels individually\\nTABLE \\n8.9\\nBMP absolute \\ncoding mode  \\noptions. In this \\nmode, the ﬁrst \\nbyte of the BMP \\npair is 0.\\nDIP4E_GLOBAL_Print_Ready.indb   568\\n6/16/2017   2:10:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 569}),\n",
       " Document(page_content='8.6\\n  \\nRun-length Coding\\n    \\n569\\nimaginary white line. The 2-D coding technique that is used is called \\nRelative Ele-\\nment Address Designate\\n (READ) coding. In the Group 3 standard, one or three \\nREAD coded lines are allowed between successive MH coded lines; this technique \\nis called \\nModified READ\\n (MR) coding. In the Group 4 standard, a greater num-\\nber of READ coded lines are allowed, and the method is called \\nModified Modified \\nREAD\\n (MMR) coding. As was previously noted, the coding is two-dimensional in \\nthe sense that information from the previous line is used to encode the current line. \\nTwo-dimensional transforms are not involved.\\nFigure 8.14 shows the basic 2-D coding process for a single scan line. Note that \\nthe initial steps of the procedure are directed at locating several key \\nchanging ele-\\nments\\n: \\na\\n0\\n, \\na\\n1\\n, \\na\\n2\\n, \\nb\\n1\\n, and \\nb\\n2\\n. A changing element is deﬁned by the standard as a pixel \\nwhose value is different from that of the previous pixel on the same line. The most \\nimportant changing element is \\na\\n0\\n (the reference element), which is either set to the \\nlocation of an imaginary white changing element to the left of the ﬁrst pixel of each \\nnew coding line, or determined from the previous coding mode. Coding modes will \\nbe discussed in the following paragraph. After \\na\\n0\\n is located, \\na\\n1\\n is identiﬁed as the \\nlocation of the next changing element to the right of \\na\\n0\\n on the current coding line, \\na\\n2\\n as the next changing element to the right of \\na\\n1\\n on the coding line, \\nb\\n1\\n as the chang-\\ning element of the opposite value (of \\na\\n0\\n) and to the right of \\na\\n0\\n on the reference (or \\nprevious) line, and \\nb\\n2\\n as the next changing element to the right of \\nb\\n1\\n on the reference \\nline. If any of these changing elements are not detected, they are set to the location \\nof an imaginary pixel to the right of the last pixel on the appropriate line. Figure 8.15 \\nprovides two illustrations of the general relationships between the various changing \\nelements.\\nAfter identiﬁcation of the current reference element and associated changing ele-\\nments, two simple tests are performed to select one of three possible coding modes: \\npass mode\\n, \\nvertical mode\\n, or \\nhorizontal mode\\n. The initial test, which corresponds \\nto the ﬁrst branch point in the ﬂowchart in Fig. 8.14, compares the location of \\nb\\n2\\n to \\nthat of \\na\\n1\\n. The second test, which corresponds to the second branch point in Fig. 8.14, \\ncomputes the distance (in pixels) between the locations of \\na\\n1\\n and \\nb\\n1\\n and compares it \\nagainst 3. Depending on the outcome of these tests, one of the three outlined coding \\nblocks of Fig. 8.14 is entered and the appropriate coding procedure is executed. A \\nnew reference element is then established, as per the ﬂowchart, in preparation for \\nthe next coding iteration.\\nTable 8.10 deﬁnes the speciﬁc codes utilized for each of the three possible cod-\\ning modes. In pass mode, which speciﬁcally excludes the case in which \\nb\\n2\\n is directly \\nabove \\na\\n1\\n, only the pass mode code word 0001 is needed. As Fig. 8.15(a) shows, this \\nmode identiﬁes white or black reference line runs that do not overlap the current \\nwhite or black coding line runs. In horizontal coding mode, the distances from \\na\\n0\\n to \\na\\n1\\n and \\na\\n1\\n to \\na\\n2\\n must be coded in accordance with the termination and makeup codes \\nof 1-D CCITT Group 3 compression, then appended to the horizontal mode code \\nword 001. This is indicated in Table 8.10 by the notation \\n001\\n01 12\\n++\\nMaa Ma a\\n() () ,\\n \\nwhere \\na\\n0\\na\\n1\\n and \\na\\n1\\na\\n2\\n denote the distances from \\na\\n0\\n to \\na\\n1\\n and \\na\\n1\\n to \\na\\n2\\n, respectively. \\nFinally, in vertical coding mode, one of six special variable-length codes is assigned \\nto the distance between \\na\\n1\\n and \\nb\\n1\\n. Figure 8.15(b) illustrates the parameters involved \\nConsult the book web-\\nsite for the coding tables \\nof the CCITT standard.\\nDIP4E_GLOBAL_Print_Ready.indb   569\\n6/16/2017   2:10:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 570}),\n",
       " Document(page_content='570\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nFIGURE 8.14\\nCCITT 2-D \\nREAD coding \\nprocedure. The \\nnotation \\nab\\n11\\n \\ndenotes the abso-\\nlute value of the \\ndistance between \\nchanging  \\nelements \\na\\n1\\n  \\nand \\nb\\n1\\n.\\nStart new\\ncoding line\\nDetect \\na\\n1\\nDetect \\nb\\n1\\nDetect \\nb\\n2\\nDetect \\na\\n2\\nNo\\nYes\\nYes\\nNo\\nPass mode\\ncoding\\nVertical mode\\ncoding\\nHorizontal\\nmode coding\\nEnd of\\ncoding line\\nEnd of\\nline?\\nb\\n2\\n left of \\na\\n1\\nPut \\na\\n0\\nunder \\nb\\n2\\nPut \\na\\n0\\n on \\na\\n2\\nPut \\na\\n0\\n on \\na\\n1\\n \\n@\\n \\na\\n1\\nb\\n1\\n \\n@\\n \\nd\\n 3\\nYes\\nNo\\nPut \\na\\n0\\n before\\nthe first pixel\\nDIP4E_GLOBAL_Print_Ready.indb   570\\n6/16/2017   2:10:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 571}),\n",
       " Document(page_content='8.6\\n  \\nRun-length Coding\\n    \\n571\\nin both horizontal and vertical mode coding. The extension mode code word at the \\nbottom of Table 8.10 is used to enter an optional facsimile coding mode. For exam-\\nple, the 0000001111 code is used to initiate an uncompressed mode of transmission.\\nEXAMPLE 8.9 :   CCITT vertical mode coding example.\\nAlthough Fig. 8.15(b) is annotated with the parameters for both horizontal and vertical mode coding \\n(to facilitate the discussion above), the depicted pattern of black and white pixels is a case for vertical \\nmode coding. That is, because \\nb\\n2\\n is to the right of \\na\\n1\\n, the ﬁrst (or pass mode) test in Fig. 8.14 fails. The \\nsecond test, which determines whether the vertical or horizontal coding mode is entered, indicates that \\nvertical mode coding should be used, because the distance from \\na\\n1\\n to \\nb\\n1\\n is less than 3. In accordance with \\nTable 8.10, the appropriate code word is 000010, implying that \\na\\n1\\n is two pixels left of \\nb\\n1\\n. In preparation \\nfor the next coding iteration, \\na\\n0\\n is moved to the location of \\na\\n1\\n.\\nMode\\nCode Word\\nPass 0001\\nHorizontal\\n001 + \\nM\\n(\\na\\n0\\na\\n1\\n) + \\nM\\n(\\na\\n1\\na\\n2\\n)\\nVertical\\na\\n1\\n below \\nb\\n1\\n1\\na\\n1\\n one to the right of \\nb\\n1\\n011\\na\\n1\\n two to the right of \\nb\\n1\\n000011\\na\\n1\\n three to the right of \\nb\\n1\\n0000011\\na\\n1\\n one to the left of \\nb\\n1\\n010\\na\\n1\\n two to the left of \\nb\\n1\\n000010\\na\\n1\\n three to the left of \\nb\\n1\\n0000010\\nExtension\\n0000001xxx\\nTABLE \\n8.10\\nCCITT  \\ntwo-dimensional \\ncode table.\\nb\\na\\nFIGURE 8.15\\nCCITT (a) pass \\nmode and  \\n(b) horizontal \\nand vertical mode \\ncoding  \\nparameters.\\n/H11005 \\n0\\n/H11005 \\n1\\nb\\n1\\na\\n0\\na\\n1\\nNext \\na\\n0\\nb\\n2\\nb\\n2\\na\\n1\\nb\\n1\\na\\n0\\na\\n1\\na\\n2\\nb\\n1\\na\\n0\\na\\n1\\na\\n1\\na\\n2\\nReference line\\nCoding line\\nReference line\\nCoding line\\nVertical mode\\nPass mode\\nHorizontal mode\\nDIP4E_GLOBAL_Print_Ready.indb   571\\n6/16/2017   2:10:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 572}),\n",
       " Document(page_content='572\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nEXAMPLE 8.10 :  CCITT compression example.\\nFigure 8.16(a) is a 300 dpi scan of a \\n79 2 5\\n*\\n.\\n inch book page displayed at about \\n13\\n scale. Note that about \\nhalf of the page contains text,\\n around 9% is occupied by a halftone image, and the rest is white space. \\nA section of the page is enlarged in Fig. 8.16(b). Keep in mind that we are dealing with a binary image; \\nthe illusion of gray tones is created, as was described in Section 4.5, by the halftoning process used in \\nprinting. If the binary pixels of the image in Fig. 8.16(a) are stored in groups of 8 pixels per byte, the \\n1\\n952 2697\\n×\\n bit scanned image, commonly called a document, requires 658,068 bytes. An uncompressed \\nPDF ﬁle of the document (created in Photoshop) requires 663,445 bytes\\n. CCITT Group 3 compression \\nreduces the ﬁle to 123,497 bytes, resulting in a compression ratio \\nC\\n=\\n53\\n7\\n..\\n CCITT Group 4 compression \\nreduces the ﬁle to 110,456 bytes\\n, increasing the compression ratio to about 6.\\n8.7 SYMBOL-BASED CODING  \\nIn \\nsymbol\\n- or \\ntoken-based\\n coding, an image is represented as a collection of fre-\\nquently occurring subimages, called \\nsymbols\\n. Each such symbol is stored in a \\nsym-\\nbol dictionary\\n and the image is coded as a set of triplets \\n(,, ) , (, ,) , ,\\nxy\\nt xyt\\n11 1 22 2\\np\\n{}\\n \\nwhere each \\n(,)\\nxy\\nii\\n pair specifies the location of a symbol in the image and \\ntoken\\n \\nt\\ni\\n is the address of the symbol or subimage in the dictionary. That is, each triplet \\nrepresents an instance of a dictionary symbol in the image. Storing repeated sym-\\nbols only once can compress images significantly, particularly in document storage \\nand retrieval applications where the symbols are often character bitmaps that are \\nrepeated many times.\\n8.7\\nWith reference to \\nTables 8.3–8.5, symbol-\\nbased coding is used in\\n• JBIG2\\ncompression.\\nb a\\nFIGURE 8.16\\nA binary scan of \\na book page: (a) \\nscaled to show \\nthe general page \\ncontent;  \\n(b) scaled to show \\nthe binary pixels \\nused in dithering.\\nDIP4E_GLOBAL_Print_Ready.indb   572\\n6/16/2017   2:10:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 573}),\n",
       " Document(page_content='8.7\\n  \\nSymbol-based Coding\\n    \\n573\\nConsider the simple bilevel image in Fig. 8.17(a). It contains the single word, \\nbanana\\n, which is composed of three unique symbols: a \\nb\\n, three \\na\\n’s, and two \\nn\\n’s. \\nAssuming that the \\nb\\n is the ﬁrst symbol identiﬁed in the coding process, its \\n97\\n*\\n bit-\\nmap is stored in location 0 of the symbol dictionary\\n. As Fig. 8.17(b) shows, the token \\nidentifying the \\nb\\n bitmap is 0. Thus, the ﬁrst triplet in the encoded image’s represen-\\ntation [see Fig. 8.17(c)] is (0, 2, 0), indicating that the upper-left corner (an arbitrary \\nconvention) of the rectangular bitmap representing the \\nb\\n symbol is to be placed \\nat location (0, 2) in the decoded image. After the bitmaps for the \\na\\n and \\nn\\n symbols \\nhave been identiﬁed and added to the dictionary, the remainder of the image can \\nbe encoded with ﬁve additional triplets. As long as the six triplets required to locate \\nthe symbols in the image, together with the three bitmaps required to deﬁne them, \\nare smaller than the original image, compression occurs. In this case, the starting \\nimage has  \\n95 11\\n**\\n or 459 bits and, assuming that each triplet is composed of three \\nbytes\\n, the compressed representation has \\n( ) () () ()\\n6\\n3 8 9 76 76 6\\n* * + ***\\n++\\n[]\\n or \\n285 bits; the resulting compression ratio \\nC\\n=\\n16\\n1\\n..\\n To decode the symbol-based rep-\\nresentation in F\\nig. 8.17(c), you simply read the bitmaps of the symbols speciﬁed in \\nthe triplets from the symbol dictionary and place them at the spatial coordinates \\nspeciﬁed in each triplet.\\nSymbol-based compression was proposed in the early 1970s (Ascher and Nagy \\n[1974]), but has become practical only recently. Advances in symbol-matching algo-\\nrithms (see Chapter 12) and increased CPU computer processing speeds have made \\nit possible to both select dictionary symbols and to ﬁnd where they occur in an \\nimage in a timely manner. And like many other compression methods, symbol-based \\ndecoding is signiﬁcantly faster than encoding. Finally, we note that both the symbol \\nbitmaps that are stored in the dictionary and the triplets used to reference them \\nthemselves can be encoded to further improve compression performance. If, as in \\nFig. 8.17, only exact symbol matches are allowed, the resulting compression is loss-\\nless; if small differences are permitted, some level of reconstruction error will be \\npresent.\\nJBIG2 COMPRESSION\\nJBIG2 is an international standard for bilevel image compression. By segmenting \\nan image into overlapping and/or non-overlapping regions of text, halftone, and \\ngeneric content, compression techniques that are specifically optimized for each \\ntype of content are employed:\\nb a\\nc\\nFIGURE 8.17\\n(a) A bi-level \\ndocument, (b) \\nsymbol dictionary, \\nand (c) the trip-\\nlets used to locate \\nthe symbols in the \\ndocument.\\nToken Symbol Triplet\\n0\\n1\\n2\\n(0, 2, 0)\\n(3,10, 1)\\n(3, 18, 2)\\n(3, 26, 1)\\n(3, 34, 2)\\n(3, 42, 1)\\nDIP4E_GLOBAL_Print_Ready.indb   573\\n6/16/2017   2:10:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 574}),\n",
       " Document(page_content='574\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\n1. \\nText regions\\n are composed of characters that are ideally suited for a symbol-\\nbased coding approach.\\n Typically, each symbol will correspond to a character \\nbitmap—a subimage representing a character of text. There is normally only \\none character bitmap (or subimage) in the symbol dictionary for each upper- \\nand lowercase character of the font being used. For example, there would be \\none “a” bitmap in the dictionary, one “A” bitmap, one “b” bitmap, and so on.\\nIn lossy JBIG2 compression, often called \\nperceptually lossless\\n or \\nvisually \\nlossless\\n, we neglect differences between dictionary bitmaps (i.e., the reference \\ncharacter bitmaps or character templates) and speciﬁc instances of the corre-\\nsponding characters in the image. In lossless compression, the differences are \\nstored and used in conjunction with the triplets encoding each character (by \\nthe decoder) to produce the actual image bitmaps. All bitmaps are encoded \\neither arithmetically or using MMR (see Section 8.6); the triplets used to access \\ndictionary entries are either arithmetically or Huffman encoded.\\n2. \\nHalftone regions\\n are similar to text regions in that they are composed of pat-\\nterns arranged in a regular grid.\\n The symbols that are stored in the dictionary, \\nhowever, are not character bitmaps but periodic patterns that represent intensi-\\nties (e.g., of a photograph) that have been dithered to produce bilevel images \\nfor printing.\\n3. \\nGeneric regions\\n contain non-text,\\n non-halftone information, like line art and \\nnoise, and are compressed using either arithmetic or MMR coding.\\nAs is true of many image compression standards, JBIG2 defines decoder behavior. It \\ndoes not explicitly define a standard encoder, but is flexible enough to allow various \\nencoder designs. Although the design of the encoder is left unspecified, it is impor-\\ntant because it determines the level of compression that is achieved. After all, the \\nencoder must segment the image into regions, choose the text and halftone symbols \\nthat are stored in the dictionaries, and decide when those symbols are essentially \\nthe same as, or different from, potential instances of the symbols in the image. The \\ndecoder simply uses that information to recreate the original image.\\nEXAMPLE 8.11 :  JBIG2 compression example.\\nConsider again the bilevel image in Fig. 8.16(a). Figure 8.18(a) shows a reconstructed section of the \\nimage after lossless JBIG2 encoding (by a commercially available document compression application). \\nIt is an exact replica of the original image. Note that the \\nds\\n in the reconstructed text vary slightly, despite \\nthe fact that they were generated from the same \\nd\\n entry in the dictionary. The differences between that   \\nd\\n and the \\nds\\n in the image were used to reﬁne the output of the dictionary. The standard deﬁnes an algo-\\nrithm for accomplishing this during the decoding of the encoded dictionary bitmaps. For the purposes of \\nour discussion, you can think of it as adding the difference between a dictionary bitmap and a speciﬁc \\ninstance of the corresponding character in the image to the bitmap read from the dictionary.\\nFigure 8.18(b) is another reconstruction of the area in Fig. 8.18(a) after perceptually lossless JBIG2 \\ncompression. Note that the \\nds\\n in this ﬁgure are identical. They have been copied directly from the sym-\\nbol dictionary. The reconstruction is called perceptually lossless because the text is readable and the font \\nis even the same. The small differences shown in Fig. 8.18(c) between the \\nds\\n in the original image and the \\nd\\n in the dictionary are considered unimportant because they do not affect readability. Remember that \\nDIP4E_GLOBAL_Print_Ready.indb   574\\n6/16/2017   2:10:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 575}),\n",
       " Document(page_content='8.8\\n  \\nBit-plane Coding\\n    \\n575\\nwe are dealing with bilevel images, so there are only three intensities in Fig. 8.18(c). Intensity 128 indi-\\ncates areas where there is no difference between the corresponding pixels of the images in Figs. 8.18(a) \\nand (b); intensities 0 (black) and 255 (white) indicate pixels of opposite intensities in the two images—\\nfor example, a black pixel in one image that is white in the other, and vice versa.\\nThe lossless JBIG2 compression that was used to generate Fig. 8.18(a) reduces the original 663,445 \\nbyte uncompressed PDF image to 32,705 bytes; the compression ratio is \\nC\\n=\\n20\\n3\\n..\\n Perceptually lossless \\nJBIG2 compression reduces the image to 23,913 bytes\\n, increasing the compression ratio to about 27.7. \\nThese compressions are 4 to 5 times greater than the CCITT Group 3 and 4 results from Example 8.10.\\n8.8 BIT-PLANE CODING  \\nThe run-length and symbol-based techniques of the previous sections can be applied \\nto images with more than two intensities by individually processing their bit planes. \\nThe technique, called \\nbit-plane coding\\n, is based on the concept of decomposing a \\nmultilevel (monochrome or color) image into a series of binary images (see Sec-\\ntion 3.2) and compressing each binary image via one of several well-known binary \\ncompression methods. In this section, we describe the two most popular decomposi-\\ntion approaches.\\nThe intensities of an \\nm\\n-bit monochrome image can be represented in the form of \\nthe base-2 polynomial\\n \\naa a a\\nm\\nm\\nm\\nm\\n-\\n-\\n-\\n-\\n1\\n1\\n2\\n2\\n1\\n1\\n0\\n0\\n22 2 2\\n++ + +\\n…\\n \\n(8-19)\\nBased on this property, a simple method of decomposing the image into a collection \\nof binary images is to separate the \\nm\\n coefficients of the polynomial into \\nm\\n 1-bit bit \\nplanes\\n. As noted in Section 3.2, the lowest-order bit plane (the plane corresponding \\nto the least significant bit) is generated by collecting the \\na\\n0\\n bits of each pixel, while \\nthe highest-order bit plane contains the \\na\\nm\\n-\\n1\\n bits or coefficients. In general, each bit \\nplane is constructed by setting its pixels equal to the values of the appropriate bits \\nor polynomial coefficients from each pixel in the original image. The inherent disad-\\nvantage of this decomposition approach is that small changes in intensity can have \\na significant impact on the complexity of the bit planes. If a pixel of intensity 127 \\n(01111111) is adjacent to a pixel of intensity 128 (10000000), for instance, every bit \\n8.8\\nWith reference to \\nTables 8.3–8.5, bit-plane \\ncoding is used in\\n• JBIG2 \\n• JPEG-2000\\ncompression standards.\\nb a\\nc\\nFIGURE 8.18\\nJBIG2 compres-\\nsion compari-\\nson: (a) lossless \\ncompression and \\nreconstruction; \\n(b) perceptually \\nlossless; and (c) \\nthe scaled differ-\\nence between the \\ntwo.\\nDIP4E_GLOBAL_Print_Ready.indb   575\\n6/16/2017   2:10:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 576}),\n",
       " Document(page_content='576\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nplane will contain a corresponding 0 to 1 (or 1 to 0) transition. For example, because \\nthe most significant bits of the binary codes for 127 and 128 are different, the highest \\nbit plane will contain a zero-valued pixel next to a pixel of value 1, creating a 0 to 1 \\n(or 1 to 0) transition at that point.\\nAn alternative decomposition approach (which reduces the effect of small inten-\\nsity variations) is to ﬁrst represent the image by an \\nm\\n-bit \\nGray code\\n. The \\nm\\n-bit Gray \\ncode \\ngg g g\\nm\\n−\\n12 1 0\\n…\\n that corresponds to the polynomial in Eq. (8-19) can be com-\\nputed from\\n \\ngaa i m\\nga\\nii i\\nmm\\n=≤ ≤\\n=\\n+\\n/H20003\\n1\\n11\\n02\\n-\\n--\\n \\n(8-20)\\nHere, \\n/H20003\\n denotes the exclusive OR operation. This code has the unique property \\nthat successive code words differ in only one bit position.\\n Thus, small changes in \\nintensity are less likely to affect all \\nm\\n bit planes. For instance, when intensity levels \\n127 and 128 are adjacent, only the highest-order bit plane will contain a 0 to 1 tran-\\nsition, because the Gray codes that correspond to 127 and 128 are 01000000 and \\n11000000, respectively.\\nEXAMPLE 8.12 :  Bit-plane coding.\\nFigures 8.19 and 8.20 show the eight binary and Gray-coded bit planes of the 8-bit monochrome image \\nof the child in Fig. 8.19(a). Note that the high-order bit planes are far less complex than their low-order \\ncounterparts. That is, they contain large uniform areas of signiﬁcantly less detail, busyness, or random-\\nness. In addition, the Gray-coded bit planes are less complex than the corresponding binary bit planes. \\nBoth observations are reﬂected in the JBIG2 coding results of Table 8.11. Note, for instance, that the \\na\\n5\\n \\nand \\ng\\n5\\n results are signiﬁcantly larger than the \\na\\n6\\n and \\ng\\n6\\n compressions, and that both \\ng\\n5\\n and \\ng\\n6\\n are smaller \\nthan their \\na\\n5\\n and \\na\\n6\\n counterparts. This trend continues throughout the table, with the single exception of \\na\\n0\\n. Gray-coding provides a compression advantage of about 1.06:1 on average. Combined together, the \\nGray-coded ﬁles compress the original monochrome image by \\n678 676 475 964\\n,,\\n or 1.43:1; the non-Gray-\\ncoded ﬁles compress the image by \\n678 676 503 916\\n,,\\n or 1.35:1.\\nF\\ninally, we note that the two least signiﬁcant bits in Fig. 8.20 have little apparent structure. Because \\nthis is typical of most 8-bit monochrome images, bit-plane coding is usually restricted to images of \\n6 bits/pixel\\n or less. JBIG1, the predecessor to JBIG2, imposes such a limit.\\n8.9 BLOCK TRANSFORM CODING  \\nIn this section, we consider a compression technique that divides an image into \\nsmall non-overlapping blocks of equal size (e.g., \\n88\\n*\\n) and processes the blocks \\nindependently using a 2-D transform.\\n In \\nblock transform coding\\n, a reversible, linear \\ntransform (such as the Fourier transform) is used to map each block or subimage \\ninto a set of transform coefficients, which are then quantized and coded. For most \\nimages, a significant number of the coefficients have small magnitudes and can be \\ncoarsely quantized (or discarded entirely) with little image distortion. A variety of  \\n8.9\\nWith reference to \\nTables 8.3–8.5, block \\ntransform coding is \\nused in\\n• JPEG \\n• M-JPEG \\n• MPEG-1,2,4 \\n• H.261, H.262, \\n H.263, and H.264 \\n• DV and HDV \\n• VC-1\\nDIP4E_GLOBAL_Print_Ready.indb   576\\n6/16/2017   2:10:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 577}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n577\\ntransformations, including the discrete Fourier transform (DFT) of Chapter 4, can \\nbe used to transform the image data.\\nFigure 8.21 shows a typical block transform coding system. The decoder imple-\\nments the inverse sequence of steps (with the exception of the quantization func-\\ntion) of the encoder, which performs four relatively straightforward operations: \\nsubimage decomposition, transformation, quantization, and coding. An \\nMN\\n*\\n input \\nimage is subdivided ﬁrst into subimages of size \\nnn\\n*\\n,\\n which are then transformed \\nto generate \\nM\\nNn\\n2\\n subimage transform arrays, each of size \\nnn\\n*\\n.\\n The goal of the \\ntransformation process is to decorrelate the pixels of each subimage\\n, or to pack as \\nmuch information as possible into the smallest number of transform coefﬁcients. \\nThe quantization stage then selectively eliminates or more coarsely quantizes the \\ncoefﬁcients that carry the least amount of information in a predeﬁned sense (several \\nmethods will be discussed later in the section). These coefﬁcients have the smallest \\nimpact on reconstructed subimage quality. The encoding process terminates by cod-\\ning (normally using a variable-length code) the quantized coefﬁcients. Any or all of \\nthe transform encoding steps can be adapted to local image content, called \\nadaptive \\ntransform coding\\n, or ﬁxed for all subimages, called \\nnonadaptive transform coding\\n.\\nTRANSFORM SELECTION\\nBlock transform coding systems based on a variety of discrete 2-D transforms have \\nbeen constructed and/or studied extensively. The choice of a particular transform in \\na given application depends on the amount of reconstruction error that can be toler-\\nated and the computational resources available. Compression is achieved during the \\nquantization of the transformed coefficients (not during the transformation step).\\nEXAMPLE 8.13 :  Block transform coding with the DFT, WHT, and DCT.\\nFigures 8.22(a) through (c) show three approximations of the \\n512 512\\n×\\n monochrome image in Fig. 8.9(a). \\nT\\nhese pictures were obtained by dividing the original image into subimages of size \\n88\\n×\\n,\\n representing \\neach subimage using three of the transforms described in Chapter 7 (the DFT\\n, WHT, and DCT trans-\\nforms), truncating 50% of the resulting coefﬁcients, and taking the inverse transform of the truncated \\ncoefﬁcient arrays.\\nIn this section, we restrict \\nour attention to square \\nsubimages (the most \\ncommonly used). It is \\nassumed that the input \\nimage is padded, if neces-\\nsary, so that both \\nM\\n and \\nN\\n are multiples of \\nn\\n.\\nCoefﬁcient\\nm\\nBinary Code\\n(PDF bits)\\nGray Code\\n(PDF bits)\\nCompression\\nRatio\\n7\\n6,999\\n6,999\\n1.00\\n6\\n12,791\\n11,024\\n1.16\\n5\\n40,104\\n36,914\\n1.09\\n4\\n55,911\\n47,415\\n1.18\\n3\\n78,915\\n67,787\\n1.16\\n2\\n101,535\\n92,630\\n1.10\\n1\\n107,909\\n105,286\\n1.03\\n0\\n99,753\\n107,909\\n0.92\\nTABLE \\n8.11\\nJBIG2 lossless \\ncoding results \\nfor the binary \\nand Gray-coded \\nbit planes of \\nFig. 8.19(a). These \\nresults include the \\noverhead of each \\nbit plane’s PDF \\nrepresentation.\\nDIP4E_GLOBAL_Print_Ready.indb   577\\n6/16/2017   2:10:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 578}),\n",
       " Document(page_content='578\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nb a\\nd c\\nf e\\nh\\ng\\nFIGURE 8.19\\n(a) A 256-bit \\nmonochrome \\nimage.  \\n(b)–(h) The four \\nmost signiﬁcant \\nbinary and  \\nGray-coded bit \\nplanes of the \\nimage in (a).\\nAll\\nbits\\na\\n7\\n, \\ng\\n7\\na\\n6\\ng\\n6\\na\\n5\\ng\\n5\\na\\n4\\ng\\n4\\nDIP4E_GLOBAL_Print_Ready.indb   578\\n6/16/2017   2:10:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 579}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n579\\nb a\\nd c\\nf e\\nh\\ng\\nFIGURE 8.20\\n(a)–(h) The four \\nleast signiﬁcant \\nbinary (left  \\ncolumn) and \\nGray-coded \\n(right column) \\nbit planes of \\nthe image in \\nFig. 8.19(a).\\na\\n3\\ng\\n3\\na\\n2\\ng\\n2\\na\\n1\\ng\\n1\\na\\n0\\ng\\n0\\nDIP4E_GLOBAL_Print_Ready.indb   579\\n6/16/2017   2:10:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 580}),\n",
       " Document(page_content='580\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nIn each case, the 32 retained coefﬁcients were selected on the basis of maximum magnitude. Note \\nthat in all cases, the 32 discarded coefﬁcients had little visual impact on the quality of the reconstructed \\nimage. Their elimination, however, was accompanied by some mean-squared error, which can be seen \\nin the scaled error images of Figs. 8.22(d) through (f). The actual rms errors were 2.32, 1.78, and 1.13 \\nintensities, respectively.\\nb\\na\\nFIGURE 8.21\\nA block transform \\ncoding system:  \\n(a) encoder;  \\n(b) decoder.\\nInput\\nimage\\n(\\nM\\n \\n*\\n \\nN\\n)\\nContruct\\nn\\n \\n*\\n \\nn\\nsubimage\\nForward\\ntransform\\nQuantizer\\nSymbol\\nencoder\\nSymbol\\ndecoder\\nInverse\\ntransform\\nMerge\\nn\\n \\n*\\n \\nn\\nsubimage\\nCompressed\\nimage\\nDecompressed\\nimage\\nCompressed\\nimage\\nb a\\nc\\ne d\\nf\\nFIGURE 8.22\\n Approximations of Fig. 8.9(a) using the (a) Fourier, (b) Walsh-Hadamard, and (c) cosine transforms, \\ntogether with the corresponding scaled error images in (d)–(f).\\nDIP4E_GLOBAL_Print_Ready.indb   580\\n6/16/2017   2:10:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 581}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n581\\nThe small differences in mean-squared reconstruction error noted in the preced-\\ning example are related directly to the energy or information packing properties of \\nthe transforms employed. In accordance with Eqs. (7-75) and (7-76) of Section 7.5, \\nan \\nnn\\n*\\n subimage \\ngxy\\n(,\\n)\\n can be expressed as a function of its 2-D transform \\nTuv\\n(,\\n) :\\n \\nGS\\n=\\n= =\\n∑ ∑\\nTuv\\nuv\\nv\\nn\\nu\\nn\\n(,)\\n0\\n1\\n0\\n1\\n− −\\n \\n(8-21)\\nfor \\nxy\\nn\\n,,\\n, , ,.\\n=\\n012 1\\n…\\n−\\n \\nG\\n,\\n the matrix containing the pixels of the input subimage, \\nis explicitly deﬁned as a linear combination of \\nn\\n2\\n basis images of size \\nnn\\n×\\n.\\n Recall \\nthat the basis images of the DFT\\n, DCT, and WHT transforms for \\nn\\n=\\n8\\n are shown \\nin F\\nigs. 7.7, 7.10, and 7.16. If we now deﬁne a transform coefﬁcient \\nmasking function\\n \\nχ\\nuv\\nTu v\\n,\\n,\\n()\\n=\\n()\\n0\\n1\\nif satisfies a specified truncation criterion\\nothe\\nerwise\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(8-22)\\nfor \\nuv\\nn\\n,,\\n, , ,,\\n=\\n012 1\\n…\\n−\\n an approximation of \\nG\\n can be obtained from the trun-\\ncated expansion\\n \\nˆ\\n,(\\n, )\\nGS\\n=\\n()\\n= =\\n∑ ∑\\nχ\\n− −\\nuvTuv\\nuv\\nv\\nn\\nu\\nn\\n0\\n1\\n0\\n1\\n \\n(8-23)\\nwhere \\nχ\\nuv\\n,\\n()\\n is constructed to eliminate the basis images that make the smallest \\ncontribution to the total sum in Eq. (8-21). The mean-squared error between subim-\\nage \\nG\\n and approximation \\nˆ\\nG\\n then is\\n \\neE\\nE T uv\\nuvT uv\\nms\\nuv\\nv\\nn\\nu\\nn\\nuv\\nv\\nn\\n=\\n{}\\n=\\n() () ()\\n=\\n==\\n∑ ∑\\nGG\\nSS\\n−\\n−\\nχ\\n−\\n−−\\nˆ\\n,, ,\\n2\\n0\\n1\\n0\\n1\\n0\\n1 1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\n1\\n∑ ∑\\n∑ ∑\\n=\\n= =\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n⎫\\n⎬\\n⎪\\n⎭\\n⎪\\n=\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\nu\\nn\\nuv\\nv\\nn\\nu\\nn\\nE T uv uv\\n−\\n− −\\n−\\nχ\\n,,\\nS\\n2 2\\n2\\n0\\n1\\n0\\n1\\n1\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n⎫\\n⎬\\n⎪\\n⎭\\n⎪\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n()\\n= =\\n∑ ∑\\ns\\nTuv\\nv\\nn\\nu\\nn\\nuv\\n,\\n,\\n− −\\n−\\nχ\\n \\n(8-24)\\nwhere \\nGG\\n−\\nˆ\\n is the norm of matrix \\nGG\\n−\\nˆ\\n()\\n and \\ns\\nTu v\\n(,)\\n2\\n is the variance of the coef-\\nficient at transform location \\n(,) .\\nuv\\n The final simplification is based on the ortho-\\nnormal nature of the basis images and the assumption that the pixels of \\nG\\n are \\ngenerated by a random process with zero mean and known covariance\\n. The total \\nmean-squared error of approximation thus is the sum of the variances of the dis-\\ncarded transform coefficients; that is, the coefficients for which \\nχ\\nuv\\n,,\\n()\\n=\\n0\\n so that \\n1\\n−\\nχ\\nuv\\n,\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n in Eq. (8-24) is 1. Transformations that redistribute or pack the most \\ninformation into the fewest coefficients provide the best subimage approximations \\nand,\\n consequently, the smallest reconstruction errors. Finally, under the assumptions \\nDIP4E_GLOBAL_Print_Ready.indb   581\\n6/16/2017   2:10:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 582}),\n",
       " Document(page_content='582\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nthat led to Eq. (8-24), the mean-squared error of the \\nM\\nNn\\n2\\n subimages of an \\nMN\\n×\\n \\nimage are identical.\\n Thus the mean-squared error (being a measure of \\naverage\\n error) \\nof the \\nMN\\n×\\n image equals the mean-squared error of a single subimage.\\nT\\nhe earlier example showed that the information packing ability of the DCT is \\nsuperior to that of the DFT and WHT. Although this condition usually holds for most \\nimages, the Karhunen-Loève transform (see Chapter 11), not the DCT, is the opti-\\nmal transform in an information packing sense. This is due to the fact that the KLT \\nminimizes the mean-squared error in Eq. (8-24) for any input image and any number \\nof retained coefﬁcients (Kramer and Mathews [1956]). However, because the KLT  \\nis data dependent, obtaining the KLT basis images for each subimage, in general, is \\na nontrivial computational task. For this reason, the KLT is used infrequently for \\nimage compression. Instead, a transform, such as the DFT, WHT, or DCT, whose \\nbasis images are ﬁxed (input independent) is normally used. Of the possible input \\nindependent transforms, the nonsinusoidal transforms (such as the WHT transform) \\nare the simplest to implement. The sinusoidal transforms (such as the DFT or DCT) \\nmore closely approximate the information packing ability of the optimal KLT.\\nHence, most transform coding systems are based on the DCT, which provides a \\ngood compromise between information packing ability and computational complex-\\nity. In fact, the properties of the DCT have proved to be of such practical value that \\nthe DCT is an international standard for transform coding systems. Compared to \\nthe other input independent transforms, it has the advantages of having been imple-\\nmented in a single integrated circuit, packing the most information into the fewest \\ncoefﬁcients\\n†\\n (for most images), and minimizing the block-like appearance, called \\nblocking artifact\\n, that results when the boundaries between subimages become \\nvisible. This last property is particularly important in comparisons with the other \\nsinusoidal transforms. As Fig. 7.11(a) of Section 7.6 shows, the implicit \\nn\\n-point peri-\\nodicity of the DFT gives rise to boundary discontinuities that result in substantial \\nhigh-frequency transform content. When the DFT transform coefﬁcients are trun-\\ncated or quantized, the Gibbs phenomenon\\n‡\\n causes the boundary points to take on \\nerroneous values, which appear in an image as blocking artifact. That is, the bound-\\naries between adjacent subimages become visible because the boundary pixels of \\nthe subimages assume the mean values of discontinuities formed at the boundary \\npoints [see Fig. 7.11(a)]. The DCT of Fig. 7.11(b) reduces this effect, because its \\nimplicit 2\\nn\\n-point periodicity does not inherently produce boundary discontinuities.\\nSUBIMAGE SIZE SELECTION\\nAnother significant factor affecting transform coding error and computational com-\\nplexity is subimage size. In most applications, images are subdivided so the correla-\\ntion (redundancy) between adjacent subimages is reduced to some acceptable level \\n† Ahmed et al. [1974] ﬁrst noticed that the KLT basis images of a ﬁrst-order Markov image source closely resem-\\nble the DCT’s basis images. As the correlation between adjacent pixels approaches one, the input-dependent \\nKLT basis images become identical to the input-independent DCT basis images (Clarke [1985]).\\n‡ This phenomenon, described in most electrical engineering texts on circuit analysis, occurs because the Fourier \\ntransform fails to converge uniformly at discontinuities. At discontinuities, Fourier expansions take the mean \\nvalues of the points of discontinuity.\\nAn additional condition \\nfor optimality is that \\nthe masking function \\nof Eq. (8-22) selects \\nthe KLT coefﬁcients of \\nmaximum variance.\\nDIP4E_GLOBAL_Print_Ready.indb   582\\n6/16/2017   2:11:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 583}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n583\\nand so \\nn\\n is an integer power of 2 where, as before, \\nn\\n is the subimage dimension. \\nThe latter condition simplifies the computation of the subimage transforms (see the \\nbase-2 successive doubling method discussed in Section 4.11). In general, both the \\nlevel of compression and computational complexity increase as the subimage size \\nincreases. The most popular subimage sizes are \\n88\\n×\\n and \\n1\\n66\\n×1\\n.\\nEXAMPLE 8.14 :  Effects of subimage size on transform coding.\\nFigure 8.23 illustrates graphically the impact of subimage size on transform coding reconstruction error. \\nThe data plotted were obtained by dividing the monochrome image of Fig. 8.9(a) into subimages of size \\nnn\\n×\\n,\\n for \\nn\\n=\\n2 4 8 16 256 512\\n, , , , ,,,\\n…\\n computing the transform of each subimage, truncating 75% of the \\nresulting coefﬁcients, and taking the inverse transform of the truncated arrays. Note that the Hadamard \\nand cosine curves ﬂatten as the size of the subimage becomes greater than \\n88\\n×\\n,\\n whereas the Fourier \\nreconstruction error continues to decrease in this region.\\n As \\nn\\n further increases, the Fourier reconstruc-\\ntion error crosses the Walsh-Hadamard curve and approaches the cosine result. This result is consistent \\nwith the theoretical and experimental ﬁndings reported by Netravali and Limb [1980] and by Pratt \\n[2001] for a 2-D Markov image source.\\nAll three curves intersect when \\n22\\n×\\n subimages are used. In this case, only one of the four coefﬁcients \\n(25%) of each transformed array was retained.\\n The coefﬁcient in all cases was the dc component, so the \\ninverse transform simply replaced the four subimage pixels by their average value [see Eq. (4-92)]. This \\ncondition is evident in Fig. 8.24(b), which shows a zoomed portion of the \\n22\\n×\\n DCT result. Note that \\nthe blocking artifact that is prevalent in this result decreases as the subimage size increases to \\n44\\n×\\n and \\n88\\n×\\n in Figs. 8.24(c) and (d). Figure 8.24(a) shows a zoomed portion of the original image for reference.\\nBIT ALLOCATION\\nThe reconstruction error associated with the truncated series expansion of Eq. (8-23) \\nis a function of the number and relative importance of the transform coefficients \\nthat are discarded, as well as the precision that is used to represent the retained \\ncoefficients. In most transform coding systems, the retained coefficients are selected \\n[that is, the masking function of Eq. (8-22) is constructed] on the basis of maximum \\nFIGURE 8.23\\nReconstruction \\nerror versus \\nsubimage size.\\nSubimage size\\nRoot-mean-square error\\nDFT\\nWHT\\nDCT\\n2 4 8 16 32 64 128 256 512\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n5.5\\n6\\n6.5\\nDIP4E_GLOBAL_Print_Ready.indb   583\\n6/16/2017   2:11:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 584}),\n",
       " Document(page_content='584\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nvariance, called \\nzonal coding\\n, or on the basis of maximum magnitude, called \\nthresh-\\nold coding\\n. The overall process of truncating, quantizing, and coding the coefficients \\nof a transformed subimage is commonly called \\nbit allocation\\n.\\nEXAMPLE 8.15 :  Bit allocation.\\nFigures 8.25(a) and (c) show two approximations of Fig. 8.9(a) in which 87.5% of the DCT coefﬁcients \\nof each \\n88\\n×\\n subimage were discarded. The ﬁrst result was obtained via threshold coding by keeping \\nthe eight largest transform coefﬁcients\\n, and the second image was generated by using a zonal coding \\napproach. In the latter case, each DCT coefﬁcient was considered a random variable whose distribution \\ncould be computed over the ensemble of all transformed subimages. The eight distributions of largest \\nvariance (12.5% of the 64 coefﬁcients in the transformed \\n88\\n×\\n subimage) were located and used to \\ndetermine the coordinates [\\nu\\n and \\nv\\n of the coefﬁcients\\n, \\nTuv\\n(,\\n)\\n], that were retained for all subimages. \\nNote that the threshold coding difference image of F\\nig. 8.25(b) contains less error than the zonal coding \\nresult in Fig. 8.25(d). Both images have been scaled to make the errors more visible. The corresponding \\nrms errors are 4.5 and 6.5 intensities, respectively.\\nZonal Coding Implementation\\nZonal coding is based on the information theory concept of viewing information as \\nuncertainty. Therefore, the transform coefﬁcients of maximum variance carry the \\nmost image information, and should be retained in the coding process. The variances \\nthemselves can be calculated directly from the ensemble of \\nM\\nNn\\n2\\n transformed \\nsubimage arrays (as in the preceding example) or based on an assumed image model \\n(say, a Markov autocorrelation function). In either case, the zonal sampling process \\ncan be viewed, in accordance with Eq. (8-23), as multiplying each \\nTuv\\n(,\\n)\\n by the cor-\\nresponding element in a \\nz\\nonal mask\\n, which is constructed by placing a 1 in the loca-\\ntions of maximum variance and a 0 in all other locations. Coefﬁcients of maximum \\nvariance usually are located around the origin of an image transform, resulting in \\nthe typical zonal mask shown in Fig. 8.26(a).\\nThe coefﬁcients retained during the zonal sampling process must be quantized \\nand coded, so zonal masks are sometimes depicted showing the number of bits used \\nb a\\nc\\nd\\nFIGURE 8.24\\n Approximations of Fig. 8.24(a) using 25% of the DCT coefﬁcients and (b) \\n22\\n×\\n subimages, (c) \\n44\\n×\\n \\nsubimages\\n, and (d) \\n88\\n×\\n subimages. The original image in (a) is a zoomed section of Fig. 8.9(a).\\nDIP4E_GLOBAL_Print_Ready.indb   584\\n6/16/2017   2:11:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 585}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n585\\nto code each coefﬁcient [see Fig. 8.26(b)]. In most cases, the coefﬁcients are allo-\\ncated the same number of bits, or some ﬁxed number of bits is distributed among \\nthem unequally. In the ﬁrst case, the coefﬁcients generally are normalized by their \\nstandard deviations and uniformly quantized. In the second case, a quantizer, such \\nas an optimal Lloyd-Max quantizer (see Optimal quantizers in Section 8.10), is \\ndesigned for each coefﬁcient. To construct the required quantizers, the zeroth or DC \\ncoefﬁcient normally is modeled by a Rayleigh density function, whereas the remain-\\ning coefﬁcients are modeled by a Laplacian or Gaussian density.\\n†\\n The number of \\nquantization levels (and thus the number of bits) allotted to each quantizer is made \\nproportional to \\nlog .\\n,\\n2\\n2\\ns\\nTuv\\n()\\n Thus, the retained coefﬁcients in Eq. (8-23)—which (in \\nthe context of the current discussion) are selected on the basis of maximum vari-\\nance—are assigned bits in proportion to the logarithm of the coefﬁcient variances.\\nThreshold Coding Implementation\\nZonal coding usually is implemented by using a single ﬁxed mask for all subimages. \\nThreshold coding, however, is inherently adaptive in the sense that the location of \\nthe transform coefﬁcients retained for each subimage vary from one subimage to \\n† As each coefﬁcient is a linear combination of the pixels in its subimage [see Eq. (7-31)], the central-limit theo-\\nrem suggests that, as subimage size increases, the coefﬁcients tend to become Gaussian. This result does not \\napply to the dc coefﬁcient, however, because nonnegative images always have positive dc coefﬁcients.\\nb a\\nd c\\nFIGURE 8.25\\nApproximations \\nof Fig. 8.9(a) using \\n12.5% of the   \\nDCT coefﬁcients: \\n(a)–(b) threshold \\ncoding results; \\n(c)–(d) zonal \\ncoding results. The \\ndifference images \\nare scaled by 4.\\nDIP4E_GLOBAL_Print_Ready.indb   585\\n6/16/2017   2:11:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 586}),\n",
       " Document(page_content='586\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nanother. In fact, threshold coding is the adaptive transform coding approach most \\noften used in practice because of its computational simplicity. The underlying con-\\ncept is that, for any subimage, the transform coefﬁcients of largest magnitude make \\nthe most signiﬁcant contribution to reconstructed subimage quality, as demonstrated \\nin the last example. Because the locations of the maximum coefﬁcients vary from \\none subimage to another, the elements of \\nχ\\nuv\\nTuv\\n,( , )\\n()\\n normally are reordered (in a \\npredeﬁned manner) to form a 1-D, run-length coded sequence. Figure 8.26(c) shows \\na typical \\nthreshold mask\\n for one subimage of a hypothetical image. This mask pro-\\nvides a convenient way to visualize the threshold coding process for the correspond-\\ning subimage, as well as to mathematically describe the process using Eq. (8-23). \\nWhen the mask is applied [via Eq. (8-23)] to the subimage for which it was derived, \\nand the resulting \\nnn\\n×\\n array is reordered to form an \\nn\\n2\\n-element\\n coefﬁcient sequence \\nin accordance with the zigzag ordering pattern of F\\nig. 8.26(d), the reordered 1-D \\nsequence contains several long runs of 0’s. [The zigzag pattern becomes evident by \\nstarting at 0 in Fig. 8.26(d) and following the numbers in sequence.] These runs nor-\\nmally are run-length coded. The nonzero or retained coefﬁcients, corresponding to \\nthe mask locations that contain a 1, are represented using a variable-length code.\\nThere are three basic ways to threshold a transformed subimage or, stated dif-\\nferently, to create a subimage threshold masking function of the form given in \\nEq. (8-22): (1) A single global threshold can be applied to all subimages; (2) a differ-\\nent threshold can be used for each subimage, or; (3) the threshold can be varied as a \\nfunction of the location of each coefﬁcient within the subimage. In the ﬁrst approach, \\nb a\\nd c\\nFIGURE 8.26\\nA typical  \\n(a) zonal mask, \\n(b) zonal bit allo-\\ncation,  \\n(c) threshold \\nmask, and  \\n(d) thresholded \\ncoefﬁcient order-\\ning sequence. \\nShading highlights \\nthe coefﬁcients \\nthat are retained.\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n000\\n0\\n000\\n00000\\n0 0 0 0 0 0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n2\\n2\\n1\\n1\\n1\\n1 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n22\\n2\\n2\\n3 3\\n3\\n33\\n33\\n3\\n3\\n5\\n44\\n4\\n4\\n4\\n5\\n6\\n6\\n6\\n7\\n7\\n8\\n8\\n0156\\n2\\n3\\n47\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27 28\\n29 42\\n30 41 43\\n31 40 44 53\\n33\\n34\\n35 36 48\\n37\\n32 39 45 52 54\\n38 46 51 55 60\\n47 50 56 59 61\\n49 57 58 62 63\\nDIP4E_GLOBAL_Print_Ready.indb   586\\n6/16/2017   2:11:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 587}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n587\\nthe level of compression differs from image to image, depending on the number of \\ncoefﬁcients that exceed the global threshold. In the second, called \\nN-largest coding\\n, \\nthe same number of coefﬁcients is discarded for each subimage. As a result, the code \\nrate is constant and known in advance. The third technique, like the ﬁrst, results in a \\nvariable code rate, but offers the advantage that thresholding \\nand\\n quantization can \\nbe combined by replacing \\nχ\\nuv\\nTuv\\n,( , )\\n()\\n in Eq. (8-23) with\\n \\nTu v\\nTu v\\nZu v\\nˆ\\n,\\n,\\n,\\n()\\n=\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nround\\n \\n(8-25)\\nwhere \\nˆ\\n(,\\n)\\nTuv\\n is a thresholded and quantized approximation of \\nTuv\\n(,\\n) ,\\n and \\nZuv\\n(,\\n)\\n \\nis an element of the following transform normalization array:\\n \\nZ\\n=\\n() () ( )\\n()\\n()\\nZZ Z n\\nZ\\nZn Zn\\n00 01 0 1\\n10\\n10 11\\n,, ,\\n,\\n,,\\n…\\n/vertellipsis… /vertellipsis\\n/vertellipsis/vertellipsis … /vertellipsis\\n/vertellipsis/vertellipsis … /vertellipsis\\n/vertellipsis/vertellipsis … /vertellipsis\\n−\\n−−\\n(\\n() ( )\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n…\\nZn n\\n−−\\n11\\n,\\n \\n(8-26)\\nBefore a normalized (thresholded and quantized) subimage transform, \\nˆ\\n(,\\n) ,\\nTuv\\n can \\nbe inverse transformed to obtain an approximation of subimage \\ngxy\\n(,\\n) ,\\n it must \\nbe multiplied by \\nZuv\\n(,\\n) .\\n The resulting denormalized array, denoted \\n/dotnosp\\nTu\\nv\\n(,\\n)\\n is an \\napproximation of \\nˆ\\n(,\\n) :\\nTuv\\n \\n/dotnosp\\nTu\\nvT\\nu v Z u v\\n(,) (,) (,)\\n=\\nˆ\\n \\n(8-27)\\nThe inverse transform of \\n/dotnosp\\nTu\\nv\\n(,\\n)\\n yields the decompressed subimage approximation.\\nF\\nigure 8.27(a) graphically depicts Eq. (8-25) for the case in which \\nZuv\\n(,\\n)\\n is \\nassigned a particular value \\nc\\n.\\n Note that \\nˆ\\n(,\\n)\\nTuv\\n assumes integer value \\nk\\n if and only if\\n \\nkc\\nc\\nTu v k c\\nc\\n−+\\n22\\n≤\\n()\\n<\\n,\\n \\n(8-28)\\nIf \\nZu v Tu v\\n,, ,\\n()\\n>\\n()\\n2\\n then \\nˆ\\n(,\\n)\\nTuv\\n=\\n0\\n and the transform coefficient is completely \\ntruncated or discarded.\\n When \\nˆ\\n(,\\n)\\nTuv\\n is represented with a variable-length code that \\nincreases in length as the magnitude of \\nk\\n increases\\n, the number of bits used to rep-\\nresent \\nTuv\\n(,\\n)\\n is controlled by the value of \\nc\\n.\\n Thus, the elements of \\nZ\\n can be scaled \\nto achieve a variety of compression levels. Figure 8.27(b) shows a typical normaliza-\\ntion array. This array, which has been used extensively in the JPEG standardization \\nefforts (see the next section), weighs each coefficient of a transformed subimage \\naccording to heuristically determined perceptual or psychovisual importance.\\nThe \\nN\\n in “\\nN\\n-largest \\ncoding” is not an image \\ndimension, but refers \\nto the number of coef-\\nﬁcients that are kept.\\nDIP4E_GLOBAL_Print_Ready.indb   587\\n6/16/2017   2:11:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 588}),\n",
       " Document(page_content='588\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nEXAMPLE 8.16 :  Illustration of threshold coding.\\nFigures 8.28(a) through (f) show six threshold-coded approximations of the monochrome image in \\nFig. 8.9(a). All images were generated using an \\n88\\n×\\n DCT and the normalization array of Fig. 8.27(b). \\nT\\nhe ﬁrst result, which provides a compression ratio of about 12 to 1 (i.e., C = 12), was obtained by direct \\napplication of that normalization array. The remaining results, which compress the original image by 19, \\n30, 49, 85, and 182 to 1, were generated after multiplying (scaling) the normalization arrays by 2, 4, 8, \\n16, and 32, respectively. The corresponding rms errors are 3.83, 4.93, 6.62, 9.35, 13.94, and 22.46 intensity \\nlevels.\\nJPEG\\nOne of the most popular and comprehensive continuous tone, still-frame compres-\\nsion standards is the JPEG standard. It defines three different coding systems: (1) a \\nlossy baseline coding system, which is based on the DCT and is adequate for most \\ncompression applications; (2) an extended coding system for greater compression, \\nhigher precision, or progressive reconstruction applications; and (3) a lossless inde-\\npendent coding system for reversible compression. To be JPEG compatible, a prod-\\nuct or system must include support for the baseline system. No particular file format, \\nspatial resolution, or color space model is specified.\\nIn the baseline system, often called the \\nsequential baseline system\\n, the input and \\noutput data precision is limited to 8 bits, whereas the quantized DCT values are \\nrestricted to 11 bits. The compression itself is performed in three sequential steps: \\nDCT computation, quantization, and variable-length code assignment. The image \\nis ﬁrst subdivided into pixel blocks of size \\n88\\n×\\n,\\n which are processed left-to-right, \\ntop-to-bottom.\\n As each \\n88\\n×\\n block or subimage is encountered, its 64 pixels are \\nlevel-shifted by subtracting the quantity \\n2\\n1\\nk\\n−\\n,\\n where \\n2\\nk\\n is the maximum number of \\nintensity levels. The 2-D discrete cosine transform of the block is then computed, \\nquantized in accordance with Eq. (8-25), and reordered, using the zigzag pattern of \\nFig. 8.26(d), to form a 1-D sequence of quantized coefﬁcients.\\nBecause the one-dimensionally reordered array generated under the zigzag \\npattern of Fig. 8.26(d) is arranged qualitatively according to increasing spatial fre-\\nquency, the JPEG coding procedure is designed to take advantage of the long runs \\nb a\\nFIGURE 8.27\\n(a) A threshold \\ncoding quantiza-\\ntion curve [see \\nEq. (8-28)]. (b) A \\ntypical normaliza-\\ntion matrix.\\nˆ\\n3\\nc\\n2\\nc\\nc\\nT\\n(\\nu\\n, \\nv\\n)\\n-\\n3\\nc\\n-\\n2\\nc\\n-\\nc\\n2\\n3\\n1\\n-\\n2\\n-\\n3\\n-\\n1\\nT\\n(\\nu\\n, \\nv\\n)\\n16 11 10 16 24 40 51 61\\n12 12 14 19 26 58 60 55\\n14 13 16 24 40 57 69 56\\n14 17 22 29 51 87 80 62\\n18 22 37 56 68 109 103 77\\n24 35 55 64 81 104 113 92\\n49 64 78 87 103 121 120 101\\n72 92 95 98 112 100 103 99\\nDIP4E_GLOBAL_Print_Ready.indb   588\\n6/16/2017   2:11:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 589}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n589\\nof zeros that normally result from the reordering. In particular, the nonzero AC\\n†\\n \\ncoefﬁcients are coded using a variable-length code that deﬁnes the coefﬁcient values \\nand number of preceding zeros. The DC coefﬁcient is difference coded relative to \\nthe DC coefﬁcient of the previous subimage. The default JPEG Huffman codes for \\nthe luminance component of a color image, or intensity of a monochrome image, are \\navailable on the book website. The JPEG recommended luminance quantization \\narray is given in Fig. 8.27(b) and can be scaled to provide a variety of compression \\nlevels. The scaling of this array allows users to select the “quality” of JPEG compres-\\nsions. Although default coding tables and quantization arrays are provided for both \\ncolor and monochrome processing, the user is free to construct custom tables and/or \\narrays, which may be adapted to the characteristics of the image being compressed.\\n† In the standard, the term AC denotes all transform coefﬁcients with the exception of the zeroth or DC coef-\\nﬁcient.\\nConsult the book web-\\nsite for the JPEG default \\nHuffman code tables:  \\n(1) a JPEG coefﬁcient \\ncategory table, (2) a \\ndefault DC code table, \\nand (3) a default AC \\ncode table.\\nb a\\nc\\ne d\\nf\\nFIGURE 8.28\\n Approximations of Fig. 8.9(a) using the DCT and normalization array of Fig. 8.27(b): (a) \\nZ\\n, (b) 2\\nZ\\n,  \\n(c) 4\\nZ\\n, (d) 8\\nZ\\n, (e) 16\\nZ\\n, and (f) 32\\nZ\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   589\\n6/16/2017   2:11:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 590}),\n",
       " Document(page_content='590\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nEXAMPLE 8.17 :  JPEG baseline coding and decoding.\\nConsider compression and reconstruction of the following \\n88\\n×\\n subimage with the JPEG baseline stan-\\ndard:\\n52 55 61 66 70 61 64 73\\n63 59 66 90 109 85 69 72\\n62 59 68 113 144 104 66 73\\n63 58 71 122 154 106 70 69\\n67 61 68 104 126 88 68 70\\n79 65 60 70 77 63 58 75\\n85 71 64 59 55 61 65 83\\n87 79 69 68 65 76 78 94\\nThe original image consists of 256 or 2\\n8\\n possible intensities, so the coding process begins by level shifting \\nthe pixels of the original subimage by \\n−\\n2\\n7\\n or \\n−1\\n28 intensity levels. The resulting shifted array is\\n-\\n76\\n-\\n73\\n-\\n67\\n-\\n62\\n-\\n58\\n-\\n67\\n-\\n64\\n-\\n55\\n-\\n65\\n-\\n69\\n-\\n62\\n-\\n38\\n-\\n19\\n-\\n43\\n-\\n59\\n-\\n56\\n-\\n66\\n-\\n69\\n-\\n60\\n-\\n15 16\\n-\\n24\\n-\\n62\\n-\\n55\\n-\\n65\\n-\\n70\\n-\\n57\\n-\\n62 6\\n-\\n22\\n-\\n58\\n-\\n59\\n-\\n61\\n-\\n67\\n-\\n60\\n-\\n24\\n-\\n2\\n-\\n40\\n-\\n60\\n-\\n58\\n-\\n49\\n-\\n63\\n-\\n68\\n-\\n58\\n-\\n51\\n-\\n65\\n-\\n70\\n-\\n53\\n-\\n43\\n-\\n57\\n-\\n64\\n-\\n69\\n-\\n73\\n-\\n67\\n-\\n63\\n-\\n45\\n-\\n41\\n-\\n49\\n-\\n59\\n-\\n60\\n-\\n63\\n-\\n52\\n-\\n50\\n-\\n34\\nwhich, when transformed in accordance with the forward DCT of Eq. (7-31) with \\nrxy uv sxy uv\\n(,\\n,,) (,,,)\\n=\\nof Eq. (7-85) for \\nn\\n = 8 becomes\\n-\\n415\\n-\\n29\\n-\\n62 25 55\\n-\\n20\\n-\\n13\\n7\\n-\\n21\\n-\\n62 9 11\\n-\\n7\\n-\\n66\\n-\\n46 8 77\\n-\\n25\\n-\\n30 10 7\\n-\\n5\\n-\\n50 13 35\\n-\\n15\\n-\\n9 603\\n11\\n-\\n8\\n-\\n13\\n-\\n2\\n-\\n11\\n-\\n41\\n-\\n10 1 3\\n-\\n3\\n-\\n10 2\\n-\\n1\\n-\\n4\\n-\\n12\\n-\\n12\\n-\\n31\\n-\\n2\\n-\\n1\\n-\\n1\\n-\\n1\\n-\\n2\\n-\\n1\\n-\\n10\\n-\\n1\\nIf the JPEG recommended normalization array of Fig. 8.27(b) is used to quantize the transformed array, \\nthe scaled and truncated [that is, normalized in accordance with Eq. (8-25)] coefﬁcients are\\nDIP4E_GLOBAL_Print_Ready.indb   590\\n6/16/2017   2:11:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 591}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n591\\n-\\n26\\n-\\n3\\n-\\n6 22000\\n1\\n-\\n2\\n-\\n4 00000\\n-\\n31 5\\n-\\n1\\n-\\n1 000\\n-\\n41 2\\n-\\n1 0000\\n10000000\\n00000000\\n00000000\\n00000000\\nwhere, for instance, the DC coefﬁcient is computed as\\n \\nT\\nT\\nZ\\nˆ\\n00\\n00\\n00\\n415\\n16\\n26\\n,\\n,\\n,\\n()\\n=\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\nround\\nround\\n−\\n−\\nNote that the transformation and normalization process produces a large number of zero-valued coefﬁ-\\ncients. When the coefﬁcients are reordered in accordance with the zigzag ordering pattern of Fig. 8.26(d), \\nthe resulting 1-D coefﬁcient sequence is\\n \\n−− − − − − −\\n−\\n− −\\n2\\n6 31 3 2 62 41 41150200 1200000 1 1E O B\\n[\\n]\\nwhere the EOB symbol denotes the end-of-block condition. A special EOB Huffman code word (see \\ncategory 0 and run-length 0 of the JPEG default AC code table on the book website) is provided to \\nindicate that the remainder of the coefﬁcients in a reordered sequence are zeros.\\nThe construction of the default JPEG code for the reordered coefﬁcient sequence begins with the \\ncomputation of the difference between the current DC coefﬁcient and that of the previously encoded \\nsubimage. Assuming the DC coefﬁcient of the transformed and quantized subimage to its immediate left \\nwas \\n−\\n17\\n,\\n the resulting DPCM difference is \\n−− −\\n26\\n17\\n()\\n[]\\n or \\n−\\n9,\\n which lies in DC difference category 4  \\nof the JPEG coefﬁcient category table (see the book website).\\n In accordance with the default Huffman \\ndifference code, the proper base code for a category 4 difference is 101 (a 3-bit code), while the total \\nlength of a completely encoded category 4 coefﬁcient is 7 bits. The remaining 4 bits must be generated \\nfrom the least signiﬁcant bits (LSBs) of the difference value. For a general DC difference category (say, \\ncategory \\nK\\n), an additional \\nK\\n bits are needed and computed as either the \\nK\\n LSBs of the positive differ-\\nence or the \\nK\\n LSBs of the negative difference minus 1. For a difference of \\n−\\n9,\\n the appropriate LSBs are \\n0111 1\\n()\\n−\\n or 0110, and the complete DPCM coded DC code word is 1010110.\\nT\\nhe nonzero AC coefﬁcients of the reordered array are coded similarly. The principal difference is \\nthat each default AC Huffman code word depends on the number of zero-valued coefﬁcients preceding \\nthe nonzero coefﬁcient to be coded, as well as the magnitude category of the nonzero coefﬁcient. (See \\nthe column labeled Run/Category in the JPEG AC code table on the book website.) Thus, the ﬁrst non-\\nzero AC coefﬁcient of the reordered array \\n()\\n−\\n3\\n is coded as 0100. The ﬁrst 2 bits of this code indicate that \\nthe coefﬁcient was in magnitude category 2 and preceded by no zero-valued coefﬁcients;\\n the last 2 bits \\nare generated by the same process used to arrive at the LSBs of the DC difference code. Continuing in \\nDIP4E_GLOBAL_Print_Ready.indb   591\\n6/16/2017   2:11:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 592}),\n",
       " Document(page_content='592\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nthis manner, the completely coded (reordered) array is\\n 1010110 0100 001 0100 0101 100001 0110 100011 001 100011 001\\n 001 100101 11100110 110110 0110 11110100 000 1010\\nwhere the spaces have been inserted solely for readability. Although it was not needed in this example, \\nthe default JPEG code contains a special code word for a run of 15 zeros followed by a zero. The total \\nnumber of bits in the completely coded reordered array (and thus the number of bits required to rep-\\nresent the entire \\n88\\n×\\n,\\n 8-bit subimage of this example) is 92. The resulting compression ratio is \\n512 92,\\n \\nor about 5.6:1.\\nT\\no decompress a JPEG compressed subimage, the decoder must ﬁrst recreate the normalized trans-\\nform coefﬁcients that led to the compressed bit stream. Because a Huffman-coded binary sequence is \\ninstantaneous and uniquely decodable, this step is easily accomplished in a simple lookup table manner. \\nHere, the regenerated array of quantized coefﬁcients is\\n-\\n26\\n-\\n3\\n-\\n6 22000\\n1\\n-\\n2\\n-\\n4 00000\\n-\\n31 5\\n-\\n1\\n-\\n1 000\\n-\\n41 2\\n-\\n1 0000\\n10000000\\n00000000\\n00000000\\n00000000\\nAfter denormalization in accordance with Eq. (8-27), the array becomes\\n-\\n416\\n-\\n33\\n-\\n60 32 48 0 0 0\\n12\\n-\\n24\\n-\\n5 6 00000\\n-\\n42 13 80\\n-\\n24\\n-\\n4 0 000\\n-\\n56 17 44\\n-\\n2 9 0000\\n1 8 0000000\\n00000000\\n00000000\\n00000000\\nwhere, for example, the DC coefﬁcient is computed as\\n \\n/dotnosp\\nTT\\nZ\\n0 0 0 0 0 0 26 16 416\\n,, ,\\n()\\n=\\n() ()\\n=\\n() ( )\\n=\\nˆ\\n−−\\nDIP4E_GLOBAL_Print_Ready.indb   592\\n6/16/2017   2:11:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 593}),\n",
       " Document(page_content='8.9\\n  \\nBlock Transform Coding\\n    \\n593\\nThe completely reconstructed subimage is obtained by taking the inverse DCT of the denormalized \\narray in accordance with Eqs. (7-32) and (7-85) to obtain\\n-\\n70\\n-\\n64\\n-\\n61\\n-\\n64\\n-\\n69\\n-\\n66\\n-\\n58\\n-\\n50\\n-\\n72\\n-\\n73\\n-\\n61\\n-\\n39\\n-\\n30\\n-\\n40\\n-\\n54\\n-\\n59\\n-\\n68\\n-\\n78\\n-\\n58\\n-\\n91 3\\n-\\n12\\n-\\n48\\n-\\n64\\n-\\n59\\n-\\n77\\n-\\n57 0 22\\n-\\n13\\n-\\n51\\n-\\n60\\n-\\n54\\n-\\n75\\n-\\n64\\n-\\n23\\n-\\n13\\n-\\n44\\n-\\n63\\n-\\n56\\n-\\n52\\n-\\n71\\n-\\n72\\n-\\n54\\n-\\n54\\n-\\n71\\n-\\n71\\n-\\n54\\n-\\n45\\n-\\n59\\n-\\n70\\n-\\n68\\n-\\n67\\n-\\n67\\n-\\n61\\n-\\n50\\n-\\n35\\n-\\n47\\n-\\n61\\n-\\n66\\n-\\n60\\n-\\n48\\n-\\n44\\n-\\n44\\nand level shifting each inverse transformed pixel by 2\\n7\\n (or \\n+1\\n28) to yield\\n58 64 67 64 59 62 70 78\\n56 55 67 89 98 88 74 69\\n60 50 70 119 141 116 80 64\\n69 51 71 128 149 115 77 68\\n74 53 64 105 115 84 65 72\\n76 57 56 74 75 57 57 74\\n83 69 59 60 61 61 67 78\\n93 81 67 62 69 80 84 84\\nAny differences between the original and reconstructed subimage are a result of the lossy nature of the \\nJPEG compression and decompression process. In this example, the errors range from \\n−\\n14\\n to \\n+\\n11\\n and \\nare distributed as follows:\\n-\\n6\\n-\\n9\\n-\\n621 1\\n-\\n1\\n-\\n6\\n-\\n5\\n74\\n-\\n111 1\\n-\\n3\\n-\\n53\\n29\\n-\\n2\\n-\\n6\\n-\\n3\\n-\\n12\\n-\\n14 9\\n-\\n67 0\\n-\\n4\\n-\\n5\\n-\\n9\\n-\\n71\\n-\\n78 4\\n-\\n1 643\\n-\\n2\\n384\\n-\\n4 2611\\n225\\n-\\n1\\n-\\n60\\n-\\n25\\n-\\n6\\n-\\n22 6\\n-\\n4\\n-\\n4\\n-\\n61 0\\nThe root-mean-squared error of the overall compression and reconstruction process is approximately \\n5.8 intensity levels.\\nDIP4E_GLOBAL_Print_Ready.indb   593\\n6/16/2017   2:11:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 594}),\n",
       " Document(page_content='594\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nEXAMPLE 8.18 :  Illustration of JPEG coding.\\nFigures 8.29(a) and (d) show two JPEG approximations of the monochrome image in Fig. 8.9(a). The \\nﬁrst result provides a compression of 25:1; the second compresses the original image by 52:1. The dif-\\nferences between the original image and the reconstructed images in Figs. 8.29(a) and (d) are shown in \\nFigs. 8.29(b) and (e), respectively. The corresponding rms errors are 5.4 and 10.7 intensities. The errors \\nare clearly visible in the zoomed images in Figs. 8.29(c) and (f). These images show a magniﬁed section \\nof Figs. 8.29(a) and (d), respectively. Note that the JPEG blocking artifact increases with compression.\\n8.10 PREDICTIVE CODING  \\nWe now turn to a simpler approach that achieves good compression without sig-\\nnificant computational overhead \\nand\\n can be either error-free or lossy. The approach, \\ncommonly referred to as \\npredictive coding\\n, is based on eliminating the redundancies \\nof closely spaced pixels—in space and/or time—by extracting and coding only the \\nnew information\\n in each pixel. The new information of a pixel is defined as the dif-\\nference between the actual and predicted value of the pixel.\\nLOSSLESS PREDICTIVE CODING\\nFigure 8.30 shows the basic components of a \\nlossless predictive coding\\n system. The \\nsystem consists of an encoder and a decoder, each containing an identical \\npredic-\\ntor\\n. As successive samples of discrete time input signal, \\nfn\\n()\\n,\\n are introduced to the \\nencoder\\n, the predictor generates the anticipated value of each sample based on a \\nspecified number of past samples. The output of the predictor is then rounded to the \\nnearest integer, denoted \\nfn\\nˆ\\n()\\n,\\n and used to form the difference or \\nprediction error\\n \\nen fn f n\\n()\\n=\\n() ()\\n−\\nˆ\\n \\n(8-29)\\nwhich is encoded using a variable-length code (by the symbol encoder) to generate \\nthe next element of the compressed data stream.\\n The decoder in Fig. 8.30(b) recon-\\nstructs \\nen\\n()\\n from the received variable-length code words and performs the inverse \\noperation\\n \\nfn e n fn\\n()\\n=+\\n() ()\\nˆ\\n \\n(8-30)\\nto decompress or recreate the original input sequence.\\nVarious local, global, and adaptive methods (see the later subsection entitled Lossy\\n \\npredictive coding) can be used to generate \\nfn\\nˆ\\n()\\n.\\n In many cases, the prediction is \\nformed as a linear combination of \\nm\\n previous samples\\n. That is,\\n \\nfn\\nfn i\\ni\\ni\\nm\\nˆ\\n()\\n=\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∑\\nround\\na\\n1\\n−\\n \\n(8-31)\\nwhere \\nm\\n is the \\nor\\nder\\n of the linear predictor, round is a function used to denote the \\nrounding or nearest integer operation, and the \\na\\ni\\n for \\nim\\n=\\n12\\n,,\\n,\\n…\\n are prediction \\n8.10\\nWith reference to \\nTables 8.3–8.5, predictive \\ncoding is used in\\n• JBIG2 \\n• JPEG \\n• JPEG-LS \\n• MPEG-1,2,4 \\n• H.261, H.262, \\n H.263, and H.264 \\n• HDV \\n• VC-1\\nand other compres-\\nsion standards and ﬁle \\nformats.\\nDIP4E_GLOBAL_Print_Ready.indb   594\\n6/16/2017   2:11:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 595}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n595\\ncoefficients. If the input sequence in Fig. 8.30(a) is considered to be samples of an \\nimage, the \\nfn\\n()\\n in Eqs. (8-29) through (8-31) are pixels—and the \\nm\\n samples used \\nto predict the value of each pixel come from the current scan line (called 1-D lin-\\near predictive coding),\\n from the current and previous scan lines (called 2-D linear \\npredictive coding), or from the current image and previous images in a sequence of \\nimages (called 3-D linear predictive coding). Thus, for 1-D linear predictive image \\ncoding, Eq. (8-31) can be written as\\n \\nfx y\\nfx y i\\ni\\ni\\nm\\nˆ\\n,,\\n()\\n=\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∑\\nround\\na\\n1\\n−\\n \\n(8-32)\\nwhere each sample is now expressed explicitly as a function of the input image’s \\nspatial coordinates\\n, \\nx\\n and \\ny\\n. Note that Eq. (8-32) indicates that the 1-D linear predic-\\ntion is a function of the previous pixels on the current line alone. In 2-D predictive \\nb a\\nc\\ne d\\nf\\nFIGURE 8.29\\n Two JPEG approximations of Fig. 8.9(a). Each row contains a result after compression and reconstruc-\\ntion, the scaled difference between the result and the original image, and a zoomed portion of the reconstructed \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   595\\n6/16/2017   2:11:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 596}),\n",
       " Document(page_content='596\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\ncoding, the prediction is a function of the previous pixels in a left-to-right, top-to-\\nbottom scan of an image. In the 3-D case, it is based on these pixels and the previous \\npixels of preceding frames. Equation (8-32) cannot be evaluated for the first \\nm\\n pix-\\nels of each line, so those pixels must be coded by using other means (such as a Huff-\\nman code) and considered as an overhead of the predictive coding process. Similar \\ncomments apply to the higher-dimensional cases.\\nEXAMPLE 8.19 :  Predictive coding and spatial redundancy.\\nConsider encoding the monochrome image of Fig. 8.31(a) using the simple ﬁrst-order (i.e., \\nm\\n = 1) linear \\npredictor from Eq. (8-32)\\n \\nfx y fx y\\nˆ\\n,,\\n()\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\nround\\na\\n−\\n1\\n \\n(8-33)\\nThis equation is a simpliﬁcation of Eq. (8-32), with \\nm\\n = 1 and the subscript of lone prediction coefﬁcient \\na\\n1\\n removed as unnecessary. A predictor of this general form is called a \\nprevious pixel\\n predictor, and the \\ncorresponding predictive coding procedure is known as \\ndifferential coding\\n or \\nprevious pixel coding\\n. Fig-\\nure 8.31(c) shows the prediction error image, \\nexy fxy f xy\\n,( , ) ( , )\\n()\\n=−\\nˆ\\n that results from Eq. (8-33) with \\na\\n=\\n1.\\n The scaling of this image is such that intensity 128 represents a prediction error of zero, while all \\nnonzero positive and negative prediction errors (under and over estimates) are displayed as lighter and \\ndarker shades of gray\\n, respectively. The mean value of the prediction image is 128.26. Because intensity \\n128 corresponds to a prediction error of 0, the average prediction error is only 0.26 bits.\\nFigures 8.31(b) and (d) show the intensity histogram of the image in Fig. 8.31(a) and the histogram \\nof prediction error \\nexy\\n(,\\n) ,\\n respectively. Note that the standard deviation of the prediction error in \\nF\\nig. 8.31(d) is much smaller than the standard deviation of the intensities in the original image. More-\\nover, the entropy of the prediction error, as estimated using Eq. (8-7), is signiﬁcantly less than the esti-\\nmated entropy of the original image (3.99 \\nbits pixel\\n as opposed to 7.25 \\nbits pixel\\n). This decrease in \\nentropy reﬂects removal of a great deal of spatial redundanc\\ny, despite the fact that for \\nk\\n-bit images, \\n()\\nk\\n+\\n1-\\nb i t\\n numbers are needed to represent accurately the prediction error sequence \\nexy\\n(,\\n) .\\n (Note that \\nthe variable-length encoded prediction error is the compressed image\\n.) In general, the maximum com-\\npression of a predictive coding approach can be estimated by dividing the average number of bits used \\nb\\na\\nFIGURE 8.30\\nA lossless predic-\\ntive coding model: \\n(a) encoder;  \\n(b) decoder.\\n/H11001\\n/H11002\\nCompressed\\nsequence\\n/H11001\\n/H11001\\nf\\n(\\nn\\n)\\nf\\n(\\nn\\n)\\nf\\n(\\nn\\n)\\nf\\n(\\nn\\n)\\ne\\n(\\nn\\n)\\ne\\n(\\nn\\n)\\n!\\n!\\nInput\\nsequence\\nCompressed\\nsequence\\nSymbol\\nencoder\\nNearest\\ninteger\\nPredictor\\nSymbol\\ndecoder\\nDecompressed\\nsequence\\nPredictor\\nDIP4E_GLOBAL_Print_Ready.indb   596\\n6/16/2017   2:11:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 597}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n597\\nb a\\nd c\\nFIGURE 8.31\\n(a) A view of the \\nEarth from an \\norbiting space \\nshuttle. (b) The \\nintensity histo-\\ngram of (a).  \\n(c) The predic-\\ntion error image \\nresulting from \\nEq. (8-33).  \\n(d) A histogram \\nof the prediction \\nerror. \\n(Original image \\ncourtesy of \\nNASA.)\\nto represent each pixel in the original image by an estimate of the entropy of the prediction error. In this \\nexample, any variable-length coding procedure can be used to code \\nexy\\n(,\\n) ,\\n but the resulting compres-\\nsion will be limited to about \\n83 9 9\\n.,\\n or 2:1.\\nT\\nhe preceding example illustrates that the compression achieved in predictive \\ncoding is related directly to the entropy reduction that results from mapping an input \\nimage into a prediction error sequence, often called a \\nprediction residual\\n. Because \\nspatial redundancy is removed by the prediction and differencing process, the prob-\\nability density function of the prediction residual is, in general, highly peaked at zero, \\nand characterized by a relatively small (in comparison to the input intensity distribu-\\ntion) variance. In fact, it is often modeled by a zero mean uncorrelated Laplacian \\nPDF\\n \\npe e\\ne\\ne\\ne\\ne\\n()\\n=\\n1\\n2\\n2\\ns\\ns\\n−\\n \\n(8-34)\\nwhere \\ns\\ne\\n is the standard deviation of \\ne\\n.\\nNumber of pixels (\\n:\\n 1,000)\\nIntensity\\nStd. dev. = 45.60\\nEntropy = 7.25\\n0 50 100 150 200 300 250\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nNumber of pixels (\\n:\\n 10,000)\\nPrediction error\\nStd. dev. = 15.58\\nEntropy = 3.99\\n-\\n300\\n-\\n200\\n-\\n100 0 100 300 200\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n2.0\\nDIP4E_GLOBAL_Print_Ready.indb   597\\n6/16/2017   2:11:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 598}),\n",
       " Document(page_content='598\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nEXAMPLE 8.20 :  Predictive coding and temporal redundancy.\\nThe image in Fig. 8.31(a) is a portion of a frame of NASA video in which the Earth is moving from left \\nto right with respect to a stationary camera attached to the space shuttle. It is repeated in Fig. 8.32(b), \\nalong with its immediately preceding frame in Fig. 8.32(a). Using the ﬁrst-order linear predictor\\n \\nf xyt\\nf xyt\\nˆ\\n,,\\n,,\\n()\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\nround\\na\\n−\\n1\\n \\n(8-35)\\nwith \\na\\n=\\n1,\\n the intensities of the pixels in Fig. 8.32(b) can be predicted from the corresponding pix-\\nels in (a).\\n Figure 8.34(c) is the resulting prediction residual image, \\nexyt f xyt f xyt\\n(,\\n, ) (,, ) (,, ) .\\n=−\\nˆ\\n  \\nF\\nigure 8.31(d) is the histogram of \\nexyt\\n(,\\n, ) .\\n Note there is very little prediction error. The standard devia-\\ntion of the error is much smaller than in the previous example:\\n 3.76 \\nbits pixel\\n as opposed to 15.58 \\nbits pixel.\\n In addition, the entropy of the prediction error [computed using Eq. (8-7)] has decreased from \\n3.99 to 2.59 \\nbits pixel.\\n (Recall again that the variable-length encoded prediction error is the compressed \\nb a\\nd c\\nFIGURE 8.32\\n(a) and (b) Two \\nviews of Earth \\nfrom an orbit-\\ning space shuttle \\nvideo. (c) The \\nprediction error \\nimage resulting \\nfrom Eq. (8-35). \\n(d) A histogram \\nof the prediction \\nerror. \\n(Original images \\ncourtesy of \\nNASA.)\\nNumber of pixels (\\n:\\n 10,000)\\nPrediction error\\nStd. dev. = 3.76\\nEntropy = 2.59\\n-\\n100\\n-\\n80\\n-\\n60\\n0 20\\n80\\n40\\n0\\n1\\n2\\n3\\n4\\n5\\n60\\n-\\n40\\n-\\n20\\n6\\n7\\nDIP4E_GLOBAL_Print_Ready.indb   598\\n6/16/2017   2:11:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 599}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n599\\nimage.) By variable-length coding the resulting prediction residual, the original image is compressed \\nby approximately \\n82 5 9\\n.\\n or 3.1:1, a 50% improvement over the 2:1 compression obtained using the  \\nspatially oriented previous pixel predictor in Example 8.19.\\nMOTION COMPENSATED PREDICTION RESIDUALS\\nAs you saw in Example 8.20, successive frames in a video sequence often are very \\nsimilar. Coding their differences can reduce temporal redundancy and provide sig-\\nnificant compression. However, when a sequence of frames contains rapidly moving \\nobjects—or involves camera zoom and pan, sudden scene changes, or fade-ins and \\nfade-outs—the similarity between neighboring frames is reduced, and compression \\nis affected negatively. That is, like most compression techniques (see Example 8.5), \\ntemporally based predictive coding works best with certain kinds of inputs, namely, \\na sequence of images with significant temporal redundancy. When used on images \\nwith little temporal redundancy, data expansion can occur. Video compression sys-\\ntems avoid the problem of data expansion in two ways:\\n1. \\nBy tracking object movement and compensating for it during the prediction \\nand differencing process\\n.\\n2. \\nBy switching to an alternate coding method when there is insufﬁcient \\ninter\\n-\\nframe\\n correlation (similarity between frames) to make predictive coding advan-\\ntageous.\\nThe first of these, called \\nmotion compensation\\n, is the subject of the remainder of this \\nsection. Before proceeding, however, we should note that when there is insufficient \\ninterframe correlation to make predictive coding effective, the second problem is \\ntypically addressed using a block-oriented 2-D transform, like JPEG’s DCT-based \\ncoding (see the previous section). Frames compressed in this way (i.e., without a \\nprediction residual) are called \\nintraframes\\n or \\nIndependent frames\\n (\\nI-frames\\n). They \\ncan be decoded without access to other frames in the video to which they belong. \\nI-frames usually resemble JPEG encoded images, and are ideal starting points for \\nthe generation of prediction residuals. Moreover, they provide a high degree of ran-\\ndom access, ease of editing, and resistance to the propagation of transmission error. \\nAs a result, all standards require the periodic insertion of I-frames into the com-\\npressed video codestream.\\nFigure 8.33 illustrates the basics of motion-compensated predictive coding. Each \\nvideo frame is divided into non-overlapping rectangular regions (typically of size \\n44\\n×\\n to \\n16 16\\n×\\n) called \\nmacrobloc\\nks\\n. (Only one macroblock is shown in Fig. 8.33.) \\nThe “movement” of each macroblock with respect to its “most likely” position in \\nthe previous (or subsequent) video frame, called the \\nreference frame\\n, is encoded \\nin a \\nmotion vector\\n. The vector describes the motion by deﬁning the horizontal and \\nvertical \\ndisplacement\\n from the “most likely” position. The displacements typically \\nare speciﬁed to the nearest pixel, ½ pixel, or ¼ pixel precision. If subpixel precision \\nis used, the predictions must be interpolated [e.g., using bilinear interpolation (see \\nSection 2.4)] from a combination of pixels in the reference frame. An encoded frame \\nthat is based on the previous frame (a \\nforward prediction\\n in Fig. 8.33) is called a \\nPre-\\nThe “most likely” \\nposition is the one that \\nminimizes an error \\nmeasure between the \\nreference macroblock \\nand the macroblock \\nbeing encoded. The two \\nblocks do not have to \\nbe representations of \\nthe same object, but \\nthey must minimize the \\nerror measure.\\nDIP4E_GLOBAL_Print_Ready.indb   599\\n6/16/2017   2:11:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 600}),\n",
       " Document(page_content='600\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\ndictive frame\\n (\\nP-frame\\n); one that is based on the subsequent frame (a \\nbackward pre-\\ndiction\\n in Fig. 8.33) is called a \\nBidirectional frame\\n (\\nB-frame\\n). B-frames require the \\ncompressed codestream to be reordered so that frames are presented to the decoder \\nin the proper decoding sequence, rather than the natural display order.\\nAs you might expect, \\nmotion estimation\\n is the key component of motion compen-\\nsation. During motion estimation, the motion of objects is measured and encoded \\ninto motion vectors. The search for the “best” motion vector requires that a criterion \\nof optimality be deﬁned. For example, motion vectors may be selected on the basis \\nof maximum correlation or minimum error between macroblock pixels and the pre-\\ndicted pixels (or interpolated pixels for sub-pixel motion vectors) from the chosen \\nreference frame. One of the most commonly used error measures is \\nmean absolute \\ndistortion\\n (\\nMAD\\n)\\n \\nMAD x y\\nmn\\nf x i y j p x i dx y j dy\\nj\\nn\\ni\\nm\\n,, ,\\n()\\n=+ +\\n()\\n++ + +\\n()\\n= =\\n∑ ∑\\n1\\n0\\n1\\n0\\n1\\n−\\n− −\\n \\n(8-36)\\nwhere \\nx\\n and \\ny\\n are the coordinates of the upper\\n-left pixel of the \\nmn\\n×\\n macroblock \\nbeing coded,\\n \\ndx\\n and \\ndy\\n are displacements from the reference frame as shown in \\nFig. 8.33, and \\np\\n is an array of predicted macroblock pixel values. For sub-pixel \\nmotion vector estimation, \\np\\n is interpolated from pixels in the reference frame. Typi-\\ncally, \\ndx\\n and \\ndy\\n must fall within a limited search region (see Fig. 8.33) around each \\nmacroblock. Values from \\n±\\n8\\n to \\n±\\n64\\n pixels are common, and the horizontal search \\narea is often slightly larger than the vertical area.\\n A more computationally efficient \\nerror measure, called the \\nsum of absolute distortions\\n (\\nSAD\\n), omits the \\n1\\nmn\\n factor \\nin Eq.\\n (8-36).\\nGiven a selection criterion like that of Eq. (8-36), motion estimation is performed \\nby searching for the \\ndx\\n and \\ndy\\n that minimize \\nMAD x y\\n(,\\n)\\n over the allowed range of \\nmotion vector displacements\\n, including subpixel displacements. This process often is \\ncalled \\nblock matching\\n. An exhaustive search guarantees the best possible result, but \\nis computationally expensive because every possible motion must be tested over the \\nentire displacement range. For \\n1\\n6\\n×16\\n macroblocks and a \\n±\\n32\\n pixel displacement \\nFIGURE 8.33\\nMacroblock \\nmotion speciﬁca-\\ntion.\\nCurrent image\\nSearch region\\nMacroblock\\nForward prediction image\\nDisplacement\\nBackward prediction image\\nDisplacement\\nTime\\nMotion vector Motion vector\\ndx\\ndy\\nDIP4E_GLOBAL_Print_Ready.indb   600\\n6/16/2017   2:11:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 601}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n601\\nrange (not out of the question for action ﬁlms and sporting events), 4225 \\n16 16\\n×\\n \\nMAD\\n calculations must be performed for each macroblock in a frame when integer \\ndisplacement precision is used.\\n If ½ or ¼ pixel precision is desired, the number of \\ncalculations is multiplied by a factor of 4 or 16, respectively. Fast search algorithms \\ncan reduce the computational burden, but may or may not yield optimal motion \\nvectors. A number of fast block-based motion estimation algorithms have been pro-\\nposed and studied in the literature (see Furht et al. [1997] or Mitchell et al. [1997]).\\nEXAMPLE 8.21 :  Motion compensated prediction.\\nFigures 8.34(a) and (b) were taken from the same NASA video sequence used in Examples 8.19 and \\n8.20. Figure 8.34(b) is identical to Figs. 8.31(a) and 8.32(b); Fig. 8.34(a) is the corresponding section of a \\nframe occurring thirteen frames earlier. Figure 8.34(c) is the difference between the two frames, scaled \\nto the full intensity range. Note that the difference is 0 in the area of the stationary (with respect to the \\ncamera) space shuttle, but there are signiﬁcant differences in the remainder of the image due to the \\nrelative motion of the Earth. The standard deviation of the prediction residual in Fig. 8.34(c) is 12.73 \\nintensity levels; its entropy [using Eq. (8-7)] is 4.17 \\nbits pixel.\\n The maximum compression achievable, \\nwhen variable-length coding the prediction residual,\\n is \\nC\\n==\\n8 4 17 1 92\\n.. .\\nFigure 8.34(d) shows a motion compensated prediction residual with a much lower standard devia-\\ntion (5.62 as opposed to 12.73 intensity levels) and slightly lower entropy (3.04 vs\\n. 4.17 \\nbits pixel\\n). The \\nentropy was computed using Eq.\\n (8-7). If the prediction residual in Fig. 8.34(d) is variable-length coded, \\nthe resulting compression ratio is \\nC\\n==\\n8 3 04 2 63\\n.. .\\n To generate this prediction residual, we divided \\nF\\nig. 8.34(b) into non-overlapping \\n16 16\\n×\\n macroblocks and compared each macroblock against every \\n16 16\\n×\\n region in Fig. 8.34(a)—the reference frame—that fell within \\n±\\n16\\n pixels of the macroblock’s posi-\\ntion in (b).\\n We then used Eq. (8-36) to determine the best match by selecting displacement \\n(,)\\ndx\\ndy\\n with \\nthe lowest \\nMAD\\n.\\n The resulting displacements are the \\nx\\n and \\ny\\n components of the motion vectors shown \\nin Fig. 8.34(e). The white dots in the ﬁgure are the heads of the motion vectors; they indicate the upper-\\nleft-hand corner of the coded macroblocks. As you can see from the pattern of the vectors, the predomi-\\nnant motion in the image is from left to right. In the lower portion of the image, which corresponds to \\nthe area of the space shuttle in the original image, there is no motion, and therefore no motion vectors \\ndisplayed. Macroblocks in this area are predicted from similarly located (i.e., the corresponding) macro-\\nblocks in the reference frame. Because the motion vectors in Fig. 8.34(e) are highly correlated, they can \\nbe variable-length coded to reduce their storage and transmission requirements\\nFigure 8.35 illustrates the increased prediction accuracy that is possible with sub-\\npixel motion compensation. Figure 8.35(a) is repeated from Fig. 8.34(c) and included \\nas a point of reference; it shows the prediction error that results without motion \\ncompensation. The images in Figs. 8.35(b), (c), and (d) are motion compensated pre-\\ndiction residuals. They are based on the same two frames that were used in Exam-\\nple 8.21 and computed with macroblock displacements to 1, ½, and ¼ pixel resolu-\\ntion (i.e., precision), respectively. Macroblocks of size \\n88\\n×\\n were used; displacements \\nwere limited to \\n±\\n8 pixels.\\nT\\nhe most signiﬁcant visual difference between the prediction residuals in Fig. 8.35 \\nis the number and size of intensity peaks and valleys—their darkest and lightest \\nareas of intensity. The ¼ pixel residual in Fig. 8.35(d) is the “ﬂattest” of the four \\nThe visual difference \\nbetween Figs. 8.34(c) \\nand 8.35(a) is due to \\nscaling. The image \\nin Fig. 8.35(a) has \\nbeen scaled to match \\nFigs. 8.35(b)–(d).\\nDIP4E_GLOBAL_Print_Ready.indb   601\\n6/16/2017   2:11:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 602}),\n",
       " Document(page_content='602\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nimages, with the fewest excursions to black or white. As would be expected, it has \\nthe narrowest histogram. The standard deviations of the prediction residuals in \\nFigs. 8.35(a) through (d) decrease as motion vector precision increases from 12.7 \\nto 4.4, 4, and 3.8 pixels, respectively. The entropies of the residuals, as determined \\nusing Eq. (8-7), are 4.17, 3.34, 3.35, and 3.34 \\nbits pixel,\\n respectively. Thus, the motion \\ncompensated residuals contain about the same amount of information,\\n despite the \\nfact that the residuals in Figs. 8.35(c) and (d) use additional bits to accommodate ½ \\nand ¼ pixel interpolation. Finally, we note that there is an obvious strip of increased \\nprediction error on the left side of each motion compensated residual. This is due \\nto the left-to-right motion of the Earth, which introduces new or previously unseen \\nareas of the Earth’s terrain into the left side of each image. Because these areas are \\nabsent from the previous frames, they cannot be accurately predicted, regardless of \\nthe precision used to compute motion vectors.\\nb a\\nc\\ne\\nd\\nFIGURE 8.34\\n (a) and (b) Two views of Earth that are thirteen frames apart in an orbiting space shuttle video. (c) A \\nprediction error image without motion compensation. (d) The prediction residual with motion compensation. (e) \\nThe motion vectors associated with (d). The white dots in (e) represent the arrow heads of the motion vectors that \\nare depicted. (Original images courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   602\\n6/16/2017   2:11:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 603}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n603\\nMotion estimation is a computationally demanding task. Fortunately, only the \\nencoder must estimate macroblock motion. Given the motion vectors of the macro-\\nblocks, the decoder simply accesses the areas of the reference frames that were used \\nin the encoder to form the prediction residuals. Because of this, motion estimation \\nis not included in most video compression standards. Compression standards focus \\non the decoder, placing constraints on macroblock dimensions, motion vector preci-\\nsion, horizontal and vertical displacement ranges, and the like. Table 8.12 gives the \\nkey predictive coding parameters of some the most important video compression \\nstandards. Note that most of the standards use an \\n88\\n×\\n DCT for I-frame encod-\\ning\\n, but specify a larger area (i.e., \\n16 16\\n×\\n macroblock) for motion compensation. In \\naddition,\\n even the P- and B-frame prediction residuals are transform coded due to \\nthe effectiveness of DCT coefﬁcient quantization. Finally, we note that the H.264 \\nand MPEG-4 A VC standards support intraframe predictive coding (in I-frames) to \\nreduce spatial redundancy.\\nb a\\nd c\\nFIGURE 8.35\\nSub-pixel motion \\ncompensated \\nprediction residu-\\nals: (a) without \\nmotion compen-\\nsation; (b) single \\npixel precision;  \\n(c) ½ pixel preci-\\nsion; and  \\n(d) ¼ pixel preci-\\nsion. (All predic-\\ntion errors have \\nbeen scaled to \\nthe full intensity \\nrange and then \\nmultiplied by 2 \\nto increase their \\nvisibility.)\\nDIP4E_GLOBAL_Print_Ready.indb   603\\n6/16/2017   2:11:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 604}),\n",
       " Document(page_content='604\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nFigure 8.36 shows a typical motion compensated video encoder. It exploits redun-\\ndancies within and between adjacent video frames, motion uniformity between \\nframes, and the psychovisual properties of the human visual system. We can think \\nof the input to the encoder as sequential macroblocks of video. For color video, \\neach macroblock is composed of a luminance block and two chrominance blocks. \\nBecause the eye has far less spatial acuity for color than for luminance, the chromi-\\nnance blocks often are sampled at half the horizontal and vertical resolution of the \\nluminance block. The dark blue elements in the ﬁgure parallel the transformation, \\nquantization, and variable-length coding operations of a JPEG encoder. The princi-\\npal difference is the input, which may be a conventional macroblock of image data \\n(for I-frames), or the difference between a conventional macroblock and a predic-\\ntion of it based on previous and/or subsequent video frames (for P- and B-frames). \\nThe encoder includes an \\ninverse quantizer\\n and \\ninverse mapper\\n (e.g., inverse DCT) so \\nthat its predictions match those of the complementary decoder. Also, it is designed \\nto produce compressed bit streams that match the capacity of the intended video \\nchannel. To accomplish this, the quantization parameters are adjusted by a \\nrate con-\\ntroller\\n as a function of the occupancy of an output \\nbuffer\\n. As the buffer becomes \\nfuller, the quantization is made coarser, so fewer bits stream into the buffer.\\nQuantization as \\ndeﬁned earlier in the \\nchapter is irreversible. \\nThe “inverse quantizer” \\nin Fig. 8.36 does not \\nprevent information \\nloss.\\nParameter H.261 MPEG-1\\nH.262\\nMPEG-2\\nH.263 MPEG-4\\nVC-1\\nWMV-9\\nH.264\\nMPEG-4\\nAV C\\nMotion vector \\nprecision\\n1½ ½ ½ ¼ ¼ ¼\\nMacroblock \\nsizes\\n16 16\\n×\\n16 16\\n×\\n16 16\\n×\\n \\n16 8\\n×\\n16 16\\n×\\n \\n88\\n×\\n16 16\\n×\\n \\n88\\n×\\n16 16\\n×\\n \\n88\\n×\\n16 16\\n×\\n \\n16 8\\n×\\n \\n88\\n×\\n \\n84\\n×\\n \\n48\\n×\\n \\n44\\n×\\nTransform\\n88\\n×\\n \\nDCT\\n88\\n×\\n \\nDCT\\n88\\n×\\n \\nDCT\\n88\\n×\\n \\nDCT\\n88\\n×\\n \\nDCT\\n88\\n×\\n \\n84\\n×\\n \\n48\\n×\\n \\n44\\n×\\n \\nInteger \\nDCT\\n44\\n×\\n \\n88\\n×\\n \\nInteger\\nInterframe \\npredictions\\nP\\nP, B P, B P, B P, B P, B P, B\\nI-frame intra-\\npredictions\\nNo No\\nNo\\nNo No No Yes\\nTABLE \\n8.12\\nPredictive coding in video compression standards.\\nDIP4E_GLOBAL_Print_Ready.indb   604\\n6/16/2017   2:11:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 605}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n605\\nEXAMPLE 8.22 :  Video compression example.\\nWe conclude our discussion of motion compensated predictive coding with an example illustrating the \\nkind of compression that is possible with modern video compression methods. Figure 8.37 shows ﬁfteen \\nframes of a 1 minute HD \\n()\\n1280\\n720\\n×\\n full-color NASA video, parts of which have been used throughout \\nthis section.\\n Although the images shown are monochrome, the video is a sequence of 1,829 full-color \\nframes. Note that there are a variety of scenes, a great deal of motion, and multiple fade effects. For \\nexample, the video opens with a 150 frame fade-in from black, which includes frames 21 and 44 in \\nFig. 8.37, and concludes with a fade sequence containing frames 1595, 1609, and 1652 in Fig. 8.37, fol-\\nlowed by a ﬁnal fade to black. There are also several abrupt scene changes, like the change involving \\nframes 1303 and 1304 in Fig. 8.37.\\nAn H.264 compressed version of the NASA video stored as a Quicktime ﬁle (see Table 8.5) requires \\n44.56 MB of storage, plus another 1.39 MB for the associated audio. The video quality is excellent. About \\n5 GB of data would be needed to store the video frames as uncompressed full-color images. It should \\nbe noted that the video contains sequences involving both rotation and scale change (e.g., the sequence \\nincluding frames 959, 1023, and 1088 in Fig. 8.37). The discussion in this section, however, has been \\nlimited to translation alone. (See the book website for the NASA video segment used in this example.)\\nLOSSY PREDICTIVE CODING\\nIn this section, we add a quantizer to the lossless predictive coding model introduced \\nearlier, and examine the trade-off between reconstruction accuracy and compres-\\nsion performance within the context of spatial predictors. As Fig. 8.38 shows, the \\nquantizer, which replaces the nearest integer function of the error-free encoder, is \\ninserted between the symbol encoder and the point at which the prediction error is \\nformed. It maps the prediction error into a limited range of outputs, denoted \\n/dotnosp\\nen\\n()\\n,\\nwhich establish the amount of compression and distortion that occurs.\\nFIGURE 8.36\\nA typical motion \\ncompensated \\nvideo encoder.\\nPrediction macroblock\\nEncoded\\nmacroblock\\nImage\\nmacroblock\\nDifference\\nmacroblock\\nDecoded\\nmacroblock\\nEncoded\\nmotion\\nvector\\nVariable-length\\ncoding\\nQuantizer\\nMapper\\n(e.g., DCT)\\nRate\\ncontroller\\nBuffer\\nInverse\\nquantizer\\nInverse\\nMapper\\n(e.g., DCT\\n-\\n1\\n)\\nVariable-length\\ncoding\\nMotion estimator and\\ncompensator w/frame delay\\n/H11001\\n/H11002\\n/H11001\\n/H11001\\nDIP4E_GLOBAL_Print_Ready.indb   605\\n6/16/2017   2:11:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 606}),\n",
       " Document(page_content='606\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nFIGURE 8.37\\n Fifteen frames from an 1829-frame, 1-minute NASA video. The original video is in HD full color. \\n(Courtesy of NASA.)\\nFrame 0021\\nFrame 0044\\nFrame 0201\\nFrame 0266 Frame 0424 Frame 0801\\nFrame 0959 Frame 1023 Frame 1088\\nFrame 1224 Frame 1303 Frame 1304\\nFrame 1595 Frame 1609 Frame 1652\\nDIP4E_GLOBAL_Print_Ready.indb   606\\n6/16/2017   2:11:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 607}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n607\\nIn order to accommodate the insertion of the quantization step, the error-free \\nencoder of Fig. 8.30(a) must be altered so the predictions generated by the encoder \\nand decoder are equivalent. As Fig. 8.38(a) shows, this is accomplished by placing \\nthe lossy encoder’s predictor within a feedback loop, where its input, denoted as \\n/dotnosp\\nfn\\n()\\n,\\n is generated as a function of past predictions and the corresponding quantized \\nerrors\\n. That is,\\n \\n/dotnosp\\n/dotnosp\\nfn\\ne n\\nfn\\n()\\n=\\n()\\n+\\n()\\nˆ\\n(8-37)\\nwhere \\nˆ\\n()\\nfn\\n is as defined earlier. This closed loop configuration prevents error \\nbuildup at the decoder’\\ns output. Note in Fig. 8.38(b) that the output of the decoder \\nis given also by Eq. (8-37).\\nEXAMPLE 8.23 :  Delta modulation.\\nDelta modulation\\n (DM) is a simple but well-known form of lossy predictive coding in which the predic-\\ntor and quantizer are deﬁned as\\n \\nfn fn\\nˆ\\n()\\n=\\n()\\na\\n/dotnosp\\n−\\n1\\n \\n(8-38)\\nand\\n \\n/dotnosp\\nen\\nen\\n()\\n=\\n()\\n>\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n+\\n−\\nz\\nz\\nfor\\notherwise\\n0\\n(8-39)\\nwhere \\na\\n is a prediction coefﬁcient (normally less than 1), and \\nz\\n is a positive constant. The output of the \\nquantizer\\n, \\n/dotnosp\\nen\\n()\\n,\\n can be represented by a single bit [see Fig. 8.39(a)], so the symbol encoder of Fig. 8.38(a) \\ncan utilize a 1-bit ﬁxed-length code\\n. The resulting DM code rate is 1 \\nbit pixel.\\nFigure 8.39(c) illustrates the mechanics of the delta modulation process, where the calculations \\nneeded to compress and reconstruct input sequence {14,\\n 15, 14, 15, 13, 15, 15, 14, 20, 26, 27, 28, 27, 27, 29, \\n37, 47, 62, 75, 77, 78, 79, 80, 81, 81, 82, 82} with \\na\\n=\\n1\\n and \\nz\\n=\\n65\\n.\\n are tabulated. The process begins with the \\nb\\na\\nFIGURE 8.38\\nA lossy predictive \\ncoding model:  \\n(a) encoder;  \\n(b) decoder.\\n/H11001\\n/H11002\\n/H11001\\n/H11001\\n/H11001\\n/H11001\\nInput\\nsequence\\nCompressed\\nsequence\\nCompressed\\nsequence\\nDecompressed\\nsequence\\nSymbol\\nencoder\\nQuantizer\\nPredictor\\nSymbol\\ndecoder\\nPredictor\\n/dotnosp\\nen\\n()\\n/dotnosp\\nen\\n()\\n/dotnosp\\nfn\\n()\\nˆ\\n()\\nfn\\nˆ\\n()\\nfn\\n/dotnosp\\nfn\\n()\\nf\\n(\\nn\\n)\\ne\\n(\\nn\\n)\\nDIP4E_GLOBAL_Print_Ready.indb   607\\n6/16/2017   2:11:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 608}),\n",
       " Document(page_content='608\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nerror-free transfer of the ﬁrst input sample to the decoder. With the initial condition \\n/dotnosp\\nff\\n()\\n()\\n00 1 4\\n==\\nestablished at both the encoder and decoder, the remaining outputs can be computed by repeatedly \\nevaluating Eqs\\n. (8-38), (8-29), (8-39), and (8-37). Thus, when \\nn\\n = 1, for example, \\nˆ\\n()\\n() ( ) ,\\nf\\n1 1 14 14\\n==\\ne\\n()\\n,\\n11 5 1 4 1\\n==\\n−\\n \\n/dotnosp\\ne\\n()\\n.\\n16 5\\n=+\\n (because \\ne\\n()\\n10\\n>\\n), \\n/dotnosp\\nf\\n()\\n. .,\\n16 4 1 4 2 0 5\\n==\\n+\\n and the resulting reconstruction \\nerror is \\n(. ) ,\\n15\\n20 5\\n−\\n or \\n−\\n55\\n..\\nFigure 8.39(b) graphically shows the tabulated data in Fig. 8.39(c). Both the input and completely \\ndecoded output \\nfn fn\\n() ()\\n and \\n/dotnosp\\n⎡\\n⎣\\n⎤\\n⎦\\n are shown. Note that in the rapidly changing area from \\nn\\n = 14 to 19, \\nwhere \\nz\\n was too small to represent the input’s largest changes, a distortion known as \\nslope o\\nverload\\n \\noccurs. Moreover, when \\nz\\n was too large to represent the input’s smallest changes, as in the relatively \\nsmooth region from \\nn\\n = 0 to \\nn\\n = 7,\\n \\ngranular noise\\n appears. In images, these two phenomena lead to \\nblurred object edges and grainy or noisy surfaces (that is, distorted smooth areas).\\nThe distortions noted in the preceding example are common to all forms of lossy \\npredictive coding. The severity of these distortions depends on a complex set of \\ninteractions between the quantization and prediction methods employed. Despite \\nthese interactions, the predictor normally is designed with the assumption of no \\nquantization error, and the quantizer is designed to minimize its own error. That is, \\nthe predictor and quantizer are designed independently of each other.\\nb a\\nc\\nFIGURE 8.39\\nAn example of \\ndelta modulation.\\nCode = 1\\nCode = 0\\n+ \\n6.5\\n- \\n6.5\\nGranular noise\\nSlope overload\\nfn fn\\n() ()\\n−\\n/dotnosp\\n/dotnosp\\nfn\\n()\\nf\\n(\\nn\\n)\\ne\\n(\\nn\\n)\\n/dotnosp\\nen\\n()\\nnf\\n(\\nn\\n)\\nˆ\\n()\\nfn e\\n(\\nn\\n)\\n/dotnosp\\nen\\n()\\n/dotnosp\\nfn\\n()\\nˆ\\n()\\nfn\\n/dotnosp\\nfn\\n()\\nInput\\nEncoder\\nDecoder\\nError\\n0\\n1\\n2\\n3\\n.\\n.\\n14\\n15\\n16\\n17\\n18\\n19\\n.\\n.\\n14\\n15\\n14\\n15\\n.\\n.\\n29\\n37\\n47\\n62\\n75\\n77\\n.\\n.\\n-\\n14.0\\n20.5\\n14.0\\n.\\n.\\n20.5\\n27.0\\n33.5\\n40.0\\n46.5\\n53.0\\n.\\n.\\n-\\n1.0\\n- \\n6.5\\n1.0\\n.\\n.\\n8.5\\n10.0\\n13.5\\n22.0\\n28.5\\n24.0\\n.\\n.\\n-\\n6.5\\n- \\n6.5\\n6.5\\n.\\n.\\n6.5\\n6.5\\n6.5\\n6.5\\n6.5\\n6.5\\n.\\n.\\n14.0\\n20.5\\n14.0\\n20.5\\n.\\n.\\n27.0\\n33.5\\n40.0\\n46.5\\n53.0\\n59.6\\n.\\n.\\n14.0\\n20.5\\n14.0\\n20.5\\n.\\n.\\n27.0\\n33.5\\n40.0\\n46.5\\n53.0\\n59.5\\n.\\n.\\n0.0\\n- \\n5.5\\n0.0\\n- \\n5.5\\n.\\n.\\n2.0\\n3.5\\n7.0\\n15.5\\n22.0\\n17.5\\n.\\n.\\n-\\n14.0\\n20.5\\n14.0\\n.\\n.\\n20.5\\n27.0\\n33.5\\n40.0\\n46.5\\n53.0\\n.\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   608\\n6/16/2017   2:11:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 609}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n609\\nOPTIMAL PREDICTORS\\nIn many predictive coding applications, the predictor is chosen to minimize the \\nencoder’s mean-squared prediction error\\n \\nEe n E fn f n\\n2\\n2\\n()\\n{}\\n=\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\n{}\\n−\\nˆ\\n \\n(8-40)\\nsubject to the constraint that\\n \\n/dotnosp\\n/dotnosp\\nf n en f n en f n\\nfn\\n()\\n=\\n()\\n+\\n()\\n≈\\n()\\n+\\n()\\n=\\n()\\nˆˆ\\n \\n(8-41)\\nand\\n \\nfn fn i\\ni\\ni\\nm\\nˆ\\n()\\n=\\n()\\n=\\n∑\\na\\n−\\n1\\n \\n(8-42)\\nThat is, the optimization criterion is minimal mean-squared prediction error, the \\nquantization error is assumed to be negligible \\n/dotnosp\\nen\\nen\\n()\\n()\\n≈\\n[]\\n, and the prediction is \\nconstrained to a linear combination of \\nm\\n previous samples. These restrictions are \\nnot essential, but they considerably simplify the analysis and, at the same time, \\ndecrease the computational complexity of the predictor. The resulting predictive \\ncoding approach is referred to as \\ndifferential pulse code modulation\\n (DPCM).\\nUnder these conditions, the optimal predictor design problem is reduced to the \\nrelatively straightforward exercise of selecting the \\nm\\n prediction coefﬁcients that \\nminimize the expression\\n \\nEe n E fn fn i\\ni\\ni\\nm\\n2\\n1\\n2\\n()\\n{}\\n=\\n() ( )\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n⎫\\n⎬\\n⎪\\n⎭\\n⎪\\n=\\n∑\\n−−\\na\\n \\n(8-43)\\nDifferentiating Eq. (8-43) with respect to each coefficient, equating the derivatives \\nto zero\\n, and solving the resulting set of simultaneous equations under the assump-\\ntion that \\nfn\\n()\\n has mean zero and variance \\ns\\n2\\n yields\\n \\nA\\n=\\nRr\\n−\\n1\\n \\n(8-44)\\nwhere \\nR\\n−\\n1\\n is the inverse of the \\nmm\\n×\\n autocorrelation matrix\\n \\nR\\n=\\n() ()\\n{}\\n() ()\\n{}\\n() ( )\\n{}\\n()\\nEfn fn Efn fn Efn fn m\\nEfn fn\\n− − −− −−\\n−−\\n11 1 2 1\\n2\\n/midhorizellipsis\\n1 1\\n12\\n()\\n{}\\n() ( )\\n{}\\n() ( )\\n{}\\n(\\n/vertellipsis/midhorizellipsis/vertellipsis\\n/vertellipsis/vertellipsis /midhorizellipsis /vertellipsis\\n/vertellipsis/vertellipsis /midhorizellipsis /vertellipsis\\n/vertellipsis/vertellipsis /midhorizellipsis /vertellipsis\\n/midhorizellipsis\\nEfn mfn Efn m fn Efn m\\n−− − − −\\n)\\n)( )\\n{}\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\nfn m\\n−\\n \\n  \\n(8-45)\\nThe notation \\nE\\ni\\n{}\\n \\ndenotes the statistical \\nexpectation operator.\\nIn general, the optimal \\npredictor for a non-\\nGaussian sequence is \\na nonlinear function \\nof the samples used to \\nform the estimate.\\nDIP4E_GLOBAL_Print_Ready.indb   609\\n6/16/2017   2:11:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 610}),\n",
       " Document(page_content='610\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nand \\nr\\n and \\nA\\n are the \\nm\\n-element vectors\\n \\nr\\n=\\n{}\\n{}\\n{}\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\nEf n f n\\nEf n f n\\nEf n f n m\\n()( )\\n()( )\\n()( )\\n−\\n−\\n−\\n1\\n2\\n/vertellipsis\\n     \\nA\\na\\na\\na\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n1\\n2\\n/vertellipsis\\nm\\n \\n(8-46)\\nThus for any input sequence, the coefficients that minimize Eq. (8-43) can be deter-\\nmined via a series of elementary matrix operations\\n. Moreover, the coefficients \\ndepend only on the autocorrelations of the samples in the original sequence. The \\nvariance of the prediction error that results from the use of these optimal coeffi-\\ncients is\\n \\nssA s\\na\\ne\\nT\\ni\\ni\\nm\\nEf n f n i\\n22 2\\n1\\n==\\n{}\\n=\\n∑\\n−− −\\nr\\n()( )\\n \\n(8-47)\\nAlthough the mechanics of evaluating Eq. (8-44) are quite simple, computation of \\nthe autocorrelations needed to form \\nR\\n and \\nr\\n is so difficult in practice that \\nlocal\\n \\npredictions (those in which the prediction coefficients are computed for each input \\nsequence) are almost never used.\\n In most cases, a set of \\nglobal\\n coefficients is com-\\nputed by assuming a simple input model and substituting the corresponding auto-\\ncorrelations into Eqs. (8-45) and (8-46). For instance, when a 2-D Markov image \\nsource (see Section 8.1) with separable autocorrelation function\\n \\nEfx y fx i y j\\nv\\ni\\nh\\nj\\n,,\\n() ( )\\n{}\\n=\\n−−\\nsrr\\n2\\n \\n(8-48)\\nand generalized fourth-order linear predictor\\n \\nfx y fx y fx y\\nfx y fx y\\nˆ\\n,, ,\\n,,\\n()\\n=\\n()\\n+\\n()\\n() ( )\\naa\\naa\\n12\\n34\\n11 1\\n11 1\\n−− −\\n+− + − +\\n \\n(8-49)\\nare assumed, the resulting optimal coefficients (Jain [1989]) are\\n \\nar a r r ar a\\n12 34\\n0\\n== ==\\nhv\\nh v\\n−\\n \\n(8-50)\\nwhere \\nr\\nh\\n and \\nr\\nh\\n are the horizontal and vertical correlation coefficients, respectively, \\nof the image under consideration.\\nFinally, the sum of the prediction coefﬁcients in Eq. (8-42) is normally required to \\nbe less than or equal to one. That is,\\n \\na\\ni\\ni\\nm\\n≤\\n1\\n1\\n=\\n∑\\n \\n(8-51)\\nDIP4E_GLOBAL_Print_Ready.indb   610\\n6/16/2017   2:11:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 611}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n611\\nThis restriction is made to ensure that the output of the predictor falls within the \\nallowed range of the input, and to reduce the impact of transmission noise [which \\ngenerally is seen as horizontal streaks in reconstructed images when the input to \\nFig. 8.38(a) is an image]. Reducing the DPCM decoder’s susceptibility to input noise \\nis important, because a single error (under the right circumstances) can propagate \\nto all future outputs. That is, the decoder’s output may become unstable. Further \\nrestricting Eq. (8-51) to be strictly less than 1 confines the impact of an input error \\nto a small number of outputs.\\nEXAMPLE 8.24 :  Comparison of prediction techniques.\\nConsider the prediction error that results from DPCM coding the monochrome image of Fig. 8.9(a) \\nunder the assumption of zero quantization error and with each of four predictors:\\n \\nfx y fx y\\nˆ\\n,.,\\n()\\n=\\n()\\n09 7 1\\n−\\n \\n(8-52)\\n \\nfx y fx y fx y\\nˆ\\n,. , . ,\\n()\\n=\\n()()\\n05 1 05 1\\n−+ −\\n \\n(8-53)\\n \\nfx y fx y fx y fx y\\nˆ\\n,., . , . ,\\n()\\n=\\n()\\n+\\n()( )\\n07 5 1 07 5 1 05 1 1\\n−− − − −\\n \\n(8-54)\\n \\nfx y\\nfx\\nyh v\\nfx y\\nˆ\\n,\\n.,\\n.,\\n()\\n=\\n()\\n()\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n09 7 1\\n09 7 1\\n−≤\\n−\\nif\\notherwise\\n/H9004/H9004\\n \\n(8-55)\\nwhere \\n/H9004\\nhf x yf x y\\n=\\n() ( )\\n−−− −\\n11 1\\n,,\\n and \\n/H9004\\nvf x y f x y\\n=\\n() ( )\\n,,\\n−− − −\\n11 1\\n denote the horizontal and \\nvertical gradients at point \\n(,) .\\nxy\\n Equations (8-52) through (8-55) deﬁne a relatively robust set of \\na\\ni\\n that \\nprovide satisfactory performance over a wide range of images. The adaptive predictor of Eq. (8-55) is \\ndesigned to improve edge rendition by computing a local measure of the directional properties of an \\nimage (\\n/H9004\\nh\\n and \\n/H9004\\nv\\n), and selecting a predictor speciﬁcally tailored to the measured behavior.\\nF\\nigures 8.40(a) through (d) show the prediction error images that result from using the predictors of \\nEqs. (8-52) through (8-55). Note that the visually perceptible error decreases as the order of the predic-\\ntor increases.\\n†\\n The standard deviations of the prediction errors follow a similar pattern. They are 11.1, 9.8, \\n9.1, and 9.7 intensity levels, respectively.\\nOPTIMAL QUANTIZATION\\nThe staircase quantization function \\ntq s\\n=\\n()\\n in Fig. 8.41 is an odd function of \\ns\\n [that is\\n, \\nqs q s\\n()\\n( )\\n−−\\n=\\n] that can be described completely by the \\nL\\n2\\n values of \\ns\\ni\\n and \\nt\\ni\\n shown \\nin the first quadrant of the graph. These break points define function discontinuities, \\nand are called the \\ndecision\\n and \\nreconstruction levels\\n of the quantizer. As a matter \\nof convention, \\ns\\n is considered to be mapped to \\nt\\ni\\n if it lies in the half-open interval \\n(, ] .\\nss\\nii\\n+\\n1\\nThe quantizer design problem is to select the best \\ns\\ni\\n and \\nt\\ni\\n for a particular opti-\\nmization criterion and input probability density function \\nps\\n()\\n.\\n If the optimization \\n† Predictors that use more than three or four previous pixels provide little compression gain for the added pre-\\ndictor complexity (Habibi [1971]).\\nDIP4E_GLOBAL_Print_Ready.indb   611\\n6/16/2017   2:11:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 612}),\n",
       " Document(page_content='612\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\ncriterion, which could be either a statistical or psychovisual measure,\\n†\\n is the minimi-\\nzation of the mean-squared quantization error \\nAEF B\\nt\\nhat is \\nEs t\\nii\\n()\\n−\\n2\\n and \\nps\\n()\\n is an \\neven function,\\n the conditions for minimal error (Max [1960]) are\\n \\ns\\ns\\ni\\ni\\nst p s d s i\\nL\\ni\\n−\\n−\\n1\\n01 2\\n2\\n2\\n() ( ) , , ,\\n==\\n…\\n \\n(8-56)\\n \\ns\\ni\\ntt\\ni\\nL\\ni\\nL\\ni\\nii\\n=\\n=\\n=\\n∞=\\n⎧\\n⎨\\n⎪\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n⎪\\n+\\n00\\n2\\n12\\n2\\n2\\n1\\n1\\n+\\n−\\n,, ,\\n…\\n \\n(8-57)\\n† See Netravali [1977] and Limb for more on psychovisual measures.\\nb a\\nd c\\nFIGURE 8.40\\nA comparison of \\nfour linear  \\nprediction  \\ntechniques.\\nDIP4E_GLOBAL_Print_Ready.indb   612\\n6/16/2017   2:11:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 613}),\n",
       " Document(page_content='8.10\\n  \\nPredictive Coding\\n    \\n613\\nand\\n \\nss tt\\nii i i\\n−−\\n−−\\n==\\n \\n(8-58)\\nEquation (8-56) indicates that the reconstruction levels are the centroids of the areas \\nunder \\nps\\n()\\n over the specified decision intervals, whereas Eq. (8-57) indicates that \\nthe decision levels are halfway between the reconstruction levels\\n. Equation (8-58) \\nis a consequence of the fact that \\nq\\n is an odd function. For any \\nL\\n, the \\ns\\ni\\n and \\nt\\ni\\n that \\nsatisfy Eqs. (8-56) through (8-58) are optimal in the mean-squared error sense; the \\ncorresponding quantizer is called an \\nL\\n-level \\nLloyd-Max\\n quantizer.\\nTable 8.13 lists the 2-, 4-, and 8-level Lloyd-Max decision and reconstruction \\nlevels for a unit variance Laplacian probability density function [see Eq. (8-34)]. \\nBecause obtaining an explicit or closed-form solution to Eqs. (8-56) through (8-58)\\nfor most nontrivial \\nps\\n()\\n is difﬁcult, these values were generated numerically (Paez \\nand Glisson [1972]).\\n The three quantizers shown provide ﬁxed output rates of 1, 2, \\nand 3 \\nbits pixel,\\n respectively. As Table 8.13 was constructed for a unit variance dis-\\ntribution,\\n the reconstruction and decision levels for the case of \\ns\\n≠\\n1\\n are obtained by \\nmultiplying the tabulated values by the standard deviation of the probability density \\nFIGURE 8.41\\nA typical  \\nquantization  \\nfunction.\\nt \\n= \\nq\\n(\\ns\\n)\\ns\\n1\\ns\\n2\\nt\\n1\\nt\\n2\\nt\\ns\\nOutput\\nInput\\ns\\nL\\n−−\\n()\\n21\\n[]\\ns\\nL\\n()\\n21\\n−\\n−\\nt\\nL\\n2\\nt\\nL\\n2\\nLevels\\n2\\n4\\n8\\ns\\ni\\nt\\ni\\ns\\ni\\nt\\ni\\ns\\ni\\nt\\ni\\n1\\n/H11009\\n0.707 1.102\\n0.395 0.504\\n0.222\\n2\\n/H11009\\n1.810 1.181\\n0.785\\n3\\n2.285\\n1.576\\n4\\n/H11009\\n2.994\\nu\\n1.414\\n1.087\\n0.731\\nTABLE \\n8.13\\nLloyd-Max  \\nquantizers for a  \\nLaplacian  \\nprobability  \\ndensity function \\nof unit variance.\\nDIP4E_GLOBAL_Print_Ready.indb   613\\n6/16/2017   2:11:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 614}),\n",
       " Document(page_content='614\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nfunction under consideration. The ﬁnal row of the table lists the step size, \\nu\\n,\\n that \\nsimultaneously satisﬁes Eqs\\n. (8-56) through (8-58) and the additional constraint that\\n \\ntt ss\\nii i i\\n−−\\n−−\\n11\\n==\\nu\\n \\n(8-59)\\nIf a symbol encoder that utilizes a variable-length code is used in the general lossy \\npredictive encoder of F\\nig. 8.38(a), an \\noptimum uniform quantizer\\n of step size \\nu\\n will \\nprovide a lower code rate (for a Laplacian PDF) than a fixed-length coded Lloyd-\\nMax quantizer with the same output fidelity (O’Neil [1971]).\\nAlthough the Lloyd-Max and optimum uniform quantizers are not adaptive\\n, \\nmuch can be gained from adjusting the quantization levels based on the local behav-\\nior of an image. In theory, slowly changing regions can be ﬁnely quantized, while the \\nrapidly changing areas are quantized more coarsely. This approach simultaneously \\nreduces both granular noise and slope overload, while requiring only a minimal \\nincrease in code rate. The trade-off is increased quantizer complexity.\\n8.11 WAVELET CODING  \\nAs with the block transform coding techniques presented earlier, wavelet coding is \\nbased on the idea that the coefficients of a transform that decorrelates the pixels of \\nan image can be coded more efficiently than the original pixels themselves. If the \\nbasis functions of the transform (in this case wavelets) pack most of the important \\nvisual information into a small number of coefficients, the remaining coefficients can \\nbe quantized coarsely or truncated to zero with little image distortion.\\nFigure 8.42 shows a typical wavelet coding system. To encode a \\n22\\nJJ\\n×\\n image, an \\nanalyzing wavelet,\\n \\nc\\n,\\n and minimum decomposition level, \\nJP\\n−\\n,\\n are selected and \\nused to compute the discrete wavelet transform of the image\\n. If the wavelet has \\na complementary scaling function \\nw\\n,\\n the fast wavelet transform (see Section 7.10) \\ncan be used.\\n In either case, the computed transform converts a large portion of the \\noriginal image to horizontal, vertical, and diagonal decomposition coefﬁcients with \\nzero mean and Laplacian-like probabilities. Because many of the computed coef-\\nﬁcients carry little visual information, they can be quantized and coded to minimize \\nintercoefﬁcient and coding redundancy. Moreover, the quantization can be adapted \\nto exploit any positional correlation across the \\nP\\n decomposition levels. One or more \\nlossless coding methods, such as run-length, Huffman, arithmetic, and bit-plane cod-\\ning, can be incorporated into the ﬁnal symbol coding step. Decoding is accomplished \\nby inverting the encoding operations, with the exception of quantization, which can-\\nnot be reversed exactly.\\nThe principal difference between the wavelet-based system of Fig. 8.42 and the \\ntransform coding system of Fig. 8.21 is the omission of the subimage processing \\nstages of the transform coder. Because wavelet transforms are both computation-\\nally efﬁcient and inherently local (i.e., their basis functions are limited in duration), \\nsubdivision of the original image is unnecessary. As you will see later in this section, \\nthe removal of the subdivision step eliminates the blocking artifact that character-\\nizes DCT-based approximations at high compression ratios.\\n8.11\\nWith reference to \\nTables 8.3–8.5, wavelet \\ncoding is used in the\\n• JPEG-2000\\ncompression standard.\\nDIP4E_GLOBAL_Print_Ready.indb   614\\n6/16/2017   2:11:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 615}),\n",
       " Document(page_content='8.11\\n  \\nWavelet Coding\\n    \\n615\\nWAVELET SELECTION\\nThe wavelets chosen as the basis of the forward and inverse transforms in Fig. 8.42 \\naffect all aspects of wavelet coding system design and performance. They impact \\ndirectly the computational complexity of the transforms and, less directly, the sys-\\ntem’s ability to compress and reconstruct images of acceptable error. When the \\ntransforming wavelet has a companion scaling function, the transformation can be \\nimplemented as a sequence of digital filtering operations, with the number of filter \\ntaps equal to the number of nonzero wavelet and scaling vector coefficients. The \\nability of the wavelet to pack information into a small number of transform coef-\\nficients determines its compression and reconstruction performance.\\nThe most widely used expansion functions for wavelet-based compression are \\nthe Daubechies wavelets and biorthogonal wavelets. The latter allow useful analysis \\nproperties, like the number of vanishing moments (see Section 7.10), to be incor-\\nporated into the decomposition ﬁlters, while important synthesis properties, like \\nsmoothness of reconstruction, are built into the reconstruction ﬁlters.\\nEXAMPLE 8.25 :  Wavelet bases in wavelet coding.\\nFigure 8.43 contains four discrete wavelet transforms of Fig. 8.9(a). Haar wavelets, the simplest and only \\ndiscontinuous wavelets considered in this example, were used as the expansion or basis functions in \\nFig. 8.43(a). Daubechies wavelets, among the most popular imaging wavelets, were used in Fig. 8.43(b), \\nand symlets, which are an extension of the Daubechies wavelets with increased symmetry, were used in \\nFig. 8.43(c). The Cohen-Daubechies-Feauveau wavelets employed in Fig. 8.43(d) are included to illus-\\ntrate the capabilities of biorthogonal wavelets. As in previous results of this type, all detail coefﬁcients \\nwere scaled to make the underlying structure more visible, with intensity 128 corresponding to coef-\\nﬁcient value 0.\\nAs you can see in Table 8.14, the number of operations involved in the computation of the transforms \\nin Fig. 8.43 increases from 4 to 28 multiplications and additions per coefﬁcient (for each decomposition \\nb\\na\\nFIGURE 8.42\\nA wavelet coding \\nsystem:  \\n(a) encoder;  \\n(b) decoder.\\nWavelet\\ntransform\\nQuantizer\\nSymbol\\nencoder\\nInput\\nimage\\nCompressed\\nimage\\nInverse\\nwavelet transform\\nCompressed\\nimage\\nDecompressed\\nimage\\nSymbol\\ndecoder\\nWavelet\\nFilter Taps\\n(Scaling + Wavelet)\\nZeroed \\nCoefﬁcients\\nHaar\\n2 + 2\\n33.3%\\nDaubechies\\n8 + 8\\n40.9%\\nSymlet\\n8 + 8\\n41.2%\\nBiorthogonal\\n17 + 11\\n42.1%\\nTABLE \\n8.14\\nWavelet trans-\\nform ﬁlter taps \\nand zeroed \\ncoefﬁcients when \\ntruncating the \\ntransforms  \\nin Fig. 8.43  \\nbelow 1.5.\\nDIP4E_GLOBAL_Print_Ready.indb   615\\n7/12/2017   10:34:15 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 616}),\n",
       " Document(page_content='616\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nlevel) as you move from Fig. 8.43(a) to (d). All four transforms were computed using a fast wavelet \\ntransform (i.e., ﬁlter bank) formulation. Note that as the computational complexity (i.e., the number \\nof ﬁlter taps) increases, the information packing performance does as well. When Haar wavelets are \\nemployed and the detail coefﬁcients below 1.5 are truncated to zero, 33.8% of the total transform is \\nzeroed. With the more complex biorthogonal wavelets, the number of zeroed coefﬁcients rises to 42.1%, \\nincreasing the potential compression by almost 10%.\\nDECOMPOSITION LEVEL SELECTION\\nAnother factor affecting wavelet coding computational complexity and reconstruc-\\ntion error is the number of transform decomposition levels. Because a \\nP\\n-scale fast \\nwavelet transform involves \\nP\\n filter bank iterations, the number of operations in \\nthe computation of the forward and inverse transforms increases with the num-\\nber of decomposition levels. Moreover, quantizing the increasingly lower-scale  \\nb a\\nd c\\nFIGURE 8.43\\nThree-scale wave-\\nlet transforms of \\nFig. 8.9(a) with \\nrespect to  \\n(a) Haar wavelets, \\n(b) Daubechies \\nwavelets,  \\n(c) symlets, \\nand (d) Cohen-\\nDaubechies-Feau-\\nveau biorthogonal \\nwavelets.\\nDIP4E_GLOBAL_Print_Ready.indb   616\\n6/16/2017   2:11:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 617}),\n",
       " Document(page_content='8.11\\n  \\nWavelet Coding\\n    \\n617\\nDecomposition Level\\n(Scales or Filter Bank \\nIterations)\\nApproximation \\nCoefﬁcient Image\\nTruncated  \\nCoefﬁcients (%)\\nReconstruction  \\nError (rms)\\n1\\n256 256\\n×\\n74.7%\\n3.27\\n2\\n1\\n28 128\\n×\\n91.7%\\n4.23\\n3\\n64 64\\n×\\n95.1%\\n4.54\\n4\\n32 32\\n×\\n95.6%\\n4.61\\n5\\n1\\n61 6\\n×\\n95.5%\\n4.63\\nTABLE \\n8.15\\nDecomposition \\nlevel impact on \\nwavelet coding \\nthe \\n512 512\\n×\\n  \\nimage of \\nF\\nig. 8.9(a).\\ncoefficients that result with more decomposition levels affects increasingly larger \\nareas of the reconstructed image. In many applications, like searching image data-\\nbases or transmitting images for progressive reconstruction, the resolution of the \\nstored or transmitted images, and the scale of the lowest useful approximations, nor-\\nmally determine the number of transform levels.\\nEXAMPLE 8.26 :  Decomposition levels in wavelet coding.\\nTable 8.15 illustrates the effect of decomposition level selection on the coding of Fig. 8.9(a) using bior-\\nthogonal wavelets and a ﬁxed global threshold of 25. As in the previous wavelet coding example, only \\ndetail coefﬁcients are truncated. The table lists both the percentage of zeroed coefﬁcients and the result-\\ning rms reconstruction errors from Eq. (8-10). Note that the initial decompositions are responsible for \\nthe majority of the data compression. There is little change in the number of truncated coefﬁcients \\nabove three decomposition levels.\\nQUANTIZER DESIGN\\nThe most important factor affecting wavelet coding compression and reconstruc-\\ntion error is coefficient quantization. Although the most widely used quantizers \\nare uniform, the effectiveness of the quantization can be improved significantly by  \\n(1) introducing a larger quantization interval around zero, called a \\ndead zone\\n, or \\n \\n(2) adapting the size of the quantization interval from scale to scale. In either case, \\nthe selected quantization intervals must be transmitted to the decoder with the \\nencoded image bit stream. The intervals themselves may be determined heuristically, \\nor computed automatically based on the image being compressed. For example, a \\nglobal coefficient threshold could be computed as the median of the absolute values \\nof the first-level detail coefficients or as a function of the number of zeroes that are \\ntruncated and the amount of energy that is retained in the reconstructed image.\\nEXAMPLE 8.27 :  Dead zone interval selection in wavelet coding.\\nFigure 8.44 illustrates the impact of dead zone interval size on the percentage of truncated detail coef-\\nﬁcients for a three-scale biorthogonal wavelet-based encoding of Fig. 8.9(a). As the size of the dead zone \\nincreases, the number of truncated coefﬁcients does as well. Above the knee of the curve (i.e., beyond 5),  \\nOne measure of the \\nenergy of a digital signal \\nis the sum of the squared \\nsamples.\\nDIP4E_GLOBAL_Print_Ready.indb   617\\n6/16/2017   2:11:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 618}),\n",
       " Document(page_content='618\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nthere is little gain. This is due to the fact that the histogram of the detail coefﬁcients is highly peaked \\naround zero.\\nThe rms reconstruction errors corresponding to the dead zone thresholds in Fig. 8.44 increase from \\n0 to 1.94 intensity levels at a threshold of 5, and to 3.83 intensity levels for a threshold of 18, where the \\nnumber of zeroes reaches 93.85%. If every detail coefﬁcient were eliminated, that percentage would \\nincrease to about 97.92% (by about 4%), but the reconstruction error would grow to 12.3 intensity levels.\\nJPEG-2000\\nJPEG-2000 extends the popular JPEG standard to provide increased flexibility in \\nboth the compression of continuous-tone still images and access to the compressed \\ndata. For example, portions of a JPEG-2000 compressed image can be extracted for \\nretransmission, storage, display, and/or editing. The standard is based on the wavelet \\ncoding techniques just described. Coefficient quantization is adapted to individual \\nscales and subbands, and the quantized coefficients are arithmetically coded on a \\nbit-plane basis (see Sections 8.4 and 8.8). Using the notation of the standard, an \\nimage is encoded as follows (ISO/IEC [2000]).\\nThe ﬁrst step of the encoding process is to DC level shift the samples of the \\nSsiz\\n-\\nbit unsigned image to be coded by subtracting \\n2\\n1\\nSsiz\\n−\\n.\\n If the image has more than one \\ncomponent, such as the red, green, and blue planes of a color image, each component \\nis shifted individually. If there are exactly three components, they may be optionally \\ndecorrelated using a reversible or nonreversible linear combination of the compo-\\nnents. The \\nirreversible component transform\\n of the standard, for example, is\\n \\nY xy I xy I xy I xy\\nYx\\ny\\n001 2\\n1\\n0 299 0 587 0 114\\n01\\n, . ,. ,. ,\\n,.\\n()\\n=\\n() () ()\\n()\\n=\\n++\\n−\\n6 6875 0 33126 0 5\\n05 0\\n01 2\\n20\\nIx y I x y I x y\\nYx y Ix y\\n,. ,. ,\\n,. ,\\n() () ()\\n()\\n=\\n()\\n−+\\n−\\n.. , . ,\\n41869 0 08131\\n12\\nIx y Ix y\\n() ()\\n−\\n \\n(8-60)\\nSsiz\\n is used in the stan-\\ndard to denote intensity \\nresolution.\\nThe irreversible com-\\nponent transform is the \\ncomponent transform \\nused for lossy compres-\\nsion. The component \\ntransform itself is not \\nirreversible. A different \\ncomponent transform \\nis used for reversible \\ncompression.\\nFIGURE 8.44\\nThe impact of \\ndead zone interval \\nselection on  \\nwavelet coding.\\n97.918%\\nCoefficient truncation (%)\\nDead zone threshold\\nRoot-mean-square error\\n(intensity levels)\\nRMSE\\n% Zeroes\\n80\\n70\\n60\\n50\\n40\\n30\\n20\\n10\\n90\\n0\\n100\\n02468 1 0 1 2 1 4 1 6 1 8\\n3.2\\n2.8\\n2.4\\n2\\n1.6\\n1.2\\n0.8\\n0.4\\n3.6\\n0\\n4.0\\nDIP4E_GLOBAL_Print_Ready.indb   618\\n6/16/2017   2:11:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 619}),\n",
       " Document(page_content='8.11\\n  \\nWavelet Coding\\n    \\n619\\nwhere \\nI\\n0\\n, \\nI\\n1\\n, and \\nI\\n2\\n are the level-shifted input components, and \\nY\\n0\\n, \\nY\\n1\\n, and \\nY\\n2\\n are \\nthe corresponding decorrelated components. If the input components are the red, \\ngreen, and blue planes of a color image, Eq. (8-60) approximates the \\n′′ ′\\nRG\\nB\\n to \\n′\\nYC\\nC\\nbr\\n color video transform (Poynton [1996]).\\n†\\n The goal of the transformation is to \\nimprove compression efficiency; transformed components \\nY\\n1\\n and \\nY\\n2\\n are difference \\nimages whose histograms are highly peaked around zero.\\nAfter the image has been level-shifted and optionally decorrelated, its compo-\\nnents can be divided into \\ntiles\\n. Tiles are rectangular arrays of pixels that are pro-\\ncessed independently. Because an image can have more than one component (e.g., it \\ncould be made up of three color components), the tiling process creates \\ntile compo-\\nnents\\n. Each tile component can be reconstructed independently, providing a simple \\nmechanism for accessing and/or manipulating a limited region of a coded image. For \\nexample, an image having a 16:9 aspect ratio could be subdivided into tiles so one \\nof its tiles is a subimage with a 4:3 aspect ratio. That tile then could be reconstructed \\nwithout accessing the other tiles in the compressed image. If the image is not subdi-\\nvided into tiles, it is a single tile.\\nThe 1-D discrete wavelet transform of the rows and columns of each tile compo-\\nnent is then computed. For error-free compression, the transform is based on a bior-\\nthogonal, 5/3 coefﬁcient scaling and wavelet vector (Le Gall and Tabatabai [1988]). \\nA rounding procedure is deﬁned for non-integer-valued transform coefﬁcients. In \\nlossy applications, a 9/7 coefﬁcient scaling-wavelet vector is employed (Antonini, \\nBarlaud, Mathieu, and Daubechies [1992]). In either case, the transform is computed \\nusing the fast wavelet transform of Section 7.10 or via a complementary \\nlifting-based \\napproach (Mallat [1999]). For example, in lossy applications, the coefﬁcients used to \\nconstruct the 9/7 FWT analysis ﬁlter bank are given in Table 7.1. The complementary \\nlifting-based implementation involves six sequential “lifting” and “scaling” opera-\\ntions:\\n \\nYn Xn Xn Xn i n i\\nYn\\nXn\\n21 21 2 22 3 21 3\\n22\\n01\\n+\\n()\\n=+\\n()\\n+\\n()\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤+ < +\\n()\\n=\\n(\\na\\n−\\n) )\\n+\\n()\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤< +\\n+\\n()\\n=+\\n()\\n+\\n()\\n+\\nb\\ng\\nYn Yn i ni\\nYn Yn Yn\\n21 21 2 2 2\\n21 21 2\\n01\\n−−\\nY\\nYn i n i\\nYn Yn Yn Yn\\n22 1 21 1\\n22 2 1 2 1\\n01\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤+ < +\\n()\\n=\\n()\\n+\\n()\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n−\\nd\\ni\\nin i\\nYn K Yn\\ni n i\\nYn Yn K\\ni ni\\n01\\n01\\n01\\n2\\n21 21\\n21\\n22\\n2\\n≤<\\n+\\n()\\n=+\\n()\\n≤+ <\\n()\\n=\\n()\\n≤<\\n−\\n \\n  \\n(8-61)\\nHere, \\nX\\n is the tile component being transformed,\\n \\nY\\n is the resulting transform, \\nand \\ni\\n0\\n and \\ni\\n1\\n define the position of the tile component within a component. That \\nis, they are the indices of the first sample of the tile-component row or column \\nbeing transformed and the one immediately following the last sample. Variable \\n† \\n′′ ′\\nRGB\\n is a gamma-corrected, nonlinear version of a linear CIE (International Commission on Illumination)  \\nRGB\\n colorimetry value\\n. \\n′\\nY\\n is luminance and \\nC\\nb\\n and \\nC\\nr\\n are color differences (i.e., scaled \\n′′\\nBY\\n−\\n and \\n′′\\nRY\\n−\\n \\nvalues).\\nLifting-based imple-\\nmentations are another \\nway to compute wavelet \\ntransforms. The coef-\\nﬁcients used in the \\napproach are directly \\nrelated to the FWT ﬁlter \\nbank coefﬁcients.\\nDIP4E_GLOBAL_Print_Ready.indb   619\\n6/16/2017   2:11:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 620}),\n",
       " Document(page_content='620\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nn\\n assumes values based on \\ni\\n0\\n, \\ni\\n1\\n, and determines which of the six operations is \\nbeing performed. If \\nni\\n<\\n0\\n or \\nni\\n>\\n1\\n, \\nXn\\n()\\n is obtained by symmetrically extending \\nX\\n.\\n For example, \\nX\\niX i\\n00\\n11\\n−\\n()\\n=+\\n()\\n,\\n \\nX\\niX i\\n00\\n22\\n−\\n()\\n=+\\n()\\n,\\n \\nXi Xi\\n11\\n2\\n()\\n=\\n()\\n−\\n,\\n and \\nXi Xi\\n11\\n13\\n+\\n()\\n=\\n()\\n−\\n.\\n At the conclusion of the lifting and scaling operations, the even-\\nindexed values of \\nY\\n are equivalent to the FWT lowpass filtered output; the odd-\\nindexed values of \\nY\\n correspond to the highpass FWT filtered result. Lifting param-\\neters \\na\\n, \\nb\\n, \\ng\\n,\\n and \\nd\\n are \\n−\\n1\\n586134342\\n.,\\n \\n−\\n0\\n052980118\\n.,\\n 0.882911075, and 0.433506852, \\nrespectively\\n, and scaling factor \\nK\\n is 1.230174105.\\nThe transformation just described produces four subbands; a low-resolution \\napproximation of the tile component and the component’s horizontal, vertical, and \\ndiagonal frequency characteristics. Repeating the transformation \\nN\\nL\\n times, with sub-\\nsequent iterations restricted to the previous decomposition’s approximation coefﬁ-\\ncients, produces an \\nN\\nL\\n-scale wavelet transform. Adjacent scales are related spatially \\nby powers of 2, and the lowest scale contains the only explicitly deﬁned approxima-\\ntion of the original tile component. As can be surmised from Fig. 8.45, where the \\nnotation of the JPEG-2000 standard is summarized for the case of \\nN\\nL\\n = 2, a general \\nN\\nL\\n-scale transform contains \\n31\\nN\\nL\\n+\\n subbands whose coefﬁcients are denoted \\na\\nb\\n for  \\nb\\n = \\nN\\nL \\nHL\\n,…, 1\\nHL\\n, 1\\nLH\\n, 1\\nHH\\n. The standard does not specify the number of scales \\nto be computed.\\nWhen each of the tile components has been processed, the total number of trans-\\nform coefﬁcients is equal to the number of samples in the original image, but the \\nimportant visual information is concentrated in a few coefﬁcients. To reduce the \\nnumber of bits needed to represent the transform, coefﬁcient \\nau v\\nb\\n(,)\\n of subband \\nb\\n \\nis quantized to value \\nqu v\\nb\\n(,)\\n using\\n \\nqu v au v\\nau v\\nbb\\nb\\nb\\n,,\\n,\\n()\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n⋅\\n()\\nΔ\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\nsign floor\\n \\n(8-62)\\nwhere the \\nquantiztion step siz\\ne\\n \\n/H9004\\nb\\n is\\n \\nΔ= +\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\nb\\nR\\nb\\nbb\\n21\\n2\\n11\\n−\\ne\\nm\\n \\n(8-63)\\nR\\nb\\n is the \\nnominal dynamic range\\n of subband \\nb\\n, while \\ne\\nb\\n and \\nm\\nb\\n are the number of \\nbits allotted to the \\nexponent\\n and \\nmantissa\\n of the subband’s coefficients. The nominal \\ndynamic range of subband \\nb\\n is the sum of the number of bits used to represent the \\noriginal image and the \\nanalysis gain\\n bits for subband \\nb\\n. Subband analysis gain bits \\nfollow the simple pattern shown in Fig. 8.45. For example, there are two analysis gain \\nbits for subband \\nb\\n = 1\\nHH\\n.\\nFor error-free compression, \\nm\\nb\\n=\\n0,\\n \\nR\\nbb\\n=\\ne\\n,\\n and \\n/H9004\\nb\\n=\\n1.\\n For irreversible com-\\npression,\\n no particular quantization step size is speciﬁed in the standard. Instead, \\nthe number of exponent and mantissa bits must be provided to the decoder on a \\nsubband basis, called \\nexpounded quantization\\n, or for the \\nN\\nL\\nLL\\n subband only, called \\nderived quantization\\n. In the latter case, the remaining subbands are quantized using \\nThese lifting-based coef-\\nﬁcients are speciﬁed in \\nthe standard.\\nRecall from Chapter 7 \\nthat the DWT decom-\\nposes an image into a set \\nof band-limited compo-\\nnents called subbands.\\nDo not confuse the \\nstandard’s deﬁnition of \\nnominal dynamic range \\nwith the closely related \\ndeﬁnition in Chapter 2.\\nDIP4E_GLOBAL_Print_Ready.indb   620\\n6/16/2017   2:11:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 621}),\n",
       " Document(page_content='8.11\\n  \\nWavelet Coding\\n    \\n621\\nextrapolated \\nN\\nL\\nLL\\n subband parameters. Letting \\ne\\n0\\n and \\nm\\n0\\n be the number of bits \\nallocated to the \\nN\\nL\\nLL\\n subband, the extrapolated parameters for subband \\nb\\n are\\nmm\\nee\\nb\\nbb L\\nnN\\n=\\n=+\\n0\\n0\\n−\\n \\n(8-64)\\nwhere \\nn\\nb\\n denotes the number of subband decomposition levels from the original \\nimage tile component to subband \\nb\\n.\\nIn the ﬁnal steps of the encoding process, the coefﬁcients of each transformed \\ntile-component’s subbands are arranged into rectangular blocks called \\ncode blocks\\n, \\nwhich are coded individually, one bit plane at a time. Starting from the most signiﬁ-\\ncant bit plane with a nonzero element, each bit plane is processed in three passes. \\nEach bit (in a bit plane) is coded in only one of the three passes, which are called \\nsigniﬁcance propagation\\n, \\nmagnitude reﬁnement\\n, and \\ncleanup\\n. The outputs are then \\narithmetically coded and grouped with similar passes from other code blocks to \\nform \\nlayers\\n. A layer is an arbitrary number of groupings of coding passes from \\neach code block. The resulting layers ﬁnally are partitioned into \\npackets\\n, providing \\nan additional method of extracting a spatial region of interest from the total code \\nstream. Packets are the fundamental unit of the encoded code stream.\\nJPEG-2000 decoders simply invert the operations previously described. After \\nreconstructing the subbands of the tile-components from the arithmetically coded \\nJPEG-2000 packets, a user-selected number of the subbands is decoded. Although \\nthe encoder may have encoded \\nM\\nb\\n bit planes for a particular subband, the user, \\ndue to the embedded nature of the code stream, may choose to decode only \\nN\\nb\\n bit \\nplanes. This amounts to quantizing the coefﬁcients of the code block using a step \\nFIGURE 8.45\\nJPEG 2000 two-\\nscale wavelet \\ntransform tile-\\ncomponent coefﬁ-\\ncient notation and \\nanalysis gain.\\na\\n2\\nLL\\n(\\nu\\n, v)\\na\\n2\\nHL\\n(\\nu\\n, v)\\na\\n2\\nLH\\n(\\nu\\n, v)\\na\\n2\\nHH\\n(\\nu\\n, v)\\na\\n1\\nHL\\n(\\nu\\n, v)\\na\\n1\\nLH\\n(\\nu\\n, v)\\na\\n1\\nHH\\n(\\nu\\n, v)\\n0\\n1\\n1\\n2\\n1\\n1\\n2\\na\\n1\\nH\\nL\\n(\\nu\\n, v\\n)\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   621\\n6/16/2017   2:11:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 622}),\n",
       " Document(page_content='622\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nsize of \\n2\\nMN\\nb\\nbb\\n−\\n⋅Δ\\n.\\n Any nondecoded bits are set to zero and the resulting coefﬁcients, \\ndenoted \\nqu v\\nb\\n(,) ,\\n are inverse quantized using\\n \\nRu v\\nqu v r\\nqu v\\nqu v r\\nq\\nb\\nMN u v\\nbb\\nb\\nM\\nb\\nbb\\nb\\n,\\n,,\\n,\\n,\\n()\\n=\\n()\\n+⋅\\n()\\n⋅Δ\\n()\\n>\\n()\\n⋅\\n()\\n20\\n2\\n−\\n−\\n−\\nN\\nNu v\\nbb\\nb\\nb\\nqu v\\nqu v\\n,\\n,\\n,\\n()\\n()\\n⋅Δ\\n()\\n<\\n()\\n=\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\n⎪\\n⎪\\n0\\n00\\n \\n(8-65)\\nwhere \\nRu v\\nq\\nb\\n(,)\\n denotes an inverse-quantized transform coefficient, and \\nNu v\\nb\\n(,)\\n is \\nthe number of decoded bit planes for \\nqu v\\nb\\n(,) .\\n \\nReconstruction parameter\\n \\nr\\n is cho-\\nsen by the decoder to produce the best visual or objective quality of reconstruction.\\n \\nGenerally, \\n01\\n≤<\\nr\\n,\\n with a common value being \\nr\\n=\\n12 .\\n The inverse-quantized coef-\\nficients then are inverse-transformed by column and by row using an \\nFWT\\n−\\n1\\n filter \\nbank whose coefficients are obtained from Table 7.1, or via the following lifting-\\nbased operations:\\n \\nXn K Yn\\ni ni\\nXn K Yn\\ni n i\\n22\\n3 2 3\\n21 1 21\\n2 21\\n01\\n01\\n()\\n=⋅\\n≤ < +\\n+\\n()\\n=\\n()\\n+\\n()\\n≤< +\\n()\\n−\\n−−\\n−\\n2 2\\n22 2 1 2 1 3 23\\n21 2\\n01\\nXn Xn Xn Xn i ni\\nXn Xn\\n()\\n=\\n() ( )\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤< +\\n+\\n()\\n=+\\n−−\\n−\\nd\\n1\\n12 2 2 2 2 1 2\\n22 2 1\\n01\\n() ( )\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤+ < +\\n()\\n=\\n() ( )\\n+\\n−−\\n−−\\ng\\nb\\nXn Xn i n i\\nXn Xn Xn X Xn i ni\\nXn Xn Xn Xn\\n21 1 2 1\\n21 21 2 22\\n01\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n≤< +\\n+\\n()\\n=+\\n() ( )\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n−\\na\\ni\\nin i\\n01\\n21\\n≤+ <\\n \\n  \\n(8-66)\\nwhere parameters \\na\\n, \\nb\\n, \\ng\\n, \\nd\\n,\\n and \\nK\\n are as defined for Eq.\\n (8-61). Inverse-quantized \\ncoefficient row or column element \\nYn\\n()\\n is symmetrically extended when necessary. \\nT\\nhe final decoding steps are the assembly of the component tiles, inverse compo-\\nnent transformation (if required), and DC level shifting. For irreversible coding, the \\ninverse component transformation is\\n \\nIx y Yx y Yx y\\nIx y Yx y Yx y\\n00 2\\n10 1\\n1 402\\n0 34413\\n,, . ,\\n,, . ,\\n()\\n=\\n()\\n+\\n()\\n()\\n=\\n() ()\\n−−\\n−\\n0 71414\\n1 772\\n2\\n20 1\\n.,\\n,, . ,\\nYx y\\nIx y Yx y Y x y\\n()\\n()\\n=\\n()\\n+\\n()\\n \\n(8-67)\\nand the transformed pixels are shifted by \\n+\\n−\\n2\\n1\\nSsiz\\n.\\nEXAMPLE 8.28 :  A comparison of JPEG-2000 wavelet-based coding and JPEG DCT-based compression.\\nFigure\\n \\n8.46 shows four JPEG-2000 approximations of the monochrome image in Figure\\n \\n8.9(a). Succes-\\nsive rows of the ﬁgure illustrate increasing levels of compression, including \\nC\\n \\n=\\n \\n25, 52, 75, and 105. The \\nimages in column 1 are decompressed JPEG-2000 encodings. The differences between these images \\nand the original image [see Fig. 8.9(a)] are shown in the second column, and the third column contains \\na zoomed portion of the reconstructions in column 1. Because the compression ratios for the ﬁrst two \\nrows are virtually identical to the compression ratios in Example 8.18, these results can be compared \\n(both qualitatively and quantitatively) to the JPEG transform-based results in Figs. 8.29(a) through (f).\\nQuantization as deﬁned \\nearlier in the chapter is \\nirreversible. The term \\n“inverse quantized” does \\nnot mean that there is \\nno information loss. This \\nprocess is lossy except \\nfor the case of reversible \\nJPEG-2000 compression, \\nwhere \\nm\\nb\\n=\\n0,  \\nR\\nbb\\n=\\ne\\n, \\nand \\n/H9004\\nb\\n=\\n1.\\nDIP4E_GLOBAL_Print_Ready.indb   622\\n6/16/2017   2:11:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 623}),\n",
       " Document(page_content='8.11\\n  \\nWavelet Coding\\n    \\n623\\nFIGURE 8.46\\n Four JPEG-2000 approximations of Fig. 8.9(a). Each row contains a result after compression and recon-\\nstruction, the scaled difference between the result and the original image, and a zoomed portion of the recon-\\nstructed image. (Compare the results in rows 1 and 2 with the JPEG results in Fig. 8.29.).\\nDIP4E_GLOBAL_Print_Ready.indb   623\\n6/16/2017   2:11:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 624}),\n",
       " Document(page_content='624\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nA visual comparison of the error images in rows 1 and 2 of Fig. 8.46 with the corresponding images \\nin Figs. 8.29(b) and (e) reveals a noticeable decrease of error in the JPEG-2000 results—3.86 and 5.77 \\nintensity levels, as opposed to 5.4 and 10.7 intensity levels for the JPEG results. The computed errors \\nfavor the wavelet-based results at both compression levels. Besides decreasing reconstruction error, \\nwavelet coding dramatically increases (in a subjective sense) image quality. Note that the blocking arti-\\nfact that dominated the JPEG results [see Figs. 8.29(c) and (f)] is not present in Fig. 8.46. Finally, we \\nnote that the compression achieved in rows 3 and 4 of Fig. 8.46 is not practical with JPEG. JPEG-2000 \\nprovides useable images that are compressed by more than 100:1, with the most objectionable degrada-\\ntion being increased image blur.\\n8.12 DIGITAL IMAGE WATERMARKING  \\nThe methods and standards of Sections 8.2 through 8.11 make the distribution of \\nimages (in photographs or videos) on digital media and over the Internet practi-\\ncal. Unfortunately, the images so distributed can be copied repeatedly and without \\nerror, putting the rights of their owners at risk. Even when encrypted for distribution, \\nimages are unprotected after decryption. One way to discourage illegal duplication \\nis to insert one or more items of information, collectively called a \\nwatermark\\n, into \\npotentially vulnerable images in such a way that the watermarks are inseparable \\nfrom the images themselves. As integral parts of the watermarked images, they pro-\\ntect the rights of their owners in a variety of ways, including:\\n1. \\nCopyright identiﬁcation\\n.\\n Watermarks can provide information that serves as \\nproof of ownership when the rights of the owner have been infringed.\\n2. \\nUser identiﬁcation\\n or \\nﬁngerprinting\\n.\\n The identity of legal users can be encoded \\nin watermarks and used to identify sources of illegal copies.\\n3. \\nAuthenticity determination\\n.\\n The presence of a watermark can guarantee that an \\nimage has not been altered, assuming the watermark is designed to be destroyed \\nby any modiﬁcation of the image.\\n4. \\nAutomated monitoring\\n.\\n Watermarks can be monitored by systems that track \\nwhen and where images are used (e.g., programs that search the Web for images \\nplaced on Web pages). Monitoring is useful for royalty collection and/or the \\nlocation of illegal users.\\n5. \\nCopy protection\\n.\\n Watermarks can specify rules of image usage and copying (e.g., \\nto DVD players).\\nIn this section, we provide a brief overview of \\ndigital image watermarking\\n, which is \\nthe process of inserting data into an image in such a way that it can be used to make \\nan assertion about the image. The methods described have little in common with \\nthe compression techniques presented in the previous sections (although they do \\ninvolve the coding of information). In fact, watermarking and compression are in \\nsome ways opposites. While the objective in compression is to reduce the amount of \\ndata used to represent images, the goal in watermarking is to add information and \\ndata (i.e., watermarks) to them. As will be seen in the remainder of the section, the \\nwatermarks themselves can be either visible or invisible.\\n8.12\\nDIP4E_GLOBAL_Print_Ready.indb   624\\n6/16/2017   2:11:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 625}),\n",
       " Document(page_content='8.12\\n  \\nDigital Image Watermarking\\n    \\n625\\nA \\nvisible watermark\\n is an opaque or semi-transparent subimage or image that is \\nplaced on top of another image (i.e., the image being watermarked) so that it is obvi-\\nous to the viewer. Television networks often place visible watermarks (fashioned \\nafter their logos) in the upper or lower right-hand corner of the television screen. As \\nthe following example illustrates, visible watermarking typically is performed in the \\nspatial domain.\\nEXAMPLE 8.29 :  A simple visible watermark.\\nThe image in Fig. 8.47(b) is the lower right-hand quadrant of the image in Fig. 8.9(a) with a scaled ver-\\nsion of the watermark in Fig. 8.47(a) overlaid on top of it. Letting \\nf\\nw\\n denote the watermarked image, we \\ncan express it as a linear combination of the unmarked image \\nf\\n and watermark \\nw\\n using\\n \\nff\\nw\\nw\\n=\\n()\\n+\\n1\\n−\\naa\\n \\n(8-68)\\nwhere constant \\na\\n controls the relative visibility of the watermark and the underlying image. If \\na\\n is 1, \\nthe watermark is opaque and the underlying image is completely obscured.\\n As \\na\\n approaches 0, more of \\nthe underlying image and less of the watermark is seen.\\n In general, \\n01\\n<≤\\na\\n;\\n in Fig. 8.47(b), \\na\\n=\\n03\\n..\\n Fig-\\nure 8.47(c) is the computed difference (scaled in intensity) between the watermarked image in (b) and \\nthe unmarked image in F\\nig. 8.9(a). Intensity 128 represents a difference of 0. Note that the underlying \\nimage is clearly visible through the “semi-transparent” watermark. This is evident in both Fig. 8.47(b) \\nand the difference image in Fig. 8.47(c).\\nUnlike the visible watermark of the previous example, \\ninvisible watermarks\\n can-\\nnot be seen with the naked eye. They are imperceptible but can be recovered with an \\nappropriate decoding algorithm. Invisibility is assured by inserting them as visually \\nredundant information [information that the human visual system ignores or cannot \\nb\\na\\nc\\nFIGURE 8.47\\nA simple visible \\nwatermark:  \\n(a) watermark;  \\n(b) the water-\\nmarked image; \\nand  \\n(c) the  \\ndifference \\nbetween the \\nwatermarked \\nimage and the \\noriginal (non-\\nwatermarked) \\nimage.\\nD\\nigital Image\\nProcessing\\nDIP4E_GLOBAL_Print_Ready.indb   625\\n6/16/2017   2:11:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 626}),\n",
       " Document(page_content='626\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nperceive (see Section 8.1)]. Figure 8.48(a) provides a simple example. Because the \\nleast signiﬁcant bits of an 8-bit image have virtually no effect on our perception of \\nthe image, the watermark from Fig. 8.47(a) was inserted or “hidden” in its two least \\nsigniﬁcant bits. Using the notation introduced above, we let\\n \\nf\\nf\\nw\\nw\\n=\\n⎛\\n⎝\\n⎜\\n⎞\\n⎠\\n⎟\\n+\\n4\\n46 4\\n \\n(8-69)\\nand use unsigned integer arithmetic to perform the calculations. Dividing and mul-\\ntiplying by 4 sets the two least significant bits of \\nf\\n to 0,\\n dividing \\nw\\n by 64 shifts its two \\nmost significant bits into the two least significant bit positions, and adding the two \\nresults generates the \\nLSB watermarked image\\n. Note that the embedded watermark \\nis not visible in Fig. 8.48(a). By zeroing the most significant 6 bits of this image and \\nscaling the remaining values to the full intensity range, however, the watermark can \\nbe extracted as in Fig. 8.48(b).\\nAn important property of invisible watermarks is their resistance to both acci-\\ndental and intentional attempts to remove them. \\nFragile invisible watermarks\\n \\nare destroyed by any modiﬁcation of the images in which they are embedded. In \\nsome applications, like image authentication, this is a desirable characteristic. As \\nFigs. 8.48(c) and (d) show, the LSB watermarked image in Fig. 8.48(a) contains a \\nfragile invisible watermark. If the image in (a) is compressed and decompressed \\nusing lossy JPEG, the watermark is destroyed. Figure 8.48(c) is the result after com-\\nb a\\nd c\\nFIGURE 8.48\\nA simple invis-\\nible watermark: \\n(a) watermarked \\nimage;  \\n(b) the extracted \\nwatermark;  \\n(c) the water-\\nmarked image \\nafter high quality \\nJPEG compres-\\nsion and decom-\\npression; and  \\n(d) the extracted \\nwatermark  \\nfrom (c).\\nDIP4E_GLOBAL_Print_Ready.indb   626\\n6/16/2017   2:11:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 627}),\n",
       " Document(page_content='8.12\\n  \\nDigital Image Watermarking\\n    \\n627\\npressing and decompressing Fig. 8.48(a); the rms error is 2.1 bits. If we try to extract \\nthe watermark from this image using the same method as in (b), the result is unintel-\\nligible [see Fig. 8.48(d)]. Although lossy compression and decompression preserved \\nthe important visual information in the image, the fragile watermark was destroyed.\\nRobust invisible watermarks\\n are designed to survive image modiﬁcation, whether \\nthe so-called \\nattacks\\n are inadvertent or intentional. Common inadvertent attacks \\ninclude lossy compression, linear and nonlinear ﬁltering, cropping, rotation, resam-\\npling, and the like. Intentional attacks range from printing and rescanning to adding \\nadditional watermarks and/or noise. Of course, it is unnecessary to withstand attacks \\nthat leave the image itself unusable.\\nFigure 8.49 shows the basic components of a typical image watermarking system. \\nThe encoder in Fig. 8.49(a) inserts watermark \\nw\\ni\\n into image \\nf\\ni\\n producing water-\\nmarked image \\nf\\ni\\nw\\n;\\n the complementary decoder in (b) extracts and validates the \\npresence of \\nw\\ni\\n in watermarked input \\nf\\ni\\nw\\n or unmarked input \\nf\\nj\\n.\\n If \\nw\\ni\\n is visible, the \\ndecoder is not needed. If it is invisible, the decoder may or may not require a copy \\nof \\nf\\ni\\n and \\nw\\ni\\n [shown in blue in Fig. 8.49(b)] to do its job. If \\nf\\ni\\n and/or \\nw\\ni\\n are used, the \\nwatermarking system is known as a \\nprivate\\n or \\nrestricted-key \\nsystem; if not, it is a \\npublic\\n or \\nunrestricted-key\\n system. Because the decoder must process both marked \\nand unmarked images, \\nw\\n∅\\n is used in Fig. 8.49(b) to denote the absence of a mark. \\nFinally, we note that to determine the presence of \\nw\\ni\\n in an image, the decoder must \\ncorrelate extracted watermark \\nw\\nj\\n with \\nw\\ni\\n and compare the result to a predeﬁned \\nthreshold. The threshold sets the degree of similarity that is acceptable for a “match.”\\nEXAMPLE 8.30 :  A DCT-based invisible robust watermark.\\nMark insertion\\n and \\nextraction\\n can be performed in the spatial domain, as in the previous examples, \\nor in the transform domain. Figures 8.50(a) and (c) show two watermarked versions of the image in \\nFig. 8.9(a) using the DCT-based watermarking approach outlined here (Cox et al. [1997]):\\n1. \\nCompute the 2-D DCT of the image to be watermarked.\\n2. \\nLocate its \\nK\\n largest coefﬁcients\\n, \\ncc c\\nK\\n12\\n,, , ,\\n…\\n by magnitude.\\n3. \\nCreate a watermark by generating a \\nK\\n-element pseudo-random sequence of numbers\\n, \\nvv v\\n12\\n,,, ,\\n…\\nK\\n \\ntaken from a Gaussian distribution with mean \\nm\\n=\\n0\\n and variance \\ns\\n2\\n1\\n=\\n.\\n (Note: A pseudo-random \\nnumber sequence approximates the properties of random numbers\\n. It is not truly random because \\nit depends on a predetermined initial value.)\\n4. \\nEmbed the watermark from Step 3 into the \\nK\\n largest DCT coefﬁcients from Step 2 using the fol-\\nlowing equation\\n \\n′\\n=⋅+\\ncc\\ni K\\nii i\\n()\\n11\\nav\\n≤≤\\n \\n(8-70)\\nfor a speciﬁed constant \\na\\n>\\n0 (that controls the extent to which \\nv\\ni\\n alters \\nc\\ni\\n). Replace the original \\nc\\ni\\n with the computed \\n′\\nc\\ni\\n from Eq. (8-70). (For the images in Fig. 8.50, \\na\\n=\\n01\\n.  and \\nK\\n = 1000.)\\n5. \\nCompute the inverse DCT of the result from Step 4.\\nBy employing watermarks made from pseudo-random numbers and spreading them across an image’\\ns \\nperceptually signiﬁcant frequency components, \\na\\n can be made small, reducing watermark visibility. At \\nDIP4E_GLOBAL_Print_Ready.indb   627\\n6/16/2017   2:11:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 628}),\n",
       " Document(page_content='628\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nthe same time, watermark security is kept high because (1) the watermarks are composed of pseudo-\\nrandom numbers with no obvious structure, (2) the watermarks are embedded in multiple frequency \\ncomponents with spatial impact over the entire 2-D image (so their location is not obvious) and \\n(3) attacks against them tend to degrade the image as well (i.e., the image’s most important frequency \\n \\ncomponents must be altered to affect the watermarks).\\nFigures 8.50(b) and (d) make the changes in image intensity that result from the pseudo-random \\nnumbers that are embedded in the DCT coefﬁcients of the watermarked images in Figs. 8.50(a) and (c) \\nb a\\nd c\\nFIGURE 8.50\\n(a) and (c) Two \\nwatermarked \\nversions of \\nFig. 8.9(a);  \\n(b) and (d) \\nthe differences \\n(scaled in inten-\\nsity) between \\nthe watermarked \\nversions and the \\nunmarked image. \\nThese two images \\nshow the inten-\\nsity contribution \\n(although scaled \\ndramatically) of \\nthe pseudo- \\nrandom water-\\nmarks on the \\noriginal image.\\nb\\na\\nFIGURE 8.49\\nA typical image \\nwatermarking  \\nsystem:  \\n(a) encoder;  \\n(b) decoder.\\nDecision\\n(mark detected\\nor not)\\nImage\\nMarked\\nimage\\nWatermark\\nMark\\ninsertion\\nWatermark\\nWatermark\\nImage\\nMark\\nextraction\\nMark\\ndetection\\nImage\\n(marked or\\nunmarked)\\nD\\nf\\ni\\nf\\ni\\nf\\ni\\nw\\nw\\ni\\nw\\ni\\nf, f\\nw\\ni\\nj\\nww\\nj\\n,\\n∅\\nDIP4E_GLOBAL_Print_Ready.indb   628\\n6/16/2017   2:11:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 629}),\n",
       " Document(page_content='8.12\\n  \\nDigital Image Watermarking\\n    \\n629\\nvisible. Obviously, the pseudo-random numbers must have an effect (even if too small to see) on the \\nwatermarked images. To display the effect, the images in Figs. 8.50(a) and (c) were subtracted from the \\nunmarked image in Fig. 8.9(a) and scaled in intensity to the range [0, 255]. Figures 8.50(b) and (d) are \\nthe resulting images; they show the 2-D spatial contributions of the pseudo-random numbers. Because \\nthey have been scaled, however, you cannot simply add these images to the image in Fig. 8.9(a) and \\nget the watermarked images in Figs. 8.50(a) and (c). As can be seen in Figs. 8.50(a) and (c), their actual \\nintensity perturbations are small to negligible.\\nTo determine whether a particular image is a copy of a previously watermarked image with water-\\nmark \\nvv v\\n12\\n,,,\\n…\\nK\\n and DCT coefﬁcients \\ncc c\\nK\\n12\\n,, , ,\\n…\\n we use the following procedure:\\n1. \\nCompute the 2-D DCT of the image in question.\\n2. \\nExtract the \\nK\\n DCT coefﬁcients (in the positions corresponding to \\ncc c\\nK\\n12\\n,, ,\\n…\\n of Step 2 in the \\nwatermarking procedure) and denote the coefﬁcients as \\nˆ\\n,\\nˆ\\n,,\\nˆ\\n.\\ncc\\nc\\nK\\n12\\n…\\n If the image in question is the \\npreviously watermarked image (without modiﬁcation),\\n \\nˆ\\ncc\\nii\\n=\\n′\\n for \\n1\\n≤≤\\niK\\n.\\n If it is a modiﬁed copy \\nof the watermarked image (i.e\\n., it has undergone some sort of attack), \\nˆ\\ncc\\nii\\n≈\\n′\\n for \\n1\\n≤≤\\niK\\n (the \\nˆ\\nc\\ni\\n \\nwill be approximations of the \\n′\\nc\\ni\\n). Otherwise, the image in question will be an unmarked image or an \\nimage with a completely different watermark, and the \\nˆ\\nc\\ni\\n will bear no resemblance to the original \\nˆ\\n.\\nc\\ni\\n3. \\nCompute watermark \\nˆ\\n,\\nˆ\\n,,\\nˆ\\nvv\\nv\\n12\\n…\\nK\\n using\\n \\nˆ\\nˆ\\nv\\na\\ni\\nii\\ni\\ncc\\nc\\nik\\n=\\n−\\n≤≤\\nfor 1\\n \\n(8-71)\\nRecall that watermarks are sequences of pseudo-random numbers.\\n4. \\nMeasure the similarity of \\nˆ\\n,\\nˆ\\n,,\\nˆ\\nvv\\nv\\n12\\n…\\nK\\n (from Step 3) and \\nvv v\\n12\\n,,,\\n…\\nK\\n (from Step 3 of the water-\\nmarking procedure) using a metric such as the correlation coefﬁcient\\n \\ng\\nvvv v\\nvv v v\\n=\\n⋅\\n≤≤\\n=\\n=\\n=\\n∑\\n∑ ∑\\n() ( )\\n() ( )\\nˆˆ\\n–\\nˆˆ\\n–\\nii\\ni\\nK\\nii\\ni\\nK\\ni\\nK\\niK\\n−−\\n−−\\n1\\n22\\n1\\n1\\n1\\n \\n(8-72)\\nwhere \\nv\\n and \\nˆ\\nv\\n are the means of the two \\nK\\n-element watermarks\\n. (Note: Correlation coefﬁcients \\nare discussed in detail in Section 12.3.)\\n5. \\nCompare the measured similarity, \\ng\\n,\\n to a predeﬁned threshold, \\nT\\n,\\n and make a binary detection deci-\\nsion:\\n \\nD\\nT\\n=\\n⎧\\n⎨\\n⎩\\n1\\n0\\nif\\n \\notherwise\\ng\\n≥\\n \\n(8-73)\\nIn other words, \\nD\\n = 1 indicates that watermark \\nvv v\\n12\\n,,,\\n…\\nK\\n is present (with respect to the speci-\\nﬁed threshold, \\nT\\n); \\nD\\n = 0 indicates that it was not.\\nUsing this procedure, the original watermarked image in Fig. 8.50(a), measured against itself, yields a \\ncorrelation coefﬁcient of 0.9999, i.e., \\ng\\n=\\n0\\n9999\\n..\\n It is an unmistakable match. In a similar manner, the \\nimage in F\\nig. 8.50(b), when measured against the image in Fig. 8.50(a), results in a \\ng\\n of 0.0417. It could \\nnot be mistaken for the watermarked image in F\\nig. 8.50(a) because the correlation coefﬁcient is so low.\\nDIP4E_GLOBAL_Print_Ready.indb   629\\n6/16/2017   2:11:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 630}),\n",
       " Document(page_content='630\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nTo conclude the section, we note that the DCT-based watermarking approach of the previous \\nexample is fairly resistant to watermark attacks, partly because it is a private or restricted-key method. \\nRestricted-key methods are always more resilient than their unrestricted-key counterparts. Using the \\nwatermarked image in Fig. 8.50(a), Fig. 8.51 illustrates the ability of the method to withstand a variety \\nof common attacks. As can be seen in the ﬁgure, watermark detection is quite good over the range of \\nattacks that were implemented; the resulting correlation coefﬁcients (shown under each image in the \\nﬁgure) vary from 0.3113 to 0.9945. When subjected to a high quality but lossy (resulting in an rms error \\nof 7 intensities) JPEG compression and decompression, \\ng\\n=\\n0\\n9945\\n..\\n Even when the compression and \\nreconstruction yields an rms error of 10 intensity levels\\n, \\ng\\n=\\n0 7395 .\\n; and the usability of this image has \\nbeen signiﬁcantly degraded.\\n Signiﬁcant smoothing by spatial ﬁltering and the addition of Gaussian noise \\ndo not reduce the correlation coefﬁcient below 0.8230. However, histogram equalization reduces \\ng\\n to \\n0.5210;\\n and rotation has the largest effect; reducing \\ng\\n to 0.3313. All attacks, except for the lossy JPEG \\ncompression and reconstruction in F\\nig. 8.51(a), have signiﬁcantly reduced the usability of the original \\nwatermarked image.\\nSummary, References, and Further Reading\\n  \\nThe principal objectives of this chapter were to present the theoretic foundation of digital image compression, to \\ndescribe the most commonly used compression methods, and to introduce the related area of digital image water-\\nmarking. Although the level of the presentation is introductory in nature, the references provide an entry into the \\nextensive body of literature dealing with the topics discussed. As evidenced by the international standards listed \\nin Tables 8.3 through 8.5, compression plays a key role in document image storage and transmission, the Internet, \\nand commercial video distribution (e.g., DVDs). It is one of the few areas of image processing that has received a \\nsufﬁciently broad commercial appeal to warrant the adoption of widely accepted standards. Image watermarking is \\nbecoming increasingly important as more and more images are distributed in compressed digital form.\\nThe introductory material of the chapter, which is generally conﬁned to Section 8.1, is basic to image compres-\\nsion, and may be found in one form or another in most of the general image processing books cited at the end of \\nChapter 1. For additional information on the human visual system, see Netravali and Limb [1980], as well as Huang \\n[1966], Schreiber and Knapp [1958], and the references cited at the end of Chapter 2. For more on information \\ntheory, see the book website or Abramson [1963], Blahut [1987], and Berger [1971]. Shannon’s classic paper, “A \\nMathematical Theory of Communication” [1948], lays the foundation for the area and is another excellent refer-\\nence. Subjective ﬁdelity criteria are discussed in Frendendall and Behrend [1960]. Throughout the chapter, a variety \\nof compression standards are used in examples. Most of them were implemented using Adobe Photoshop (with \\nfreely available compression plug-ins) and/or MATLAB, which is described in Gonzalez et al. [2004]. Compression \\nstandards, as a rule, are lengthy and complex; we have not attempted to cover any of them in their entirety. For more \\ninformation on a particular standard, see the published documents of the appropriate standards organization—the \\nInternational Standards Organization, International Electrotechnical Commission, and/or the International Tele-\\ncommunications Union.\\nThe lossy and error-free compression techniques described in Sections 8.2 through 8.11 and watermarking tech-\\nniques in Section 8.12 are, for the most part, based on the original papers cited in the text. The algorithms covered \\nare representative of the work in this area, but are by no means exhaustive. The material on LZW coding has its \\norigins in the work of Ziv and Lempel [1977, 1978]. The material on arithmetic coding follows the development in \\nWitten, Neal, and Cleary [1987]. One of the more important implementations of arithmetic coding is summarized in \\nPennebaker et al. [1988]. For a good discussion of lossless predictive coding, see the tutorial by Rabbani and Jones \\n[1991]. The adaptive predictor of Eq. (8-55) is from Graham [1958]. For more on motion compensation, see S. Solari \\n[1997], which also contains an introduction to general video compression and compression standards, and Mitchell \\net al. [1997]. The DCT-based watermarking technique in Section 8.12 is based on the paper by Cox et al. [1997]. For \\nmore on watermarking, see the books by Cox et al. [2001] and Parhi and Nishitani [1999]. See also the paper by S. \\nMohanty [1999].\\nDIP4E_GLOBAL_Print_Ready.indb   630\\n6/16/2017   2:11:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 631}),\n",
       " Document(page_content='  \\n  \\nSummary, References, and Further Reading\\n    \\n631\\nb a\\nc\\ne d\\nf\\nFIGURE 8.51\\n Attacks on the watermarked image in Fig. 8.50(a): (a) lossy JPEG compression and decompression \\nwith an rms error of seven intensity levels; (b) lossy JPEG compression and decompression with an rms error of 10 \\nintensity levels (note the blocking artifact); (c) smoothing by spatial ﬁltering; (d) the addition of Gaussian noise; \\n \\n(e) histogram equalization; and (f) rotation. Each image is a modiﬁed version of the watermarked image in \\nFig. 8.50(a). After modiﬁcation, they retain their watermarks to varying degrees, as indicated by the correlation \\ncoefﬁcients below each image.\\ng\\n=\\n0.9945\\ng\\n=\\n0.7395\\ng\\n=\\n0.8390\\ng\\n=\\n0.8230\\ng\\n=\\n0.5210\\ng\\n=\\n0.3113\\nMany survey articles have been devoted to the ﬁeld of image compression. Noteworthy are Netravali and Limb \\n[1980], A. K. Jain [1981], a special issue on picture communication systems in the \\nIEEE Transactions on Communi-\\ncations\\n [1981], a special issue on the encoding of graphics in the \\nProceedings of IEEE\\n [1980], a special issue on visual \\ncommunication systems in the \\nProceedings of the IEEE \\n[1985], a special issue on image sequence compression in \\nthe \\nIEEE Transactions on Image Processing\\n [1994], and a special issue on vector quantization in the \\nIEEE Transac-\\ntions on Image Processing\\n [1996]. In addition, most issues of the \\nIEEE Transactions on Image Processing\\n, \\nIEEE \\nTransactions on Circuits and Systems for Video Technology\\n, and \\nIEEE Transactions on Multimedia\\n include articles \\non video and still image compression, motion compensation, and watermarking.\\nDIP4E_GLOBAL_Print_Ready.indb   631\\n6/16/2017   2:11:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 632}),\n",
       " Document(page_content='632\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n8.1 \\nAnswer the following.\\n(a) \\nCan variable-length coding procedures be used \\nto compress a histogram equalized i\\nm\\nage with \\n2\\nn\\n intensity levels? Explain.\\n(b) \\nCan such an image contain spatial or tempo-\\nral redundancies that could be exploited for \\ndata compression?\\n8.2 \\nOne variation of run-length coding involves  \\n(1) coding only the runs of 0’\\ns or 1’s (not both) \\nand (2) assigning a special code to the start of \\neach line to reduce the effect of transmission \\nerrors. One possible code pair is \\n(, ) ,\\nxr\\nkk\\n where \\nx\\nk\\n and \\nr\\nk\\n represent the \\nk\\nth run’s starting coordi-\\nnate and run length, respectively. The code (0, 0) \\nis used to signal each new line.\\n(a) \\nDerive a general expression for the maxi-\\nmum average runs per scan line required \\nto guarantee data compression when run-\\nlength coding a \\n22\\nnn\\n×\\n binary image.\\n(b) \\nCompute the maximum allowable value for \\nn\\n = 10.\\n8.3 \\nConsider an 8-pixel line of intensity data, {108, \\n139,\\n 135, 244, 172, 173, 56, 99}. If it is uniformly \\nquantized with 4-bit accuracy, compute the rms \\nerror and rms signal-to-noise ratios for the quan-\\ntized data.\\n8.4 * \\nAlthough quantization results in information loss, \\nit is sometimes invisible to the eye\\n. For example, \\nwhen 8-bit pixels are uniformly quantized to \\nfewer \\nbits pixel,\\n false contouring often occurs. \\nIt can be reduced or eliminated using \\nimproved \\ngray-scale\\n (IGS) \\nquantization\\n. A sum (initially set \\nto zero) is formed from the current 8-bit intensity \\nvalue and the four least signiﬁcant bits of the pre-\\nviously generated sum. If the four most signiﬁ-\\ncant bits of the intensity value are 1111\\n2\\n, however, \\n0000\\n2\\n is added instead. The four most signiﬁcant \\nbits of the resulting sum are used as the coded \\npixel value.\\n(a) \\nConstruct the IGS code for the intensity data \\nin Problem 8.3.\\n(b) \\nCompute the rms error and rms signal-to-\\nnoise ratios for the IGS data.\\n8.5 \\nA \\n1024 024\\n×1\\n 8-bit image with 5.3 \\nbits pixel\\n \\nentropy [computed from its histogram using \\nEq.\\n (8-7)] is to be Huffman coded.\\n(a) \\nWhat is the maximum compression that can \\nbe expected?\\n(b) \\nWill it be obtained?\\n(c) \\nIf a greater level of lossless compression is \\nrequired,\\n what else can be done?\\n8.6 * \\nThe base \\ne\\n unit of information is commonly \\ncalled a \\nnat\\n,\\n and the base-10 information unit is \\ncalled a \\nHartley\\n. Compute the conversion factors \\nneeded to relate these units to the base-2 unit of \\ninformation (the bit).\\n8.7 * \\nProve that, for a zero-memory source with \\nq\\n sym-\\nbols\\n, the maximum value of the entropy is log \\nq\\n, \\nwhich is achieved if and only if all source symbols \\nare equiprobable. [\\nHint:\\n Consider the quantity   \\nlog ( )\\nqH\\nz\\n−\\n and note the inequality \\nln .\\nxx\\n≤−\\n1 ]\\n8.8 \\nAnswer the following.\\n(a) \\nHow many unique Huffman codes are there \\nfor a three-symbol source?\\n(b) \\nConstruct them.\\n8.9 \\nConsider the simple \\n48\\n×\\n, 8-bit image:\\n21   21   21   95   169   243   243   243\\n21   21   21   95   169   243   243   243\\n21   21   21   95   169   243   243   243\\n21   21   21   95   169   243   243   243\\n(a) \\nCompute the entropy of the image.\\n(b) \\nCompress the image using Huffman coding.\\n(c) \\nCompute the compression achieved and the \\neffectiveness of the Huffman coding\\n.\\n(d) * \\nConsider Huffman encoding pairs of pixels \\nrather than individual pixels\\n. That is, con-\\nsider the image to be produced by the sec-\\nond extension of the zero-memory source \\nDIP4E_GLOBAL_Print_Ready.indb   632\\n6/16/2017   2:11:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 633}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n633\\nthat produced the original image. What is the \\nentropy of the image when looked at as pairs \\nof pixels?\\n(e) \\nConsider coding the differences between \\nadjacent pixels\\n. What is the entropy of the \\nnew difference image? What does this tell us \\nabout compressing the image?\\n(f) \\nExplain the entropy differences in (a), (d) \\nand (e).\\n8.10 \\nUsing the Huffman code in Fig. 8.8, decode the \\nencoded string 0101000001010111110100.\\n8.11 \\nCompute Golomb code \\nGn\\n3\\n()\\n for \\n01 5\\n≤≤\\nn\\n.\\n8.12 \\nWrite a general procedure for decoding Golomb \\ncode \\nGn\\nm\\n() .\\n8.13 \\nWhy is it not possible to compute the Huffman \\ncode of the nonnegative integers\\n, \\nn\\n≥\\n0,\\n with the \\nprobability mass function of Eq.\\n (8-13)?\\n8.14 \\nCompute exponential Golomb code \\nGn\\nexp\\n()\\n2\\n for \\n01 5\\n≤≤\\nn\\n.\\n8.15 * \\nWrite a general procedure for decoding exponen-\\ntial Golomb code \\nGn\\nk\\nexp\\n() .\\n8.16 \\nPlot the optimal Golomb coding parameter \\nm\\n as \\na function of \\nr\\n for \\n01\\n<<\\nr\\n in Eq. (8-14).\\n8.17 \\nGiven a four-symbol source {\\na\\n, \\nb\\n, \\nc\\n, \\nd\\n} with source \\nprobabilities {0.1,\\n 0.4, 0.3, 0.2}, arithmetically \\nencode the sequence \\nbbadc\\n.\\n8.18 * \\nThe arithmetic decoding process is the reverse \\nof the encoding procedure\\n. Decode the message \\n0.23355 given the coding model\\nSymbol Probability\\na\\n0.2\\ne\\n0.3\\ni\\n0.1\\no 0.2\\nu 0.1\\n! 0.1\\n8.19 \\nUse the LZW coding algorithm to encode the \\n7-bit ASCII \\nstring “aaaaaaaaaaa”.\\n8.20 * \\nDevise an algorithm for decoding the LZW \\nencoded output of Example 8.7.\\n Since the dic-\\ntionary that was used during the encoding is not \\navailable, the code book must be reproduced as \\nthe output is decoded.\\n8.21 \\nDecode the BMP encoded sequence {3, 4, 5, 6, 0, 3, \\n103,\\n 125, 67, 0, 2, 47}.\\n8.22 \\nDo the following:\\n(a) \\nConstruct the entire 4-bit Gray code.\\n(b) \\nCreate a general procedure for converting a \\nGray-coded number to its binary equivalent \\nand use it to decode 0111010100111.\\n8.23 \\nUse the CCITT Group 4 compression algorithm \\nto code the second line of the following two-line \\nsegment:\\n01100111001111111100001\\n11111110001110000111111\\nAssume that the initial reference element \\na\\n0\\n is \\nlocated on the ﬁrst pixel of the second line seg-\\nment. (\\nNote:\\n Employ the CCITT 2-D code table \\nfrom the book website.)\\n8.24 * \\nDo the following.\\n(a) \\nList all the members of JPEG DC coefﬁcient \\ndifference category 3.\\n(b) \\nCompute their default Huffman codes using \\nusing the appropriate Huffamn code table \\nfrom the book website\\n.\\n8.25 \\nHow many computations are required to ﬁnd the \\noptimal motion vector of a macroblock of size \\n88\\n×\\n using the MAD optimality criterion, single \\npixel precision,\\n and a maximum allowable dis-\\nplacement of 8 pixels? What would it become for \\n¼ pixel precision?\\n8.26 \\nWhat are the advantages of using B-frames for \\nmotion compensation?\\n8.27 * \\nDraw the block diagram of the companion \\nmotion compensated video decoder for the \\nencoder in F\\nig. 8.36.\\n8.28 \\nAn image whose autocorrelation function is of \\nthe form of Eq.\\n (8-48) with \\nr\\nh\\n=\\n0\\n is to be DPCM \\ncoded using a second-order predictor\\n.\\n(a) \\nForm the autocorrelation matrix \\nR\\n and vec-\\ntor \\nr\\n.\\n(b) \\nFind the optimal prediction coefﬁcients.\\n(c) \\nCompute the variance of the prediction error \\nthat would result from using the optimal \\ncoefﬁcients\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   633\\n6/16/2017   2:11:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 634}),\n",
       " Document(page_content='634\\n    \\nChapter\\n \\n8\\n  \\nImage Compression and Watermarking\\n8.29 * \\nDerive the Lloyd-Max decision and reconstruc-\\ntion levels for \\nL\\n = 4 and the uniform probability \\ndensity function\\n \\nps\\nA\\nAsA\\n()\\n=\\n≤≤\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n2\\n0\\n−\\notherwise\\n8.30 \\nA radiologist from a well-known research hospital \\nrecently attended a medical conference at which \\na system that could transmit \\n4096 096\\n×4\\n 12-bit \\ndigitized X-ray images over standard \\nT1 (1.544 \\nMb/s) phone lines was exhibited. The system \\ntransmitted the images in a compressed format \\nusing a progressive technique in which a reason-\\nably good approximation of the X-ray was ﬁrst \\nreconstructed at the viewing station, then reﬁned \\ngradually to produce an error-free display. The \\ntransmission of the data needed to generate the \\nﬁrst approximation took approximately 5 or 6 s. \\n \\nReﬁnements were made every 5 or 6 s (on the \\naverage) for the next 1 min, with the ﬁrst and last \\nreﬁnements having the most and least signiﬁcant \\nimpact on the reconstructed X-ray, respectively. \\nThe physician was favorably impressed with the \\nsystem, because she could begin her diagnosis by \\nusing the ﬁrst approximation of the X-ray and \\ncomplete it as the error-free reconstruction of \\nthe X-ray was being generated. Upon returning \\nto her ofﬁce, she submitted a purchase request \\nto the hospital administrator. Unfortunately, the \\nhospital was on a relatively tight budget, which \\nrecently had been stretched by the hiring of an \\naspiring young electrical engineering graduate. To \\nappease the radiologist, the administrator gave \\nthe young engineer the task of designing such a \\nsystem. (He thought it might be cheaper to design \\nand build a similar system in-house. The hospital \\ncurrently owned some of the elements of such \\na system, but the transmission of the raw X-ray \\ndata took more than 2 min.) The administrator \\nasked the engineer to have an initial block dia-\\ngram by the afternoon staff meeting. With little \\ntime and only a copy of \\nDigital Image Processing \\nfrom his recent school days in hand, the engineer \\nwas able to devise a system conceptually to sat-\\nisfy the transmission and associated compression \\nrequirements. Construct a conceptual block dia-\\ngram of such a system, specifying the compression \\ntechniques you would recommend.\\n8.31 \\nShow that the lifting-based wavelet transform \\ndeﬁned by Eq.\\n (8-61) is equivalent to the tradi-\\ntional FWT ﬁlter bank implementation using the \\ncoefﬁcients in Table 7.1. Deﬁne the ﬁlter coefﬁ-\\ncients in terms of \\na\\n, \\nb\\n, \\ng\\n, \\nd\\n, and \\nK\\n.\\n8.32 \\nCompute the quantization step sizes of the sub-\\nbands for a JPEG-2000 encoded image in which \\nderived quantization is used and 8 bits are allot-\\nted to the mantissa and exponent of the 2\\nLL\\n sub-\\nband.\\n8.33 \\nHow would you add a visible watermark to an \\nimage in the frequenc\\ny domain?\\n8.34 * \\nDesign an invisible watermarking system based \\non the discrete F\\nourier transform.\\n8.35 \\nDesign an invisible watermarking system based \\non the discrete wavelet transform.\\nDIP4E_GLOBAL_Print_Ready.indb   634\\n6/16/2017   2:11:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 635}),\n",
       " Document(page_content='6359\\nMorphological Image \\nProcessing\\nPreview\\nThe word \\nmorphology\\n commonly denotes a branch of biology that deals with the form and structure of \\nanimals and plants. We use the same word here in the context of \\nmathematical morphology\\n as a tool for \\nextracting image components that are useful in the representation and description of region shape, such \\nas boundaries, skeletons, and the convex hull. We are interested also in morphological techniques for \\npre- or postprocessing, such as morphological ﬁltering, thinning, and pruning.\\nIn the following sections, we will develop a number of fundamental concepts in mathematical mor-\\nphology, and illustrate how they are applied in image processing. The material in this chapter begins a \\ntransition from methods whose inputs and outputs are images, to methods whose outputs are image \\nattributes, for tasks such as object extraction and description. Morphology is one of several tools devel-\\noped in the remainder of the book—such as segmentation, feature extraction, and object recognition—\\nthat form the foundation of techniques for extracting “meaning” from an image. The material in the \\nfollowing sections of this chapter deals with methods for processing both binary and grayscale images. \\nUpon completion of this chapter, readers should:\\n Understand basic concepts of mathematical \\nmorphology, and how to apply them to digital \\nimage processing.\\n Be familiar with the tools used for binary \\nimage morphology, including erosion, dilation, \\nopening, closing, and how to combine them to \\ngenerate more complex tools.\\n Be able to develop algorithms based on bi-\\nnary image morphology for performing tasks \\nsuch as morphological smoothing, edge de-\\ntection, extracting connected components, \\nand skeletonizing.\\n Be familiar with how binary image morphol-\\nogy can be extended to grayscale images.\\n Be able to develop algorithms for grayscale \\nimage processing for tasks such as textural \\nsegmentation, granulometry, computing gray-\\nscale image gradients, and others.\\nIn form and feature, face and limb, \\nI grew so like my brother \\nThat folks got taking me for him \\nAnd each for one another.\\nHenry Sambrook Leigh, Carols of Cockayne, The Twins\\nDIP4E_GLOBAL_Print_Ready.indb   635\\n6/16/2017   2:11:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 636}),\n",
       " Document(page_content='636\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n9.1 PRELIMINARIES  \\nThe language of mathematical morphology is set theory. As such, morphology offers \\na unified and powerful approach to numerous image processing problems. When \\nworking with images, sets in mathematical morphology represent objects in those \\nimages. In binary images, the sets in question are members of the 2-D integer space \\nZ\\n2\\n,\\n where each element of a set is a tuple (2-D vector) whose coordinates are the \\ncoordinates of an object (typically foreground) pixel in the image\\n. Grayscale digital \\nimages can be represented as sets whose components are in \\nZ\\n3\\n.\\n In this case, two \\ncomponents of each element of the set refer to the coordinates of a pixel,\\n and the \\nthird corresponds to its discrete intensity value. Sets in higher dimensional spaces \\ncan contain other image attributes, such as color and time-varying components.\\nMorphological operations are deﬁned in terms of sets. In image processing, we use \\nmorphology with two types of sets of pixels: \\nobjects\\n and \\nstructuring\\n \\nelements\\n (SE’\\ns). \\nTypically, objects are deﬁned as sets of foreground pixels. Structuring elements can \\nbe speciﬁed in terms of both foreground and background pixels. In addition, struc-\\nturing elements sometimes contain so-called “don’t care” elements, denoted by \\n×\\n, \\nsignifying that the value of that particular element in the SE does not matter\\n. In this \\nsense, the value can be ignored, or it can be made to ﬁt a desired value in the evalu-\\nation of an expression; for example, it might take on the value of a pixel in an image \\nin applications in which value matching is the objective. \\nBecause the images with which we work are rectangular arrays, and sets in general \\nare of arbitrary shape, applications of morphology in image processing require that \\nsets be embedded in rectangular arrays. In forming such arrays, we assign a back-\\nground value to all pixels that are not members of object sets. The top row in Fig. 9.1 \\nshows an example. On the left are sets in the graphical format you are accustomed \\nto seeing in book ﬁgures. In the center, the sets have been embedded in a rectangular \\nbackground (white) to form a graphical image.\\n†\\n On the right, we show a digital image \\n(notice the grid) which is the format we use for digital image processing. \\nStructuring elements are deﬁned in the same manner, and the second row in Fig. 9.1 \\nshows an example. There is an important difference between the way we represent \\ndigital images and digital structuring elements. Observe on the top right that there is \\na border of background pixels surrounding the objects, while there is none in the SE. \\nAs you will learn shortly, structuring elements are used in a form similar to spatial \\nconvolution kernels (see Fig. 3.28), and the image border just described is similar \\nto the padding we discussed in Section 3.4 and 3.5. The operations are different in \\nmorphology, but the padding and sliding operations are the same as in convolution. \\nIn addition to the set deﬁnitions given in Section 2.6, the concept of set reﬂection \\nand translation are used extensively in morphology in connection with structuring \\nelements. The \\nreﬂection\\n of a set (structuring element) \\nB \\nabout its origin, denoted by \\nˆ\\n,\\nB\\n is deﬁned as\\n†\\n Sets are shown as drawings of objects (e.g. squares and triangles) of arbitrary shape. A graphical image contains \\nsets that have been embedded into a background to form a rectangular array. When we intend for a drawing to \\nbe interpreted as a \\ndigital\\n image (or structuring element), we include a grid in illustrations that might otherwise \\nbe ambiguous. Objects in all drawings are shaded, and the background is shown in white. When working with \\nactual binary images, we say that objects are \\nforeground\\n pixels. All other pixels are \\nbackground\\n. \\n9.1\\nBefore proceeding, you \\nwill ﬁnd it helpful to \\nreview the discussion in \\nSection 2.4 dealing with \\nrepresenting images, the \\ndiscussion on  \\nconnectivity in Section \\n2.5, and the discussion on \\nsets in Section 2.6. \\nDIP4E_GLOBAL_Print_Ready.indb   636\\n6/16/2017   2:11:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 637}),\n",
       " Document(page_content='9.1\\n  \\nPreliminaries\\n    \\n637\\nˆ\\n,\\nBb b B\\n== − ∈\\n{}\\nww\\nfor\\n(9-1)\\nThat is, if \\nB\\n is a set of points in 2-D\\n, then \\nˆ\\nB\\n is the set of points in \\nB\\n whose \\n(,)\\nxy\\ncoordinates have been replaced by \\n(,) .\\n−−\\nxy\\n Figure 9.2 shows several examples of \\ndigital sets (structuring elements) and their reflection.\\n The dot denotes the origin of \\nthe SE. Note that reflection consists simply of rotating an SE by \\n180\\n°\\n about its origin, \\nand that all elements\\n, including the background and don’t care elements, are rotated. \\nThe \\ntranslation\\n of a set \\nB\\n by point \\nzz z\\n=\\n()\\n12\\n,,\\n denoted \\nB\\nz\\n(\\n)\\n, is deﬁned as\\nBc c b z b B\\nz\\n()\\n== + ∈\\n{}\\n,f o r\\n(9-2)\\nThat is, if \\nB\\n is a set of pixels in 2-D\\n, then \\nB\\nz\\n(\\n)\\n is the set of pixels in \\nB\\n whose \\n(,)\\nxy\\ncoordinates have been replaced by \\nxz yz\\n++\\n()\\n12\\n,.\\n This construct is used to trans-\\nlate (slide) a structuring element over an image\\n, and each location perform a set \\nReﬂection is the same \\noperation we performed \\nwith kernels prior to \\nspatial convolution, as \\nexplained in Section 3.4.\\nFIGURE 9.1\\n Top row. \\nLeft:\\n Objects represented as graphical sets. \\nCenter:\\n Objects embedded in a background to form \\na graphical image. \\nRight:\\n Object and background are digitized to form a digital image (note the grid). Second row: \\nExample of a structuring element represented as a set, a graphical image, and ﬁnally as a digital SE. \\nObjects representeed\\nas sets\\nObjects represented as\\na graphical image\\nDigital image\\nStructuring element\\nrepresented as a set\\nStructuring element \\nrepresented as a graphical image\\nDigital \\nstructuring element\\nFIGURE 9.2\\nStructuring  \\nelements and their \\nreﬂections about the \\norigin (the \\n×\\n’\\ns\\n are \\ndon’\\nt care elements, \\nand the dots denote \\nthe origin). Reﬂec-\\ntion is rotation by \\n180\\n°\\n of an SE about \\nits origin.\\n××\\n×\\n××\\n×\\nˆ\\nB\\nB\\n×\\n×\\nB\\n×\\n×\\nˆ\\nB\\nB\\nˆ\\nB\\nˆ\\nB\\nB\\nDIP4E_GLOBAL_Print_Ready.indb   637\\n6/16/2017   2:11:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 638}),\n",
       " Document(page_content='638\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\noperation between the structuring element and the area of the image directly under \\nit, as we explained in Fig. 3.28 for correlation and convolution. Both reflection and \\ntranslation are defined with respect to the \\norigin\\n of \\nB\\n.\\nAs an introduction to how morphological operations between images and struc-\\nturing elements are performed, consider Fig 9.3, which shows a simple binary image, \\nI\\n, \\nconsisting of an object (set) \\nA\\n, shown shaded, and a \\n33\\n×\\n SE whose elements are \\nall 1’\\ns (foreground pixels). The background pixels (0’s) are shown in white. We are \\ninterested in performing the following morphological operations: (1) form a new \\nimage, of the same size as \\nI\\n, consisting only of background values initially, (2) trans-\\nlate (slide) \\nB\\n over image \\nI\\n, and (3) at each increment of translation, if \\nB\\n is \\ncompletely\\n \\ncontained in \\nA\\n, mark the \\nlocation\\n of the origin of \\nB\\n as a \\nforeground\\n pixel in the new \\nimage; otherwise, leave it as a \\nbackground\\n point. Figure 9.3(c) is the result after the \\norigin of \\nB\\n has visited every element of \\nI\\n. We see that, when the origin of \\nB\\n is on \\na border element of \\nA\\n, part of \\nB\\n ceases to be contained in \\nA\\n, thus eliminating that \\nlocation of the origin of \\nB\\n as a possible foreground point of the new image. The net \\nresult is that the boundary of set \\nA\\n is \\neroded\\n, as Fig. 9.3(e) shows. Because of the way \\nin which we deﬁned the operation, the maximum excursion needed for \\nB\\n in \\nI\\n is when \\nthe origin of \\nB\\n (which is at its center) is contained in \\nA\\n. With \\nB\\n being of size \\n33\\n×\\n, \\nthe narrowest background padding we needed was one pixel wide\\n, as shown in Fig. \\n9.3(a). By using the smallest border needed for an operation, we keep the drawings \\nsmaller. In practice, we specify the width of padding based on the maximum dimen-\\nsions of the structuring elements used, regardless of the operations being performed.\\nWhen we use terminology such as “the structuring element \\nB\\n is contained in \\nset  \\nA\\n,” we mean \\nspeciﬁcally\\n that the \\nforeground\\n elements of \\nB\\n overlap \\nonly\\n ele-\\nments of \\nA\\n. This becomes an important issue when \\nB\\n also contains background and, \\npossibly, don’t care elements. Also, we use set \\nA\\n to denote \\nall\\n foreground pixels of \\nI\\n. \\nThose foreground elements can be a \\nsingle\\n object, as in Fig. 9.3, or they can represent \\ndisjoint\\n subsets of foreground elements, as in the ﬁrst row of Fig. 9.1. We will discuss \\nbinary images and structuring elements from Sections 9.2 through 9.7. Then, in Sec-\\ntion 9.8, we will extend the binary ideas to grayscale images and structuring elements. \\n9.2 EROSION AND DILATION  \\nWe begin the discussion of morphology by studying two operations: \\nerosion\\n and \\ndilation\\n. These operations are fundamental to morphological processing. In fact, \\nmany of the morphological algorithms discussed in this chapter are based on these \\ntwo primitive operations.\\nThe reason we gener-\\nally specify the padding \\nborder to be of the same \\ndimensions as \\nB\\n, is that \\nsome morphological \\noperations are deﬁned \\nfor an entire structuring \\nelement, and cannot be \\ninterpreted with respect \\nto the location of its \\norigin.\\n9.2\\nb a\\nc\\nFIGURE 9.3\\n  \\n(a) A binary image \\ncontaining one object \\n(set), \\nA\\n. (b) A struc-\\nturing element, \\nB\\n.  \\n(c) Image resulting \\nfrom a morphological \\noperation (see text). \\nImage \\nI\\nA\\nB\\nImage after morphological operation\\nDIP4E_GLOBAL_Print_Ready.indb   638\\n6/16/2017   2:11:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 639}),\n",
       " Document(page_content='9.2\\n  \\nErosion and Dilation\\n    \\n639\\nEROSION\\nMorphological expressions are written in terms of structuring elements and a set, \\nA\\n, of foreground pixels, or in terms of structuring elements and an image, \\nI\\n, that \\ncontains \\nA\\n. We consider the former approach first. With \\nA\\n and \\nB\\n as sets in \\nZ\\n2\\n,\\n the \\nerosion\\n of \\nA\\n by \\nB\\n,\\n denoted \\nAB\\n|\\n, is defined as\\n \\nAB z B A\\nz\\n|8\\n=\\n(\\n)\\n{}\\n \\n(9-3)\\nwhere \\nA\\n is a set of foreground pixels\\n, \\nB\\n is a structuring element, and the \\nz\\n’s are \\nforeground values (1’s). In words, this equation indicates that the erosion of \\nA\\n by \\nB\\n \\nis the set of all points \\nz\\n such that \\nB\\n, translated by \\nz,\\n is contained in \\nA. \\n(Remember, \\ndisplacement is defined with respect to the \\norigin\\n of \\nB.\\n)\\n \\nEquation (9-3) is the formu-\\nlation that resulted in the \\nforeground\\n pixels of the image in Fig. 9.3(c).\\nAs noted, we work with sets of foreground pixels embedded in a set of back-\\nground pixels to form a complete image, \\nI\\n. Thus, inputs and outputs of our morpho-\\nlogical procedures are images, not individual sets. We \\ncould\\n make this fact explicit \\nby writing Eq. (9-3) as\\n \\nIB z B A A I A AI\\nz\\ncc\\n|8 8 ´ 8\\n=\\n()\\n{}\\n{}\\n and \\n \\n(9-4)\\nwhere \\nI\\n is a rectangular array of foreground and background pixels\\n. The contents of \\nthe first braces say the same thing as Eq. (9-3), with the added clariﬁcation that \\nA\\n is \\na subset of (i.e., is contained in) \\nI\\n. The union with the operation inside the second set \\nof braces “adds” the pixels that are not in subset \\nA\\n (i.e., \\nA\\nc\\n,\\n which is the set of back-\\nground pixels) to the result from the ﬁrst braces\\n, requiring also that the background \\npixels be part of the rectangle deﬁned by \\nI\\n. In words, all this equation says is that \\nerosion of \\nI\\n by \\nB\\n is the set of all points, \\nz,\\n such that \\nB\\n, translated by \\nz,\\n is contained in \\nA. \\nThe equation also makes explicit that \\nA\\n is contained in \\nI\\n, that the result is embed-\\nded in a set of background pixels, and that the entire process is of the same size as \\nI\\n. \\nOf course, we do not use the cumbersome notation of Eq. (9-4), which we show \\nonly to emphasize an important point. Instead, we use the notation \\nAB\\n|\\n when a \\nmorphological operation uses \\nonly\\n foreground elements\\n, and \\nIB\\n|\\n when the oper-\\nation uses foreground \\nand\\n background elements\\n. This distinction may seem trivial, \\nbut suppose that we want to perform erosion with Eq. (9-3), using the foreground \\nelements of the structuring element in the last column in Fig. 9.2. This structuring \\nelement also has background elements, but Eq. (9-3) assumes that \\nB\\n only has fore-\\nground elements. In fact, erosion is \\ndefined\\n only for operations between foreground \\nelements, so writing \\nIB\\n|\\n would be meaningless without the “explanation” embed-\\nded in Eq.\\n (9-4). To avoid confusion, we use \\nA\\n in morphological expressions when \\nthe operation involves only foreground elements, and \\nI\\n when the operation also \\ninvolves background and/or “don’t-care” elements. We also avoid using standard \\nmorphological symbols like \\n| \\nwhen working with “mixed” SEs. For example, later \\nin Eq. (9-17) we use the symbol \\n\\x04\\x01\\nin the expression \\nIB z B I\\nz\\n\\x04\\n=\\n()\\n{} ,\\nP\\n8\\n which has \\nthe same \\nform\\n as Eq.\\n (9-3), but instead involves an entire image and the mixed-value \\nSE in the last column of Fig. 9.2. As you will see, using SE’s with mixed values adds \\nconsiderable power to morphological operations.\\nRemember, set \\nA\\n can \\nrepresent (be the union \\nof) multiple disjoint sets \\nof foreground pixels  \\n(i.e., objects).\\nDIP4E_GLOBAL_Print_Ready.indb   639\\n6/16/2017   2:11:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 640}),\n",
       " Document(page_content='640\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nReturning to our discussion of Eq. (9-3), because the statement that \\nB\\n has to be \\ncontained in \\nA\\n is equivalent to \\nB\\n not sharing any common elements with the back-\\nground (i.e., the set complement of \\nA\\n), we can express erosion equivalently as\\n \\nAB z B A\\nz\\nc\\n|¨\\n=\\n(\\n)\\n=∅\\n{}\\n(9-5)\\nwhere, as defined in Section 2.6, \\n∅\\n is the empty set.\\nF\\nigure 9.4 shows an example of erosion. The elements of set \\nA\\n (shaded) are the \\nforeground pixels of image \\nI\\n, and, as before, the background is shown in white. The \\nsolid boundary inside the dashed boundary in Fig. 9.4(c) is the limit beyond which \\nfurther displacements of the origin of \\nB\\n would cause some elements of the struc-\\nturing element to cease being completely contained in \\nA\\n. Thus, the locus of points \\n(locations of the origin of \\nB\\n) within (and including) this boundary constitutes the \\nforeground elements of the erosion of \\nA\\n by \\nB. \\nWe show the resulting erosion shaded \\nin Fig. 9.4(c), and the background as white. Erosion is the \\nset\\n of values of \\nz\\n that sat-\\nisfy Eqs. (9-3) or (9-5). The boundary of \\nA\\n is shown dashed in Figs. 9.4(c) and (e) as a \\nreference; it is not part of the erosion. Figure 9.4(d) shows an elongated structuring \\nelement, and Fig. 9.4(e) shows the erosion of \\nA\\n by this element. Note that the origi-\\nnal object was eroded to a line. As you can see, the result of erosion is controlled by \\nthe shape of the structuring element. In both cases, the assumption is that the image \\nwas padded to accommodate all excursions of \\nB\\n, and that the result was cropped to \\nthe same size as the original image, just as we did  with images processed by spatial \\nconvolution in Chapter 3. \\nEquations (9-3) and (9-5) are not the only deﬁnitions of erosion (see Problems 9.12 \\nand 9.13 for two additional, equivalent deﬁnitions). However, the former equations \\nhave the advantage of being more intuitive when the structuring element \\nB\\n is viewed \\nas if it were a spatial kernel that slides over a set, as in convolution.\\nb a\\nc\\ne\\nd\\nFIGURE 9.4\\n(a) Image \\nI,\\n  \\nconsisting of a set \\n(object) \\nA\\n, and back-\\nground.  \\n(b) Square SE, \\nB \\n(the \\ndot is the origin)\\n.\\n  \\n(c) Erosion of \\nA\\n by \\nB\\n (shown shaded in \\nthe resulting image).  \\n(d) Elongated SE.  \\n(e) Erosion of \\nA\\n  \\nby \\nB.\\n (The erosion \\nis a line.) The dotted \\nborder in (c) and (e) \\nis the boundary of \\nA\\n, \\nshown for reference.  \\nd\\n/\\n4\\nd\\n/\\n4\\nB\\nd\\n/\\n8\\nd\\n/\\n83\\nd\\n/\\n4\\nB\\nd\\n/\\n4\\nd\\nd\\n/\\n2\\nd\\n/\\n2\\nd\\n/\\n8\\nd\\n/\\n83\\nd\\n/\\n4\\nd\\nd\\nImage \\nI\\nA\\nBackground\\nA\\n \\n/H20001\\n \\nB\\nA\\n \\n/H20001\\n \\nB\\nI\\n \\n/H20001\\n \\nB\\nI\\n \\n/H20001\\n \\nB\\nDIP4E_GLOBAL_Print_Ready.indb   640\\n6/16/2017   2:11:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 641}),\n",
       " Document(page_content='9.2\\n  \\nErosion and Dilation\\n    \\n641\\nb a\\nd c\\nFIGURE 9.5\\nUsing erosion to \\nremove image \\ncomponents.  \\n(a) A \\n486 486\\n×\\n \\nbinary image of a \\nwire-bond mask \\nin which fore-\\nground pixels are \\nshown in white\\n. \\n(b)–(d) Image \\neroded using \\nsquare structuring \\nelements of sizes \\n11 11\\n×\\n, \\n15 15\\n×\\n, \\nand \\n45 45\\n×\\n  \\nelements\\n,  \\nrespectively, all \\nvalued 1.\\nEXAMPLE 9.1 :  Using erosion to remove image components.\\nFigure 9.5(a) is a binary image depicting a simple wire-bond mask. As mentioned previously, we gener-\\nally show the foreground pixels in binary images in white and the background in black. Suppose that \\nwe want to remove the lines connecting the center region to the border pads in Fig. 9.5(a). Eroding the \\nimage (i.e., eroding the \\nforeground\\n pixels of the image) with a square structuring element of size \\n11 11\\n×\\n \\nwhose components are all 1’\\ns removed most of the lines, as Fig. 9.5(b) shows. The reason that the two \\nvertical lines in the center were thinned but not removed completely is that their width is greater than \\n11 pixels. Changing the SE size to \\n15 15\\n×\\n elements and eroding the original image again did remove all \\nthe connecting lines\\n, as Fig. 9.5(c) shows. An alternate approach would have been to erode the image \\nin Fig. 9.5(b) again, using the same \\n11 11\\n×\\n,\\n or smaller, SE. Increasing the size of the structuring element \\neven more would eliminate larger components\\n. For example, the connecting lines and the border pads \\ncan be removed with a structuring element of size \\n45 45\\n×\\n elements applied to the original image, as \\nF\\nig. 9.5(d) shows.\\nWe see from this example that erosion shrinks or thins objects in a binary image. In fact, we can \\nview erosion as a \\nmorphological ﬁltering\\n operation in which image details smaller than the structuring \\nelement are ﬁltered (removed) from the image. In Fig. 9.5, erosion performed the function of a “line \\nﬁlter.” We will return to the concept of morphological ﬁlters in Sections 9.4\\n \\nand 9.8.\\nDILATION\\nWith \\nA\\n and \\nB\\n as sets in \\nZ\\n2\\n, the dilation of \\nA\\n by \\nB\\n,\\n denoted as \\nAB\\n{\\n, is defined as\\n \\nAB z B A\\nz\\n{¨\\n=∅\\n{}\\nP\\n()\\nˆ\\n≠\\n \\n(9-6)\\nDIP4E_GLOBAL_Print_Ready.indb   641\\n6/16/2017   2:11:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 642}),\n",
       " Document(page_content='642\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nThis equation is based on reflecting \\nB\\n about its origin and translating the reflection \\nby \\nz\\n, as in erosion. The dilation of \\nA\\n by \\nB\\n then is the set of all displacements, \\nz\\n, such \\nthat the foreground elements of \\nˆ\\nB\\n overlap at least one element of \\nA.\\n (Remember\\n, \\nz\\n is the displacement of the origin of \\nˆ\\n.)\\nB\\n Based on this interpretation, Eq. (9-6) can \\nbe written equivalently as\\n \\nAB z B A A\\nz\\n{¨ 8\\n=\\n{}\\nP\\n[( )\\nˆ\\n]\\n  \\n(9-7)\\nEquations (9-6) and (9-7) are not the only deﬁnitions of dilation currently in use \\n(see Problems 9.14 and 9.15 for two different,\\n yet equivalent, deﬁnitions). As with \\nerosion, the preceding deﬁnitions have the advantage of being more intuitive when \\nstructuring element \\nB\\n is viewed as a convolution kernel. As noted earlier, the basic \\nprocess of ﬂipping (rotating) \\nB\\n about its origin and then successively displacing it \\nso that it slides over set \\nA\\n is analogous to spatial convolution. However, keep in \\nmind that dilation is based on set operations and therefore is a nonlinear operation, \\nwhereas convolution is a sum of products, which is a linear operation.\\nUnlike erosion, which is a shrinking or thinning operation, dilation “grows” or \\n“thickens” objects in a binary image. The manner and extent of this thickening is con-\\ntrolled by the shape and size of the structuring element used. Figure 9.6(a) shows the \\nsame object used in Fig. 9.4 (the background area is larger to accommodate the dila-\\ntion), and Fig. 9.6(b) shows a structuring element (in this case \\nˆ\\nBB\\n=\\n because the SE \\nis symmetric about its origin).\\n The dashed line in Fig. 9.6(c) shows the boundary of \\nthe original object for reference, and the solid line shows the limit beyond which any \\nfurther displacements of the origin of \\nˆ\\nB\\n by \\nz\\n would cause the intersection of \\nˆ\\nB\\n and \\nA\\n to be empty\\n. Therefore, all points on and inside this boundary constitute the dila-\\ntion of \\nA\\n by \\nB\\n. Figure 9.6(d) shows a structuring element designed to achieve more \\ndilation vertically than horizontally, and Fig. 9.6(e) shows the dilation achieved with \\nthis element. \\nEXAMPLE 9.2 :  Using dilation to repair broken characters in an image.\\nOne of the simplest applications of dilation is for bridging gaps. Figure 9.7(a) shows the same image \\nwith broken characters that we studied in Fig. 4.48 in connection with lowpass ﬁltering. The maximum \\nlength of the breaks is known to be two pixels. Figure 9.7(b) shows a structuring element that can be \\nused for repairing the gaps. As noted earlier, we use white (1) to denote the foreground and black (0) for \\nthe background when working with images. Figure 9.7(c) shows the result of dilating the original image \\nwith the structuring element. The gaps were bridged. One important advantage of the morphological \\napproach over the lowpass ﬁltering method we used to bridge the gaps in Fig. 4.48 is that the morpho-\\nlogical method resulted directly in a binary image. Lowpass ﬁltering, on the other hand, started with \\na binary image and produced a grayscale image that would require thresholding to convert it back to \\nbinary form (we will discuss thresholding in Chapter 10). Observe that set \\nA\\n in this application consists \\nof numerous disjointed objects of foreground pixels.\\nDIP4E_GLOBAL_Print_Ready.indb   642\\n6/16/2017   2:11:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 643}),\n",
       " Document(page_content='9.2\\n  \\nErosion and Dilation\\n    \\n643\\nb a\\nc\\ne\\nd\\nFIGURE 9.6\\n(a) Image \\nI\\n,  \\ncomposed of set \\n(object) \\nA \\nand  \\nbackground.  \\n(b) Square SE (the \\ndot is the origin).  \\n(c) Dilation of \\nA\\n by \\nB\\n (shown shaded).  \\n(d) Elongated SE.  \\n(e) Dilation of \\nA\\n by \\nthis element. The  \\ndotted line in (c)  \\nand (e) is the  \\nboundary of \\nA\\n, \\nshown for  \\nreference. \\nd\\n/\\n4\\nd\\n/\\n4\\nB\\n \\n/H11005\\n \\nB\\nˆ\\nd\\nd\\n/\\n4\\nB\\n \\n/H11005\\n \\nB\\nˆ\\nA\\n \\n/H20003\\n \\nB\\nd\\n/\\n8\\nd\\n/\\n8\\nd\\nd\\n/\\n2\\nd\\nd\\n/\\n2\\nd\\nd\\nd\\n/\\n8\\nd\\n/\\n8\\nd\\nA\\n \\n/H20003\\n \\nB\\nA\\nImage\\n, \\nI\\nI\\n \\n/H20003\\n \\nB\\nI\\n \\n/H20003\\n \\nB\\nBackground\\n111\\n111\\n111\\nb\\na\\nc\\nFIGURE 9.7\\n(a) Low-resolution \\ntext showing \\nbroken characters \\n(see magniﬁed \\nview).  \\n(b) Structuring \\nelement.  \\n(c) Dilation of (a) \\nby (b). Broken \\nsegments were \\njoined.\\nDIP4E_GLOBAL_Print_Ready.indb   643\\n6/16/2017   2:11:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 644}),\n",
       " Document(page_content='644\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nDUALITY\\nErosion and dilation are \\nduals\\n of each other with respect to set complementation \\nand reflection. That is,\\n \\nAB A B\\nc\\nc\\n|{\\n(\\n)\\n=\\nˆ\\n \\n(9-8)\\nand\\n \\nAB A B\\nc\\nc\\n{|\\n(\\n)\\n=\\nˆ\\n \\n(9-9)\\nEquation (9-8) indicates that erosion of \\nA\\n by \\nB\\n is the complement of the dilation of \\nA\\nc\\n by \\nˆ\\n,\\nB\\n and vice versa. The duality property is useful when the structuring element \\nvalues are symmetric with respect to its origin (as often is the case),\\n so that \\nˆ\\n.\\nBB\\n=\\n \\nT\\nhen, we can obtain the erosion of \\nA\\n simply by dilating its background (i.e., dilating \\nA\\nc\\n) with the same structuring element and complementing the result. Similar com-\\nments apply to Eq. (9-9).\\nWe proceed to prove formally the validity of Eq. (9-8) in order to illustrate a typi-\\ncal approach for establishing the validity of morphological expressions. Starting with \\nthe deﬁnition of erosion, it follows that\\n \\nAB z B A\\nc\\nz\\nc\\n|8\\n(\\n)\\n=\\n(\\n)\\n{}\\nP\\n \\nIf set \\n()\\nB\\nz\\n is contained in \\nA\\n, then it follows that \\nBA\\nz\\nc\\n(\\n)\\n=∅\\n¨\\n,\\n in which case the \\npreceding expression becomes\\n \\nAB z B A\\nc\\nz\\nc\\nc\\n|¨\\n(\\n)\\n=\\n(\\n)\\n=∅\\n{}\\nP\\nBut the \\ncomplement\\n of the set of \\nz\\n’s that satisfy \\nBA\\nz\\nc\\n(\\n)\\n=∅\\n¨\\n is the set of \\nz\\n’\\ns such \\nthat \\nBA\\nz\\nc\\n(\\n)\\n∅\\n¨\\n≠\\n.\\n Therefore,\\n \\nAB z B A\\nAB\\nc\\nz\\nc\\nc\\n|¨\\n{\\n()\\n=\\n()\\n≠∅\\n{}\\n=\\n|\\nˆ\\n \\nwhere the last step follows from the definition of dilation in Eq.\\n (9-6) and its equiva-\\nlent form in Eq. (9-7). This concludes the proof. A similar line of reasoning can be \\nused to prove Eq. (9-9) (see Problem 9.16).\\n9.3 OPENING AND CLOSING  \\nAs you saw in the previous section, dilation expands the components of a set and \\nerosion shrinks it. In this section, we discuss two other important morphological \\noperations: opening and closing. Opening generally smoothes the contour of an \\nobject, breaks narrow isthmuses, and eliminates thin protrusions. Closing also tends \\n9.3\\nDIP4E_GLOBAL_Print_Ready.indb   644\\n6/16/2017   2:11:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 645}),\n",
       " Document(page_content='9.3\\n  \\nOpening and Closing\\n    \\n645\\nto smooth sections of contours, but, as opposed to opening, it generally fuses narrow \\nbreaks and long thin gulfs, eliminates small holes, and fills gaps in the contour.\\nThe \\nopening\\n of set \\nA\\n by structuring element \\nB\\n, denoted by \\nAB\\n/H11568\\n, is deﬁned as\\n \\nAB A B B\\n/H11568\\n=\\n(\\n)\\n|{\\n(9-10)\\nThus, the opening \\nA\\n by \\nB\\n is the erosion of \\nA\\n by \\nB\\n,\\n followed by a dilation of the result \\nby \\nB\\n.\\nSimilarly, the \\nclosing\\n of set \\nA\\n by structuring element \\nB\\n, denoted \\nAB\\n/H17033\\n,\\n is deﬁned \\nas\\n \\nAB A B B\\n/H17033\\n=\\n(\\n)\\n{|\\n(9-11)\\nwhich says that the closing of \\nA\\n by \\nB\\n is simply the dilation of \\nA\\n by \\nB\\n,\\n followed by \\nerosion of the result by \\nB\\n.\\nEquation (9-10) has a simple geometrical interpretation: The opening of \\nA\\n by \\nB\\n is \\nthe union of all the translations of \\nB\\n so that \\nB\\n ﬁts entirely in \\nA\\n. Figure 9.8(a) shows \\nan image containing a set (object) \\nA\\n and Fig. 9.8(b) is a solid, circular structuring ele-\\nment, \\nB\\n. Figure 9.8(c) shows some of the translations of \\nB\\n such that it is contained \\nwithin \\nA\\n, and the set shown shaded in Fig. 9.8(d) is the union of all such possible \\ntranslations. Observe that, in this case, the opening is a set composed of two disjoint \\nsubsets, resulting from the fact that \\nB\\n could not ﬁt in the narrow segment in the cen-\\nter of \\nA\\n. As you will see shortly, the ability to eliminate regions narrower than the \\nstructuring element is one of the key features of morphological opening. \\nThe interpretation that the opening of \\nA\\n by \\nB\\n is the union of all the translations \\nof \\nB\\n such that \\nB\\n ﬁts entirely within \\nA\\n can be written in equation form as \\n \\nAB B B A\\nzz\\n∘\\n∪\\n=\\n(\\n)\\n(\\n)\\n{}\\n8\\n \\n(9-12)\\nwhere \\n´\\n denotes the union of the sets inside the braces.\\nWhen a circular  \\nstructuring element is \\nused for opening, the \\nanalogy is often made of \\nthe shape of the opening \\nbeing determined by a \\n“rolling ball” reaching as \\nfar as it can on the inner  \\nboundary of a set. For \\nmorphological closing \\nthe ball rolls outside, and \\nthe shape of the closing \\nis determined by how far \\nthe ball can reach into \\nthe boundary.\\nb a\\nd c\\nFIGURE 9.8\\n(a) Image \\nI\\n, \\ncomposed of set \\n(object) \\nA\\n and \\nbackground.  \\n(b) Structuring  \\nelement, \\nB\\n. \\n(c) Translations \\nof \\nB\\n while being \\ncontained in \\nA. \\n(\\nA\\n \\nis shown dark for  \\nclarity.)  \\n(d) Opening of \\nA\\n \\nby \\nB\\n.\\nA\\nB\\nAB\\n/H11568\\nImage, \\nI\\nBackground\\nDIP4E_GLOBAL_Print_Ready.indb   645\\n6/16/2017   2:11:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 646}),\n",
       " Document(page_content='646\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nClosing has a similar geometric interpretation, except that now we translate \\nB \\noutside\\n \\nA\\n. The closing is then the \\ncomplement\\n of the union of all translations of \\nB\\n \\nthat \\ndo not\\n overlap \\nA\\n. Figure 9.9 illustrates this concept. Note that the boundary of \\nthe closing is determined by the furthest points \\nB\\n could reach without going inside \\nany part of \\nA\\n. Based on this interpretation, we can write the closing of \\nA\\n by \\nB\\n as\\n  \\nAB B B A\\nzz\\nc\\n/H17033\\n=\\n(\\n)\\n(\\n)\\n=∅\\n{}\\n⎡\\n⎣\\n⎤\\n⎦\\n¨\\n∪\\n \\n(9-13)\\nEXAMPLE 9.3 :  Morphological opening and closing.\\nFigure 9.10 shows in more detail the process and properties of opening and closing. Unlike Figs. 9.8 \\nand 9.9, whose main objectives are overall geometrical interpretations, this ﬁgure shows the individual \\nprocesses and also pays more attention to the relationship between the scale of the ﬁnal results and the \\nsize of the structuring elements. \\nFigure 9.10(a) shows an image containing a single object (set) \\nA\\n, and a disk structuring element. \\nFigure 9.10(b) shows various positions of the structuring element during erosion. This process resulted \\nin the disjoint set in Fig. 9.10(c). Note how the bridge between the two main sections was eliminated. \\nIts width was thin in relation to the diameter of the structuring element, which could not be completely \\ncontained in this part of the set, thus violating the deﬁnition of erosion. The same was true of the two \\nrightmost members of the object. Protruding elements where the disk did not ﬁt were eliminated. Figure \\n9.10(d) shows the process of dilating the eroded set, and Fig. 9.10(e) shows the ﬁnal result of opening. \\nMorphological opening removes regions that cannot contain the structuring element, smoothes object \\ncontours, breaks thin connections, and removes thin protrusions. \\nFigures 9.10(f) through (i) show the results of closing \\nA\\n with the same structuring element. As with \\nopening, closing also smoothes the contours of objects. However, unlike opening, closing tends to join \\nnarrow breaks, ﬁlls long thin gulfs, and ﬁlls objects smaller than the structuring element. In this example, \\nthe principal result of closing was that it ﬁlled the small gulf on the left of set \\nA\\n.\\nb a\\nd c\\nFIGURE 9.9\\n(a) Image \\nI\\n,  \\ncomposed of set \\n(object) \\nA, \\nand \\nbackground.  \\n(b) Structuring  \\nelement \\nB\\n. \\n(c) Translations of \\nB\\n \\nsuch that \\nB\\n does not \\noverlap any part  \\nof \\nA. \\n(\\nA\\n is shown \\ndark for clarity.)  \\n(d) Closing of \\nA\\n \\nby \\nB\\n.\\nB\\nAB\\n/H17033\\nBackground\\nA\\nImage, \\nI\\nDIP4E_GLOBAL_Print_Ready.indb   646\\n6/16/2017   2:11:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 647}),\n",
       " Document(page_content='9.3\\n  \\nOpening and Closing\\n    \\n647\\nAs with erosion and dilation, opening and closing are duals of each other with \\nrespect to set complementation and reﬂection: \\nAB A B\\nc\\nc\\n/H11568\\n/H17033\\n(\\n)\\n=\\n()\\nˆ\\n(9-14)\\nand\\nAB A B\\nc\\nc\\n/H17033\\n/H11568\\n(\\n)\\n=\\n()\\nˆ\\n(9-15)\\nWe leave the proof of these equations as an exercise (see Problem 9.20).\\na\\nb\\nc\\ne\\nd\\nf\\nh\\ng\\ni\\nFIGURE 9.10\\nMorphological \\nopening and  \\nclosing.  \\n(a) Image \\nI\\n, \\ncomposed of a \\nset (object ) \\nA\\n \\nand background; \\na solid, circular \\nstructuring element \\nis shown also. (The \\ndot is the origin.)  \\n(b) Structuring  \\nelement in  \\nvarious positions. \\n(c)-(i) The  \\nmorphological \\noperations used to \\nobtain the opening \\nand closing.\\nA\\nA\\nBackground\\nImage, \\nI\\nA\\n/H20001\\nB\\nA\\n/H20003\\nB\\nA\\n/H11554\\nB\\n/H11005\\n (\\nA\\n/H20001\\nB\\n)\\n/H20003\\nB\\nA\\n/H11554\\nB\\n/H11005\\n (\\nA\\n/H20003\\nB\\n)\\n/H20001\\nB\\nB\\nDIP4E_GLOBAL_Print_Ready.indb   647\\n6/16/2017   2:11:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 648}),\n",
       " Document(page_content='648\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nMorphological opening has the following properties:\\n(a) \\nAB\\n/H11568\\n is a subset of \\nA\\n.\\n(b) \\nIf \\nC\\n is a subset of \\nD\\n,\\n then \\nCB\\n/H11568\\n is a subset of \\nDB\\n/H11568\\n.\\n(c) \\n()\\n.\\nAB B AB\\n/H11568/H11568 /H11568\\n=\\nSimilarly, closing satisfies the following properties:\\n(a) \\nA\\n is a subset of \\nAB\\n/H17033\\n.\\n(b) \\nIf \\nC\\n is a subset of \\nD\\n,\\n then \\nCB\\n/H17033\\n is a subset of \\nDB\\n/H17033\\n.\\n(c) \\n() .\\nAB B AB\\n/H17033/H17033 /H17033\\n=\\nNote from condition (c) in both cases that multiple openings or closings of a set have \\nno effect after the operation has been applied once\\n.\\nEXAMPLE 9.4 : Using opening and closing for morphological ﬁltering.\\nMorphological operations can be used to construct ﬁlters similar in concept to the spatial ﬁlters discussed \\nin Chapter 3. The binary image in Fig. 9.11(a) shows a section of a ﬁngerprint corrupted by noise. In \\nterms of our previous notation, \\nA\\n is the set of all foreground (white) pixels, which includes objects of \\ninterest (the ﬁngerprint ridges) as well as white specks of random noise. The background is black, as \\nbefore. The noise manifests itself as white specks on a dark background and dark specks on the white \\ncomponents of the ﬁngerprint. The objective is to eliminate the noise and its effects on the print, while \\ndistorting it as little as possible. A morphological ﬁlter consisting of an opening followed by a closing can \\nbe used to accomplish this objective.\\nFigure 9.11(b) shows the structuring element we used. The rest of Fig. 9.11 shows the sequence of \\nsteps in the ﬁltering operation. Figure 9.11(c) is the result of eroding \\nA\\n by \\nB\\n. The white speckled noise \\nin the background was eliminated almost completely in the erosion stage of opening because in this case \\nmost noise components are smaller than the structuring element. The size of the noise elements (dark \\nspots) contained within the ﬁngerprint actually increased in size. The reason is that these elements are \\ninner boundaries that increase in size as objects are eroded. This enlargement is countered by perform-\\ning dilation on Fig. 9.11(c). Figure 9.11(d) shows the result.\\nThe two operations just described constitute the opening of \\nA\\n by \\nB\\n. We note in Fig. 9.11(d) that the \\nnet effect of opening was to reduce all noise components in both the background and the ﬁngerprint \\nitself. However, new gaps between the ﬁngerprint ridges were created. To counter this undesirable effect, \\nwe perform a dilation on the opening, as shown in Fig. 9.11(e). Most of the breaks were restored, but the \\nridges were thickened, a condition that can be remedied by erosion. The result, shown in Fig. 9.11(f), is \\nthe closing of the opening of Fig. 9.11(d). This ﬁnal result is remarkably clean of noise specks, but it still \\nshows some specks of noise that appear as single pixels. These could be eliminated by methods we will \\ndiscuss later in this chapter.\\n9.4 THE HIT-OR-MISS TRANSFORM  \\nThe morphological \\nhit-or-miss transform\\n (HMT) is a basic tool for shape detection. \\nLet \\nI\\n be a binary image composed of foreground (\\nA\\n) and background  pixels, respec-\\ntively. Unlike the morphological methods discussed thus far, the HMT utilizes \\ntwo\\n \\n9.4\\nDIP4E_GLOBAL_Print_Ready.indb   648\\n6/16/2017   2:11:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 649}),\n",
       " Document(page_content='9.4\\n  \\nThe Hit-or-Miss Transform\\n    \\n649\\n[(\\nA\\n/H11554\\nB\\n)\\n/H20003\\nB\\n]\\n/H20001\\nB\\n/H11005\\n (\\nA\\n/H11554\\nB\\n)\\n/H11554\\nB\\n(\\nA\\n/H11554\\nB\\n)\\n/H20003\\nB\\n(\\nA\\n/H20001\\nB\\n)\\n/H20003\\nB\\n/H11005\\nA\\n/H11554\\nB\\nA\\n/H20001\\nB\\nB\\n111\\n111\\n111\\nA\\n (foreground pixels)\\nb\\na\\nd\\nc\\nf\\ne\\nFIGURE 9.11\\n(a) Noisy image. \\n(b) Structuring  \\nelement.  \\n(c) Eroded image. \\n(d) Dilation of the \\nerosion (opening \\nof \\nA\\n). (e) Dilation \\nof the opening. \\n(f) Closing of the \\nopening. \\n(Original image \\ncourtesy of the \\nNational Institute \\nof Standards and \\nTechnology.)\\nstructuring elements: \\nB\\n1\\n,\\n for detecting shapes in the foreground, and \\nB\\n2\\n,\\n for detect-\\ning shapes in the background.\\n The HMT of image \\nI\\n is defined as\\n \\nIB z A B A\\nAB\\nA B\\nB\\nzz\\nc\\nc\\n\\x04\\n12\\n1\\n2\\n12\\n,\\n=\\n() ( )\\n{}\\n=\\n()\\n(\\n)\\nP\\n88\\n|¨|\\n and \\n  \\n(9-16)\\nwhere the second line follows from the definition of erosion in Eq. (9-3). In words, \\nthis equation says that the morphological HMT is the set of translations\\n, \\nz\\n, of struc-\\nturing elements \\nB\\n1\\n and \\nB\\n2\\n such that, \\nsimultaneously\\n, \\nB\\n1\\n found a match in the fore-\\nground (i.e., \\nB\\n1\\n is contained in \\nA\\n) \\nand\\n \\nB\\n2\\n found a match in the background (i.e., \\nB\\n2\\n \\nis contained in \\nA\\nc\\n).\\n The word “simultaneous” implies that \\nz\\n is the \\nsame\\n translation \\nof both structuring elements\\n. The word “miss” in the HMT arises from the fact that \\nB\\n2\\n finding a match in \\nA\\nc\\n is the same as \\nB\\n2\\n not finding (missing) a match in \\nA\\n.\\nFigure 9.12 illustrates the concepts just introduced. Suppose that we want to ﬁnd \\nthe location of the origin of object (set) \\nD\\n in image \\nI\\n. Here, \\nA\\n is the union of all \\nobject sets, so \\nD\\n is a subset of \\nA\\n. The need for two structuring elements capable \\nWith reference to the  \\nexplanation of Eq. (9-4), \\nwe show the  \\nmorphological HMT  \\noperation working \\ndirectly on image \\nI\\n, to \\nmake it explicit that the \\nstructuring elements \\nwork on sets of  \\nforeground \\nand\\n back-\\nground pixels  \\nsimultaneously. \\nDIP4E_GLOBAL_Print_Ready.indb   649\\n6/16/2017   2:11:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 650}),\n",
       " Document(page_content='650\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nof detecting properties of both the foreground and background becomes immedi-\\nately obvious. All three objects are composed of foreground pixels, and one way of \\nexplaining why they appear as different shapes is because each occupies a different \\narea of the background. In other words, the nature of a shape is determined by the \\ngeometrical arrangement of both foreground and background pixels.\\nFigure 9.12(a) shows that \\nI\\n is composed of foreground (\\nA\\n) and background pixels. \\nFigure 9.12(b) is \\nI\\nc\\n,\\n the complement of \\nI\\n.\\n The foreground of \\nI\\nc\\n is deﬁned as the set of \\npixels in \\nA\\nc\\n,\\n and the background is the union of the complement of the three objects. \\nF\\nigure 9.12(c) shows the two structuring elements needed to detect \\nD\\n. Element \\nB\\n1\\n is \\nequal to \\nD\\n itself. As Fig. 9.12(d) shows, the erosion of \\nA\\n by \\nB\\n1\\n contains a single point: \\nthe origin of \\nD\\n, as desired, but it also contains parts of object \\nC\\n. \\nb a\\nd c\\nf e\\nFIGURE 9.12\\n(a) Image  \\nconsisting of a \\nforeground (1’s) \\nequal to the union, \\nA\\n, of set of objects, \\nand a background \\nof 0’s.  \\n(b)  Image with \\nits foreground \\ndeﬁned as \\nA\\nc\\n. \\n(c) Structuring ele-\\nments designed to \\ndetect object \\nD\\n.  \\n(d) Erosion of \\nA\\n \\nby \\nB\\n1\\n.\\n \\n(e) Erosion of \\nA\\nc\\n \\nby \\nB\\n2\\n.\\n \\n(f) Intersection of \\n(d) and (e),  \\nshowing the  \\nlocation of the \\norigin of \\nD\\n, as \\ndesired. The dots \\nindicate the origin \\nof their respective  \\ncomponents. Each \\ndot is a single \\npixel.\\nACD E\\n=\\n/H33371/H33371\\nImage, \\nI\\nBackground\\n1\\nB\\n2\\nB\\n1\\nAB\\n|\\n2\\nc\\nAB\\n|\\n1,2\\n1\\n2\\nc\\nIB ABA B\\n=\\n||\\n\\x04\\n/H33370\\nOrigin of \\nD\\nBackground\\nImage:\\nd\\nd\\nd\\nd\\nd\\nd\\nBackground\\nD\\nE\\nC\\n¨\\nccc\\nCDE\\n¨\\nc\\nA\\nForeground = =\\nForeground\\npixels\\nc\\nI\\nDIP4E_GLOBAL_Print_Ready.indb   650\\n6/16/2017   2:11:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 651}),\n",
       " Document(page_content='9.4\\n  \\nThe Hit-or-Miss Transform\\n    \\n651\\nStructuring element \\nB\\n2\\n is designed to detect \\nD\\n in \\nI\\nc\\n.\\n Because \\nD\\n is composed of \\nbackground elements in \\nI\\nc\\n,\\n and erosion works with foreground elements, \\nB\\n2\\n has to \\nbe designed to detect the \\nborder\\n of \\nD\\n, which is composed of foreground pixels in \\nI\\nc\\n. \\nT\\nhe SE in Fig. 9.12(c) does precisely this. It consists of a rectangle of foreground ele-\\nments one pixel thick. The size of the rectangle is such that is encloses the size of \\nD\\n. \\nFigure 9.12(e) shows (shaded) the erosion of the foreground of \\nI\\nc\\n by \\nB\\n2\\n.\\n It contains \\nthe origin of \\nD\\n,\\n but is also contains parts of sets \\nA\\nc\\n and \\nC.\\n (The outer shaded area \\nin Fig. 9.12(e) is larger than shown (see Problem 9.25); the result was cropped to \\nthe same size as image \\nI \\nfor consistency.) The only elements that are common in \\nFigs. 9.12(d) and (e) is the origin of \\nD\\n, so the intersection of these two sets of ele-\\nments gives the location of that point, as desired. Figure 9.12(f) shows the ﬁnal result. \\nThe preceding explanation is the classic way of presenting the HMT using erosion, \\nwhich is deﬁned only for foreground pixels. A good question at this point is: Why \\nnot try to detect \\nD\\n directly in image \\nI\\n using a single structuring element, instead \\nof going thorough such a laborious process? The answer is that it is possible to do \\nso, but not in the “traditional” context of erosion the way we deﬁned it in Eqs. (9-3) \\nand (9-5). In order to detect \\nD\\n directly in image \\nI\\n, we would have to be able to pro-\\ncess foreground and background pixels \\nsimultaneously\\n, rather than processing just \\nforeground pixels, as required by the deﬁnition of erosion. \\nTo show how this can be done for the example in Fig. 9.12, we deﬁne a structuring \\nelement, \\nB\\n, identical to \\nD\\n, but having in addition a border of \\nbackground\\n elements \\nwith a width of one pixel. We can use a structuring element formed in such a way to \\nrestate the HMT as\\n \\nIB z B I\\nz\\n\\x04\\n=\\n()\\n{}\\nP\\n8\\n \\n(9-17)\\nThe \\nform\\n is the same as Eq.\\n (9-3), but now we test to see if \\n()\\nB\\nz\\n is a subset of \\nimage\\n \\nI\\n,  \\nwhich is composed of \\nboth\\n foreground and background pixels. This formulation is \\ngeneral, in the sense that \\nB\\n can be structured to detect any arrangement of pixels in \\nimage \\nI\\n, as Figs. 9.13 and 9.14 will illustrate. \\nFigure 9.13 shows graphically the same solution as Fig. 9.12(f), but using the \\nsingle structuring element discussed in the previous paragraph. Figure 9.14 shows \\nseveral examples based on using Eq. (9-17). The ﬁrst row shows the result of using \\na small SE composed of both foreground (shaded) and background elements. This \\nSE is designed to detect one-pixel holes (i.e., one background pixel surrounded by a \\nconnected border of foreground pixels) contained in image \\nI\\n.  The SE in the second \\nrow is capable of detecting the foreground corner pixel of the top, right corner of \\nthe object in \\nI\\n. Using this SE in Eq. (9-17) yielded the image on the right. As you \\ncan see, the correct pixel was identified. The last row of Fig. 9.14 is more interest-\\ning, as it shows a structuring element composed of foreground, background, and \\n“don’t care” elements which, as mentioned earlier, we denote by \\n×\\n’s.\\n You can think \\nof the value of a don’\\nt care element as always matching its corresponding pixel in \\nan image. In this example, when the SE is centered on the top, right corner pixel, \\nthe don’t care elements in the top of the SE can be considered to be background, \\nand the don’t care elements on the bottom row as foreground, producing a correct \\nDIP4E_GLOBAL_Print_Ready.indb   651\\n6/16/2017   2:11:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 652}),\n",
       " Document(page_content='652\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nmatch. When the SE is centered on the bottom, right corner pixel, the role of the \\ndon’t care elements is reversed, again resulting in a correct match. The other border \\npixels between the two corners were similarly detected by considering all don’t care \\nelements as foreground. Thus, using don’t care elements increases the flexibility of \\nstructuring elements to perform multiple roles. \\n9.5 SOME BASIC MORPHOLOGICAL ALGORITHMS  \\nWith the preceding discussion as a foundation, we are now ready to consider some \\npractical uses of morphology. When dealing with binary images, one of the principal \\napplications of morphology is in extracting image components that are useful in the \\n9.5\\nb a\\nc\\n \\nFIGURE 9.13\\n Same solution as in Fig. 9.12, but using Eq. (9-17) with a single structuring element. \\nC\\nD\\nE\\nImage, \\nI\\nBackground\\nBorder of\\nbackground pixels\\nB\\nOrigin of \\nD\\nBackground\\nImage,\\nIB\\n\\x04\\nImage, \\nI\\nB\\nImage,\\nIB\\n\\x04\\nImage, \\nI\\nB\\nImage,\\nIB\\n\\x04\\nImage, \\nI\\nB\\nImage,\\nIB\\n\\x04\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\n \\nFIGURE 9.14\\nThree examples \\nof using a single \\nstructuring  \\nelement and \\nEq. (9-17) to \\ndetect speciﬁc \\nfeatures. First \\nrow: detection \\nof single-pixel \\nholes. Second \\nrow: detection of \\nan upper-right \\ncorner. Third row: \\ndetection of  \\nmultiple features. \\nDIP4E_GLOBAL_Print_Ready.indb   652\\n6/16/2017   2:11:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 653}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n653\\nrepresentation and description of shape. In particular, we consider morphological \\nalgorithms for extracting boundaries, connected components, the convex hull, and \\nthe skeleton of a region. We also develop several methods (for region filling, thinning, \\nthickening, and pruning) that are used frequently for pre- or post-processing. We \\nmake extensive use in this section of “mini-images,” designed to clarify the mechan-\\nics of each morphological method as we introduce it. These binary images are shown \\ngraphically with foreground (1’s) shaded and background (0’s) in white, as before.\\nBOUNDARY EXTRACTION\\nThe boundary of a set \\nA\\n of foreground pixels, denoted by \\nb\\n()\\n,\\nA\\n can be obtained by \\nfirst eroding \\nA\\n by a suitable structuring element \\nB\\n,\\n and then performing the set dif-\\nference between \\nA\\n and its erosion. That is,\\n \\nb\\n()\\n( )\\nAA A B\\n=−\\n|\\n \\n(9-18)\\nFigure 9.15 illustrates the mechanics of boundary extraction. It shows a simple binary \\nobject,\\n a structuring element \\nB\\n, and the result of using Eq. (9-18). The structuring \\nelement in Fig. 9.15(b) is among the most frequently used, but it is not unique. For \\nexample, using a \\n55\\n×\\n structuring element of 1’s would result in a boundary between \\n2 and 3 pixels thick.\\n It is understood that the image in Fig. 9.15(a) was padded with \\na border of background elements, and that the results were cropped back to the \\noriginal size after the morphological operations were completed.\\nEXAMPLE 9.5 :  Boundary extraction.\\nFigure 9.16 further illustrates the use of Eq. (9-18) using a \\n33\\n×\\n structuring element of 1’s. As before \\nwhen working with images\\n, we show foreground pixels (1’s) in white and background pixels (0’s) in \\nblack. The elements of the SE, which are 1’s, also are treated as white. Because of the size of the structur-\\ning element used, the boundary in Fig. 9.16(b) is one pixel thick.\\nHOLE FILLING\\nAs mentioned in the discussion of Fig. 9.14, a \\nhole\\n may be defined as a background \\nregion surrounded by a connected border of foreground pixels. In this section, we \\ndevelop an algorithm based on set dilation, complementation, and intersection for \\nA\\n \\n/H20001\\n \\nB\\nA\\nB\\n() ( )\\nAA A B\\n=−\\nb\\n|\\nb a\\nd c\\n \\nFIGURE 9.15\\n(a) Set, \\nA\\n, of  \\nforeground pixels.  \\n(b) Structuring \\nelement.  \\n(c) \\nA\\n eroded by \\nB\\n. \\n(d) Boundary of \\nA\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   653\\n6/16/2017   2:11:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 654}),\n",
       " Document(page_content='654\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nfilling holes in an image. Let \\nA\\n denote a set whose elements are 8-connected bound-\\naries, with each boundary enclosing a background region (i.e., a hole). Given a point \\nin each hole, the objective is to fill all the holes with foreground elements (1’s).\\nWe begin by forming an array, \\nX\\n0\\n,\\n of 0’s (the same size as \\nI\\n,\\n the image containing \\nA\\n), except at locations in \\nX\\n0\\n that correspond to pixels that are known to be holes, \\nwhich we set to 1. Then, the following procedure ﬁlls all the holes with 1’s:\\n \\nXX B I k\\nkk\\nc\\n=\\n()\\n=\\n−\\n1\\n123\\n{¨\\n,,,\\n…\\n \\n(9-19)\\nwhere \\nB\\n is the symmetric structuring element in F\\nig. 9.17(c) . The algorithm termi-\\nnates at iteration step \\nk\\n if \\nXX\\nkk\\n=\\n−\\n1\\n.\\n Then, \\nX\\nk\\n contains all the filled holes. The set \\nunion of \\nX\\nk\\n and \\nI\\n contains all the filled holes and their boundaries.\\nThe dilation in Eq. (9-19) would ﬁll the entire area if left unchecked, but the \\nintersection at each step with \\nI\\nc\\n limits the result to inside the region of interest. This is \\nour ﬁrst example of how a morphological process can be \\nconditioned\\n to meet a desired \\nproperty. In the current application, the process is appropriately called \\nconditional \\ndilation\\n. The rest of Fig. 9.17 illustrates further the mechanics of Eq. (9-19). This exam-\\nple only has one hole, but the concept applies to any ﬁnite number of holes, assuming \\nthat a point inside each hole is given (we remove this requirement in Section 9.6).\\nEXAMPLE 9.6 :  Morphological hole ﬁlling.\\nFigure 9.18(a) shows an image of white circles with black holes. An image such as this might result from \\nthresholding into two levels a scene containing polished spheres (e.g., ball bearings). The dark circular \\nareas inside the spheres would result from reﬂections. The objective is to eliminate the reﬂections by \\nﬁlling the holes in the image. Figure 9.18(b) shows the result of ﬁlling all the spheres. Because it must be \\nknown whether black points are background points or sphere inner points (i.e., holes), fully automating \\nthis procedure requires that additional “intelligence” be built into the algorithm. We will give a fully \\nautomatic approach in Section 9.6 based on morphological reconstruction (see Problem 9.36\\n \\nalso).\\nRemember, the dila-\\ntion of image \\nX\\n by \\nB\\n \\nis the dilation of the \\nforeground\\n  \\nelements of \\nX\\n by \\nB\\n.\\nb a\\nFIGURE 9.16\\n(a) A binary  \\nimage. \\n(b) Result of \\nusing Eq. (9-18) \\nwith the  \\nstructuring  \\nelement in  \\nFig. 9.15(b).\\nDIP4E_GLOBAL_Print_Ready.indb   654\\n6/16/2017   2:11:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 655}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n655\\nX\\n0\\nX\\n1\\nX\\n2\\nX\\n6\\nX\\n8\\nB\\nA\\nI\\nc\\nI\\n8\\nXI\\n/H33371\\nc\\nA\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 9.17\\nHole ﬁlling. \\n(a) Set \\nA\\n (shown \\nshaded) contained \\nin image \\nI\\n.  \\n(b) Complement \\nof \\nI\\n.  \\n(c) Structuring  \\nelement \\nB.\\n \\nOnly\\n \\nthe foreground \\nelements are  \\nused in  \\ncomputations \\n(d) Initial point  \\ninside hole, set \\nto 1.  \\n(e)–(h) Various \\nsteps of Eq. (9-19). \\n(i) Final result \\n[union of (a) and \\n(h)].\\nEXTRACTION OF CONNECTED COMPONENTS\\nBeing able to extract connected components from a binary image is central to many \\nautomated image analysis applications. Let \\nA\\n be a set of foreground pixels consist-\\ning of one or more connected components, and form an image \\nX\\n0\\n (of the same size \\nas \\nI\\n, the image containing \\nA\\n) whose elements are 0’s (background values), except \\nat each location known to correspond to a point in each connected component in \\nA\\n, \\nConnectivity and  \\nconnected components \\nare discussed in  \\nSection 2.5.\\nb a\\nFIGURE 9.18\\n (a) Binary image. \\nThe white dots \\ninside the regions \\n(shown enlarged \\nfor clarity) are the \\nstarting points for \\nthe hole-ﬁlling \\nalgorithm.  \\n(b) Result of  \\nﬁlling all holes.\\nDIP4E_GLOBAL_Print_Ready.indb   655\\n6/16/2017   2:11:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 656}),\n",
       " Document(page_content='656\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nB\\nAX\\n0\\nX\\n1\\nX\\n3\\nX\\n6\\nX\\n2\\nI\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nFIGURE 9.19\\n(a) Structuring \\nelement. \\n(b) Image  \\ncontaining a set \\nwith one connected \\ncomponent. \\n(c) Initial array \\ncontaining a 1 in \\nthe region of the \\nconnected  \\ncomponent. \\n(d)–(g) Various \\nsteps in the  \\niteration of  \\nEq. (9-20)\\nwhich we set to 1 (foreground value). The objective is to start with \\nX\\n0\\n and find all \\nthe connected components in \\nI\\n. The following iterative procedure accomplishes this:\\n \\nXX B I k\\nkk\\n=\\n()\\n=\\n−\\n1\\n123\\n{¨\\n,,,\\n…\\n \\n(9-20)\\nwhere \\nB\\n is the SE in F\\nig. 9.19(a). The procedure terminates when \\nXX\\nkk\\n=\\n−\\n1\\n,\\n with \\nX\\nk\\n containing all the connected components of foreground pixels in the image. \\nBoth Eqs. (9-19) and (9-20) use conditional dilation to limit the growth of set dila-\\ntion, but Eq. (9-20) uses \\nI\\n instead of \\nI\\nc\\n.\\n This is because here we are looking for \\nforeground points\\n, while the objective of (9-19) is to find background points. Figure \\n9.19 illustrates the mechanics of Eq. (9-20), with convergence being achieved for \\nk\\n=\\n6.\\n Note that the shape of the structuring element used is based on 8-connec-\\ntivity between pixels\\n. As in the hole-filling algorithm, Eq. (9-20) is applicable to \\nany finite number of connected components contained in \\nI\\n, assuming that a point \\nis known in each. See Problem 9.37 for a completely automated procedure that \\nremoves this requirement.\\nEXAMPLE 9.7 :  Using connected components to detect foreign objects in packaged food.\\nConnected components are used frequently for automated inspection. Figure 9.20(a) shows an X-ray \\nimage of a chicken breast that contains bone fragments. It is important to be able to detect such foreign \\nobjects in processed foods before shipping. In this application, the density of the bones is such that their \\nnominal intensity values are signiﬁcantly different from the background. This makes extraction of the \\nbones from the background a simple matter by using a single threshold (thresholding was introduced in \\nSection 3.1 and we will discuss in more detail in Section 10.3). The result is the binary image in Fig. 9.20(b).\\nThe most signiﬁcant feature in this ﬁgure is the fact that the points that remain after thresholding \\nare clustered into objects (bones), rather than being scattered. We can make sure that only objects of \\nDIP4E_GLOBAL_Print_Ready.indb   656\\n6/16/2017   2:11:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 657}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n657\\n“signiﬁcant” size are contained in the binary image by eroding its foreground. In this example, we deﬁne \\nas signiﬁcant any object that remains after erosion with a \\n55\\n×\\n SE of 1’s. Figure 9.20(c) shows the result \\nof erosion.\\n The next step is to analyze the size of the objects that remain. We label (identify) these \\nobjects by extracting the connected components in the image. The table in Fig. 9.20(d) lists the results \\nof the extraction. There are 15 connected components, with four of them being dominant in size. This is \\nenough evidence to conclude that signiﬁcant, undesirable objects are contained in the original image. If \\nneeded, further characterization (such as shape) is possible using the techniques discussed in Chapter 11.\\nCONVEX HULL\\nA set, \\nS,\\n of points in the Euclidean plane is said to be \\nconvex\\n if and only if a straight \\nline segment joining any two points in \\nS\\n lies entirely within \\nS\\n. The \\nconvex hull\\n, \\nH\\n, \\nof \\nS\\n is the smallest convex set containing \\nS\\n. The \\nconvex deficiency\\n of \\nS\\n is defined as \\nthe set difference \\nHS\\n−\\n.\\n Unlike the Euclidean plane, the digital image plane (see \\nF\\nig. 2.19) only allows points at discrete coordinates. Thus, the sets with which we \\nwork are \\ndigital sets\\n. The same concepts of convexity are applicable to digital sets, \\nbut the definition of a convex digital set\\n \\nis slightly different. A \\ndigital set\\n, \\nA\\n,\\n is said \\nto be \\nconvex\\n if and only if its Euclidean convex hull only contains digital points \\nConnected\\ncomponent\\nNo. of pixels in\\nconnected comp\\n01\\n02\\n03\\n04\\n05\\n06\\n07\\n08\\n09\\n10\\n11\\n12\\n13\\n14\\n15\\n11\\n9\\n9\\n39\\n133\\n1\\n1\\n743\\n7\\n11\\n11\\n9\\n9\\n674\\n85\\nb\\na\\nd c\\nFIGURE 9.20\\n(a) X-ray image of \\na chicken ﬁlet with \\nbone fragments.  \\n(b) Thresholded \\nimage (shown as \\nthe negative for \\nclarity).  \\n(c) Image eroded \\nwith a \\n55\\n×\\n SE \\nof 1’s.  \\n(d) Number of \\npixels in the  \\nconnected  \\ncomponents of (c). \\n(Image (a)  \\ncourtesy of NTB \\nElektronische \\nGeraete GmbH, \\nDiepholz,  \\nGermany,  \\nwww.ntbxray.com.)\\nDIP4E_GLOBAL_Print_Ready.indb   657\\n6/16/2017   2:11:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 658}),\n",
       " Document(page_content='658\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nbelonging to \\nA\\n. A simple way to visualize if a digital set of foreground points is con-\\nvex is to join its boundary points by straight (continuous) Euclidean line segments. \\nIf only foreground points are contained within the set formed by the line segments, \\nthen the set is convex; otherwise it is not. The definitions of convex hull and convex \\ndeficiency given above for \\nS\\n, extend directly to digital sets. The following morpho-\\nlogical algorithm can be used to obtain an approximation of the convex hull of a set \\nA \\nof foreground pixels, embedded in a binary image, \\nI\\n.\\nLet \\nBi\\ni\\n,, , , ,\\n=\\n1234\\n denote the four structuring elements in Fig. 9.21(a). The pro-\\ncedure consists of implementing the morphological equation\\n \\nXX BX i\\nk\\nk\\ni\\nk\\nii\\nk\\ni\\n=\\n(\\n)\\n==\\n−−\\n11\\n1234 123\\n\\x04\\n´\\n,,,\\n,,,\\nand\\n…\\n \\n(9-21)\\nwith \\nXI\\ni\\n0\\n=\\n.\\n When the procedure converges using the \\ni\\nth structuring element (i.e\\n., \\nwhen \\nXX\\nk\\ni\\nk\\ni\\n=\\n−\\n1\\n),\\n we let \\nDX\\ni\\nk\\ni\\n=\\n.\\n Then, the convex hull of \\nA\\n is the union of the four \\nresults:\\n \\nCA D\\ni\\ni\\n(\\n)\\n=\\n=\\n1\\n4\\n∪\\n \\n(9-22)\\nThus, the method consists of iteratively applying the hit-or-miss transform to \\nI\\n with \\nB\\n1\\n until convergence, then letting \\nDX\\nk\\n11\\n=\\n,\\n where \\nk\\n is the step at which convergence \\noccurred.\\n The procedure is repeated with \\nB\\n2\\n (applied to \\nI\\n) until no further changes \\noccur, and so on. The union of the four resulting \\nD\\ni\\n \\nconstitutes the convex hull of \\nA\\n. \\nThe algorithm is initialized with \\nk\\n=\\n0\\n and \\nXI\\ni\\n0\\n=\\n every time that \\ni\\n (i.e\\n., the structur-\\ning element) changes. \\nFigure 9.21 illustrates the use of Eqs. (9-21) and (9-22). Figure 9.21(a) shows the \\nstructuring elements used to extract the convex hull. The origin of each element is \\nat its center. As before, the × entries indicate “don’t care” elements. Recall that the \\nHMT is said to have found a match of structuring element \\nB\\ni\\n in a \\n33\\n×\\n region of \\nI\\n,\\n if all the elements of \\nB\\ni\\n ﬁnd corresponding matches in that region. As noted ear-\\nlier, when computing a match, a “don’t care” element can be interpreted as always \\nmatching the value of its corresponding element in the image. Note in Fig. 9.21(a) \\nthat \\nB\\ni\\n is a clockwise rotation of \\nB\\ni\\n−\\n1\\n by 90\\n°\\n.\\nFigure 9.21(b) shows a set \\nA\\n for which the convex hull is sought.\\n As before, the \\nset is embedded in an array of background elements to form an image, \\nI\\n. Starting \\nwith \\nXI\\n0\\n1\\n=\\n resulted in the set in Fig. 9.21(c) after ﬁve iterations of Eq. (9-21). Then, \\nletting \\nXI\\n0\\n2\\n=\\n and again using Eq. (9-21) resulted in the set in Fig. 9.21(d) (con-\\nvergence was achieved in only two steps in this case).\\n The next two results were \\nobtained in the same manner. Finally, forming the union of the sets in Figs. 9.21(c), \\n(d), (e), and (f) resulted in the convex hull in Fig. 9.21(g). The contribution of each \\nstructuring element is highlighted in the composite set shown in Fig. 9.21(h).\\nOne obvious shortcoming of the procedure just discussed is that the convex hull \\ncan grow beyond the minimum dimensions required to guarantee convexity, thus \\nviolating the deﬁnition of the convex hull. This, in fact, is what happened in this \\ncase. One simple approach to reduce this growth is to place limits so that it does \\nnot extend beyond the vertical and horizontal dimensions of set \\nA\\n. Imposing this \\nDIP4E_GLOBAL_Print_Ready.indb   658\\n6/16/2017   2:11:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 659}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n659\\nlimitation on the example in Fig. 9.21 resulted in Fig. 9.22(a). Joining the boundary \\npixels of the reduced set (remember, the pixels are the center points of the squares) \\nshow that no set points lie outside these lines, indicating that the set is convex. By \\ninspection, you can see that no points can be deleted from this set without losing \\nconvexity, so the reduced set is the convex hull of \\nA\\n.\\nOf course, the limits we used to produce Fig. 9.22 do not constitute a general \\napproach for obtaining the minimum convex set enclosing a set in question; it is \\nsimply an easy-to-implement heuristic. The reason why the convex hull algorithm \\ndid not yield a closer approximation of the actual convex hull is because of the \\nstructuring elements used. The SEs in Fig. 9.21(a) “look” only in four orthogonal \\ndirections. We could achieve greater accuracy by looking in additional directions, \\nsuch as the diagonals, for example. The price paid is increased algorithm complexity \\nand a higher computational load.\\nX\\n0\\n1\\n \\n/H11005\\n \\nI\\nX\\n2\\n4\\nX\\n2\\n2\\nC\\n(\\nA\\n)\\nB\\n1\\n**\\n*\\n**\\nB\\n2\\n**\\n*\\n**\\nB\\n3\\n**\\n*\\n**\\nB\\n4\\n**\\n*\\n**\\nB\\n1\\nB\\n2\\nB\\n3\\nB\\n4\\nA\\nI\\n1\\n5\\nX\\n3\\n7\\nX\\na\\nb\\nc\\nd\\nh\\ne\\nf\\ng\\nFIGURE 9.21\\n(a) Structuring \\nelements.  \\n(b) Set \\nA\\n.  \\n(c)–(f) Results of \\nconvergence with \\nthe structuring  \\nelements shown \\nin (a).  \\n(g) Convex hull. \\n(h) Convex hull \\nshowing the \\ncontribution of \\neach structuring \\nelement. \\nDIP4E_GLOBAL_Print_Ready.indb   659\\n6/16/2017   2:11:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 660}),\n",
       " Document(page_content='660\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nTHINNING\\nThinning of a set \\nA\\n of foreground pixels by a structuring element \\nB\\n, denoted \\nAB\\nz\\n,\\ncan be defined in terms of the hit-or-miss transform:\\n \\nABA AB\\nAA\\nB\\nc\\nz\\n¨\\n=−\\n()\\n=\\n()\\n\\x04\\n\\x04\\n \\n(9-23)\\nwhere the second line follows from the definition of set difference given in Eq. (2-40). \\nA more useful expression for thinning \\nA\\n symmetrically is based on a \\nsequence\\n of \\nstructuring elements:\\n \\nBB B B B\\nn\\n{}\\n=\\n{}\\n123\\n,,, ,\\n…\\n \\n(9-24)\\nUsing this concept, we now define thinning by a sequence of structuring elements as\\n \\nAB A B B B\\nn\\nzz z z\\n{}\\n=\\n(\\n)\\n(\\n)\\n()\\n()\\n……\\n12\\n(9-25)\\nThe process is to thin \\nA\\n by one pass with \\nB\\n1\\n,\\n then thin the result with one pass of \\nB\\n2\\n, \\nand so on,\\n until \\nA\\n is thinned with one pass of \\nB\\nn\\n.\\n The entire process is repeated until \\nno further changes occur after one complete pass through all structuring elements\\n. \\nEach individual thinning pass is performed using Eq. (9-23).\\nFigure 9.23(a) shows a set of structuring elements used routinely for thinning \\n(note that \\nB\\ni\\n is equal to \\nB\\ni\\n−\\n1\\n rotated clockwise by \\n45\\n°\\n),\\n and Fig. 9.23(b) shows a \\nset \\nA\\n to be thinned,\\n using the procedure just discussed. Figure 9.23(c) shows the \\nresult of thinning \\nA \\nwith one pass of \\nB\\n1\\n to obtain \\nA\\n1\\n.\\n Figure 9.23(c) is the result of \\nthinning \\nA\\n1\\n with \\nB\\n2\\n,\\n and Figs. 9.21(e) through (k) show the results of passes with \\nthe remaining structuring elements (there were no changes from \\nA\\n7\\n to \\nA\\n8\\n or from \\nA\\n9\\n to \\nA\\n11\\n.)\\n Convergence was achieved after the second pass of \\nB\\n6\\n.\\n Figure 9.23(l) \\nshows the thinned result.\\n Finally, Fig. 9.23(m) shows the thinned set converted to \\nm\\n-connectivity (see Section 2.5\\n \\nand Problem 9.29) to eliminate multiple paths.\\nTHICKENING\\nThickening is the morphological dual of thinning and is defined by the expression\\n \\nABA AB\\n}´\\n=\\n()\\n\\x04\\n \\n(9-26)\\nAs before, we assume \\nthat the image containing \\nA\\n was padded to accom-\\nmodate all excursions  \\nof \\nB\\n, and that the result \\nwas cropped. We show \\nonly \\nA\\n for simplicity.\\nb a\\n \\nFIGURE 9.22\\n(a) Result of limiting \\ngrowth of the convex \\nhull algorithm. \\n(b) Straight lines \\nconnecting the \\nboundary points \\nshow that the new set \\nis convex also.\\nA\\nDIP4E_GLOBAL_Print_Ready.indb   660\\n6/16/2017   2:11:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 661}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n661\\nwhere \\nB\\n is a structuring element suitable for thickening. As in thinning, thickening \\ncan be defined as a sequential operation:\\n \\nAB A B B B\\nn\\n}} } }\\n{}\\n=\\n(\\n)\\n(\\n)\\n()\\n()\\n……\\n12\\n `\\n(9-27)\\nThe structuring elements used for thickening have the same form as those shown \\nin F\\nig. 9.23(a), but with all 1’s and 0’s interchanged. However, a separate algorithm \\nfor thickening is seldom used in practice. Instead, the usual procedure is to thin the \\nbackground of the set in question, then complement the result. In other words, to \\nthicken a set \\nA\\n we form \\nA\\nc\\n,\\n thin \\nA\\nc\\n,\\n and then complement the thinned set to obtain \\nthe thickening of \\nA\\n.\\n Figure 9.24 illustrates this procedure. As before, we show only \\nset \\nA\\n and image \\nI\\n, and not the padded version of \\nI\\n.\\nDepending on the structure of \\nA\\n, this procedure can result in disconnected points, \\nas Fig. 9.24(d) shows. Hence thickening by this method usually is followed by post-\\nprocessing to remove disconnected points. Note from Fig. 9.24(c) that the thinned \\nbackground forms a boundary for the thickening process. This useful feature is not \\npresent in the direct implementation of thickening using Eq. (9-27), and it is one of \\nthe principal reasons for using background thinning to accomplish thickening.\\n* *\\nB\\n2\\n*\\n*\\nB\\n3\\n* *\\nB\\n4\\n*\\n*\\nB\\n5\\n* *\\nB\\n6\\n*\\n*\\nB\\n7\\n* *\\nB\\n8\\n*\\n*\\nOrigin\\nA\\n1\\n \\n/H11005\\n \\nA\\n \\n/H20002\\n \\nB\\n1\\nA\\n2\\n \\n/H11005\\n \\nA\\n1\\n \\n/H20002\\n \\nB\\n2\\nA\\n3\\n \\n/H11005\\n \\nA\\n2\\n \\n/H20002\\n \\nB\\n3\\nA\\n6\\n \\n/H11005\\n \\nA\\n5\\n \\n/H20002\\n \\nB\\n6\\nA\\n7\\n \\n/H11005\\n \\nA\\n6\\n \\n/H20002\\n \\nB\\n7\\nA\\n9\\n \\n/H11005\\n \\nA\\n8\\n \\n/H20002\\n \\nB\\n1\\nA\\n12\\n \\n/H11005\\n \\nA\\n11\\n \\n/H20002\\n \\nB\\n4\\nA\\n14\\n \\n/H11005\\n \\nA\\n13\\n \\n/H20002\\n \\nB\\n6\\nA\\n14 \\nconverted to\\nm\\n-connectivity.\\nNo more changes after this. \\nA\\n4\\n \\n/H11005\\n \\nA\\n3\\n \\n/H20002\\n \\nB\\n4\\nA\\n5\\n \\n/H11005\\n \\nA\\n4\\n \\n/H20002\\n \\nB\\n5\\n(\\nA\\n8\\n \\n/H11005\\n \\nA\\n7\\n \\n/H20002\\n \\nB\\n8\\n \\n/H11005\\n \\nA\\n7\\n)\\n(\\nA\\n11\\n \\n/H11005\\n \\nA\\n10\\n \\n/H11005\\n \\nA\\n9\\n)\\nA\\nImage, \\nI\\nB\\n1\\na\\nb\\nc\\ne\\nd\\nf\\nh\\ng\\ni\\nk\\nj\\nm\\nl\\nFIGURE 9.23\\n(a) Structuring  \\nelements. \\n(b) Set \\nA\\n. \\n(c) Result of thinning \\nA\\n with \\nB\\n1\\n (shaded). \\n(d) Result of thinning \\nA\\n1\\n with \\nB\\n2\\n.\\n \\n(e)–(i) Results of \\nthinning with the next \\nsix SEs. (There was no \\nchange between \\nA\\n7\\n \\nand \\nA\\n8\\n.)  \\n(j)–(k) Result of using \\nthe ﬁrst four elements \\nagain.\\n  \\n(l) Result after  \\nconvergence.  \\n(m) Result converted \\nto \\nm\\n-connectivity.\\nDIP4E_GLOBAL_Print_Ready.indb   661\\n6/16/2017   2:12:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 662}),\n",
       " Document(page_content='662\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nA\\nImage, \\nI\\nc\\nA\\nb a\\nd c\\ne\\nFIGURE 9.24\\n(a) Set \\nA\\n.  \\n(b) Complement of \\nA\\n. \\n(c) Result of  \\nthinning the  \\ncomplement.  \\n(d) Thickened set \\nobtained by  \\ncomplementing (c). \\n(e) Final result, with \\nno disconnected \\npoints.\\nSKELETONS\\nAs Fig. 9.25 shows, the notion of a \\nskeleton\\n \\nSA\\n()\\n of a set \\nA\\n is intuitively simple\\n. We \\ndeduce from this figure that\\n(a) \\nIf \\nz\\n is a point of \\nSA\\n()\\n,\\n and \\nD\\nz\\n()\\n is the largest disk centered at \\nz\\n and contained \\nin \\nA\\n, one cannot ﬁnd a larger disk (not necessarily centered at \\nz\\n) containing \\nD\\nz\\n()\\n and simultaneously included in \\nA\\n. A disk \\nD\\nz\\n()\\n satisfying these conditions \\nis called a \\nmaximum disk\\n.\\n(b) \\nIf \\nD\\nz\\n()\\n is a maximum disk, it touches the boundary of \\nA\\n at two or more differ-\\nent places.\\nThe skeleton of \\nA\\n can be expressed in terms of erosions and openings. That is, it can \\nbe shown (Serra [1982]) that\\n \\nSA S A\\nk\\nk\\nK\\n()\\n=\\n()\\n=\\n0\\n∪\\n(9-28)\\nwith\\n \\nSA A k B A k B B\\nk\\n()\\n=\\n()\\n−\\n()\\n||\\n∘\\n \\n(9-29)\\nwhere \\nB\\n is a structuring element,\\n and \\nAk B\\n|\\n()\\n indicates \\nk\\n successive erosions start-\\ning with \\nA\\n; that is, \\nA\\n is first eroded by \\nB\\n, the result is eroded by \\nB\\n, and so on:\\n \\nAk B AB B B\\n||\\n|\\n| |\\n()\\n=\\n()\\n()\\n(\\n)\\n(\\n)\\n……\\n(9-30)\\nk\\n times\\n. \\nK\\n in E q. (9-28) is the last iterative step before \\nA\\n erodes to an empty set. In \\nother words,\\n \\nKk A k B\\n=\\n()\\n≠∅\\n{}\\nmax\\n|\\n \\n(9-31)\\nThe formulation in Eqs. (9-28) and (9-29) indicate that \\nSA\\n()\\n can be obtained as the \\nunion of the skeleton subsets \\nSA\\nk\\n()\\n, \\nkK\\n=\\n012\\n,,\\n, , .\\n…\\n \\nWe will discuss skeletons \\nin more detail in Section \\n11.2.\\nDIP4E_GLOBAL_Print_Ready.indb   662\\n6/16/2017   2:12:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 663}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n663\\nIt can be \\nshown (Serra [1982]) that \\nA\\n can be reconstructed from these subsets:\\n \\nAS A k B\\nk\\nk\\nK\\n=\\n()\\n()\\n=\\n{\\n0\\n∪\\n \\n(9-32)\\nwhere \\nSA k B\\nk\\n()\\n()\\n{\\n denotes \\nk\\n successive dilations\\n, starting with \\nSA\\nk\\n()\\n; that is,\\n \\nSA k B SA B B B\\nkk\\n()\\n()\\n=\\n()\\n()\\n(\\n)\\n(\\n)\\n()\\n{\\n{{{{\\n……\\n(9-33)\\nEXAMPLE 9.8 :  Computing the skeleton of a simple set.\\nFigure 9.26 illustrates the concepts just discussed. The ﬁrst column shows the original set (at the top) \\nand two erosions by the structuring element \\nB\\n shown in the ﬁgure. Note that one more erosion would \\nyield the empty set, so \\nK\\n=\\n2\\n in this case. The second column shows the opening by \\nB\\n of the sets in the \\nﬁrst column.\\n These results are easily explained by the ﬁtting characterization of the opening operation \\ndiscussed in connection with Fig. 9.8. The third column contains the set differences between the ﬁrst and \\nsecond columns. Thus, the three entries in the third column are \\nSA\\n0\\n() ,\\n \\nSA\\n1\\n() ,\\n and \\nSA\\n2\\n() ,\\n respectively.\\nT\\nhe fourth column contains two partial skeletons, and the ﬁnal result at the bottom of the column. \\nThe ﬁnal skeleton not only is thicker than it needs to be but, more important, it is not connected. This \\nresult is not unexpected, as nothing in the preceding formulation of the morphological skeleton guar-\\nantees connectivity. Morphology produces an elegant formulation in terms of erosions and openings of \\nthe given set. However, heuristic formulations (see Section 11.2) are needed if, as is usually the case, the \\nskeleton must be maximally thin, connected, and minimally eroded.\\nA\\nSkeleton of \\nA\\nb a\\nd c\\nFIGURE 9.25\\n(a) Set \\nA\\n.  \\n(b) Various  \\npositions of  \\nmaximum disks \\nwhose centers \\npartially deﬁne \\nthe skeleton of \\nA\\n. \\n(c) Another  \\nmaximum disk, \\nwhose center \\ndeﬁnes a different \\nsegment of the \\nskeleton of \\nA\\n.  \\n(d) Complete \\nskeleton (dashed).\\nDIP4E_GLOBAL_Print_Ready.indb   663\\n6/16/2017   2:12:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 664}),\n",
       " Document(page_content='664\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n1\\n0\\nk\\n2\\nB\\nA\\nS\\n(\\nA\\n)\\nA \\n/H20001\\n kB\\n(\\nA \\n/H20001\\n kB\\n)\\n/H11568\\nB\\nS\\nk\\n(\\nA\\n)\\nS\\nk\\n(\\nA\\n) \\n/H20003\\n kB\\n/H20668\\nS\\nk\\n(\\nA\\n) \\n/H20003\\n kB\\nk \\n/H11005 \\n0\\nK\\n/H20668\\nS\\nk\\n(\\nA\\n)\\nk \\n/H11005 \\n0\\nK\\nFIGURE 9.26\\nImplementation \\nof Eqs. (9-28) \\nthrough (9-33). \\nThe original set is \\nat the top left, and \\nits morphologi-\\ncal skeleton is at \\nthe bottom of the \\nfourth column. \\nThe reconstructed \\nset is at the  \\nbottom of the \\nsixth column.\\nThe entries  in the ﬁfth and sixth columns deal with reconstructing the original set from its skeleton subsets. \\nThe ﬁfth column are the dilations of \\nSA\\nk\\n()\\n;  that is,  \\nSA\\n0\\n() ,\\n \\nSk B\\n1\\n() ,\\n{\\n and  \\nSA B SA B B\\n22\\n2\\n() () .\\n{{ {\\n=\\n()\\n \\nFinally, the last column shows reconstruction of set \\nA\\n which, according to Eq. (9-32), is the union of the \\ndilated skeleton subsets shown in the ﬁfth column.\\nPRUNING\\nPruning methods are an essential complement to thinning and skeletonizing algo-\\nrithms, because these procedures tend to leave \\nspurs\\n (“parasitic” components) that \\nneed to be “cleaned up” by postprocessing. We begin the discussion with a pruning \\nproblem, then develop a solution based on the material introduced in the preceding \\nsections. Thus, we take this opportunity to illustrate how to solve a problem by com-\\nbining several of the morphological techniques discussed up to this point.\\nA common approach in the automated recognition of hand-printed characters is \\nto analyze the shape of the skeleton of a character. These skeletons often contain \\nspurs,\\n \\ncaused during erosion by noise and non-uniformities in the character strokes. \\nDIP4E_GLOBAL_Print_Ready.indb   664\\n6/16/2017   2:12:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 665}),\n",
       " Document(page_content='9.5\\n  \\nSome Basic Morphological Algorithms\\n    \\n665\\nIn this section we develop a morphological technique for handling this problem, \\nstarting with the assumption that the length of a parasitic component does not \\nexceed a speciﬁed number of pixels.\\nFigure 9.27(a) shows the skeleton of a hand-printed letter “a.” The spur on the \\nleftmost part of the character exempliﬁes what we are interested in removing. The \\nsolution is based on suppressing a spur branch by successively eliminating its end \\npoint. Of course, this also shortens (or eliminates) other branches in the character \\nbut, in the absence of other structural information, the assumption in this example is \\nthat any branch with three or less pixels is to be eliminated. Thinning of a set \\nA,\\n with \\na sequence of structuring elements designed to detect only end points, achieves the \\ndesired result. That is, let\\n \\nXA B\\n1\\n=\\n{}\\nz\\n \\n(9-34)\\nwhere \\nB\\n{}\\n denotes the structuring element sequence in Fig. 9.27(b) [see Eq. (9-24) \\nregarding structuring-element sequences]. The sequence of structuring elements \\nconsists of two different structures, each of which is rotated 90° for a total of eight \\nelements. The \\n×\\n in Fig. 9.27(b) signifies a “don’t care” condition, as defined earlier. \\n(Note that each SE is a detector for an end point in a particular orientation.) \\nWe may deﬁne an \\nend \\npoint\\n as the center point \\nof a 3 \\n×\\n 3 region that  \\nsatisﬁes any of the  \\narrangements in  \\nFig. 9.27(b).\\nB\\n5\\nB\\n6\\nB\\n7\\nB\\n8\\nB\\n1\\nB\\n2\\nB\\n4\\nB\\n3\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\nb a\\nd c\\nf e\\nFIGURE 9.27\\n (a) Set \\nA\\n of  \\nforeground pixels \\n(shaded).  \\n(b) SEs used for \\ndeleting end points.  \\n(c) Result of three \\ncycles of thinning. \\n(d) End points  \\nof (c).  \\n(e) Dilation of end \\npoints conditioned \\non (a).  \\n(f) Pruned image.\\nDIP4E_GLOBAL_Print_Ready.indb   665\\n6/16/2017   2:12:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 666}),\n",
       " Document(page_content='666\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nApplying Eq. (9-34) to \\nA\\n three times yielded the set \\nX\\n1\\n in Fig. 9.27(c). The next \\nstep is to “restore” the character to its original form, but with the parasitic branches \\nremoved. This requires that we ﬁrst form a set \\nX\\n2\\n containing all end points in \\nX\\n1\\n \\n[Fig. 9.27(e)]:\\n \\nXX B\\nk\\nk\\n21\\n1\\n8\\n=\\n(\\n)\\n=\\n\\x04\\n∪\\n \\n(9-35)\\nwhere the \\nB\\nk\\n are the end-point detectors in Fig. 9.27(b). The next step is dilation \\nof the end points. Typically, the number of dilations is less than the number of end-\\npoint removals to reduce the probability of “growing” back some of the spurs. In \\nthis case, we know by inspection that no new spurs are created, so we dilate the end \\npoints three times using \\nA\\n as a delimiter. This is the same number of thinning passes:\\n \\nXXHA\\n32\\n=\\n()\\n{¨\\n \\n(9-36)\\nwhere \\nH\\n is a \\n33\\n×\\n structuring element of 1’s, and the intersection with \\nA\\n is applied \\nafter each step\\n. As in the case of region filling, this type of conditional dilation pre-\\nvents the creation of 1-valued elements outside the region of interest, as illustrated \\nby the result in Fig. 9.27(e). Finally, the union of \\nX\\n1\\n and \\nX\\n3\\n,\\n \\nXX X\\n41 3\\n=\\n´\\n \\n(9-37)\\nyields the desired result in Fig. 9.27(f).\\nIn more complex scenarios\\n, using Eq. (9-36) sometimes picks up the “tips” of \\nsome branches. This can occur when the end points of these branches are near the \\nskeleton. Although Eq. (9-36) may eliminate them, they can be picked up again \\nduring dilation because they are valid points in \\nA\\n. However, unless entire parasitic \\nelements are picked up again (a rare case if these elements are short with respect to \\nvalid strokes), detecting and eliminating the reconstructed elements is easy because \\nthey are disconnected regions.\\nA natural thought at this juncture is that there must be easier ways to solve this \\nproblem. For example, we could just keep track of all deleted points and simply \\nreconnect the appropriate points to all end points left after application of Eq. (9-34). \\nThis argument is valid, but the advantage of the formulation just presented is that \\nwe used existing morphological constructs to solve the problem. When a set of such \\ntools is available, the advantage is that no new algorithms have to be written. We \\nsimply combine the necessary morphological functions into a sequence of opera-\\ntions.\\nSometimes you will encounter end point detectors based on a single structuring \\nelement, similar to the ﬁrst SE in Fig. 9.27(b), but having “don’t care” conditions \\nalong the entire ﬁrst column instead having a foreground element separating the \\ncorner \\n×\\n’s.\\n This is incorrect. For example, the former element would identify the \\npoint located in the eighth row\\n, fourth column of Fig. 9.27(a) as an end point, thus \\neliminating it and breaking the connectivity of that part of the stroke.\\nDIP4E_GLOBAL_Print_Ready.indb   666\\n6/16/2017   2:12:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 667}),\n",
       " Document(page_content='9.6\\n  \\nMorphological Reconstruction\\n    \\n667\\n9.6 MORPHOLOGICAL RECONSTRUCTION  \\nThe morphological concepts discussed thus far involve a single image and one or \\nmore structuring elements. In this section, we discuss a powerful morphological \\ntransformation called \\nmorphological reconstruction\\n that involves two images and \\na structuring element. One image, the \\nmarker\\n, which we denote by \\nF\\n, contains the \\nstarting points for reconstruction. The other image, the \\nmask\\n, denoted by \\nG\\n, con-\\nstrains (conditions) the reconstruction. The structuring element is used to define \\nconnectivity.\\n†\\n For 2-D applications, connectivity typically is defined as 8-connectivity, \\nwhich is implied by a structuring element of size \\n33\\n×\\n whose elements are all 1’s.\\nGEODESIC DILATION AND EROSION\\nCentral to morphological reconstruction are the concepts of geodesic dilation and \\ngeodesic erosion. Let \\nF\\n denote the marker image and \\nG\\n the mask image. We assume \\nin this discussion that both are binary images and that \\nFG\\n8\\n.\\n The \\ngeodesic dila-\\ntion of siz\\ne\\n 1 of the marker image with respect to the mask, denoted by \\nD\\nF\\nG\\n1\\n()\\n()\\n,\\n is \\ndefined as\\n \\nD\\nFF B G\\nG\\n1\\n()\\n()\\n=\\n()\\n{¨\\n \\n(9-38)\\nwhere, as usual, \\n¨\\n denotes the set intersection (here \\n¨\\n may be interpreted as a logi-\\ncal \\nAND because we are dealing with binary quantities). The \\ngeodesic dilation of \\nsize\\n \\nn\\n of \\nF\\n with respect to \\nG\\n is defined as\\n \\nDF DD F\\nG\\nn\\nGG\\nn\\n() ( )\\n−\\n()\\n()\\n=\\n()\\n(\\n)\\n11\\n \\n(9-39)\\nwhere \\nn\\n≥\\n1\\n is an integer, and \\nD\\nFF\\nG\\n0\\n()\\n()\\n=\\n.\\n In this recursive expression, the set inter-\\nsection indicated in Eq.\\n (9-38) is performed at each step.\\n‡\\n Note that the intersec-\\ntion operation guarantees that mask \\nG\\n will limit the growth (dilation) of marker \\nF\\n. \\nFigure 9.28 shows a simple example of a geodesic dilation of size 1. The steps in the \\nfigure are a direct implementation of Eq. (9-38). Note that the marker \\nF\\n consists of \\njust one point from the object in \\nG.\\n The idea is to grow (dilate) this point succes-\\nsively, masking of the result at each step by \\nG\\n. Continuing with this process would \\nyield a result whose shape is influenced by the structure of \\nG\\n. In this simple case, \\nthe reconstruction would eventually result in an image identical to \\nG\\n (see Fig. 9.30).\\nThe \\ngeodesic erosion of size\\n 1 of marker \\nF\\n with respect to mask \\nG\\n is deﬁned as\\n \\nEF FB G\\nG\\n1\\n()\\n()\\n=\\n()\\n|´\\n \\n(9-40)\\n†\\n  In much of the literature on morphological reconstruction, the structuring element is tacitly assumed to be \\nisotropic and typically is called an \\nelementary isotropic structuring element\\n. In the context of this chapter, an \\nexample of such an SE is a \\n33\\n×\\n array of 1’s with the origin at the center.\\n‡\\n  Although it is more intuitive to develop morphological reconstruction methods using recursive formulations \\n(as we do here), their practical implementation typically is based on more computationally efﬁcient algorithms \\n(see, for example, Vincent [1993] and Soille [2003]). \\n9.6\\nSee Section 2.5 regarding \\nconnectivity.\\nDIP4E_GLOBAL_Print_Ready.indb   667\\n6/16/2017   2:12:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 668}),\n",
       " Document(page_content='668\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nMarker, \\nF\\nMask, \\nG\\nMarker dilated by \\nB\\nB\\n/H20669\\nGeodesic dilation, \\nD \\n(1)\\n(\\nF\\n)\\nG\\n(\\nThis is the dilated marker \\nimage masked by\\n \\nG.)\\nFIGURE 9.28\\nIllustration of a \\ngeodesic  \\ndilation of  \\nsize 1. Note that \\nthe marker image \\ncontains a point \\nfrom the object  \\nin \\nG\\n. If continued, \\nsubsequent dila-\\ntions and maskings \\nwould eventually \\nresult in the object \\ncontained in \\nG\\n. \\nwhere \\n´\\n denotes set union (or logical OR operation). The geodesic erosion of size \\nn\\n \\nof \\nF\\n with respect to \\nG\\n is defined as\\n \\nEFEE F\\nG\\nn\\nGG\\nn\\n() ( )\\n−\\n()\\n()\\n=\\n()\\n(\\n)\\n11\\n \\n(9-41)\\nwhere \\nn\\n≥\\n1\\n is an integer and \\nEF F\\nG\\n0\\n()\\n()\\n=\\n.\\n The set union in Eq. (9-40) is performed \\nat each step\\n, and guarantees that geodesic erosion of an image remains greater than \\nor equal to its mask image. As you might have expected from the forms in Eqs. (9-38) \\nand (9-40), geodesic dilation and erosion are duals with respect to set complementa-\\ntion (see Problem 9.41). Figure 9.29 shows an example of a geodesic erosion of size 1.  \\nThe steps in the figure are a direct implementation of Eq. (9-40).\\nGeodesic dilation and erosion converge after a ﬁnite number of iterative steps, \\nbecause propagation or shrinking of the marker image is constrained by the mask.\\nMORPHOLOGICAL RECONSTRUCTION BY DILATION AND BY EROSION\\nBased on the preceding concepts, \\nmorphological reconstruction by dilation\\n of a \\nmarker image \\nF\\n with respect to a mask image \\nG, \\ndenoted \\nRF\\nG\\nD\\n()\\n,\\n is defined as the \\ngeodesic dilation of \\nF\\n with respect to \\nG\\n,\\n iterated until stability is achieved; that is,\\n \\nRF D F\\nG\\nD\\nG\\nk\\n()\\n=\\n()\\n()\\n \\n(9-42)\\nwith \\nk\\n such that \\nDF D F\\nG\\nk\\nG\\nk\\n()\\n+\\n()\\n()\\n=\\n()\\n1\\n. \\nF\\nigure 9.30 illustrates reconstruction by dilation. Figure 9.30(a) continues the pro-\\ncess begun in Fig. 9.28. The next step in reconstruction after obtaining \\nDF\\nG\\n()\\n1\\n()\\n is to \\ndilate this result, then AND it with mask \\nG\\n to yield \\nDF\\nG\\n()\\n,\\n2\\n()\\n as Fig. 9.30(b) shows. \\nDilation of \\nDF\\nG\\n()\\n2\\n()\\n and masking with \\nG\\n then yields \\nD\\nF\\nG\\n()\\n,\\n3\\n()\\n and so on. This pro-\\ncedure is repeated until stability is reached. \\nCarrying out this example one more \\nstep would give \\nDF DF\\nGG\\n() ()\\n,\\n56\\n()\\n=\\n()\\n so the image, morphologically reconstructed by \\ndilation, is given by \\nRF D F\\nG\\nD\\nG\\n()\\n=\\n()\\n()\\n5\\n,\\n as indicated in Eq. (9-42). The reconstructed \\nimage is identical to the mask,\\n as expected.\\nDIP4E_GLOBAL_Print_Ready.indb   668\\n6/16/2017   2:12:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 669}),\n",
       " Document(page_content='9.6\\n  \\nMorphological Reconstruction\\n    \\n669\\nIn a similar manner, the \\nmorphological reconstruction by erosion\\n of a marker \\nimage \\nF\\n with respect to a mask image \\nG\\n, denoted \\nRF\\nG\\nE\\n()\\n,\\n is deﬁned as the geodesic \\nerosion of \\nF\\n with respect to \\nG\\n,\\n iterated until stability; that is,\\n \\nRF E F\\nG\\nE\\nG\\nk\\n()\\n=\\n()\\n()\\n \\n(9-43)\\nwith \\nk\\n such that \\nEFE F\\nG\\nk\\nG\\nk\\n()\\n+\\n()\\n()\\n=\\n()\\n1\\n.\\n As an exercise, generate a figure similar to \\nF\\nig. 9.30 for morphological reconstruction by erosion. Reconstruction by dilation \\nand erosion are duals with respect to set complementation (see Problem 9.42).\\nSAMPLE APPLICATIONS\\nMorphological reconstruction has a broad spectrum of practical applications, each \\ndetermined by the selection of the marker and mask images, by the structuring \\nMarker, \\nF\\nMask, \\nG\\nMarker eroded by \\nB\\nB\\n/H20668\\nGeodesic erosion, \\nE \\n(1)\\n(\\nF\\n)\\nG\\n(\\nThis is the eroded maker\\nimage masked by\\n \\nG.\\n)\\nFIGURE 9.29\\nIllustration of a \\ngeodesic erosion \\nof size 1.\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 9.30\\nIllustration of \\nmorphological \\nreconstruction \\nby dilation. Sets\\nDF\\nG\\n()\\n() ,\\n1\\n \\nG\\n, \\nB\\n  \\nand \\nF\\n are from \\nFig. 9.28. The  \\nmask (\\nG\\n) is \\nshown dotted for \\nreference.\\n(1)\\n( ) dilated by \\nG\\nDF B\\n(2)\\nResult of masking = ( ) \\nG\\nDF\\n(2)\\n( ) dilated by \\nG\\nDF B\\n(3)\\nResult of masking = ( ) \\nG\\nDF\\n(3)\\n( ) dilated by \\nG\\nDF B\\n(4)\\nResult of masking = ( ) \\nG\\nDF\\n(4)\\n( ) dilated by \\nG\\nDF B\\n(5)\\nResult of masking = ( ) \\nG\\nDF\\nNo changes after this point,\\nso\\n \\n(5)\\n() ()\\nD\\nGG\\nRF D F\\n=\\nDIP4E_GLOBAL_Print_Ready.indb   669\\n6/16/2017   2:12:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 670}),\n",
       " Document(page_content='670\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nelements, and by combinations of the morphological operations defined in the pre-\\nceding discussion. The following examples illustrate the usefulness of these concepts.\\nOpening by Reconstruction\\nIn morphological opening, erosion removes small objects and then dilation attempts \\nto restore the shape of the objects that remain. The accuracy of this restoration \\ndependents on the similarity of the shapes and the structuring element(s) used. \\nOpening by reconstruction restores exactly the shapes of the objects that remain \\nafter erosion. The \\nopening by reconstruction\\n of size \\nn\\n of an image \\nF\\n is defined as the \\nreconstruction by dilation of the erosion of size \\nn\\n of \\nF\\n with respect to \\nF\\n; that is,\\n \\nOF R Fn B\\nR\\nn\\nF\\nD\\n()\\n()\\n=\\n()\\n|\\n \\n(9-44)\\nwhere \\nFn B\\n|\\n indicates \\nn\\n erosions by \\nB\\n starting with \\nF\\n,\\n as defined in Eq. (9-30). \\nNote that \\nF\\n itself is used as the mask. By comparing this equation with Eq. (9-42), \\nwe see that Eq. (9-44) indicates that the opening by reconstruction uses an eroded \\nversion of \\nF\\n as the marker in reconstruction by dilation. \\nAs you will see in Fig. 9.31, Eq. (9-44) can lead to some interesting results. Typically, \\nthe structuring element, \\nB\\n, used in Eq. (9-44) is designed to extract some feature of \\ninterest, based on erosion. However, as mentioned at the beginning of this section, \\nthe structuring element used in reconstruction (i.e., in the dilation that is performed \\nto obtain \\nR\\nF\\nD\\n)\\n is designed to deﬁne connectivity and, for 2-D, that structuring ele-\\nment typically is a \\n33\\n×\\n array of 1’s. It is important that you do not confuse this SE \\nwith the structuring element,\\n \\nB\\n, used for erosion in Eq. (9-44). Finally, we point out \\nthat this equation is most commonly used with \\nn\\n=\\n1. \\nF\\nigure 9.31 shows an example of opening by reconstruction. We are interested in \\nextracting from Fig. 9.31(a) the characters that contain long, vertical strokes. This \\nobjective determines the nature of \\nB\\n in Eq. (9-44). The average height of the tall \\ncharacters in the ﬁgure is 51 pixels. By eroding the image with a thin structuring \\nelement of size \\n51 1\\n×\\n,\\n we should be able to isolate these characters. Figure 9.31(b) \\nshows one erosion \\n[\\nn\\n=\\n1\\n in Eq. (9-44)] of Fig. 9.31(a) with the structuring element \\njust mentioned.\\n As you can see, the locations of the tall characters were extracted \\nsuccessively. For the purpose of comparison, we computed the opening (remember \\nthis is erosion followed by the dilation) of the image using the same structuring ele-\\nment. Figure 9.31(c) shows the result. As noted earlier, simply dilating an eroded \\nimage does not always restore the original. Finally, Fig. 9.31(d) is the reconstruction \\nby dilation of the original image using that image as the mask and the eroded image \\nas the marker. The dilation in the reconstruction was done using a \\n33\\n×\\n SE of 1’s, \\nfor the reason mentioned earlier\\n. Because we only performed one erosion, the steps \\njust followed constitute the opening by reconstruction (of size 1) of \\nF\\n [i.e., \\nOF\\nR\\n()\\n1\\n()\\n ] \\ngiven in Eq. (9-44). As the ﬁgure shows, characters containing long vertical strokes \\nwere restored accurately from the eroded image (i.e., the marker); all other charac-\\nters were removed.\\nA expression similar to Eq. (9-44) can be written for \\nclosing by reconstruction\\n \\n(see Table 9.1 and Problem 9.44). The difference is that the marker used for closing \\nby reconstruction is the dilation of \\nF\\n and, instead of \\nR\\nF\\nD\\n,\\n we use \\nR\\nF\\nE\\n.\\n As you saw, \\nA expression similar \\nto this equation can be \\nwritten for closing by \\nreconstruction (see  \\nTable 9.1 and  \\nProblem 9.44).\\nDIP4E_GLOBAL_Print_Ready.indb   670\\n6/16/2017   2:12:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 671}),\n",
       " Document(page_content='9.6\\n  \\nMorphological Reconstruction\\n    \\n671\\nopening by reconstruction works with images in which the background is black (0) \\nand the foreground is white (1). Closing by reconstruction works with the opposite \\nscenario. For example, if we were working with the complement of Fig. 9.31(a), the \\nbackground would be white and the foreground black. To solve the same problem of \\nextracting the tall characters, we would use opening by reconstruction. All the other \\nimages in Fig. 9.31 would be identical, except that they would be black on white. The \\nstructuring element used would be the \\nsame\\n in both cases, so the operations of clos-\\ning by reconstruction would be performed on background pixels.\\nAutomatic Algorithm for Filling Holes\\nIn Section 9.5, we developed an algorithm for filling holes based on knowing a starting \\npoint in each hole. Here, we develop a fully automated procedure based on morpho-\\nlogical reconstruction. Let \\nIxy\\n(,\\n)\\n denote a binary image, and suppose that we form \\na marker image \\nF\\n that is 0 everywhere\\n, except at the image border, where it is set to \\n1\\n−\\nI\\n,\\n that is,\\n \\nFxy\\nIx\\ny x\\nyI\\n,\\n,,\\n()\\n=\\n−\\n() ()\\n⎧\\n⎨\\n⎩\\n1\\n0\\nif is on the border of\\notherwise\\n \\n(9-45)\\nThen,\\n \\nHR F\\nI\\nD\\nc\\nc\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n \\n(9-46)\\nis a binary image equal to \\nI\\n with all holes filled.\\nT\\no see how Eqs. (9-45) and (9-46) cause holes in an image to be ﬁlled, consider \\nFigs. 9.32(a) and (b), which show an image, \\nI,\\n containing one hole, and the image \\nb a\\nd c\\nFIGURE 9.31\\n (a) Text image of size \\n918 2018\\n×\\n pixels. The approximate average height of the tall characters is 51 \\n \\npixels\\n. (b) Erosion of (a) with a structuring element of size \\n51 1\\n×\\n elements (all 1’s). (c) Opening of (a) with the \\nsame structuring element, shown for comparison. (d) Result of opening by reconstruction.\\nDIP4E_GLOBAL_Print_Ready.indb   671\\n6/16/2017   2:12:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 672}),\n",
       " Document(page_content='672\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\ncomplement, respectively. The complement of \\nI\\n sets all foreground (1-valued) pixels \\nto background (0-valued) pixels, and vice versa. By deﬁnition, a hole is surrounded \\nby foreground pixels. Therefore, this operation builds a “wall” of 0’s around the hole. \\nBecause \\nI\\nc\\n is used as an AND mask, what we are doing is protecting all foreground \\npixels from changing during iteration. Figure 9.32(c) is array \\nF\\n, formed according to \\nEq. (9-45), and Fig. 9.32(d), using a \\n33\\n×\\n SE of 1’s. The marker \\nF\\n has a border of 1’\\ns \\n(except at locations where \\nI\\n is 1), so the dilation of the marker points starts at the \\nborder and proceeds inward. Figure 9.32(e) shows the geodesic dilation of \\nF\\n using \\nI\\nc\\n as the mask. We see that all locations in this result that correspond to foreground \\npixels of \\nI\\n are now 0, and that this is true for the hole pixels as well. Another itera-\\ntion will yield the same result which, when complemented as required by Eq. (9-46), \\ngives the result in Fig. 9.32(f). The hole is now ﬁlled and the rest of image \\nI\\n was \\nunchanged. The operation \\nHI\\nc\\n¨\\n yields an image containing 1-valued pixels in the \\nlocations corresponding to the holes in \\nI\\n and 0’\\ns elsewhere, as Fig. 9.32(g) shows.\\nFigure 9.33 shows a more practical example. Figure 9.33(b) shows the comple-\\nment of the text image in Fig. 9.33(a), and Fig. 9.33(c) is the marker image, \\nF\\n, gen-\\nerated using Eq. (9-45). This image is all black with a white (1’s) border, except at \\nlocations corresponding to 1’s in the border of the original image (the border values \\nare not easily discernible by eye at the magniﬁcation shown, and also because the \\npage is nearly white). Finally, Fig. 9.33(d) shows the image with all the holes ﬁlled.\\nBorder Clearing\\nExtracting objects from an image for subsequent shape analysis is a fundamental \\ntask in automated image processing. An algorithm for detecting objects that touch \\n(i.e., are connected to) the border is a useful tool because (1) it can be used to screen \\nimages so that only complete objects remain for further processing, or (2) it can be \\nused as a signal that partial objects are present in the field of view. As a final illustra-\\ntion of the concepts introduced in this section, we develop a border-clearing proce-\\ndure based on morphological reconstruction. In this application, we use the original \\nimage as the mask and the following marker image:\\n \\nFx y\\nIx\\ny xy\\nI\\n,\\n(,) (,)\\n()\\n=\\n⎧\\n⎨\\n⎩\\nif  is on the border of \\notherwise\\n0\\n \\n(9-47)\\nThe border-clearing algorithm first computes the morphological reconstruction \\nRF\\nI\\nD\\n()\\n (which extracts the objects touching the border), and then computes the \\nfollowing difference:\\nII\\nc\\nFF  \\n/H20003\\n B\\nF \\n/H20003\\n B \\n/H20669 \\nI\\nc\\nH \\n/H20669 \\nI\\nc\\nH\\nb a\\nc\\ne d\\nf\\ng\\nFIGURE 9.32\\nHole ﬁlling using \\nmorphological \\nreconstruction.\\nDIP4E_GLOBAL_Print_Ready.indb   672\\n6/16/2017   2:12:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 673}),\n",
       " Document(page_content='9.7\\n  \\nSummary of Morphological Operations on Binary Images\\n    \\n673\\n \\nXI RF\\nI\\nD\\n=−\\n()\\n \\n(9-48)\\nto obtain an image, \\nX\\n, with no objects touching the border.\\nAs an example\\n, consider the original text image from Fig. 9.31(a) again. \\nFigure 9.34(a) shows the reconstruction \\nRF\\nI\\nD\\n()\\n obtained using a \\n33\\n×\\n structuring \\nelement of 1’\\ns. The objects touching the border of the original image are visible \\nin the right side of Fig. 9.34(a). Figure 9.34(b) shows image \\nX\\n, computed using Eq. \\n(9-48). If the task at hand were automated character recognition, having an image in \\nwhich no characters touch the border is most useful because the problem of having \\nto recognize partial characters (a difﬁcult task at best) is avoided.\\n9.7 SUMMARY OF MORPHOLOGICAL OPERATIONS ON BINARY \\nIMAGES  \\nFigure 9.35 summarizes the types of structuring elements used in the various binary \\nmorphological methods discussed thus far. The shaded elements are foreground \\nvalues (typically denoted by 1’s in numerical arrays), the elements in white are \\nbackground values (typically denoted by 0’s), and the \\n×\\n’s\\n are “don’t care” elements. \\nT\\nable 9.1 summarizes the binary morphological results developed in the preceding \\nsections. The Roman numerals in the third column of Table 9.1 refer to the structur-\\ning elements in Fig. 9.35.\\n9.7\\nb a\\nd c\\nFIGURE 9.33\\n(a) Text image of \\nsize \\n918 2018\\n×\\n \\npixels\\n.  \\n(b) Complement \\nof (a) for use as a \\nmask image.  \\n(c) Marker image. \\n(d) Result of \\nhole-ﬁlling using \\nEqs. (9-45) and \\n(9-46).\\nb a\\nFIGURE 9.34\\n(a) Reconstruction \\nby dilation of marker \\nimage. (b) Image \\nwith no objects \\ntouching the border. \\nThe original image is \\nFig. 9.31(a).\\nDIP4E_GLOBAL_Print_Ready.indb   673\\n6/16/2017   2:12:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 674}),\n",
       " Document(page_content='674\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n9.8 GRAYSCALE MORPHOLOGY  \\nIn this section, we extend to grayscale images the basic operations of dilation, ero-\\nsion, opening, and closing. We then use these operations to develop several basic \\ngrayscale morphological algorithms. Throughout the discussion that follows, we deal \\nwith digital functions of the form \\nfx y\\n(,\\n)\\n and \\nbxy\\n(,\\n) ,\\n where \\nfx y\\n(,\\n)\\n is a grayscale \\nimage and \\nbxy\\n(,\\n)\\n is a structuring element. The assumption is that these functions \\nare discrete in the sense defined in Section 2.4.\\n That is, if \\nZ\\n denotes the set of real \\nintegers\\n, then the coordinates \\n(,)\\nxy\\n are integers from the Cartesian product \\nZ\\n2\\n,\\n and \\nfx y\\n(,\\n)\\n and \\nbxy\\n(,\\n)\\n are functions that assign an intensity value (a real number from \\nthe set of real numbers\\n, \\nR\\n) to each distinct pair of coordinates \\n(,) .\\nxy\\n If the intensity \\nlevels are integers also\\n, then \\nZ\\n replaces \\nR\\n.\\nStructuring elements in grayscale morphology perform the same basic functions \\nas their binary counterparts: They are used as “probes” to examine a given image for \\nspeciﬁc properties. Structuring elements in grayscale morphology belong to one of \\ntwo categories: \\nnonﬂat\\n and \\nﬂat\\n. Figure 9.36 shows an example of each. Figure 9.36(a) \\nis a hemispherical grayscale SE shown as an image, and Fig. 9.36(c) is a horizontal \\nintensity proﬁle through its center. Figure 9.34(b) shows a ﬂat structuring element \\nin the shape of a disk, and Fig. 9.36(d) is its corresponding intensity proﬁle. (The \\nshape of this proﬁle explains the origin of the word “ﬂat.”) The elements in Fig. 9.36 \\nare shown as continuous quantities for clarity; their computer implementation is \\nbased on digital approximations. Because of a number of difﬁculties discussed later \\nin this section, grayscale nonﬂat SEs are not used frequently in practice. Finally, we \\nmention that, as in the binary case, the origin of grayscale structuring elements must \\nbe clearly identiﬁed. Unless mentioned otherwise, all the examples in this section \\nare based on symmetrical, ﬂat structuring elements of unit height whose origins \\nare at the center. The reﬂection of an SE in grayscale morphology is as deﬁned in \\nSection 9.1; we denote it in the following discussion by \\nˆ\\n,,\\n.\\nbxy b x y\\n()\\n=− −\\n()\\nGRAYSCALE EROSION AND DILATION\\nThe \\ngrayscale erosion\\n of \\nf\\n  by a flat structuring element \\nb\\n at location \\n(,)\\nxy\\n is defined \\nas the \\nminimum\\n value of the image in the region coincident with \\nbxy\\n(,\\n)\\n when the \\norigin of \\nb\\n is at \\n(,) .\\nxy\\n In equation form, the erosion at \\n(,)\\nxy\\n of an image \\nf\\n  \\nby a struc-\\nturing element \\nb\\n is given as\\n \\nfb x y f x s y t\\nst b\\n|\\n[]\\n()\\n=+ +\\n()\\n{}\\n()\\n∈\\n,m i n ,\\n,\\n \\n(9-49)\\n9.8\\nFIGURE 9.35\\nFive basic types \\nof structuring \\nelements used for \\nbinary  \\nmorphology. \\nB\\nI\\nB\\ni \\n i\\n \\n/H11005\\n 1, 2, 3, 4\\n(rotate 90\\n/H11034\\n)\\n/H11003\\n/H11003\\nB\\ni\\n  i\\n \\n/H11005\\n 5, 6, 7, 8\\n(rotate 90\\n/H11034\\n)\\nV\\nB\\nII\\nB\\ni \\n i\\n \\n/H11005\\n 1, 2, 3, 4\\n(rotate 90\\n/H11034\\n)\\nIII\\n/H11003\\n/H11003\\n/H11003\\n/H11003\\n/H11003\\nB\\ni \\n i\\n \\n/H11005\\n 1, 2, . . . , 8\\n(rotate 45\\n/H11034\\n)\\nIV\\n/H11003\\n/H11003\\n/H11003\\n= origin\\n= don’t care\\nDIP4E_GLOBAL_Print_Ready.indb   674\\n6/16/2017   2:12:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 675}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n675\\nOperation\\nEquation\\nComments\\nTranslation\\nBc c b z b B\\nz\\n(\\n)\\n== + ∈\\n{}\\n,f o r\\nTranslates the origin of \\nB\\n to \\npoint \\nz\\n. \\nReﬂection\\nˆ\\n,\\nBb b B\\n== − ∈\\n{}\\nww\\nfor\\nReﬂects \\nB\\n about its origin.\\nComplement\\nAA\\nc\\n=∉\\n{}\\nww\\nSet of points not in \\nA\\n.\\nDifference\\nAB A B\\nAB\\nc\\n−= ∈ ∉\\n{}\\n=\\nww w\\n,\\n/H20669\\nSet of points in \\nA\\n,\\n but not \\nin \\nB\\n.\\nErosion\\nAB z B A\\nz\\n|8\\n=\\n(\\n)\\n{}\\nErodes the boundary of \\nA. \\n(I)\\nDilation\\nAB zB A\\nz\\n{¨\\n=∅\\n{}\\nP\\n()\\nˆ\\n≠\\nDilates the boundary of \\nA\\n. \\n(I)\\nOpening\\nAB A B B\\n/H11568\\n=\\n(\\n)\\n|{\\nSmoothes contours, breaks \\nnarrow isthmuses\\n, and  \\neliminates small islands and \\nsharp peaks. (I)\\nClosing\\nAB A B B\\n/H17033\\n=\\n(\\n)\\n{|\\nSmoothes contours, fuses \\nnarrow breaks and long thin \\ngulfs\\n, and eliminates small \\nholes. (I)\\nHit-or-miss transform\\nIB z B I\\nz\\n\\x04\\n=\\n(\\n)\\n{}\\nP\\n8\\nFinds instances of \\nB\\n in image \\nI\\n. \\nB\\n contains \\nboth\\n foreground \\nand background elements\\n.\\nBoundary extraction\\nb\\n()\\n( )\\nAA A B\\n=−\\n|\\nSet of points on the bound-\\nary of set \\nA\\n.\\n (I)\\nHole ﬁlling\\nXX B I\\nk\\nkk\\nc\\n=\\n()\\n=\\n−\\n1\\n123\\n{¨\\n,,,\\n…\\nFills holes in \\nA\\n. \\nX\\n0\\n is of same \\nsize as \\nI\\n, with a 1 in each hole \\nand 0’s elsewhere. (II)\\nConnected  \\ncomponents\\nXX B I\\nk\\nkk\\n=\\n()\\n=\\n−\\n1\\n123\\n{¨\\n,,,\\n…\\nFinds connected components \\nin \\nI\\n. \\nX\\n0\\n is a set, the same size \\nas \\nI\\n, with a 1 in each  \\nconnected component and 0’s \\nelsewhere. (I)\\nConvex hull\\nXX BX\\nik\\nXI\\nDX\\nC A\\nk\\ni\\nk\\nii\\nk\\ni\\nii\\nconv\\ni\\n=\\n()\\n==\\n==\\n−−\\n11\\n0\\n1234 123\\n\\x04\\n´\\n;\\n,,, ,,,\\n…\\n;;  \\n( (\\n)\\n=\\n=\\nD\\ni\\ni\\n1\\n4\\n∪\\nFinds the convex hull, \\nCA\\n(\\n)\\n, \\nof a set,\\n \\nA\\n, of foreground \\npixels contained in image \\nI\\n. \\nX\\nconv\\ni\\n means that \\nXX\\nk\\ni\\nk\\ni\\n=\\n−\\n1\\n. \\n(III)\\nTABLE \\n9.1\\nSummary of \\nbinary morpho-\\nlogical operations \\nand their  \\nproperties. \\nA\\n is a \\nset of foreground \\npixels contained \\nin binary image \\nI\\n, \\nand \\nB\\n is a struc-\\nturing element. \\nI\\n \\nis a binary image \\n(containing \\nA\\n) , \\nwith 1’s  \\ncorresponding to \\nthe elements of \\nA\\n \\nand 0’s elsewhere.\\nThe Roman \\nnumerals refer to \\nthe structuring \\nelements in  \\nFig. 9.35.\\nDIP4E_GLOBAL_Print_Ready.indb   675\\n6/16/2017   2:12:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 676}),\n",
       " Document(page_content='676\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nOperation\\nEquation\\nComments\\nThinning\\nABA AB\\nAA\\nB\\nAB\\nAB\\nBB\\nBB B B\\nc\\nn\\nz\\n¨\\nz\\nzz z\\n=−\\n(\\n)\\n=\\n(\\n)\\n{}\\n=\\n(\\n)\\n(\\n)\\n()\\n(\\n)\\n{}\\n=\\n\\x04\\n\\x04\\n……\\n12\\n123\\n,,, , ,\\n…\\nB\\nn\\n{}\\nThins set \\nA\\n. The ﬁrst two \\nequations give the basic  \\ndeﬁnition of thinning. The \\nlast two equations denote  \\nthinning by a sequence of \\nstructuring elements. This \\nmethod is normally used in \\npractice. (IV)\\nThickening\\nABA AB\\nAB\\nAB\\nB B\\nn\\n}´\\n}\\n}} }\\n=\\n(\\n)\\n{}\\n=\\n(\\n)\\n(\\n)\\n()\\n(\\n)\\n\\x04\\n……\\n12\\nThickens set \\nA\\n using a \\nsequence of structuring ele-\\nments, as above. Uses (IV) \\nwith 0’s and 1’s reversed.\\nSkeletons\\nSA S A\\nSA\\nA k B\\nAk BB\\nA\\nA\\nk\\nk\\nK\\nk\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n=\\n(\\n)\\n−\\n(\\n)\\n=\\n=\\n0\\n∪\\n∘\\n|\\n|\\nReconstruction of :\\nS\\nSA k B\\nk\\nk\\nK\\n(\\n)\\n()\\n=\\n{\\n0\\n∪\\nFinds the skeleton \\nSA\\n()\\n of \\nset \\nA\\n. The last equation indi-\\ncates that \\nA\\n can be  \\nreconstructed from its skel-\\neton subsets \\nSA\\nk\\n(\\n)\\n. \\nK\\n is the \\nvalue of the iterative step af-\\nter which the set \\nA\\n erodes to \\nthe empty set.\\n The notation \\nAk B\\n|\\n(\\n)\\n denotes the \\nk\\nth  \\niteration of successive  \\nerosions of \\nA\\n by \\nB\\n. (I)\\nPruning\\nXAB\\nXX\\nB\\nXX\\nHA\\nXXX\\nk\\nk\\n1\\n21\\n1\\n8\\n32\\n41 3\\n=\\n{}\\n=\\n(\\n)\\n=\\n()\\n=\\n=\\nz\\n{¨\\n´\\n\\x04\\n∪\\nX\\n4\\n is the result of pruning set \\nA\\n. The number of times that \\nthe ﬁrst equation is applied \\nto obtain \\nX\\n1\\n must be speci-\\nﬁed. Structuring elements \\n(V) are used for the ﬁrst two \\nequations. In the third equa-\\ntion \\nH\\n denotes structuring \\nelement. (I)\\nGeodesic dilation–size 1\\nDF F B G\\nG\\n1\\n()\\n(\\n)\\n=\\n(\\n)\\n{¨\\nF\\n and \\nG\\n are called the \\nmarker\\n and the \\nmask\\n images\\n, \\nrespectively. (I)\\nGeodesic dilation–size \\nn\\nDF DD F\\nG\\nn\\nGG\\nn\\n() ( )\\n−\\n()\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n11\\nSame comment as above.\\nGeodesic erosion–size 1\\nEF FB G\\nG\\n1\\n()\\n(\\n)\\n=\\n(\\n)\\n|´\\nSame comment as above.\\nGeodesic erosion–size \\nn\\nEF EE F\\nG\\nn\\nGG\\nn\\n() ( )\\n−\\n()\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n11\\nSame comment as above.\\nMorphological recon-\\nstruction by dilation\\nRF D F\\nG\\nD\\nG\\nk\\n(\\n)\\n=\\n(\\n)\\n()\\nWith \\nk\\n is such that \\nDF D F\\nG\\nk\\nG\\nk\\n()\\n+\\n()\\n(\\n)\\n=\\n(\\n)\\n1\\n.\\nTABLE 9.1 \\n(\\nContinued\\n)\\nDIP4E_GLOBAL_Print_Ready.indb   676\\n6/16/2017   2:12:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 677}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n677\\nOperation\\nEquation\\nComments\\nMorphological recon-\\nstruction by erosion\\nRF E F\\nG\\nE\\nG\\nk\\n(\\n)\\n=\\n(\\n)\\n()\\nWith \\nk\\n such that \\nEFE F\\nG\\nk\\nG\\nk\\n()\\n+\\n()\\n(\\n)\\n=\\n(\\n)\\n1\\n.\\nOpening by  \\nreconstruction\\nOF R Fn B\\nR\\nn\\nF\\nD\\n()\\n(\\n)\\n=\\n(\\n)\\n|\\nFn B\\n|\\n indicates \\nn\\n succes-\\nsive erosions by \\nB,\\n starting \\nwith \\nF\\n.\\n The form of \\nB\\n is \\napplication-dependent.\\nClosing by  \\nreconstruction\\nCF R Fn B\\nR\\nn\\nF\\nE\\n()\\n(\\n)\\n=\\n(\\n)\\n{\\nFn B\\n{\\n \\nindicates \\nn\\n succes-\\nsive dilations by \\nB,\\n starting \\nwith \\nF\\n. The form of \\nB\\n is \\napplication-dependent.\\nHole ﬁlling\\nHR F\\nI\\nD\\nc\\nc\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\nH\\n is equal to the input image \\nI\\n,\\n but with all holes ﬁlled. See \\nEq. (9-45) for the deﬁnition \\nof marker image \\nF.\\n \\nBorder clearing\\nXI RF\\nI\\nD\\n=−\\n(\\n)\\nX\\n is equal to the input image \\nI\\n, but with all objects that \\ntouch (are connected to) \\nthe boundary removed. See \\nEq. (9-47) for the deﬁnition \\nof marker image \\nF.\\nTABLE 9.1\\n(\\nContinued\\n)\\nNonflat SE\\nIntensity profile Intensity profile\\nFlat SE\\nb a\\nd c\\nFIGURE 9.36\\nNonﬂat and ﬂat \\nstructuring  \\nelements, and  \\ncorresponding \\nhorizontal  \\nintensity proﬁles \\nthrough their  \\ncenters. All \\nexamples in this \\nsection are based \\non ﬂat SEs.\\nDIP4E_GLOBAL_Print_Ready.indb   677\\n6/16/2017   2:12:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 678}),\n",
       " Document(page_content='678\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nwhere, in a manner similar to spatial correlation (see Section 3.4), \\nx\\n and \\ny\\n are incre-\\nmented through all values required so that the origin of \\nb\\n visits every pixel in \\nf\\n.  That \\nis, to find the erosion of \\nf\\n by \\nb\\n, we place the origin of the structuring element at every \\npixel location in the image. The erosion at any location is determined by selecting \\nthe minimum value of \\nf\\n in the region coincident with \\nb\\n. For example, if \\nb\\n is a square \\nstructuring element of size \\n33\\n×\\n,\\n obtaining the erosion at a point requires finding \\nthe minimum of the nine values of \\nf\\n contained in the \\n33\\n×\\n region spanned by \\nb\\n when \\nits origin is at that point.\\nSimilarly\\n, the \\ngrayscale dilation\\n of \\nf\\n by a ﬂat structuring element \\nb\\n at any location \\n(,)\\nxy\\n is deﬁned as the \\nmaximum\\n value of the image in the window spanned by \\nˆ\\nb\\n \\nwhen the origin of \\nˆ\\nb\\n is at \\n(,) .\\nxy\\n That is,\\n \\nfb x y f x s y t\\nst b\\n{\\n⁄\\n[]\\n()\\n=− −\\n()\\n{}\\n∈\\n, max ,\\n(,)\\n \\n(9-50)\\nwhere we used the fact stated earlier that \\nˆ\\n(,\\n) ( , ) .\\nbcd b c d\\n=− −\\n The explanation of \\nthis equation is identical to the explanation in the previous paragraph,\\n but using \\nthe maximum, rather than the minimum operation, and keeping in mind that the \\nstructuring element is reflected about its origin, which we take into account by using \\n(,)\\n−−\\nst\\n in the argument of the function. This is analogous to spatial convolution, as \\nexplained in Section 3.4.\\nEXAMPLE 9.9 :  Grayscale erosion and dilation.\\nBecause grayscale erosion with a ﬂat SE computes the minimum intensity value of \\nf\\n in every neighbor-\\nhood of \\n(,)\\nxy\\n coincident with \\nb\\n,\\n we expect in general that an eroded grayscale image will be darker than \\nthe original, that the sizes (with respect to the size of the SE) of bright features will be reduced, and that \\nthe sizes of dark features will be increased. Figure 9.37(b) shows the erosion of Fig. 9.37(a) using a disk \\nSE of unit height and a radius of 2 pixels. The effects just mentioned are clearly visible in the eroded \\nimage. For instance, note how the intensities of the small bright dots were reduced, making them barely \\nvisible in Fig. 9.37(b), while the dark features grew in thickness. The general background of the eroded \\nimage is slightly darker than the background of the original image. \\nSimilarly, Fig. 9.37(c) is the result of dilation with the same SE. The effects are the opposite of using \\nerosion. The bright features were thickened and the intensities of the darker features were reduced. \\nIn particular, the thin black connecting wires in the left, middle, and right bottom of Fig. 9.37(a) are \\nbarely visible in Fig. 9.37(c). The sizes of the dark dots were reduced as a result of dilation, but, unlike \\nthe eroded small white dots in Fig. 9.37(b), they still are easily visible in the dilated image. The reason is \\nthat the black dots were originally larger than the white dots with respect to the size of the SE. Finally, \\nobserve that the background of the dilated image is slightly lighter than that of Fig. 9.37(a).\\nNonﬂat SEs have grayscale values that vary over their domain of deﬁnition. The \\nerosion of image \\nf\\n  by nonﬂat structuring element, \\nb\\nN\\n, is deﬁned as\\n \\nfbx y f x s y t b s t\\nN\\nst b\\nN\\nN\\n|\\n[]\\n()\\n=+ +\\n()\\n−\\n()\\n{}\\n()\\n∈\\n,m i n , ,\\n,\\n \\n(9-51)\\nDIP4E_GLOBAL_Print_Ready.indb   678\\n6/16/2017   2:12:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 679}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n679\\nHere, we subtract values from \\nf\\n  to determine the erosion at any point. Unlike \\nEq. (9-49), erosion using a nonflat SE is not bounded in general by the values of \\nf\\n, \\nwhich can be problematic in interpreting results. Grayscale SEs are seldom used in \\npractice because of this, the potential difficulties in selecting meaningful elements \\nfor \\nb\\nN\\n, and the added computational burden when compared with Eq. (9-49).\\nIn a similar manner\\n, dilation using a nonﬂat SE is deﬁned as\\n \\nfbx y f x s y t b s t\\nN\\nst b\\nN\\nN\\n{\\n⁄\\n⁄\\n[]\\n()\\n=− −\\n()\\n+\\n{}\\n()\\n∈\\n,m a x , ( , )\\n,\\n \\n(9-52)\\nThe same comments made in the previous paragraph are applicable to dilation with \\nnonflat SEs\\n. When all the elements of \\nb\\nN\\n are constant (i.e., the SE is flat), Eqs. (9-51) \\nand (9-52) reduce to Eqs. (9-49) and (9-50), respectively, within a scalar constant \\nequal to the amplitude of the SE.\\nAs in the binary case, grayscale erosion and dilation are duals with respect com-\\nplementation and reﬂection; that is,\\n \\nf b xy f b xy\\nc\\nc\\n|{\\n⁄\\n[]\\n()\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n()\\n,,\\n \\n(9-53)\\nwhere \\nfx y f x y\\nc\\n,,\\n()\\n=−\\n()\\n and \\nˆ\\n(,\\n) , .\\nbxy b x y\\n=− −\\n()\\n The same expression holds for \\nnonflat structuring elements. Except as needed for clarity, we simplify the notation \\nin the following discussion by suppressing the arguments of all functions, in which \\ncase the preceding equation is written as\\n \\nfb f b\\nc\\nc\\n|{\\n()\\n=\\nˆ\\n \\n(9-54)\\nSimilarly,\\n \\nfb f b\\nc\\nc\\n{|\\n()\\n=\\nˆ\\n \\n(9-55)\\nErosion and dilation by themselves are not particularly useful in grayscale image \\nprocessing\\n. As with their binary counterparts, these operations become powerful \\nwhen used in combination to derive higher-level algorithms.\\nb a\\nc\\nFIGURE 9.37\\n(a) Gray-scale \\nX-ray image of \\nsize \\n448 425\\n×\\n \\npixels\\n. (b) Erosion \\nusing a ﬂat disk SE \\nwith a radius of 2 \\npixels. (c) Dilation \\nusing the same SE. \\n(Original image \\ncourtesy of Lixi, \\nInc.)\\nDIP4E_GLOBAL_Print_Ready.indb   679\\n6/16/2017   2:12:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 680}),\n",
       " Document(page_content='680\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nGRAYSCALE OPENING AND CLOSING\\nThe expressions for opening and closing grayscale images have the same form as \\ntheir binary counterparts. The \\ngrayscale opening\\n of image \\nf\\n by structuring element \\nb\\n, \\ndenoted \\nfb\\n/H11568\\n,\\n is\\n \\nfb f b b\\n/H11568\\n=\\n()\\n|{\\n \\n(9-56)\\nAs before, opening is simply the erosion of \\nf\\n  \\nby \\nb\\n, followed by a dilation of the result \\nby \\nb\\n. Similarly, the \\ngrayscale closing\\n of \\nf\\n  by \\nb\\n, denoted \\nfb\\n/H17033\\n, is\\n \\nfb f b b\\n/H17033\\n=\\n()\\n{|\\n \\n(9-57)\\nThe opening and closing for grayscale images are duals with respect to complemen-\\ntation and SE reflection:\\n \\nfb f b\\nc\\nc\\n/H17033\\n/H11568\\n()\\n=\\nˆ\\n \\n(9-58)\\nand\\n \\nfb f b\\nc\\nc\\n/H11568\\n/H17033\\n()\\n=\\nˆ\\n \\n(9-59)\\nBecause \\nff\\nc\\n=−\\n,\\n we can write Eq. (9-58) as \\n−= −\\n()\\n( ) ,\\nfb fb\\n/H17033\\n/H11568\\n and similarly for \\nEq.\\n (9-59).\\nOpening and closing of grayscale images have a simple geometric interpretation. \\nSuppose that an image function \\nfx y\\n(,\\n)\\n is viewed as a 3-D surface; that is, its intensity \\nvalues are interpreted as height values over the \\nxy\\n-plane\\n, as in Fig. 2.18(a). Then the \\nopening of \\nf\\n by \\nb\\n can be interpreted geometrically as pushing the structuring ele-\\nment up from below against the undersurface of \\nf\\n.\\n At each location of the origin of \\nb\\n, \\nthe opening is the highest value reached by any part of \\nb\\n as it pushes up against the \\nundersurface of \\nf\\n. The complete opening is then the set of all such values obtained \\nby the origin of \\nb\\n visiting every \\n(,)\\nxy\\n coordinate of \\nf\\n.\\nF\\nigure 9.38 illustrates the concept in one dimension. Suppose the curve in \\nFig. 9.38(a) is the intensity proﬁle along a single row of an image. Figure 9.38(b) \\nshows a ﬂat structuring element in several positions, pushed up against the bottom \\nof the curve. The heavy curve in Fig. 9.38(c) is the complete opening. Because the \\nstructuring element is too large to ﬁt completely inside the upward peaks of the \\ncurve, the tops of the peaks are clipped by the opening, with the amount removed \\nbeing proportional to how far the structuring element was able to reach into the \\npeak. In general, openings are used to remove small, bright details, while leaving the \\noverall intensity levels and larger bright features relatively undisturbed.\\nFigure 9.38(d) is a graphical illustration of closing. Observe that the structuring \\nelement is pushed down on top of the curve while being translated to all locations. \\nThe closing, shown in Fig. 9.38(e), is constructed by ﬁnding the lowest points reached \\nby any part of the structuring element as it slides against the upper side of the curve.\\nThe grayscale opening satisfies the following properties:\\nAlthough we deal with \\nﬂat SEs in the following \\ndiscussion, the concepts \\ndiscussed are applicable \\nalso to nonﬂat  \\nstructuring elements.\\nDIP4E_GLOBAL_Print_Ready.indb   680\\n6/16/2017   2:12:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 681}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n681\\n(a) \\nfbf\\n/H11568\\n↵\\n(b) \\nIf \\nff\\n12\\n↵\\n, then \\nfb fb\\n12\\n/H11568/H11568\\n() ()\\n↵\\n(c) \\nfb b fb\\n/H11568/H11568 /H11568\\n()\\n=\\nThe notation \\nqr\\n↵\\n is used to indicate that the domain of \\nq\\n is a subset of the domain \\nof \\nr\\n,\\n and also that \\nqxy rxy\\n(,\\n) (,)\\n≤\\n for any \\n(,)\\nxy\\n in the domain of \\nq\\n.\\nSimilarly\\n, the closing operation satisﬁes the following properties:\\n(a) \\nff\\nb\\n↵\\n/H17033\\n(b) \\nIf \\nff\\n12\\n↵\\n, then \\nfb fb\\n12\\n/H17033/H17033\\n() ()\\n↵\\n(c) \\nfb b fb\\n/H17033/H17033 /H17033\\n()\\n=\\nThe usefulness of these properties is similar to that of their binary counterparts. \\nEXAMPLE 9.10 :  Grayscale opening and closing.\\nFigure 9.39 extends to 2-D the 1-D concepts illustrated in Fig. 9.38. Figure 9.39(a) is the same image we \\nused in Example 9.9, and Fig. 9.39(b) is the opening obtained using a disk structuring element of unit \\nheight and radius of 3 pixels. As expected, the intensity of all bright features decreased, depending on \\nthe sizes of the features relative to the size of the SE. Comparing this ﬁgure with Fig. 9.37(b), we see \\nthat, unlike the result of erosion, opening had negligible effect on the dark features of the image, and \\nthe effect on the background was negligible. Similarly, Fig. 9.39(c) shows the closing of the image with \\na disk of radius 5 (the small round black dots are larger than the small white dots, so a larger disk was \\nneeded to achieve results comparable to the opening). In this image, the bright details and background \\nwere relatively unaffected, but the dark features were attenuated, with the degree of attenuation being \\ndependent on the relative sizes of the features with respect to the SE.\\nFlat SE\\nIntensity profile\\nOpening\\nClosing\\nb\\na\\nc\\nd\\ne\\nFIGURE 9.38\\nGrayscale opening and \\nclosing in one  \\ndimension.  \\n(a) Original 1-D signal. \\n(b) Flat structuring \\nelement pushed up \\nunderneath the signal.  \\n(c) Opening.  \\n(d) Flat structuring \\nelement pushed down \\nalong the top of the \\nsignal.  \\n(e) Closing.\\nDIP4E_GLOBAL_Print_Ready.indb   681\\n6/16/2017   2:12:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 682}),\n",
       " Document(page_content='682\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nSOME BASIC GRAYSCALE MORPHOLOGICAL ALGORITHMS\\nNumerous grayscale morphological techniques are based on the grayscale morpho-\\nlogical concepts introduced thus far. We illustrate some of these algorithms in the \\nfollowing discussion.\\nMorphological Smoothing\\nBecause opening suppresses bright details smaller than the specified SE while leav-\\ning dark details relatively unaffected, and closing generally has the opposite effect, \\nthese two operations are used often in combination as \\nmorphological filters\\n for \\nimage smoothing and noise removal. Consider Fig. 9.40(a), which shows an image \\nof the Cygnus Loop supernova taken in the X-ray band (see Fig. 1.7 for details \\nabout this image). For purposes of the present discussion, suppose that the cen-\\ntral light region is the object of interest, and that the smaller components are noise. \\nOur objective is to remove the noise. Figure 9.40(b) shows the result of opening the \\noriginal image with a flat disk of radius 1, then closing the opening with an SE of the \\nsame size. Figures 9.40(c) and (d) show the results of the same operation using SEs \\nof radii 3 and 5, respectively. As expected, this sequence shows progressive removal \\nof small components as a function of SE size. In the last result, we see that the noise \\nhas been almost eliminated. The noise components on the lower right side of the \\nimage could not be removed completely because their sizes are larger than the other \\nimage elements that were successfully removed.\\nThe results in Fig. 9.40 are based on opening the original image, then closing the \\nopening. A procedure used sometimes is to perform \\nalternating sequential ﬁltering\\n, \\nin which the opening–closing sequence starts with the original image, but subse-\\nquent steps perform the opening and closing on the results of the previous step. This \\ntype of ﬁltering is useful in automated image analysis, in which results at each step \\nare compared against a speciﬁed metric. This approach generally results in more \\nblurring for the same size SE than the method illustrated in Fig. 9.40.\\nMorphological Gradient\\nDilation and erosion can be used in combination with image subtraction to obtain \\nthe morphological gradient, \\ng\\n, of a grayscale image \\nf\\n, as follows:\\nSee Section 3.6 for a \\ndeﬁnition of the image \\ngradient.\\nb a\\nc\\nFIGURE 9.39\\n(a) A grayscale  \\nX-ray image of \\nsize \\n448 425\\n×\\n  \\npixels\\n.  \\n(b) Opening using \\na disk SE with a \\nradius of 3 pixels. \\n(c) Closing using \\nan SE of radius 5.\\nDIP4E_GLOBAL_Print_Ready.indb   682\\n6/16/2017   2:12:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 683}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n683\\n \\ngfb fb\\n=\\n()\\n−\\n()\\n{|\\n \\n(9-60)\\nwhere \\nb\\n is a suitable structuring element.\\n The overall effect achieved by using this \\nequation is that dilation thickens regions in an image, and erosion shrinks them. \\nTheir difference emphasizes the boundaries between regions. Homogenous areas \\nare not affected (provided that the SE is not too large relative to the resolution of \\nthe image) so the subtraction operation tends to eliminate them. The net result is an \\nimage in which the edges are enhanced and the contribution of the homogeneous \\nareas is suppressed, thus producing a “derivative-like” (gradient) effect.\\nFigure 9.41 shows an example. Figure 9.41(a) is a head CT scan, and the next two \\nﬁgures are the opening and closing with a \\n33\\n×\\n ﬂat SE of 1’s. Note the thickening \\nand shrinking just mentioned.\\n Figure 9.41(d) is the morphological gradient obtained \\nusing Eq. (9-60). As you can see, the boundaries between regions were clearly delin-\\neated, as expected of a 2-D derivative image.\\nTop-Hat and Bottom-Hat Transformations\\nCombining image subtraction with openings and closings results in so-called top-hat \\nand bottom-hat\\n \\ntransformations. The \\ntop-hat transformation\\n of a grayscale image \\nf\\n is \\ndefined as \\nf\\n minus its opening:\\nb a\\nd c\\nFIGURE 9.40\\n(a) 566 566\\n×\\n image \\nof the Cygnus Loop \\nsupernova,\\n taken \\nin the X-ray band \\nby NASA’s Hubble \\nTelescope.  \\n(b)–(d) Results of \\nperforming opening \\nand closing  \\nsequences on the \\noriginal image with \\ndisk structuring  \\nelements of radii, 1, \\n3, and 5, respectively. \\n(Original image \\ncourtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   683\\n6/16/2017   2:12:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 684}),\n",
       " Document(page_content='684\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n \\nTf f f b\\nhat\\n()\\n=−\\n()\\n/H11568\\n \\n(9-61)\\nSimilarly, the \\nbottom-hat transformation\\n of \\nf\\n is defined as the closing of \\nf\\n minus \\nf \\n: \\n \\nBf f b f\\nhat\\n()\\n=\\n()\\n−\\n/H17033\\n \\n(9-62)\\nOne of the principal applications of these transformations is in removing objects \\nfrom an image by using a structuring element in the opening or closing operation \\nthat does not fit the objects to be removed.\\n The difference operation then yields an \\nimage in which only the removed components remain. The top-hat transformation \\nis used for light objects on a dark background, and the bottom-hat transformation \\nis used for the opposite situation. For this reason, the names \\nwhite top-hat\\n and \\nblack \\ntop-hat\\n, respectively, are used frequently when referring to these two transformations.\\nAn important use of top-hat transformations is for correcting the effects of non-\\nuniform illumination. As you will learn in Chapter 10, proper (uniform) illumination \\nplays a central role in being able to extract objects from the background in an image. \\nThis process is fundamental in automated image analysis, and is often used in conjunc-\\ntion with thresholding, as you will learn in Chapter 10.\\nTo illustrate, consider Fig. 9.42(a), which shows an image of grains of rice. This \\nimage was obtained under nonuniform lighting, as evidenced by the darker area in \\nthe bottom rightmost part of the image. Figure 9.42(b) shows the result of thresh-\\nolding using Otsu’s method, an optimal thresholding method to be discussed in \\nSection 10.3. The net result of nonuniform illumination was to cause segmentation \\nb a\\nd c\\nFIGURE 9.41\\n(a) 512 512\\n×\\n  \\nimage of a head \\nCT scan.\\n  \\n(b) Dilation.  \\n(c) Erosion.  \\n(d) Morphological \\ngradient,  \\ncomputed as the \\ndifference  \\nbetween (b)  \\nand (c). (Original \\nimage courtesy of \\nDr. David R.  \\nPickens,  \\nVanderbilt  \\nUniversity.)\\nDIP4E_GLOBAL_Print_Ready.indb   684\\n6/16/2017   2:12:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 685}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n685\\nerrors in the dark area (several grains of rice were not extracted from the back-\\nground), as well as in the top left part of the image, where parts of the background \\nwere interpreted as rice. Figure 9.42(c) shows the opening of the image with a disk \\nof radius 40. This SE was large enough so that it would not ﬁt in any of the objects. \\nAs a result, the objects were eliminated, leaving only an approximation of the back-\\nground. The shading pattern is clear in this image. By subtracting this image from the \\noriginal (i.e., by applying a top-hat transformation), the background should become \\nmore uniform. This is indeed the case, as Fig. 9.42(d) shows. The background is not \\nperfectly uniform, but the differences between light and dark extremes are less, and \\nthis was enough to yield a correct thresholding result, in which all the rice grains \\nwere properly extracted using Otsu’s method, as Fig. 9.42(e) shows.\\nGranulometry\\nIn the context of this discussion, \\ngranulometry\\n is a field that deals with determining \\nthe size distribution of particles in an image. Particles seldom are neatly separated, \\nb a\\nc\\ne\\nd\\nFIGURE 9.42\\n Using the top-hat transformation for \\nshading correction\\n. (a) Original image of size \\n600 600\\n×\\n pixels.  \\n(b) \\nThresholded image. (c) Image opened using a disk SE of radius 40. (d) Top-hat transformation (the image minus \\nits opening). (e) Thresholded top-hat image.\\nDIP4E_GLOBAL_Print_Ready.indb   685\\n6/16/2017   2:12:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 686}),\n",
       " Document(page_content='686\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nwhich makes counting based on identifying individual particles a difficult task. Mor-\\nphology can be used to estimate particle size distribution indirectly, without having \\nto identify and measure individual particles. \\nThe approach is simple. With particles having regular shapes that are lighter than \\nthe background, the method consists of applying openings with SEs of increasing \\nsizes. The basic idea is that opening operations of a particular size should have the \\nmost effect on regions of the input image that contain particles of similar size. For \\neach image resulting from an opening, we compute the sum of the pixel values. This \\nsum, called the\\n surface area\\n, decreases as a function of increasing SE size because, \\nas we discussed earlier, openings decrease the intensity of light features in an image. \\nThis procedure yields a 1-D array each element of which is the sum of the pixels in \\nthe opening for the size SE corresponding to that location in the array. To emphasize \\nchanges between successive openings, we compute the difference between adjacent \\nelements of the 1-D array. If the differences are plotted, the peaks in the plot are an \\nindication of the predominant size distributions of the particles in the image.\\nAs an example, consider the image of wood dowel plugs of two dominant sizes \\nin Fig. 9.43(a). The wood grain in the dowels is likely to introduce variations in the \\nopenings, so smoothing is a sensible preprocessing step. Figure Fig. 9.43(b) shows the \\nimage smoothed using the morphological smoothing ﬁlter discussed earlier, with a \\ndisk of radius 5. Figures 9.43(c) through (f) show image openings with disks of radii \\n10, 20, 25, and 30, respectively. Note in Fig. 9.43(d) that the intensity contribution \\ndue to the small dowels has been almost eliminated. In Fig. 9.43(e) the contribution \\nof the large dowels has been reduced signiﬁcantly, and in Fig. 9.43(f) even more so. \\nObserve in Fig. 9.43(e) that the large dowel near the top right of the image is much \\ndarker than the others because its size is smaller than other lager dowels. This would \\nbe useful information if we had been attempting to detect\\n \\ndefective dowels.\\nF\\nigure 9.44 shows a plot of the difference array. As mentioned previously, we \\nexpect signiﬁcant differences (peaks in the plot) around radii at which the SE is \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 9.43\\n(a) 531 675\\n×\\n image \\nof wood dowels\\n.  \\n(b) Smoothed \\nimage.  \\n(c)–(f) Openings  \\nof (b) with disks of  \\nradii equal  \\nto 10, 20, 25,  \\nand 30 pixels, \\nrespectively. \\n(Original image \\ncourtesy of Dr. \\nSteve Eddins, \\nMathWorks, Inc.)\\nDIP4E_GLOBAL_Print_Ready.indb   686\\n6/16/2017   2:12:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 687}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n687\\nlarge enough to encompass a set of particles of approximately the same diameter. \\nThe result in Fig. 9.44 has two distinct peaks, clearly indicating the presence of two \\ndominant object sizes in the image.\\nTextural Segmentation\\nFigure 9.45(a) shows a noisy image of dark blobs superimposed on a light back-\\nground. The image has two textural regions: a region composed of large blobs on the \\nright and a region on the left composed of smaller blobs. The objective is to find a \\nboundary between the two regions based on their textural content, which in this case \\nis determined by the sizes and spatial distribution of the blobs (we discuss texture in \\nChapter 11). The process of partitioning an image into regions is called \\nsegmentation\\n, \\nwhich is the topic of Chapter 10.\\nThe objects of interest are darker than the background, and we know that if we \\nclose the image with a structuring element larger than the small blobs, these blobs \\nwill be removed. The result in Fig. 9.45(b), obtained by closing the input image using \\na disk with a radius of 30 pixels, shows that indeed this is the case. (The radius of the \\nsmaller blobs is approximately 25 pixels.) So, at this point, we have an image with \\nlarge, dark blobs on a light background. If we open this image with a structuring ele-\\nment that is large relative to the separation between these blobs, the net result should \\nbe an image in which the light patches between the blobs are removed, leaving the \\ndark blobs, and also the now dark patches between these blobs. Figure 9.45(c) shows \\nthe result, obtained using a disk of radius 60.\\nPerforming a morphological gradient on this image with, say, a \\n33\\n×\\n SE of 1’s, will \\ngive us the boundary between the two regions\\n. Figure 9.45(d) shows the boundary \\nobtained from the morphological gradient operation, superimposed on the original \\nimage. All pixels to the right of this boundary are said to belong to the texture region \\ncharacterized by large blobs, and conversely for the pixels on the left of the bound-\\nary. You will ﬁnd it instructive to work through this example in more detail using the \\ngraphical analogy for opening and closing illustrated in Fig. 9.38.\\n0\\n1 5 2 53 03 5\\n20\\n10 5\\n0\\n0.5\\n1.5\\n1\\n2\\n2.5\\n/H11003\\n 10\\n6\\nDifferences in surface area\\nr\\nFIGURE 9.44\\nDifferences in \\nsurface area as \\na function of SE \\ndisk radius, \\nr\\n. \\nThe two peaks \\nindicate that there \\nare two dominant \\nparticle sizes in \\nthe image.\\nDIP4E_GLOBAL_Print_Ready.indb   687\\n6/16/2017   2:12:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 688}),\n",
       " Document(page_content='688\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nGRAYSCALE MORPHOLOGICAL RECONSTRUCTION\\nGrayscale morphological reconstruction is defined in the same manner introduced \\nin Section 9.6 for binary images. Let \\nf\\n and \\ng\\n denote the \\nmarker\\n and \\nmask\\n images, \\nrespectively. We assume that both are grayscale images of the same size and that \\nfg\\n≤\\n,\\n meaning that the intensity of \\nf\\n at any point in the image is less than the inten-\\nsity of \\ng\\n at that point.\\n The \\ngeodesic dilation\\n \\nof size\\n 1 of \\nf\\n with respect to \\ng\\n is defined as\\n \\nDf fb g\\ng\\n1\\n()\\n(\\n)\\n=\\n(\\n)\\n{\\n/H11625\\n \\n(9-63)\\nwhere \\n/H11625\\n denotes the \\npoint-wise minimum operator\\n,\\n and \\nb\\n is a suitable structuring \\nelement. We see that the geodesic dilation of size 1 is obtained by first computing the \\ndilation of \\nf\\n  by \\nb,\\n then selecting the minimum between the result and \\ng\\n at every point \\n(,) .\\nxy\\n The dilation is given by Eq. (9-50) if \\nb\\n is a flat SE,\\n or by Eq. (9-52) if it is not. \\nThe \\ngeodesic dilation of size\\n \\nn\\n of \\nf\\n with respect to \\ng\\n is deﬁned as\\n \\nDf D D f\\ng\\nn\\ngg\\nn\\n() ( )\\n−\\n()\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n11\\n \\n(9-64)\\nwith \\nDf f\\ng\\n0\\n()\\n(\\n)\\n=\\n.\\nAs mentioned earlier, it \\nis understood that \\nf\\n and \\ng\\n \\nare functions of \\nx\\n and \\ny\\n. \\nWe omit the coordinates \\nto simplify the notation.\\nb a\\nd c\\nFIGURE 9.45\\nTextural  \\nsegmentation.  \\n(a) A \\n600 600\\n×\\n \\nimage consisting \\nof two types of \\nblobs\\n.  \\n(b) Image with \\nsmall blobs  \\nremoved by  \\nclosing (a).  \\n(c) Image with \\nlight patches \\nbetween large \\nblobs removed by \\nopening (b).  \\n(d) Original  \\nimage with \\nboundary  \\nbetween the two \\nregions in (c) \\nsuperimposed. \\nThe boundary was \\nobtained using \\na morphological \\ngradient.\\nDIP4E_GLOBAL_Print_Ready.indb   688\\n6/16/2017   2:12:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 689}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n689\\nSimilarly, the \\ngeodesic erosion of size\\n 1 of \\nf\\n  with respect to \\ng\\n is deﬁned as\\n \\nEf fb g\\ng\\n1\\n()\\n(\\n)\\n=\\n(\\n)\\n|\\n/H11626\\n \\n(9-65)\\nwhere \\n/H11626\\n denotes the \\npoint-wise maximum operator\\n.\\n The \\ngeodesic erosion of size n\\n \\nis defined as\\n \\nEfE E f\\ng\\nn\\ngg\\nn\\n() ( )\\n−\\n()\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n11\\n \\n(9-66)\\nwith \\nEf f\\ng\\n0\\n()\\n(\\n)\\n=\\n.\\nThe \\nmorphological reconstruction b\\ny dilation\\n of a \\ngrayscale mask image\\n, \\ng\\n, by a \\ngrayscale marker image\\n, \\nf\\n, denoted by \\nRf\\ng\\nD\\n()\\n,\\n()\\n is deﬁned as the geodesic dilation of \\nf\\n with respect to \\ng\\n, iterated until stability is reached; that is,\\n \\nRf D f\\ng\\nD\\ng\\nk\\n()\\n=\\n()\\n()\\n \\n(9-67)\\nwith \\nk\\n such that \\nD\\nfD f\\ng\\nk\\ng\\nk\\n()\\n+\\n()\\n=\\n() () .\\n1\\n The\\n morphological reconstruction by erosion \\nof \\ng\\n by \\nf\\n , denoted by \\nRf\\ng\\nE\\n(\\n)\\n, is similarly defined as\\n \\nRf E f\\ng\\nE\\ng\\nk\\n(\\n)\\n=\\n(\\n)\\n()\\n \\n(9-68)\\nwith \\nk\\n such that \\nEfE f\\ng\\nk\\ng\\nk\\n()\\n+\\n()\\n(\\n)\\n=\\n(\\n)\\n1\\n.\\nAs in the binary case, opening by reconstruction of grayscale images ﬁrst erodes \\nthe input image and uses it as a marker\\n, and uses the image itself as the mask. The \\nopening by reconstruction of size n\\n of an image \\nf\\n is deﬁned as the reconstruction by \\ndilation of the erosion of size \\nn\\n of \\nf  \\nwith respect to\\n f \\n; that is,\\n \\nOfR fn b\\nR\\nn\\nf\\nD\\n()\\n(\\n)\\n=\\n(\\n)\\n|\\n \\n(9-69)\\nwhere \\nfn b\\n|\\n denotes \\nn\\n successive erosions by \\nb\\n,\\n starting with \\nf \\n, as explained in \\nconnection with Eq. (9-30) (note that \\nf\\n itself is used as the mask). Recall also from \\nthe discussion of Eq. (9-44) for binary images that the objective of opening by recon-\\nstruction is to preserve the shape of the image components that remain after erosion.\\nSimilarly, the \\nclosing by reconstruction of size n\\n of an image \\nf\\n is deﬁned as the \\nreconstruction by erosion of the dilation of size \\nn\\n of \\nf\\n  with respect to \\nf\\n ; that is,\\n \\nCfR f n b\\nR\\nn\\nf\\nE\\n()\\n(\\n)\\n=\\n(\\n)\\n{\\n \\n(9-70)\\nwhere \\nfn b\\n{\\n denotes \\nn\\n successive dilations by \\nb\\n,\\n starting with \\nf\\n. Because of duality, \\nthe closing by reconstruction of an image can be obtained by complementing the \\nimage, obtaining the opening by reconstruction, and complementing the result. Finally, \\nas the following example shows, a useful technique called \\ntop-hat by reconstruction\\n \\nconsists of subtracting from an image its opening by reconstruction.\\nEXAMPLE 9.11 :  Using grayscale morphological reconstruction to ﬂatten a complex background.\\nIn this example, we illustrate the use of grayscale reconstruction in several steps. The objective is to \\nnormalize the irregular background of the image in Fig. 9.46(a), leaving only text on a background of \\nSee Problem 9.33 for a \\nlist of dual relationships \\nbetween expressions in \\nthis section.\\nDIP4E_GLOBAL_Print_Ready.indb   689\\n6/16/2017   2:12:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 690}),\n",
       " Document(page_content='690\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 9.46\\n (a) Original image of size \\n1134 1360\\n×\\n pixels. (b) Opening by reconstruction of (a), using a structur-\\ning element consisting of a horizontal line 71 pixels long in the erosion.\\n (c) Opening of (a) using the same SE. \\n \\n(d) Top-hat by reconstruction. (e) Result of applying just a top-hat transformation. (f) Opening by reconstruction \\nof (d), using a horizontal line 11 pixels long. (g) Dilation of (f) using a horizontal line 21 pixels long. (h) Minimum \\n \\nof (d) and (g). (i) Final reconstruction result. (Images courtesy of Dr. Steve Eddins, MathWorks, Inc.)\\nconstant intensity. The solution of this problem is a good illustration of the power of grayscale mor-\\nphology. We begin by suppressing the horizontal reﬂection on the top of the keys. The reﬂections are \\nwider than any single character in the image, so we should be able to suppress them by performing an \\nopening by reconstruction using a long horizontal line in the erosion operation. This operation will \\nyield the background containing the keys and their reﬂections. Subtracting this from the original image \\n(i.e., performing a top-hat by reconstruction) will eliminate the horizontal reﬂections and variations in \\nbackground from the original image.\\nDIP4E_GLOBAL_Print_Ready.indb   690\\n6/16/2017   2:12:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 691}),\n",
       " Document(page_content='9.8\\n  \\nGrayscale Morphology\\n    \\n691\\nFigure 9.46(b) shows the result of opening by reconstruction of the original image using a hori-\\nzontal line of size \\n17 1\\n×\\n pixels for the SE in the erosion operation. We could have used an opening to \\nremove the characters\\n, but the resulting background would not have been as uniform, as Fig. 9.46(c) \\nshows (compare the regions between the keys in the two images). Figure 9.46(d) shows the result \\nof subtracting Fig. 9.46(b) from Fig. 9.46(a). As expected, the horizontal reﬂections and variations \\nin background were suppressed. For comparison, Fig. 9.46(e) shows the result of performing just a \\ntop-hat transformation (i.e., subtracting the “standard” opening from the image). As expected from \\nthe characteristics of the background in Fig. 9.46(c), the background in Fig. 9.46(e) is not nearly as \\nuniform as in Fig. 9.46(d).\\nThe next step is to remove the vertical reﬂections from the edges of the keys, visible in Fig. 9.46(d). \\nWe can do this by performing an opening by reconstruction with a line SE whose width is approximately \\nequal to the reﬂections (about 11 pixels in this case). Figure 9.46(f) shows the result of performing this \\noperation on Fig. 9.46(d). The vertical reﬂections were suppressed, but so were thin, vertical strokes \\nthat are valid characters (for example, the I in SIN), so we have to ﬁnd a way to restore \\nthe latter. \\nThe suppressed characters are very close to the other characters so, if we dilate the remaining characters \\nhorizontally, the dilated characters will overlap the area previously occupied by the suppressed characters. \\nFigure 9.46(g), obtained by dilating Fig. 9.46(f) with a line SE of size \\n12 1\\n×\\n elements, shows that indeed \\nthis is case\\n.\\nAll that remains at this point is to restore the suppressed characters. Consider an image formed as \\nthe point-wise minimum between the dilated image in Fig. 9.46(g) and the top-hat by reconstruction in \\nFig. 9.46(d). Figure 9.46(h) shows the minimum image (although this result appears to be close to our \\nobjective, note that the I in SIN is still missing). By using this image as a marker and the dilated image as \\nthe mask in grayscale reconstruction [Eq. (9-67)], we obtained the ﬁnal result in Fig. 9.46(i). This image \\nshows that all characters were properly extracted from the original, irregular background, including the \\nbackground of the keys. The background in Fig. 9.46(i) is uniform throughout.\\nSummary, References, and Further Reading\\n \\nThe morphological concepts and techniques introduced in this chapter constitute a powerful set of tools for extract-\\ning features of interest in an image. One of the most appealing aspects of morphological image processing is the \\nextensive set-theoretic foundation from which morphological techniques have evolved. A signiﬁcant advantage in \\nterms of implementation is that dilation and erosion are primitive operations, which are the basis for a broad class \\nof morphological algorithms. As will be shown in the following chapter, morphology can be used as the basis for \\ndeveloping image segmentation procedures with numerous applications. As we will discuss in Chapter 11, morpho-\\nlogical techniques also play a major role in procedures for image feature extraction.\\nThe book by Serra [1982] is a fundamental reference on morphological image processing. See also Serra [1988], \\nGiardina and Dougherty [1988], and Haralick and Shapiro [1992]. For an overview of both binary and gray-scale \\nmorphology, see Basart and Gonzalez [1992] and Basart et al. [1992]. This set of references provides ample basic \\nbackground for the material covered in Sections 9.1 through 9.4. For a good overview of the material in Sections 9.5 \\nand 9.6, see the book by Soille [2003].\\nImportant issues of implementing morphological algorithms such as the ones given in Section 9.5 and 9.6 are \\nexempliﬁed in the papers by Jones and Svalbe [1994], Sussner and Ritter [1997], and Shaked and Bruckstein [1998]. \\nA paper by Vincent [1993] is especially important in terms of practical details for implementing gray-scale morpho-\\nlogical algorithms. For additional reading on the theory and applications of morphological image processing, see the \\nbooks by Goutsias and Bloomberg [2000], and by Beyerer et al. [2016]. To get an idea of the state of the art in fast \\ncomputer implementation of morphological algorithms, see Thurley and Danell [ 2012]. For details on the software \\naspects of many of the examples in this chapter, see Gonzalez, Woods, and Eddins [2009].\\nDIP4E_GLOBAL_Print_Ready.indb   691\\n6/16/2017   2:12:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 692}),\n",
       " Document(page_content='692\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n9.1 \\nFind the reﬂection, \\nˆ\\n,\\nB\\n of each of the following \\nstructuring elements\\n. The dot indicates the origin \\nof the SE.\\n(a)*\\n(b)\\n(c)\\n9.2 \\nSketch the result of eroding Fig. 9.3(a) with each \\nof the following structuring elements\\n.\\n(a)*\\n(b)\\n(c)\\n9.3 * \\nErosion of a set \\nA\\n by structuring element \\nB\\n is \\na subset of \\nA\\n,\\n provided that the origin of \\nB\\n lies \\nwithin \\nB\\n. Give an example in which the erosion \\nAB\\n|\\n lies outside, or partially outside, \\nA\\n.\\n9.4 \\nLet \\nB\\n be a structuring element containing a single \\npoint,\\n valued 1, and let \\nA\\n be a set of foreground \\npixels. \\n(a) * \\nWhat do you think would happen if we erode \\nA\\n by \\nB\\n?\\n(b) \\nWhat do you think would happen if we dilate \\nA\\n by \\nB\\n?\\n9.5 \\nYou are given a “black-box” function that com-\\nputes erosion.\\n You are told that this function \\nautomatically pads the input image with a border \\nwhose width is the thinnest border possible, as \\ndetermined by the dimensions of the structuring \\nelement (e.g., for a \\n33\\n×\\n structuring element the \\nborder would be one pixel wide).\\n However, you \\nare not told whether the padding is composed of \\nbackground (0) or foreground (1) values. Propose \\nan experiment for answering this question.\\n9.6 \\nDo the following:\\n(a) * \\nDilate Fig. 9.3(a) using the structuring element \\nin ﬁgure (a) of Problem 9.2.\\n(b) \\nRepeat (a) using the structuring element in \\nﬁgure (b).\\n(c) \\nRepeat (a) using the structuring element in \\nﬁgure (c).\\n9.7 \\nDilation of a set \\nA\\n by structuring element \\nB\\n is \\nthe set of locations of the origin of \\nB\\n such that \\nA\\n \\ncontains at least one (foreground) element of \\nB\\n. \\nGive an example in which the dilation of \\nA\\n by \\nB\\n \\nlies completely outside of \\nA\\n. (\\nHint:\\n Let \\nA\\n and \\nB\\n \\nbe disks of different radii.)\\n9.8 \\nWith reference to the image at the top of the ﬁg-\\nure shown below\\n, answer the following:\\n(a) * \\nGive the structuring element and morpho-\\nlogical operation(s) that produced image (a).\\n \\nShow the origin of the structuring element. \\nThe dashed lines denote the boundary of the \\noriginal object and are shown for reference; \\nthey are not part of the result. (The white \\nelements are foreground pixels.)\\n(b) \\nRepeat part (a) for the output shown in \\nimage (b).\\n(c) * \\nRepeat part (a) for the output shown in \\nimage (c).\\n(d) \\nRepeat part (a) for the solution shown in ﬁg-\\nure (d).\\n Note that in image (d) all corners are \\nrounded.\\n(a)* (b) (c)* (d)\\n9.9 \\nLet \\nA\\n denote the set shown shaded in the follow-\\ning ﬁgure\\n, and refer to the structuring elements \\nshown (the black dots denote the origin). Sketch \\nthe result of the following operations:\\n(a) * \\nAB\\nB\\n|{\\n42\\n()\\n.\\n(b) \\nAB B\\n|{\\n13\\n()\\n.\\n(c) \\nAB B\\n{{\\n13\\n()\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   692\\n6/16/2017   2:12:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 693}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n693\\nL\\nL\\n/\\n2\\nL\\n/\\n2\\nL\\n/\\n4\\nL\\n/\\n4\\nL\\n/\\n4\\nB\\n1\\nB\\n2\\nB\\n3\\nB\\n4\\nL\\nA\\nL\\nL\\n9.10 \\nBe speciﬁc in answering the following:\\n(a) * \\nWhat is the limiting effect of repeatedly dilating \\na set of foreground pixels in an image? \\nAssume \\nthat a trivial (one point) structuring element is \\nnot used.\\n(b) \\nWhat is the smallest set from which you can \\nstart in order for your answer in (a) to hold?\\n9.11 \\nBe speciﬁc in answering the following:\\n(a) \\nWhat is the limiting effect of repeatedly erod-\\ning a set of foreground pixels in an image? \\nAssume that a trivial (one point) structuring \\nelement is not used.\\n(b) \\nWhat is the smallest set of foreground pixels \\nfrom which you can start in order for your \\nanswer in (a) to hold?\\n9.12 * \\nAn alternative deﬁnition of erosion is\\n \\nAB Z b A b B\\n|\\n=∈ + ∈ ∈\\n{}\\nww\\n2\\nfor every \\nShow that this deﬁnition is equivalent to the \\ndeﬁnition in Eq.\\n (9-3).\\n9.13 \\nDo the following:\\n(a) \\nShow that the deﬁnition of erosion given in \\nProblem 9.12 is equivalent to yet another \\ndeﬁnition of erosion:\\n \\nAB A\\nbB\\nb\\n|\\n=\\n(\\n)\\n∈\\n−\\n∩\\n(If \\n−\\nb\\n is replaced with \\nb\\n,\\n this expression is \\ncalled the \\nMinkowsky subtraction\\n of two \\nsets.)\\n(b) * \\nShow that the expression in (a) is equivalent \\nto the deﬁnition in Eq.\\n (9-3).\\n9.14 * \\nAn alternative deﬁnition of dilation is\\nAB Z a b aA bB\\n{\\n=∈ = + ∈ ∈\\n{}\\nww\\n2\\n,  for some  and \\nShow that this deﬁnition and the deﬁnition in \\nEq.\\n (9-6) are equivalent.\\n9.15 \\nDo the following:\\n(a) \\nShow that the deﬁnition of dilation given in \\nProblem 9.14 is equivalent to yet another \\ndeﬁnition of dilation:\\n \\nAB A\\nbB\\nb\\n{\\n=\\n(\\n)\\n∈\\n∪\\n(This expression is called the \\nMinkowsky \\naddition\\n of two sets.)\\n(b) * \\nShow that the expression in (a) is equivalent \\nalso to the deﬁnition in Eq.\\n (9-6). \\n9.16 \\nProve the validity of the duality expression given \\nin Eq.\\n (9-9).\\n9.17 \\nAnswer the following:\\n(a) * \\nThe curved portions the black border of \\nF\\nig. 9.8(d) delineate the opening of set \\nA\\n \\nin Fig. 9.8(a), but those curved segments \\nare not part of the boundary of \\nA\\n. Are the \\nblack straight-line portions in (d) part of the \\nboundary of \\nA\\n? Explain.\\n(b) \\nThe curved portions the black border of \\nF\\nig. 9.9(d) delineate the closing of set \\nA\\n in \\nFig. 9.9(a), but those curved segments are \\nnot part of the boundary of \\nA\\n. Are the black \\nstraight line portions of the boundary in (d) \\npart of the boundary of \\nA\\n? Explain.\\nDIP4E_GLOBAL_Print_Ready.indb   693\\n6/16/2017   2:12:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 694}),\n",
       " Document(page_content='694\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n9.18 \\nShow all intermediate steps of your computations \\nfor the following:\\n(a) * \\nObtain the opening of the ﬁgure below using \\na \\n33\\n×\\n SE of 1’s. Do all operations manually.\\n(b) \\nRepeat (a) for the closing operation.\\nA\\nB\\n9.19 \\nA\\n is a solid rectangle of 1’\\ns of size \\nMN\\n×\\n with a \\n1-pixel border of 0’\\ns, and \\nm\\n and \\nn\\n below are odd \\nintegers. Discuss what the result will be in each \\ncase.\\n(a) * \\nA\\n is opened with a structuring element of 1’\\ns \\nof size \\nmn\\n×\\n.\\n(b) \\nA\\n is closed with a structuring element of 1’\\ns \\nof size \\nmn\\n×\\n.\\n9.20 \\nShow the validity of the following duality expres-\\nsions [these are Eqs\\n. (9-14) and (9-15)]:\\n(a) * \\n \\nAB A B\\nc\\nc\\n/H11568\\n/H17033\\n(\\n)\\n=\\nˆ\\n.\\n(b) \\nAB A B\\nc\\nc\\n/H17033\\n/H11568\\n(\\n)\\n=\\nˆ\\n.\\n9.21 \\nShow the validity of the following expressions: \\n(a) * \\nAB\\n/H11568\\n is a subset of \\nA\\n.\\n You may assume that \\nEq. (9-12) is valid. (\\nHint:\\n Start with this equa-\\ntion and Fig. 9.8.)\\n(b) * \\nIf \\nC\\n is a subset of \\nD\\n,\\n then \\nCB\\n/H11568\\n is a subset of \\nDB\\n/H11568\\n.\\n [\\nHint:\\n Start with Eq.\\n (9-12).]\\n(c) \\nAB\\nB AB\\n/H11568/H11568 /H11568\\n(\\n)\\n=\\n.\\n [\\nHint:\\n Start with the \\ndeﬁnition of opening\\n.]\\n9.22 \\nShow the validity of the following expressions. \\n(\\nHint:\\n Study the solution to Problem 9.21.)\\n(a) \\nA\\n is a subset of \\nAB\\n/H17033\\n.\\n(b) \\nIf \\nC\\n is a subset of \\nD\\n,\\n then \\nCB\\n/H17033\\n is a subset \\nof \\nDB\\n/H17033\\n.\\n(c) \\nAB B AB\\n/H17033/H17033\\n/H17033\\n(\\n)\\n=\\n.\\n9.23 \\nRefer to the image and the disk structuring ele-\\nment shown in the lower right of the image\\n. Sketch \\nwhat the sets \\nC\\n, \\nD\\n, \\nE\\n, and \\nF\\n would look like for \\nthe following sequence of operations: \\nCAB\\n=\\n|\\n;\\nDC\\nB\\n=\\n{\\n; \\nEDB\\n=\\n{\\n;\\n and \\nFEB\\n=\\n|\\n.\\n Set \\nA\\n \\nconsists of all the foreground pixels (white),\\n \\nexcept the structuring element, \\nB\\n, which you may \\nassume is just large enough to encompass any of \\nthe random elements in the image. Note that the \\nsequence of operations above is simply the open-\\ning of \\nA\\n by \\nB\\n followed by a closing of the result \\nby \\nB\\n.\\n9.24 * \\nAssume that SE \\nB\\n2\\n in Fig. 9.12 has a border of \\nforeground pixels that is more than one pixel \\nwide. Assuming that all four sides of the border \\nare the same, what is the maximum width of a \\nborder we can use around \\nB\\n2\\n before the solution \\nshown in Fig. 9.12(f) fails?\\n9.25 \\nWe mentioned when discussing Fig. 9.12(e) that \\nthe image had been cropped for consistenc\\ny. \\nAssume that Fig. 9.12(b) was padded with the \\nminimum border required to encompass the \\nmaximum excursions of \\nB\\n2\\n after which no further \\nchanges would occur in the erosion. What did \\nFig. 9.12(e) look like before it was cropped?\\n9.26 \\nSketch the result of applying the hit-or-miss \\ntransform to the image below\\n, using the SE shown. \\nIndicate clearly the origin and border you select-\\ned for the structuring element.\\nImage\\nStructuring element\\nDIP4E_GLOBAL_Print_Ready.indb   694\\n6/16/2017   2:12:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 695}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n695\\n9.27 * \\nGive the foundation of an algorithm for convert-\\ning an 8-connected,\\n closed curve to a 4-connected \\ncurve (see Section 2.5 regarding connectivity). \\nThe input is a binary image, \\nI\\n, in which the curve \\nconsists of 1-valued pixels embedded in a back-\\nground of 0’s. The output should be a binary image \\nalso, containing the new curve. You may assume \\nthat the curve is fully connected, is one pixel thick, \\nand has no branches. You do not need to (but you \\nmay) state the algorithm in a step-by-step man-\\nner. An overall plan containing all the information \\nneeded to implement a working algorithm is suf-\\nﬁcient.\\n9.28 \\nGive the foundation of an algorithm for convert-\\ning a 4-connected closed curve to a curve con-\\ntaining \\nonly\\n 8-connected pixels (see Section 2.5\\n \\nregarding connectivity). The input is a binary \\nimage\\n, \\nI\\n, in which the curve consists of 1-valued \\npixels embedded in a background of 0’s. The \\noutput should be a binary image also, containing \\nthe new curve. You may assume that the curve is \\nfully connected, it is one-pixel-wide, and has no \\nbranches. You do not need to (but you may) state \\nthe algorithm in a step-by-step manner. An over-\\nall plan containing all the information needed to \\nimplement a working algorithm is sufﬁcient.\\n9.29 \\nGive the foundation of an algorithm for convert-\\ning an 8-connected closed curve to an \\nm\\n-connect-\\ned curve (see Section 2.5 regarding connectiv-\\nity).\\n The input is a binary image, \\nI\\n, in which the \\ncurve consists of 1-valued pixels embedded in a \\nbackground of 0’s. The output should be a binary \\nimage also, containing the new curve. You may \\nassume that the curve is fully connected, it is one-\\npixel-wide, and has no branches. You do not need \\nto (but you may) state the algorithm in a step-\\nby-step manner. An overall plan containing all \\nthe information needed to implement a working \\nalgorithm is sufﬁcient.\\n9.30 * \\nThree curve types (lake, bay, and line segment) \\nuseful for differentiating thinned objects in an \\nimage are shown in the following ﬁgure\\n. Develop \\na morphological/logical algorithm for differentiat-\\ning between these shapes. The input to your algo-\\nrithm would be one of these three curves. The out-\\nput must be the type of the input. You may assume \\nthat the curves are 1 pixel thick and are fully con-\\nnected. They can appear in any orientation.\\nLake\\nBay\\nLine segment\\n9.31 \\nWrite Eq. (9-18) in terms of a dilation, instead of \\nan erosion,\\n of \\nA\\n. (\\nHint:\\n Take a look at the deﬁni-\\ntion of set difference in Eq. (2-40) and then con-\\nsider the duality relationship between erosion \\nand dilation.)\\n9.32 \\nAnswer the following:\\n(a) * \\nDiscuss the effect of using the structuring \\nelement in F\\nig. 9.17(c) for boundary extrac-\\ntion, instead of the element in Fig. 9.15(b).\\n(b) \\nWhat would be the effect of using a \\n33\\n×\\nstructuring element composed of all 1’s in the \\nhole ﬁlling algorithm of Eq.\\n (9-19), instead of \\nthe structuring element in Fig. 9.17(c)?\\n9.33 \\nDiscuss what you would expect the result to be in \\neach of the following cases:\\n(a) * \\nThe starting point of the hole ﬁlling algo-\\nrithm of Eq.\\n (9-19) is a point \\non\\n the outer \\nboundary of the object containing the hole.\\n(b) \\nThe starting point in the hole ﬁlling algo-\\nrithm is \\noutside\\n of the boundary (i.e\\n., the \\nstarting point is a background pixel).\\n9.34 \\nSketch the convex hull of the large ﬁgure in Prob-\\nlem 9.9.\\n Assume that \\nL\\n = 3 pixels.\\n9.35 \\nObtain the convex deﬁciency of set \\nA\\n shown in \\nF\\nig. 9.21(b). Use the convex hull in Fig. 9.22(a).\\n9.36 \\nDo the following:\\n(a) * \\nPropose a method using any of the methods \\ndeveloped in this chapter for automating the \\nexample in F\\nig. 9.18. You may assume that \\nthe spheres do not touch each other and that \\nnone touch the border of the image.\\n(b) \\nRepeat (a), but allowing the spheres to touch \\nin arbitrary ways\\n, including the border.\\nDIP4E_GLOBAL_Print_Ready.indb   695\\n6/16/2017   2:12:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 696}),\n",
       " Document(page_content='696\\n    \\nChapter\\n \\n9\\n  \\nMorphological Image Processing\\n9.37 * \\nThe algorithm for extracting connected compo-\\nnents discussed in Section 9.5 requires that a point \\nbe known in each connected component in order \\nto extract them all.\\n Suppose that you are given a \\nbinary image containing an arbitrary (unknown) \\nnumber of connected components. Propose a \\ncompletely automated procedure for extracting \\nall connected components. Assume that points \\nbelonging to connected components are labeled \\n1 and background points are labeled 0.\\n9.38 \\nGive an expression based on reconstruction by \\ndilation capable of extracting all the holes in a \\nbinary image\\n.\\n9.39 \\nWith reference to the hole-ﬁlling algorithm in \\nEqs\\n. (9-45) and (9-46):\\n(a) * \\nExplain what would happen if all border \\npoints of \\nI\\n are 1 (foreground).\\n(b) \\nIf the result in (a) gives the result that you \\nwould expect,\\n explain why. If it does not, \\nexplain how you would modify the algorithm \\nso that it works as expected.\\n9.40 * \\nAs explained in Eqs. (9-44) and (9-69), opening by \\nreconstruction preserves the shape of the image \\ncomponents that remain after erosion.\\n What does \\nclosing by reconstruction do?\\n9.41 \\nShow that geodesic erosion and dilation (Sec-\\ntion 9.6) are duals with respect to set comple-\\nmentation.\\n That is, assuming that the structuring \\nelement is symmetric about its origin, show that:\\n(a) * \\n \\nEF DD F\\nG\\nn\\nGG\\nn\\nc\\nc\\ncc\\n(\\n)\\n(\\n)\\n−\\n(\\n)\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n11\\n and, conversely, that\\n(b) \\n \\nDF EE F\\nG\\nn\\nGG\\nn\\nc\\nc\\ncc\\n(\\n)\\n(\\n)\\n−\\n(\\n)\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n11\\n.\\n \\n(\\nHint:\\n Use proof by induction.)\\n9.42 \\nShow that reconstruction by dilation and recon-\\nstruction by erosion (Section 9.6) are duals with \\nrespect to set complementation.\\n That is, assum-\\ning that the structuring element is symmetric \\nabout its origin, show that \\nRF R F\\nG\\nD\\nG\\nEc\\nc\\nc\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n and, \\nconversely, that \\nRF R F\\nG\\nE\\nG\\nDc\\nc\\nc\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n (\\nHint:\\n Consider \\nusing the results from Problem 9.41.)\\n9.43 \\nShow that: \\n(a) * \\nFn B F n B\\nc\\nc\\n|{\\n(\\n)\\n=\\nˆ\\n,\\n where \\nFn B\\n|\\n indicates \\nn\\n successive erosions, starting with \\nB\\n; and \\nsimilarly, that \\n(b) \\n \\nFn B F n B\\nc\\nc\\n{|\\n(\\n)\\n=\\nˆ\\n.\\n9.44 \\nShow the validity of the following binary morpho-\\nlogical expressions\\n. You may assume that the struc-\\nturing element is symmetric about its origin.\\n(a) * \\nOF CF\\nR\\nn\\nR\\nn\\nc\\nc\\n(\\n)\\n(\\n)\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n(b) \\nCF OF\\nR\\nn\\nR\\nn\\nc\\nc\\n(\\n)\\n(\\n)\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n9.45 \\nProve the validity of the following grayscale mor-\\nphological expressions\\n. Recall from the discus-\\nsion in Section 9.8 that \\nfx y f x y\\nc\\n(,) (,)\\n=−\\n and \\nthat \\nˆ\\n(,\\n) ( , ) .\\nbxy b x y\\n=− −\\n(a) * \\n() .\\nfbf b\\nc\\n|{\\n=\\n(b) \\nfb f b\\nc\\nc\\n{|\\n(\\n)\\n=\\nˆ\\n.\\n(c) \\nfb f b\\nc\\nc\\n/H17033\\n/H11568\\n(\\n)\\n=\\nˆ\\n.\\n(d) * \\nfb f b\\nc\\nc\\n/H11568\\n/H17033\\n(\\n)\\n=\\nˆ\\n.\\n9.46 \\nProve the validity of the following gray-\\nscale morphological expressions\\n. Recall that \\nfx y f x y\\nc\\n(,) (,)\\n=−\\n and that \\nˆ\\n(,\\n) ( , ) .\\nbxy b x y\\n=− −\\n \\n(\\nHint:\\n \\nUse proof by induction.)\\n(a) * \\nDf E E f\\ng\\nn\\ngg\\nn\\nc\\nc\\ncc\\n(\\n)\\n(\\n)\\n−\\n(\\n)\\n()\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n11\\n[( ) ] .\\n Assume a symmetric \\nstructuring element.\\n(b) \\nEf D D f\\ng\\nn\\ngg\\nn\\nc\\nc\\ncc\\n(\\n)\\n(\\n)\\n−\\n(\\n)\\n()\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n11\\n[( ) ] .\\n Assume a symmetric \\nstructuring element.\\n9.47 \\nProve the validity of the following grayscale mor-\\nphological expressions\\n.\\n(a) * \\nRf Rf\\ng\\nD\\ng\\nEc\\nc\\nc\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n(b) \\nRf Rf\\ng\\nE\\ng\\nDc\\nc\\nc\\n()\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n9.48 \\nProve the validity of the following grayscale mor-\\nphological expressions\\n.\\n(a) * \\nfn b f n b\\nc\\nc\\n|{\\n(\\n)\\n=\\n(\\n)\\nˆ\\n,\\n where \\nfn b\\n|\\n(\\n)\\n indicates \\nn\\n successive erosions, starting with \\nb\\n.\\n(b) \\nfn b f n b\\nc\\nc\\n{|\\n(\\n)\\n=\\n(\\n)\\nˆ\\n.\\n9.49 \\nProve the validity of the following gray-\\nscale morphological expressions\\n. Recall that \\nfx y f x y\\nc\\n(,) (,)\\n=−\\n  and  that \\nˆ\\n(,\\n) ( , ) .\\nbxy b x y\\n=− −\\n \\nAssume a symmetric structuring element.\\n(a) * \\nOf Cf\\nR\\nn\\nR\\nn\\nc\\nc\\n()\\n()\\n(\\n)\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n \\n(b) \\nCf Of\\nR\\nn\\nR\\nn\\nc\\nc\\n()\\n()\\n(\\n)\\n=\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n.\\n9.50 \\nConsider the image below, which shows a region \\nof small circles enclosed by a region of larger \\ncircles\\n.\\n(a) \\nWould you expect the method used to gen-\\nerate F\\nig. 9.45(d) to work with this image \\nalso? Explain your reasoning, including any \\nDIP4E_GLOBAL_Print_Ready.indb   696\\n6/16/2017   2:12:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 697}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n697\\nassumptions that you need to make for the \\nmethod to work.\\n(b) * \\nIf your answer to (a) is yes, sketch what the \\nboundary will look like\\n.\\n9.51 \\nA preprocessing step in an application of mi- \\ncroscopy is concerned with the issue of isolating \\nindividual round particles from similar particles \\nthat overlap in groups of two or more particles \\n(see the following image).\\n Assuming that all parti-\\ncles are of the same size, propose a morphological \\nalgorithm that produces three images consisting \\nrespectively of:\\n(a) * \\nOnly particles that have merged with the \\nboundary of the image\\n.\\n(b) \\nOnly overlapping particles.\\n(c) \\nOnly nonoverlapping particles.\\n9.52 \\nA high-technology manufacturing plant is award-\\ned a government contract to manufacture high-\\nprecision washers of the form shown.\\n The terms \\nof the contract require that the shape of all wash-\\ners be inspected by an imaging system. In this con-\\ntext, shape inspection refers to deviations from \\nround on the inner and outer edges of the wash-\\ners. You may assume the following: (1) A “golden” \\n(perfect with respect to the problem) image of an \\nacceptable washer is available; and (2) the imag-\\ning and positioning components ultimately used \\nin the system will have an accuracy high enough \\nto allow you to ignore errors due to digitalization \\nand positioning. You are hired as a consultant to \\nhelp specify the visual inspection part of the sys-\\ntem. Propose a solution based on morphological/\\nlogical operations.\\nDIP4E_GLOBAL_Print_Ready.indb   697\\n6/16/2017   2:12:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 698}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 699}),\n",
       " Document(page_content='69910\\nImage Segmentation\\nPreview\\nThe material in the previous chapter began a transition from image processing methods whose inputs \\nand outputs are images, to methods in which the inputs are images but the outputs are attributes extract-\\ned from those images. Most of the segmentation algorithms in this chapter are based on one of two basic \\nproperties of image intensity values: \\ndiscontinuity\\n and \\nsimilarity\\n. In the ﬁrst category, the approach is \\nto partition an image into regions based on abrupt changes in intensity, such as edges. Approaches in \\nthe second category are based on partitioning an image into regions that are similar according to a set \\nof predeﬁned criteria. Thresholding, region growing, and region splitting and merging are examples of \\nmethods in this category. We show that improvements in segmentation performance can be achieved \\nby combining methods from distinct categories, such as techniques in which edge detection is combined \\nwith thresholding. We discuss also image segmentation using clustering and superpixels, and give an \\nintroduction to graph cuts, an approach ideally suited for extracting the principal regions of an image. \\nThis is followed by a discussion of image segmentation based on morphology, an approach that com-\\nbines several of the attributes of segmentation based on the techniques presented in the ﬁrst part of the \\nchapter. We conclude the chapter with a brief discussion on the use of motion cues for segmentation. \\nUpon completion of this chapter, readers should:\\n Understand the characteristics of various types \\nof edges found in practice.\\n Understand how to use spatial ﬁltering for \\nedge detection.\\n Be familiar with other types of edge detection \\nmethods that go beyond spatial ﬁltering.\\n Understand image thresholding using several \\ndifferent approaches.\\n Know how to combine thresholding and spa-\\ntial ﬁltering to improve segmentation.\\n Be familiar with region-based segmentation, \\nincluding clustering and superpixels.\\n Understand how graph cuts and morphologi-\\ncal watersheds are used for segmentation. \\n Be familiar with basic techniques for utilizing \\nmotion in image segmentation.\\nThe whole is equal to the sum of its parts.\\nEuclid\\nThe whole is greater than the sum of its parts.\\nMax Wertheimer\\nDIP4E_GLOBAL_Print_Ready.indb   699\\n6/16/2017   2:12:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 700}),\n",
       " Document(page_content='700\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n10.1  FUNDAMENTALS  \\nLet \\nR\\n represent the entire spatial region occupied by an image. We may view image \\nsegmentation as a process that partitions \\nR\\n into \\nn\\n subregions, \\nRR R\\nn\\n12\\n,,,,\\n…\\n such \\nthat\\n(a) \\nRR\\ni\\ni\\nn\\n=\\n=\\n.\\n1\\n∪\\n(b) \\nR\\ni\\n is a connected set, for \\nin\\n=\\n012\\n,,\\n, ,.\\n…\\n(c) \\nRR\\nij\\n/H20669\\n=∅\\n for all \\ni\\n and \\nj\\n, \\nij\\n≠\\n.\\n(d) \\nQR\\ni\\n()\\n=\\nTRUE for \\nin\\n=\\n012\\n,,\\n, ,.\\n…\\n(e) \\nQR R\\nij\\n/H20668\\n()\\n=\\nFALSE\\n for any adjacent regions \\nR\\ni\\n and \\nR\\nj\\n.\\nwhere \\nQR\\nk\\n()\\n is a logical predicate defined over the points in set \\nR\\nk\\n,\\n and \\n∅\\n is the \\nnull set.\\n The symbols \\n´\\n and \\n¨\\n represent set union and intersection, respectively, as \\ndefined in Section 2.6.\\n Two regions \\nR\\ni\\n and \\nR\\nj\\n are said to be \\nadjacent\\n if their union \\nforms a connected set, as defined in Section 2.5. If the set formed by the union of two \\nregions is not connected, the regions are said to \\ndisjoint\\n.\\nCondition (a) indicates that the segmentation must be \\ncomplete\\n, in the sense that \\nevery pixel must be in a region. Condition (b) requires that points in a region be con-\\nnected in some predeﬁned sense (e.g., the points must be 8-connected). Condition \\n(c) says that the regions must be disjoint. Condition (d) deals with the properties that \\nmust be satisﬁed by the pixels in a segmented region—for example, \\nQR\\ni\\n()\\n=\\nTRUE \\nif all pixels in \\nR\\ni\\n have the same intensity. Finally, condition (e) indicates that two \\nadjacent regions \\nR\\ni\\n and \\nR\\nj\\n must be different in the sense of predicate \\nQ\\n.\\n†\\nThus, we see that the fundamental problem in segmentation is to partition an \\nimage into regions that satisfy the preceding conditions. Segmentation algorithms \\nfor monochrome images generally are based on one of two basic categories dealing \\nwith properties of intensity values: \\ndiscontinuity\\n and \\nsimilarity\\n. In the ﬁrst category, \\nwe assume that boundaries of regions are sufﬁciently different from each other, and \\nfrom the background, to allow boundary detection based on local discontinuities in \\nintensity. \\nEdge-based\\n segmentation is the principal approach used in this category. \\nRegion-based\\n segmentation approaches in the second category are based on parti-\\ntioning an image into regions that are similar according to a set of predeﬁned criteria.\\nFigure 10.1 illustrates the preceding concepts. Figure 10.1(a) shows an image of a \\nregion of constant intensity superimposed on a darker background, also of constant \\nintensity. These two regions comprise the overall image. Figure 10.1(b) shows the \\nresult of computing the boundary of the inner region based on intensity discontinui-\\nties. Points on the inside and outside of the boundary are black (zero) because there \\nare no discontinuities in intensity in those regions. To segment the image, we assign \\none level (say, white) to the pixels on or inside the boundary, and another level (e.g., \\nblack) to all points exterior to the boundary. Figure 10.1(c) shows the result of such \\na procedure. We see that conditions (a) through (c) stated at the beginning of this \\n†\\n In general, \\nQ\\n can be a compound expression such as, \\n“\\nQR\\ni\\n()\\n=\\nTRUE\\n if the average intensity of the pixels in \\nregion \\nR\\ni\\n is less than \\nm\\ni\\n AND if the standard deviation of their intensity is greater than \\ns\\ni\\n,”\\n where \\nm\\ni\\nand \\ns\\ni\\n \\nare speciﬁed constants. \\n10.1\\nDIP4E_GLOBAL_Print_Ready.indb   700\\n6/16/2017   2:12:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 701}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n701\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.1\\n(a) Image of a \\nconstant intensity \\nregion.  \\n(b) Boundary \\nbased on intensity \\ndiscontinuities.  \\n(c) Result of \\nsegmentation.  \\n(d) Image of a \\ntexture region.  \\n(e) Result of \\nintensity discon-\\ntinuity computa-\\ntions (note the \\nlarge number of \\nsmall edges).  \\n(f) Result of \\nsegmentation \\nbased on region \\nproperties.\\nsection are satisﬁed by this result. The predicate of condition (d) is: If a pixel is on, \\nor inside the boundary, label it white; otherwise, label it black. We see that this predi-\\ncate is TRUE for the points labeled black or white in Fig. 10.1(c). Similarly, the two \\nsegmented regions (object and background) satisfy condition (e).\\nThe next three images illustrate region-based segmentation. Figure 10.1(d) is \\nsimilar to Fig. 10.1(a), but the intensities of the inner region form a textured pattern. \\nFigure 10.1(e) shows the result of computing intensity discontinuities in this image. \\nThe numerous spurious changes in intensity make it difﬁcult to identify a unique \\nboundary for the original image because many of the nonzero intensity changes are \\nconnected to the boundary, so edge-based segmentation is not a suitable approach. \\nHowever, we note that the outer region is constant, so all we need to solve this seg-\\nmentation problem is a predicate that differentiates between textured and constant \\nregions. The standard deviation of pixel values is a measure that accomplishes this \\nbecause it is nonzero in areas of the texture region, and zero otherwise. Figure 10.1(f) \\nshows the result of dividing the original image into subregions of size \\n88\\n×\\n.\\n Each \\nsubregion was then labeled white if the standard deviation of its pixels was posi-\\ntive (i.e\\n., if the predicate was TRUE), and zero otherwise. The result has a “blocky” \\nappearance around the edge of the region because groups of \\n88\\n×\\n squares were \\nlabeled with the same intensity (smaller squares would have given a smoother \\nregion boundary).\\n Finally, note that these results also satisfy the ﬁve segmentation \\nconditions stated at the beginning of this section.\\n10.2  POINT, LINE, AND EDGE DETECTION  \\nThe focus of this section is on segmentation methods that are based on detecting \\nsharp, \\nlocal\\n changes in intensity. The three types of image characteristics in which \\n10.2\\nDIP4E_GLOBAL_Print_Ready.indb   701\\n6/16/2017   2:12:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 702}),\n",
       " Document(page_content='702\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nwe are interested are isolated points, lines, and edges. \\nEdge pixels\\n are pixels at which \\nthe intensity of an image changes abruptly, and \\nedges\\n (or \\nedge segments\\n) are sets of \\nconnected edge pixels (see Section 2.5 regarding connectivity). \\nEdge detectors\\n are \\nlocal image processing tools designed to detect edge pixels. A \\nline\\n may be viewed as \\na (typically) thin edge segment in which the intensity of the background on either \\nside of the line is either much higher or much lower than the intensity of the line \\npixels. In fact, as we will discuss later, lines give rise to so-called “roof edge\\ns.” F\\ninally, \\nan \\nisolated point\\n may be viewed as a foreground (background) pixel surrounded by \\nbackground (foreground) pixels.\\nBACKGROUND\\nAs we saw in Section 3.5, local averaging smoothes an image. Given that averaging \\nis analogous to integration, it is intuitive that abrupt, local changes in intensity can \\nbe detected using derivatives. For reasons that will become evident shortly, first- and \\nsecond-order derivatives are particularly well suited for this purpose.\\nDerivatives of a digital function are deﬁned in terms of \\nﬁnite differences\\n. There \\nare various ways to compute these differences but, as explained in Section 3.6, we \\nrequire that any approximation used for ﬁrst derivatives (1) must be zero in areas \\nof constant intensity; (2) must be nonzero at the onset of an intensity step or ramp; \\nand (3) must be nonzero at points along an intensity ramp. Similarly, we require that \\nan approximation used for second derivatives (1) must be zero in areas of constant \\nintensity; (2) must be nonzero at the onset and end of an intensity step or ramp; and \\n(3) must be zero along intensity ramps. Because we are dealing with digital quanti-\\nties whose values are ﬁnite, the maximum possible intensity change is also ﬁnite, and \\nthe shortest distance over which a change can occur is between adjacent pixels.\\nWe obtain an approximation to the ﬁrst-order derivative at an arbitrary point \\nx\\n of \\na one-dimensional function \\nfx\\n()\\n by expanding the function \\nfx x\\n()\\n+\\n/H9004\\n into a Taylor \\nseries about \\nx\\nfx x fx x\\nfx\\nx\\nx\\nfx\\nx\\nx\\nfx\\nx\\n() ( )\\n()\\n!\\n()\\n!\\n()\\n+= +\\n∂\\n∂\\n+\\n()\\n∂\\n∂\\n+\\n()\\n∂\\n∂\\n+⋅\\n/H9004/H9004\\n/H9004/H9004\\n2\\n2\\n2\\n3\\n3\\n3\\n23\\n⋅ ⋅⋅⋅⋅⋅\\n=\\n()\\n∂\\n∂\\n=\\n∑\\n/H9004\\n/H11009\\nx\\nn\\nfx\\nx\\nn\\nn\\nn\\nn\\n!\\n()\\n0\\n \\n(10-1)\\nwhere \\n/H9004\\nx\\n is the separation between samples of \\nf\\n.\\n For our purposes, this separation \\nis measured in pixel units. Thus, following the convention in the book, \\n/H9004\\nx\\n=\\n1\\n for \\nthe sample preceding \\nx\\n and \\n/H9004\\nx\\n=−\\n1\\n for the sample following \\nx\\n.\\n When \\n/H9004\\nx\\n=\\n1,\\n Eq. \\n(10-1) becomes\\n \\nfx fx\\nfx\\nx\\nfx\\nx\\nfx\\nx\\nn\\nn\\n() ( )\\n()\\n!\\n()\\n!\\n()\\n!\\n+= +\\n∂\\n∂\\n+\\n∂\\n∂\\n+\\n∂\\n∂\\n+ ⋅⋅⋅⋅⋅⋅\\n=\\n∂\\n1\\n1\\n2\\n1\\n3\\n1\\n2\\n2\\n3\\n3\\nf\\nfx\\nx\\nn\\nn\\n()\\n∂\\n=\\n∑\\n0\\n/H11009\\n \\n(10-2)\\nWhen we refer to lines, \\nwe are referring to thin \\nstructures, typically just \\na few pixels thick. Such \\nlines may correspond, for \\nexample, to elements of \\na digitized architectural \\ndrawing, or roads in a \\nsatellite image.\\nRemember, the notation \\nn\\n! means “\\nn\\n factorial”:  \\nn\\n! = 1\\n/H11003\\n2\\n/H11003\\n· · ·\\n/H11003 \\nn\\n.\\nAlthough this is an \\nexpression of only one \\nvariable, we used partial \\nderivatives notation for \\nconsistency when we \\ndiscuss functions of two \\nvariables later in this \\nsection.\\nDIP4E_GLOBAL_Print_Ready.indb   702\\n6/16/2017   2:12:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 703}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n703\\nSimilarly, when \\n/H9004\\nx\\n=−\\n1,\\n \\nfx fx\\nfx\\nx\\nfx\\nx\\nfx\\nx\\nn\\n() ( )\\n()\\n!\\n()\\n!\\n()\\n−= −\\n∂\\n∂\\n+\\n∂\\n∂\\n−\\n∂\\n∂\\n+ ⋅⋅⋅⋅⋅⋅\\n=\\n−\\n()\\n1\\n1\\n2\\n1\\n3\\n1\\n2\\n2\\n3\\n3\\nn n\\nfx\\nx\\nn\\nn\\nn\\n!\\n()\\n∂\\n∂\\n=\\n∑\\n0\\n/H11009\\n \\n(10-3)\\nIn what follows, we compute \\nintensity differences\\n using just a few terms of the \\nTaylor \\nseries. For first-order derivatives we use only the linear terms, and we can form dif-\\nferences in one of three ways.\\nThe \\nforward difference\\n is obtained from Eq. (10-2):\\n \\n∂\\n∂\\n== + −\\nfx\\nx\\nf x fx fx\\n()\\n() ( )\\n()\\n/H11032\\n1\\n \\n(10-4)\\nwhere, as you can see, we kept only the linear terms. The \\nbac\\nkward difference\\n is simi-\\nlarly obtained by keeping only the linear terms in Eq. (10-3):\\n \\n∂\\n∂\\n==− −\\nfx\\nx\\nf x fx fx\\n()\\n() () ( )\\n/H11032\\n1\\n \\n(10-5)\\nand the \\ncentral difference\\n is obtained by subtracting Eq.\\n (10-3) from Eq. (10-2):\\n \\n∂\\n∂\\n==\\n+− −\\nfx\\nx\\nfx\\nfx fx\\n()\\n()\\n()\\n()\\n/H11032\\n11\\n2\\n \\n(10-6)\\nThe higher terms of the series that we did not use represent the error between an \\nexact and an approximate derivative expansion.\\n In general, the more terms we use \\nfrom the Taylor series to represent a derivative, the more accurate the approxima-\\ntion will be. To include more terms implies that more points are used in the approxi-\\nmation, yielding a lower error. However, it turns out that central differences have \\na lower error for the same number of points (see Problem 10.1). For this reason, \\nderivatives are usually expressed as central differences.\\nThe \\nsecond order\\n derivative based on a central difference, \\n∂∂\\n22\\nfx x\\n() ,\\n is obtained \\nby adding Eqs\\n. (10-2) and (10-3):\\n \\n∂\\n∂\\n== + − −\\n2\\n2\\n12 1\\nfx\\nx\\nf x fx fx fx\\n()\\n() ( ) () ( )\\n/H11032/H11032\\n+\\n \\n(10-7)\\nTo obtain the \\nthir\\nd order, central derivative\\n we need one more point on either side \\nof \\nx\\n. That is, we need the Taylor expansions for \\nfx\\n()\\n+\\n2\\n and \\nfx\\n()\\n,\\n−\\n2\\n which we \\nobtain from Eqs\\n. (10-2) and (10-3) with \\n/H9004\\nx\\n=\\n2\\n and \\n/H9004\\nx\\n=−\\n2,\\n respectively. The strat-\\negy is to combine the two \\nTaylor expansions to eliminate all derivatives lower than \\nthe third. The result after ignoring all higher-order terms [see Problem 10.2(a)] is\\nDIP4E_GLOBAL_Print_Ready.indb   703\\n6/16/2017   2:12:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 704}),\n",
       " Document(page_content='704\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n∂\\n∂\\n==\\n+− ++ + −\\n−−\\n3\\n3\\n22 10 2 1 2\\n2\\nfx\\nx\\nfx\\nfx fx fx fx fx\\n()\\n()\\n() () ( ) ()\\n()\\n/H11032/H11032/H11032\\n \\n(10-8)\\nSimilarly [see Problem 10.2(b)], the \\nfourth\\n finite difference (the highest we use in \\nthe book) after ignoring all higher order terms is given by\\n∂\\n∂\\n== + − + +− − + −\\n4\\n4\\n24 16 4 1 2\\nfx\\nx\\nf x fx fx fx fx fx\\n()\\n() ( ) ( ) () ( ) ( )\\n/H11032/H11032/H11032/H11032\\n \\n(10-9)\\nTable 10.1 summarizes the ﬁrst four central derivatives just discussed. Note the \\nsymmetry of the coefﬁcients about the center point.\\n This symmetry is at the root \\nof why central differences have a lower approximation error for the same number \\nof points than the other two differences. For two variables, we apply the results in \\nTable 10.1 to each variable independently. For example,\\n \\n∂\\n()\\n∂\\n=+\\n()\\n−\\n()\\n+−\\n()\\n2\\n2\\n12 1\\nfx y\\nx\\nfx y fx y fx y\\n,\\n,, ,\\n  \\n(10-10)\\nand\\n \\n∂\\n()\\n∂\\n=+\\n()\\n−\\n()\\n+−\\n()\\n2\\n2\\n12\\n1\\nfx y\\ny\\nfx y fx y fx y\\n,\\n,, ,\\n \\n(10-11)\\nIt is easily verified that the first and second-order derivatives in Eqs. (10-4) \\nthrough (10-7) satisfy the conditions stated at the beginning of this section regarding \\nderivatives of the first and second order\\n. To illustrate this, consider Fig. 10.2. Part (a) \\nshows an image of various objects, a line, and an isolated point. Figure 10.2(b) shows \\na horizontal intensity proﬁle (scan line) through the center of the image, including \\nthe isolated point. Transitions in intensity between the solid objects and the back-\\nground along the scan line show two types of edges: \\nramp edges\\n (on the left) and \\nstep edges\\n (on the right). As we will discuss later, intensity transitions involving thin \\nobjects such as lines often are referred to as \\nroof edges\\n. \\nFigure 10.2(c) shows a simpliﬁed proﬁle, with just enough points to make it possi-\\nble for us to analyze manually how the ﬁrst- and second-order derivatives behave as \\nthey encounter a point, a line, and the edges of objects. In this diagram the transition \\nfx\\n()\\n+\\n2\\nfx\\n()\\n+\\n1\\nfx\\n()\\nfx\\n()\\n−\\n1\\nfx\\n()\\n−\\n2\\n2\\nfx\\n/H11032\\n() 1 0\\n−\\n1\\nfx\\n/H11032/H11032\\n()\\n1\\n−\\n21\\n2\\nfx\\n/H11032/H11032/H11032\\n() 1\\n−\\n20 2\\n−\\n1\\nfx\\n/H11032/H11032/H11032/H11032\\n() 1\\n−\\n46\\n−\\n41\\nTABLE \\n10.1\\nFirst four central \\ndigital derivatives \\n(ﬁnite differenc-\\nes) for samples \\ntaken uniformly, \\n/H9004\\nx\\n=\\n1 units apart.\\nDIP4E_GLOBAL_Print_Ready.indb   704\\n6/16/2017   2:12:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 705}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n705\\nSecond derivative\\nFirst derivative\\nIntensity values \\nIntensity\\n5\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n54321000600001310000777\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n10 0 6\\n/H11002\\n6\\n0 0012\\n/H11002\\n2\\n/H11002\\n1 0007000\\n/H11002\\n1 0000106\\n/H11002\\n12\\n6 0011\\n/H11002\\n4 11007\\n/H11002\\n70 0\\n7\\nRamp\\nIsolated point\\nLine\\nFlat segment\\nStep\\nb\\na\\nc\\nFIGURE 10.2\\n(a) Image.  \\n(b) Horizontal \\nintensity proﬁle \\nthat includes the \\nisolated point \\nindicated by the \\narrow.  \\n(c) Subsampled \\nproﬁle; the dashes \\nwere added \\nfor clarity. The \\nnumbers in the \\nboxes are the \\nintensity values \\nof the dots shown \\nin the proﬁle. The \\nderivatives were \\nobtained using \\nEqs. (10-4) for the \\nﬁrst derivative \\nand Eq. (10-7) for \\nthe second.\\nin the ramp spans four pixels, the noise point is a single pixel, the line is three pixels \\nthick, and the transition of the step edge takes place between adjacent pixels. The \\nnumber of intensity levels was limited to eight for simplicity. \\nConsider the properties of the ﬁrst and second derivatives as we traverse the \\nproﬁle from left to right. Initially, the ﬁrst-order derivative is nonzero at the onset \\nand along the entire intensity ramp, while the second-order derivative is nonzero \\nonly at the onset and end of the ramp. Because the edges of digital images resemble \\nthis type of transition, we conclude that ﬁrst-order derivatives produce “thick” edges, \\nand second-order derivatives much thinner ones. Next we encounter the isolated \\nnoise point. Here, the magnitude of the response at the point is much stronger for \\nthe second- than for the ﬁrst-order derivative. This is not unexpected, because a \\nsecond-order derivative is much more aggressive than a ﬁrst-order derivative in \\nenhancing sharp changes. Thus, we can expect second-order derivatives to enhance \\nﬁne detail (including noise) much more than ﬁrst-order derivatives. The line in this \\nexample is rather thin, so it too is ﬁne detail, and we see again that the second deriva-\\ntive has a larger magnitude. Finally, note in both the ramp and step edges that the \\nDIP4E_GLOBAL_Print_Ready.indb   705\\n6/16/2017   2:12:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 706}),\n",
       " Document(page_content='706\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nsecond derivative has opposite signs (negative to positive or positive to negative) \\nas it transitions into and out of an edge. This “double-edge” effect is an important \\ncharacteristic that can be used to locate edges, as we will show later in this section. \\nAs we move into the edge, the sign of the second derivative is used also to determine \\nwhether an edge is a transition from light to dark (negative second derivative), or \\nfrom dark to light (positive second derivative)\\nIn summary, we arrive at the following conclusions: (1) First-order derivatives gen-\\nerally produce thicker edges. (2) Second-order derivatives have a stronger response to \\nﬁne detail, such as thin lines, isolated points, and noise. (3) Second-order derivatives \\nproduce a double-edge response at ramp and step transitions in intensity. (4) The sign \\nof the second derivative can be used to determine whether a transition into an edge is \\nfrom light to dark or dark to light.\\nThe approach of choice for computing ﬁrst and second derivatives at every pix-\\nel location in an image is to use spatial convolution. For the \\n33\\n×\\n ﬁlter kernel in \\nF\\nig. 10.3, the procedure is to compute the sum of products of the kernel coefﬁcients \\nwith the intensity values in the region encompassed by the kernel, as we explained \\nin Section 3.4. That is, the response of the ﬁlter at the center point of the kernel is\\n \\nZz z z\\nz\\nkk\\nk\\n=+ + +\\n=\\n=\\n∑\\nww w\\nw\\n11 22\\n99\\n1\\n9\\n…\\n \\n(10-12)\\nwhere \\nz\\nk\\n is the intensity of the pixel whose spatial location corresponds to the loca-\\ntion of the \\nk\\nth kernel coefficient. \\nDETECTION OF ISOLATED POINTS\\nBased on the conclusions reached in the preceding section, we know that point \\ndetection should be based on the second derivative which, from the discussion in \\nSection 3.6, means using the Laplacian:\\n \\n∇=\\n∂\\n∂\\n+\\n∂\\n∂\\n2\\n2\\n2\\n2\\n2\\nfx y\\nf\\nx\\nf\\ny\\n(,)\\n \\n(10-13)\\nThis equation is an \\nexpansion of Eq. (3-35) \\nfor a 3\\n/H11003\\n3 kernel, valid \\nat one point, and using \\nsimpliﬁed subscript \\nnotation for the kernel \\ncoefﬁcients.\\nw\\n1\\nw\\n2\\nw\\n3\\nw\\n4\\nw\\n5\\nw\\n6\\nw\\n7\\nw\\n8\\nw\\n9\\nFIGURE 10.3\\nA general \\n33\\n×\\n \\nspatial ﬁlter  \\nkernel.\\n The \\nw\\n’s\\n \\nare the kernel  \\ncoefﬁcients \\n(weights).\\nDIP4E_GLOBAL_Print_Ready.indb   706\\n6/16/2017   2:12:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 707}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n707\\nwhere the partial derivatives are computed using the second-order finite differences \\nin Eqs. (10-10) and (10-11). The Laplacian is then\\n     \\n∇\\n()\\n= + + − ++ +− −\\n2\\n11 11 4\\nfx y fx y fx y fx y fx y fx y\\n, ( ,) ( ,) (, ) (, ) (,)\\n \\n(10-14)\\nAs explained in Section 3.6, this expression can be implemented using the Lapla-\\ncian kernel in F\\nig. 10.4(a) in Example 10.1. We then we say that a point has been \\ndetected at a location \\n(,)\\nxy\\n on which the kernel is centered if the absolute value of \\nthe response of the filter at that point exceeds a specified threshold.\\n Such points are \\nlabeled 1 and all others are labeled 0 in the output image, thus producing a binary \\nimage. In other words, we use the expression: \\n  \\ngxy\\nZxy T\\n(,)\\n(,)\\n=\\n>\\n⎧\\n⎨\\n⎩\\n1\\n0\\nif\\notherwise\\n \\n(10-15)\\nwhere \\ngxy\\n(,\\n)\\n is the output image, \\nT\\n is a nonnegative threshold,\\n and \\nZ\\n is given by \\nEq. (10-12). \\nThis formulation simply measures the weighted differences between a \\npixel and its 8-neighbors. Intuitively, the idea is that the intensity of an isolated point \\nwill be quite different from its surroundings, and thus will be easily detectable by \\nthis type of kernel. Differences in intensity that are considered of interest are those \\nlarge enough (as determined by \\nT \\n) to be considered isolated points. Note that, as \\nusual for a derivative kernel, the coefficients sum to zero, indicating that the filter \\nresponse will be zero in areas of constant intensity.\\nEXAMPLE 10.1 :  Detection of isolated points in an image.\\nFigure 10.4(b) is an X-ray image of a turbine blade from a jet engine. The blade has a porosity mani-\\nfested by a single black pixel in the upper-right quadrant of the image. Figure 10.4(c) is the result of ﬁl-\\ntering the image with the Laplacian kernel, and Fig. 10.4(d) shows the result of Eq. (10-15) with \\nT\\n equal \\nto 90% of the highest absolute pixel value of the image in Fig. 10.4(c). The single pixel is clearly visible \\nin this image at the tip of the arrow (the pixel was enlarged to enhance its visibility). This type of detec-\\ntion process is specialized because it is based on abrupt intensity changes at single-pixel locations that \\nare surrounded by a homogeneous background in the area of the detector kernel. When this condition \\nis not satisﬁed, other methods discussed in this chapter are more suitable for detecting intensity changes.\\nLINE DETECTION\\nThe next level of complexity is line detection. Based on the discussion earlier in this \\nsection, we know that for line detection we can expect second derivatives to result \\nin a stronger filter response, and to produce thinner lines than first derivatives. Thus, \\nwe can use the Laplacian kernel in Fig. 10.4(a) for line detection also, keeping in \\nmind that the double-line effect of the second derivative must be handled properly. \\nThe following example illustrates the procedure.\\nDIP4E_GLOBAL_Print_Ready.indb   707\\n6/16/2017   2:12:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 708}),\n",
       " Document(page_content='708\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nEXAMPLE 10.2 :  Using the Laplacian for line detection.\\nFigure 10.5(a) shows a \\n486 486\\n×\\n (binary) portion of a wire-bond mask for an electronic circuit, and \\nF\\nig. 10.5(b) shows its Laplacian image. Because the Laplacian image contains negative values (see the \\ndiscussion after Example 3.18), scaling is necessary for display. As the magniﬁed section shows, mid gray \\nrepresents zero, darker shades of gray represent negative values, and lighter shades are positive. The \\ndouble-line effect is clearly visible in the magniﬁed region.\\nAt ﬁrst, it might appear that the negative values can be handled simply by taking the absolute value \\nof the Laplacian image. However, as Fig. 10.5(c) shows, this approach doubles the thickness of the lines. \\nA more suitable approach is to use only the positive values of the Laplacian (in noisy situations we use \\nthe values that exceed a positive threshold to eliminate random variations about zero caused by the \\nnoise). As Fig. 10.5(d) shows, this approach results in thinner lines that generally are more useful. Note \\nin Figs. 10.5(b) through (d) that when the lines are wide with respect to the size of the Laplacian kernel, \\nthe lines are separated by a zero “valley.” This is not unexpected. For example, when the \\n33\\n×\\n kernel is \\ncentered on a line of constant intensity 5 pixels wide\\n, the response will be zero, thus producing the effect \\njust mentioned. When we talk about line detection, the assumption is that lines are thin with respect to \\nthe size of the detector. Lines that do not satisfy this assumption are best treated as regions and handled \\nby the edge detection methods discussed in the following section.\\nThe Laplacian detector kernel in Fig. 10.4(a) is isotropic, so its response is inde-\\npendent of direction (with respect to the four directions of the \\n33\\n×\\n kernel: verti-\\ncal,\\n horizontal, and two diagonals). Often, interest lies in detecting lines in \\nspeciﬁed\\n \\n1\\n1\\n1\\n1\\n/H11002\\n8\\n1\\n1\\n1\\n1\\nb\\na\\nd c\\nFIGURE 10.4\\n(a) Laplacian ker-\\nnel used for point \\ndetection.  \\n(b) X-ray image \\nof a turbine blade \\nwith a porosity \\nmanifested by a \\nsingle black pixel. \\n(c) Result of con-\\nvolving the kernel \\nwith the image.  \\n(d) Result of \\nusing Eq. (10-15) \\nwas a single point \\n(shown enlarged \\nat the tip of the \\narrow). (Original \\nimage courtesy of \\nX-TEK Systems, \\nLtd.)\\nDIP4E_GLOBAL_Print_Ready.indb   708\\n6/16/2017   2:12:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 709}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n709\\nb a\\nd c\\nFIGURE 10.5\\n(a) Original  \\nimage.  \\n(b) Laplacian  \\nimage; the  \\nmagniﬁed  \\nsection shows the \\npositive/negative \\ndouble-line effect \\ncharacteristic of \\nthe Laplacian.  \\n(c) Absolute value \\nof the Laplacian. \\n(d) Positive values \\nof the Laplacian.\\ndirections. Consider the kernels in Fig. 10.6. Suppose that an image with a constant \\nbackground and containing various lines (oriented at 0°, \\n±°\\n45\\n,\\n and 90°) is ﬁltered \\nwith the ﬁrst kernel.\\n The maximum responses would occur at image locations in \\nwhich a horizontal line passes through the middle row of the kernel. This is easily \\nveriﬁed by sketching a simple array of 1’s with a line of a different intensity (say, 5s) \\nrunning horizontally through the array. A similar experiment would reveal that the \\nsecond kernel in Fig. 10.6 responds best to lines oriented at \\n+°\\n45\\n;\\n the third kernel \\nto vertical lines;\\n and the fourth kernel to lines in the \\n−°\\n45\\n direction. The preferred \\ndirection of each kernel is weighted with a larger coefﬁcient (i.e\\n., 2) than other possi-\\nble directions. The coefﬁcients in each kernel sum to zero, indicating a zero response \\nin areas of constant intensity.\\nLet \\nZZZ\\n123\\n,,,\\n and \\nZ\\n4\\n denote the responses of the kernels in Fig. 10.6, from left \\nto right, where the \\nZ\\ns are given by Eq. (10-12). Suppose that an image is ﬁltered \\nwith these four kernels, one at a time. If, at a given point in the image, \\nZZ\\nkj\\n>\\n, \\nfor all \\njk\\n≠\\n,\\n that point is said to be more likely associated with a line in the direc-\\ntion of kernel \\nk\\n.\\n For example, if at a point in the image, \\nZZ\\nj\\n1\\n>\\n for \\nj\\n=\\n23\\n4\\n,,,\\n that \\nDIP4E_GLOBAL_Print_Ready.indb   709\\n6/16/2017   2:12:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 710}),\n",
       " Document(page_content='710\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\npoint is said to be more \\nlikely associated with a horizontal line. If we are interested \\nin detecting all the lines in an image in the direction deﬁned by a given kernel, we \\nsimply run the kernel through the image and threshold the absolute value of the \\nresult, as in Eq. (10-15). The nonzero points remaining after thresholding are the \\nstrongest responses which, for lines one pixel thick, correspond closest to the direc-\\ntion deﬁned by the kernel. The following example illustrates this procedure.\\nEXAMPLE 10.3 :  Detecting lines in speciﬁed directions.\\nFigure 10.7(a) shows the image used in the previous example. Suppose that we are interested in ﬁnd-\\ning all the lines that are one pixel thick and oriented at \\n+°\\n45\\n.\\n For this purpose, we use the kernel in \\nF\\nig. 10.6(b). Figure 10.7(b) is the result of ﬁltering the image with that kernel. As before, the shades \\ndarker than the gray background in Fig. 10.7(b) correspond to negative values. There are two principal \\nsegments in the image oriented in the \\n+°\\n45\\n direction, one in the top left and one at the bottom right. Fig-\\nures 10.7(c) and (d) show zoomed sections of F\\nig. 10.7(b) corresponding to these two areas. The straight \\nline segment in Fig. 10.7(d) is brighter than the segment in Fig. 10.7(c) because the line segment in the \\nbottom right of Fig. 10.7(a) is one pixel thick, while the one at the top left is not. The kernel is “tuned” \\nto detect one-pixel-thick lines in the \\n+°\\n45\\n direction, so we expect its response to be stronger when such \\nlines are detected.\\n Figure 10.7(e) shows the positive values of Fig. 10.7(b). Because we are interested in \\nthe strongest response, we let \\nT\\n equal 254 (the maximum value in Fig. 10.7(e) minus one). Figure 10.7(f) \\nshows in white the points whose values satisﬁed the condition \\ngT\\n>\\n,\\n where \\ng\\n is the image in Fig. 10.7(e). \\nT\\nhe isolated points in the ﬁgure are points that also had similarly strong responses to the kernel. In the \\noriginal image, these points and their immediate neighbors are oriented in such a way that the kernel \\nproduced a maximum response at those locations. These isolated points can be detected using the kernel \\nin Fig. 10.4(a) and then deleted, or they can be deleted using morphological operators, as discussed in the \\nlast chapter.\\nEDGE MODELS\\nEdge detection is an approach used frequently for segmenting images based on \\nabrupt (local) changes in intensity. We begin by introducing several ways to model \\nedges and then discuss a number of approaches for edge detection.\\nb\\na\\nc\\nd\\nFIGURE 10.6\\n  Line detection kernels. Detection angles are with respect to the axis system in Fig. 2.19, with positive \\nangles measured counterclockwise with respect to the (vertical) \\nx\\n-axis.\\n/H11002\\n1\\n2\\n/H11002\\n1\\nHorizontal Vertical\\n/H11001\\n45\\n/H11034/H11002\\n45\\n/H11034\\n/H11002\\n1\\n2\\n/H11002\\n1\\n/H11002\\n1\\n2\\n/H11002\\n1\\n2\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n2\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n2\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n2\\n2\\n2\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n2\\n/H11002\\n1\\n2\\n/H11002\\n1\\n2\\n/H11002\\n1\\n/H11002\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   710\\n6/16/2017   2:12:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 711}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n711\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.7\\n (a) Image of a wire-bond template. (b) Result of processing with the \\n+°\\n45\\n line detector kernel in Fig. \\n10.6.\\n (c) Zoomed view of the top left region of (b). (d) Zoomed view of the bottom right region of (b). (e) The image \\nin (b) with all negative values set to zero. (f) All points (in white) whose values satisﬁed the condition \\ngT\\n>\\n,\\n where \\ng\\n is the image in (e) and \\nT\\n=\\n254\\n (the maximum pixel value in the image minus 1). (The points in (f) were enlarged \\nto make them easier to see\\n.)\\nEdge models are classiﬁed according to their intensity proﬁles. A \\nstep edge\\n is \\ncharacterized by a transition between two intensity levels occurring ideally over the \\ndistance of one pixel. Figure 10.8(a) shows a section of a vertical step edge and \\na horizontal intensity proﬁle through the edge. Step edges occur, for example, in \\nimages generated by a computer for use in areas such as solid modeling and ani-\\nmation. These clean, \\nideal\\n edges can occur over the distance of one pixel, provided \\nthat no additional processing (such as smoothing) is used to make them look “real.” \\nDigital step edges are used frequently as edge models in algorithm development. \\nFor example, the Canny edge detection algorithm discussed later in this section was \\nderived originally using a step-edge model.\\nIn practice, digital images have edges that are blurred and noisy, with the degree \\nof blurring determined principally by limitations in the focusing mechanism (e.g., \\nlenses in the case of optical images), and the noise level determined principally by \\nthe electronic components of the imaging system. In such situations, edges are more \\nDIP4E_GLOBAL_Print_Ready.indb   711\\n6/16/2017   2:12:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 712}),\n",
       " Document(page_content='712\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nb a\\nc\\nFIGURE 10.8\\nFrom left to right, \\nmodels (ideal \\nrepresentations) of \\na step, a ramp, and \\na roof edge, and \\ntheir corresponding \\nintensity proﬁles.\\nclosely modeled as having an intensity \\nramp\\n proﬁle, such as the edge in Fig. 10.8(b). \\nThe slope of the ramp is inversely proportional to the degree to which the edge is \\nblurred. In this model, we no longer have a single “edge point” along the proﬁle. \\nInstead, an edge point now is any point contained in the ramp, and an edge segment \\nwould then be a set of such points that are connected.\\nA third type of edge is the so-called \\nroof edge\\n, having the characteristics illus-\\ntrated in Fig. 10.8(c). Roof edges are models of lines through a region, with the \\nbase (width) of the edge being determined by the thickness and sharpness of the \\nline. In the limit, when its base is one pixel wide, a roof edge is nothing more than \\na one-pixel-thick line running through a region in an image. Roof edges arise, for \\nexample, in range imaging, when thin objects (such as pipes) are closer to the sensor \\nthan the background (such as walls). The pipes appear brighter and thus create an \\nimage similar to the model in Fig. 10.8(c). Other areas in which roof edges appear \\nroutinely are in the digitization of line drawings and also in satellite images, where \\nthin features, such as roads, can be modeled by this type of edge.\\nIt is not unusual to ﬁnd images that contain all three types of edges. Although \\nblurring and noise result in deviations from the ideal shapes, edges in images that \\nare reasonably sharp and have a moderate amount of noise do resemble the charac-\\nteristics of the edge models in Fig. 10.8, as the proﬁles in Fig. 10.9 illustrate. What the \\nmodels in Fig. 10.8 allow us to do is write mathematical expressions for edges in the \\ndevelopment of image processing algorithms. The performance of these algorithms \\nwill depend on the differences between actual edges and the models used in devel-\\noping the algorithms.\\nFigure 10.10(a) shows the image from which the segment in Fig. 10.8(b) was extract-\\ned. Figure 10.10(b) shows a horizontal intensity proﬁle. This ﬁgure shows also the ﬁrst \\nand second derivatives of the intensity proﬁle. Moving from left to right along the \\nintensity proﬁle, we note that the ﬁrst derivative is positive at the onset of the ramp \\nand at points on the ramp, and it is zero in areas of constant intensity. The second \\nderivative is positive at the beginning of the ramp, negative at the end of the ramp, \\nzero at points on the ramp, and zero at points of constant intensity. The signs of the \\nderivatives just discussed would be reversed for an edge that transitions from light to \\ndark. The intersection between the zero \\nintensity axis and a line extending \\nbetween \\nthe extrema of the second derivative marks a point called the \\nzero crossing\\n of the \\nsecond derivative.\\nWe conclude from these observations that the \\nmagnitude\\n of the ﬁrst derivative \\ncan be used to detect the presence of an edge at a point in an image. Similarly, the \\nsign\\n of the second derivative can be used to determine whether an edge pixel lies on \\nDIP4E_GLOBAL_Print_Ready.indb   712\\n6/16/2017   2:12:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 713}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n713\\nFIGURE 10.9\\n  A \\n1508 1970\\n×\\n image showing (zoomed) actual ramp (bottom, left), step (top, \\nright),\\n and roof edge proﬁles. The proﬁles are from dark to light, in the areas enclosed by the \\nsmall circles. The ramp and step proﬁles span 9 pixels and 2 pixels, respectively. The base of the \\nroof edge is 3 pixels. (Original image courtesy of Dr. David R. Pickens, Vanderbilt University.)\\nSecond\\nderivative\\nFirst\\nderivative\\nHorizontal intensity\\nprofile\\nZero crossing\\nb a\\n \\nFIGURE 10.10\\n(a) Two regions of \\nconstant  \\nintensity  \\nseparated by an  \\nideal ramp edge.  \\n(b) Detail near \\nthe edge, showing \\na horizontal  \\nintensity proﬁle, \\nand its ﬁrst and \\nsecond  \\nderivatives.\\nthe dark or light side of an edge. Two additional properties of the second derivative \\naround an edge are: (1) it produces two values for every edge in an image; and (2) \\nits zero crossings can be used for locating the centers of thick edges, as we will show \\nlater in this section. Some edge models utilize a smooth transition into and out of \\nDIP4E_GLOBAL_Print_Ready.indb   713\\n6/16/2017   2:12:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 714}),\n",
       " Document(page_content='714\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nthe ramp (see Problem 10.9). However, the conclusions reached using those models \\nare the same as with an ideal ramp, and working with the latter simpliﬁes theoretical \\nformulations. Finally, although attention thus far has been limited to a 1-D horizon-\\ntal proﬁle, a similar argument applies to an edge of any orientation in an image. We \\nsimply deﬁne a proﬁle perpendicular to the edge direction at any desired point, and \\ninterpret the results in the same manner as for the vertical edge just discussed.\\nEXAMPLE 10.4 :  Behavior of the ﬁrst and second derivatives in the region of a noisy edge.\\nThe edge models in Fig. 10.8 are free of noise. The image segments in the ﬁrst column in Fig. 10.11 show \\nclose-ups of four ramp edges that transition from a black region on the left to a white region on the right \\n(keep in mind that the entire transition from black to white is a single edge). The image segment at the \\ntop left is free of noise. The other three images in the ﬁrst column are corrupted by additive Gaussian \\nnoise with zero mean and standard deviation of 0.1, 1.0, and 10.0 intensity levels, respectively. The graph \\nbelow each image is a horizontal intensity proﬁle passing through the center of the image. All images \\nhave 8 bits of intensity resolution, with 0 and 255 representing black and white, respectively.\\nConsider the image at the top of the center column. As discussed in connection with Fig. 10.10(b), the \\nderivative of the scan line on the left is zero in the constant areas. These are the two black bands shown \\nin the derivative image. The derivatives at points on the ramp are constant and equal to the slope of the \\nramp. These constant values in the derivative image are shown in gray. As we move down the center col-\\numn, the derivatives become increasingly different from the noiseless case. In fact, it would be difﬁcult \\nto associate the last proﬁle in the center column with the ﬁrst derivative of a ramp edge. What makes \\nthese results interesting is that the noise is almost visually undetectable in the images on the left column. \\nThese examples are good illustrations of the sensitivity of derivatives to noise.\\nAs expected, the second derivative is even more sensitive to noise. The second derivative of the noise-\\nless image is shown at the top of the right column. The thin white and black vertical lines are the positive \\nand negative components of the second derivative, as explained in Fig. 10.10. The gray in these images \\nrepresents zero (as discussed earlier, scaling causes zero to show as gray). The only noisy second deriva-\\ntive image that barely resembles the noiseless case corresponds to noise with a standard deviation of 0.1. \\nThe remaining second-derivative images and proﬁles clearly illustrate that it would be difﬁcult indeed to \\ndetect their positive and negative components, which are the truly useful features of the second deriva-\\ntive in terms of edge detection.\\nThe fact that such little visual noise can have such a signiﬁcant impact on the two key derivatives \\nused for detecting edges is an important issue to keep in mind. In particular, image smoothing should be \\na serious consideration prior to the use of derivatives in applications where noise with levels similar to \\nthose we have just discussed is likely to be present.\\nIn summary, the three steps performed typically for edge detection are:\\n1. \\nImage smoothing for noise reduction\\n.\\n The need for this step is illustrated by the \\nresults in the second and third columns of Fig. 10.11.\\n2. \\nDetection of edge points.\\n \\nAs mentioned earlier, this is a local operation that \\nextracts from an image all points that are potential edge-point candidates.\\n3. \\nEdge localization.\\n \\nThe objective of this step is to select from the candidate \\npoints only the points that are members of the set of points comprising an edge.\\nThe remainder of this section deals with techniques for achieving these objectives.\\nDIP4E_GLOBAL_Print_Ready.indb   714\\n6/16/2017   2:12:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 715}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n715\\nFIGURE 10.11\\n First column: 8-bit images with values in the range \\n[, ] ,\\n0\\n255\\n and intensity proﬁles \\nof a ramp edge corrupted by Gaussian noise of zero mean and standard deviations of 0.0,\\n 0.1, \\n1.0, and 10.0 intensity levels, respectively. Second column: First-derivative images and inten-\\nsity proﬁles. Third column: Second-derivative images and intensity proﬁles.\\nDIP4E_GLOBAL_Print_Ready.indb   715\\n6/16/2017   2:12:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 716}),\n",
       " Document(page_content='716\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nBASIC EDGE DETECTION\\nAs illustrated in the preceding discussion, detecting changes in intensity for the pur-\\npose of finding edges can be accomplished using first- or second-order derivatives. \\nWe begin with first-order derivatives, and work with second-order derivatives in the \\nfollowing subsection.\\nThe Image Gradient and Its Properties \\nThe tool of choice for finding edge strength \\nand\\n direction at an arbitrary location \\n(,)\\nxy\\n of an image, \\nf\\n,\\n is the \\ngradient\\n, denoted by \\n/H11612\\nf\\n and defined as the \\nvector\\n \\n∇≡\\n[]\\n≡\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∂\\n∂\\n∂\\nfx y fx y\\ngx y\\ngx y\\nfx y\\nx\\nfx y\\nx\\ny\\n(,) (,)\\n(,)\\n(,)\\n(,)\\n(,)\\ngrad\\n∂ ∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\ny\\n \\n(10-16)\\nThis vector has the well-known property that it points in the direction of maximum \\nrate of change of \\nf\\n at \\n(,)\\nxy\\n (see Problem 10.10). Equation (10-16) is valid at an \\narbitrary (but \\nsingle\\n) point \\n(,) .\\nxy\\n When evaluated for all applicable values of \\nx\\n \\nand \\ny\\n, \\n/H11612\\nfx\\ny\\n(,)\\n becomes a \\nvector image\\n,\\n each element of which is a vector given by \\nEq. (10-16). The \\nmagnitude\\n, \\nMxy\\n(,\\n) ,\\n of this gradient vector at a point \\n(,)\\nxy\\n is given \\nby its Euclidean vector norm:\\n \\n \\nMxy f xy g xy g xy\\nxy\\n(,) (,) (,) (,)\\n=∇ = +\\n22\\n \\n(10-17)\\nThis is the \\nvalue\\n of the rate of change in the direction of the gradient vector at point \\n(,) .\\nxy\\n Note that \\nMxy\\n(,\\n) ,\\n \\n/H11612\\nfx y\\n(,) ,\\n \\ngx y\\nx\\n(,) ,\\n and \\ngx y\\ny\\n(,)\\n are arrays of the same \\nsize as \\nf\\n,\\n created when \\nx\\n and \\ny\\n are allowed to vary over all pixel locations in \\nf.\\n It is \\ncommon practice to refer to \\nMxy\\n(,\\n)\\n and \\n/H11612\\nfx y\\n(,)\\n as the \\ngradient image\\n,\\n or simply \\nas the \\ngradient\\n when the meaning is clear. The summation, square, and square root \\noperations are elementwise operations, as defined in Section 2.6.\\nThe \\ndirection\\n of the gradient vector at a point \\n(,)\\nxy\\n is given by\\n \\na\\n(, ) t a n\\n(,)\\n(,)\\nxy\\ngx y\\ngx y\\ny\\nx\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n1\\n \\n(10-18)\\nAngles are measured in the counterclockwise direction with respect to the \\nx\\n-axis \\n(see F\\nig. 2.19). This is also an image of the same size as \\nf\\n, created by the elementwise \\ndivision of \\ng\\nx\\n and \\ng\\ny\\n over all applicable values of \\nx\\n and \\ny\\n. The following example \\nillustrates, the direction of an edge at a point \\n(,)\\nxy\\n is orthogonal to the direction, \\na\\n(,\\n) ,\\nxy\\n of the gradient vector at the point. \\nEXAMPLE 10.5 :  Computing the gradient.\\nFigure 10.12(a) shows a zoomed section of an image containing a straight edge segment. Each square \\ncorresponds to a pixel, and we are interested in obtaining the strength and direction of the edge at the \\npoint highlighted with a box. The shaded pixels in this ﬁgure are assumed to have value 0, and the white \\nFor convenience, we \\nrepeat here some of the \\ngradient concepts and \\nequations introduced in \\nChapter 3.\\nDIP4E_GLOBAL_Print_Ready.indb   716\\n6/16/2017   2:12:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 717}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n717\\npixels have value 1. We discuss after this example an approach for computing the derivatives in the \\nx\\n- \\nand \\ny\\n-directions using a \\n33\\n×\\n neighborhood centered at a point. The method consists of subtracting the \\npixels in the top row of the neighborhood from the pixels in the bottom row to obtain the partial deriva-\\ntive in the \\nx\\n-direction.\\n Similarly, we subtract the pixels in the left column from the pixels in the right col-\\numn of the neighborhood to obtain the partial derivative in the \\ny\\n-direction. It then follows, using these \\ndifferences as our estimates of the partials, that \\n∂∂ = −\\nfx\\n2\\n and \\n∂∂ =\\nfy\\n2\\n at the point in question. Then,\\n \\n∇=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∂\\n∂\\n∂\\n∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nf\\ng\\ng\\nf\\nx\\nf\\ny\\nx\\ny\\n2\\n2\\nfrom which we obtain \\n/H11612\\nf\\n=\\n22\\n at that point. Similarly, the direction of the gradient vector at the \\nsame point follows from Eq.\\n (10-18): \\na\\n=\\n()\\n=− °\\n−\\ntan\\n,\\n1\\n45\\ngg\\nyx\\n which is the same as 135° measured in \\nthe positive (counterclockwise) direction with respect to the \\nx\\n-axis in our image coordinate system (see \\nFig. 2.19). Figure 10.12(b) shows the gradient vector and its direction angle. \\nAs mentioned earlier, the direction of an edge at a point is orthogonal to the gradient vector at that \\npoint. So the direction angle of the edge in this example is \\na\\n−°\\n= ° −° =°\\n90 135 90 45 ,\\n as Fig. 10.12(c) \\nshows\\n. All edge points in Fig. 10.12(a) have the same gradient, so the entire edge segment is in the same \\ndirection. The gradient vector sometimes is called the \\nedge normal\\n. When the vector is normalized to unit \\nlength by dividing it by its magnitude, the resulting vector is referred to as the \\nedge unit normal\\n.\\nGradient Operators\\nObtaining the gradient of an image requires computing the partial derivatives \\n∂∂\\nfx\\n \\nand \\n∂∂\\nfy\\n at every pixel location in the image. For the gradient, we typically use a \\nforward or centered finite difference (see \\nTable 10.1). Using forward differences we \\nobtain\\n  \\ngx y\\nfx y\\nx\\nfx y fx y\\nx\\n(,)\\n(,)\\n(, )( , )\\n=\\n∂\\n∂\\n=+ −\\n1\\n \\n(10-19)\\nx\\ny\\nGradient vector Gradient vector\\nEdge direction\\na\\na\\n \\n/H11002\\n 90\\n/H11034\\na\\nOrigin\\nb a\\nc\\nFIGURE 10.12\\n Using the gradient to determine edge strength and direction at a point. Note that the edge direction \\nis perpendicular to the direction of the gradient vector at the point where the gradient is computed. Each square \\nrepresents one pixel. (Recall from Fig. 2.19 that the origin of our coordinate system is at the top, left.)\\nDIP4E_GLOBAL_Print_Ready.indb   717\\n6/16/2017   2:12:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 718}),\n",
       " Document(page_content='718\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nand\\n \\ngx y\\nfx y\\ny\\nfx y fx y\\ny\\n(,)\\n(,)\\n(, ) (,)\\n=\\n∂\\n∂\\n=+ −\\n1  \\n(10-20)\\nThese two equations can be implemented for all values of \\nx\\n and \\ny\\n by filtering \\nfx y\\n(,\\n)\\n \\nwith the 1-D kernels in F\\nig. 10.13.\\nWhen diagonal edge direction is of interest, we need 2-D kernels. The \\nRoberts \\ncross-gradient operators\\n (Roberts [1965]) are one of the earliest attempts to use 2-D \\nkernels with a diagonal preference. Consider the \\n33\\n×\\n region in Fig. 10.14(a). The \\nRoberts operators are based on implementing the diagonal differences\\n \\ng\\nf\\nx\\nzz\\nx\\n=\\n∂\\n∂\\n=−\\n()\\n95\\n \\n(10-21)\\nand\\n \\ng\\nf\\ny\\nzz\\ny\\n=\\n∂\\n∂\\n=−\\n()\\n86\\n \\n(10-22)\\nThese derivatives can be implemented by filtering an image with the kernels shown \\nin F\\nigs. 10.14(b) and (c).\\nKernels of size \\n22\\n×\\n are simple conceptually, but they are not as useful for com-\\nputing edge direction as kernels that are symmetric about their centers\\n, the smallest \\nof which are of size \\n33\\n×\\n.\\n These kernels take into account the nature of the data on \\nopposite sides of the center point,\\n and thus carry more information regarding the \\ndirection of an edge. The simplest digital approximations to the partial derivatives \\nusing kernels of size \\n33\\n×\\n are given by\\n \\ng\\nf\\nx\\nzzz zzz\\nx\\n=\\n∂\\n∂\\n=+ + −+ +\\n() ()\\n789 123\\n \\nand             \\n(10-23)\\n \\ng\\nf\\ny\\nzzz zzz\\ny\\n=\\n∂\\n∂\\n=+ + −+ +\\n() ()\\n369 147\\nIn this formulation, the difference between the third and first rows of the \\n33\\n×\\n region \\napproximates the derivative in the \\nx\\n-direction,\\n and the difference between the third \\nand first columns approximate the derivative in the \\ny\\n-direction. Intuitively, we would \\nexpect these approximations to be more accurate than the approximations obtained \\nusing the Roberts operators. Equations (10-22) and (10-23) can be implemented over \\nan entire image by filtering it with the two kernels in Figs. 10.14(d) and (e). These \\nkernels are called the \\nPrewitt operators\\n (Prewitt [1970]). \\nA slight variation of the preceding two equations uses a weight of 2 in the center \\ncoefﬁcient:\\nFilter kernels used to \\ncompute the derivatives \\nneeded for the gradient \\nare often called \\ngradient \\noperators\\n, \\ndifference \\noperators\\n, \\nedge operators\\n, \\nor \\nedge detectors\\n.\\nObserve that these two \\nequations are ﬁrst-order \\ncentral differences as \\ngiven in Eq. (10-6), but \\nmultiplied by 2.\\n/H11002\\n1\\n1\\n/H11002\\n1\\n1\\nb a\\nFIGURE 10.13\\n1-D kernels used to \\nimplement Eqs.  \\n(10-19) and (10-20).\\nDIP4E_GLOBAL_Print_Ready.indb   718\\n6/16/2017   2:12:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 719}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n719\\nz\\n1\\nz\\n2\\nz\\n3\\nz\\n4\\nz\\n5\\nz\\n6\\nz\\n7\\nz\\n8\\nz\\n9\\n/H11002\\n1\\n0\\n0\\n1\\n0\\n/H11002\\n1\\n1\\n0\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n/H11002\\n1\\n0\\n1\\n/H11002\\n1\\n0\\n1\\n/H11002\\n1\\n0\\n1\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n0\\n0\\n0\\n1\\n2\\n1\\n/H11002\\n1\\n0\\n1\\n/H11002\\n2\\n0\\n2\\n/H11002\\n1\\n0\\n1\\nRoberts\\nPrewitt\\nSobel\\ng\\nf\\ne\\nd\\nc\\nb\\na\\nFIGURE 10.14\\nA \\n33\\n×\\n region \\nof an image (the \\nz\\n’\\ns are intensity \\nvalues), and  \\nvarious kernels \\nused to compute \\nthe gradient at the \\npoint labeled \\nz\\n5\\n.\\n \\n \\ng\\nf\\nx\\nzz z zz z\\nx\\n=\\n∂\\n∂\\n=++ −+ +\\n() ()\\n78 9 12 3\\n22\\n \\n(10-24)\\nand\\n \\ng\\nf\\ny\\nzz z zz z\\ny\\n=\\n∂\\n∂\\n=+ + −+ +\\n() ()\\n36 9 14 7\\n22\\n \\n(10-25)\\nIt can be demonstrated (see Problem 10.12) that using a 2 in the center location pro-\\nvides image smoothing\\n. Figures 10.14(f) and (g) show the kernels used to implement \\nEqs. (10-24) and (10-25). These kernels are called the \\nSobel operators\\n (Sobel [1970]).\\nThe Prewitt kernels are simpler to implement than the Sobel kernels, but the \\nslight computational difference between them typically is not an issue. The fact \\nthat the Sobel kernels have better noise-suppression (smoothing) characteristics \\nmakes them preferable because, as mentioned earlier in the discussion of Fig. 10.11, \\nnoise suppression is an important issue when dealing with derivatives. Note that the \\nDIP4E_GLOBAL_Print_Ready.indb   719\\n6/16/2017   2:12:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 720}),\n",
       " Document(page_content='720\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\ncoefﬁcients of all the kernels in Fig. 10.14 sum to zero, thus giving a response of zero \\nin areas of constant intensity, as expected of derivative operators.\\nAny of the pairs of kernels from Fig. 10.14 are convolved with an image to obtain \\nthe gradient components \\ng\\nx\\n and \\ng\\ny\\n at every pixel location. These two partial deriva-\\ntive arrays are then used to estimate edge strength and direction. Obtaining the \\nmagnitude of the gradient requires the computations in Eq. (10-17). This imple-\\nmentation is not always desirable because of the computational burden required \\nby squares and square roots, and an approach used frequently is to approximate the \\nmagnitude of the gradient by absolute values:\\n \\nMxy g g\\nxy\\n(,)\\n≈+\\n \\n(10-26)\\nThis equation is more attractive computationally, and it still preserves relative \\nchanges in intensity levels\\n. The price paid for this advantage is that the resulting \\nfilters will not be isotropic (invariant to rotation) in general. However, this is not an \\nissue when kernels such as the Prewitt and Sobel kernels are used to compute \\ng\\nx\\n \\nand \\ng\\ny\\n because these kernels give isotropic results only for vertical and horizontal \\nedges. This means that results would be isotropic only for edges in those two direc-\\ntions anyway, regardless of which of the two equations is used. That is, Eqs. (10-17) \\nand (10-26) give identical results for vertical and horizontal edges when either the \\nSobel or Prewitt kernels are used (see Problem 10.11).\\nThe \\n33\\n×\\n kernels in Fig. 10.14 exhibit their strongest response predominantly for \\nvertical and horizontal edges\\n. The \\nKirsch compass kernels\\n (Kirsch [1971]) in Fig. 10.15, \\nare designed to detect edge magnitude \\nand\\n direction (angle) in all eight compass \\ndirections. Instead of computing the magnitude using Eq. (10-17) and angle using \\nEq. (10-18), Kirsch’s approach was to determine the edge magnitude by convolv-\\ning an image with all eight kernels and assign the edge magnitude at a point as the \\nresponse of the kernel that gave strongest convolution value at that point. The edge \\nangle at that point is then the direction associated with that kernel. For example, if \\nthe strongest value at a point in the image resulted from using the north (N) kernel, \\nthe edge magnitude at that point would be assigned the response of that kernel, and \\nthe direction would be \\n0\\n°\\n (because compass kernel pairs differ by a rotation of \\n180\\n°;\\n \\nchoosing the maximum response will always result in a positive number).\\n Although \\nwhen working with, say, the Sobel kernels, we think of a north or south edge as \\nbeing vertical, the N and S compass kernels differentiate between the two, the differ-\\nence being the direction of the intensity transitions deﬁning the edge. For example, \\nassuming that intensity values are in the range \\n[, ] ,\\n01\\n the binary edge in Fig. 10.8(a) \\nis deﬁned by black (0) on the left and white (1) on the right.\\n When all Kirsch kernels \\nare applied to this edge, the N kernel will yield the highest value, thus indicating an \\nedge oriented in the north direction (at the point of the computation). \\nEXAMPLE 10.6 :  Illustration of the 2-D gradient magnitude and angle.\\nFigure 10.16 illustrates the Sobel absolute value response of the two components of the gradient, \\ng\\nx\\n \\nand \\ng\\ny\\n,\\n as well as the gradient image formed from the sum of these two components. The directionality \\nof the horizontal and vertical components of the gradient is evident in F\\nigs. 10.16(b) and (c). Note, for \\nRecall the important \\nresult in Problem 3.32 \\nthat using a kernel \\nwhose coefﬁcients sum \\nto zero produces a \\nﬁltered image whose \\npixels also sum to zero. \\nThis implies in general \\nthat some pixels will be \\nnegative. Similarly, if the \\nkernel coefﬁcients sum \\nto 1, the sum of pixels in \\nthe original and ﬁltered \\nimages will be the same \\n(see Problem 3.31).\\nDIP4E_GLOBAL_Print_Ready.indb   720\\n6/16/2017   2:12:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 721}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n721\\nexample, how strong the roof tile, horizontal brick joints, and horizontal segments of the windows are in \\nFig. 10.16(b) compared to other edges. In contrast, Fig. 10.16(c) favors features such as the vertical com-\\nponents of the façade and windows. It is common terminology to use the term \\nedge map\\n when referring \\nto an image whose principal features are edges, such as gradient magnitude images. The intensities of the \\nimage in Fig. 10.16(a) were scaled to the range \\n[, ] .\\n01\\n We use values in this range to simplify parameter \\nselection in the various methods for edge detection discussed in this section.\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 10.15\\nKirsch compass \\nkernels. The edge \\ndirection of  \\nstrongest response \\nof each kernel is \\nlabeled below it. \\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nNN\\nW W S\\nW\\nSS\\nEE N\\nE\\n55\\n5\\n55\\n5\\n5\\n5\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n5\\n55\\n5 5 5\\n5 5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n3\\n/H11002\\n1\\n1\\n/H11002\\n1\\n1\\nb a\\nd c\\nFIGURE 10.16\\n (a) Image of size \\n834 1114\\n×\\n pixels, \\nwith intensity \\nvalues scaled to \\nthe range \\n[,] .\\n01\\n \\n(b) \\ng\\nx\\n, the \\ncomponent of \\nthe gradient in \\nthe \\nx\\n-direction,\\n \\nobtained using the \\nSobel kernel in \\nFig. 10.14(f) to  \\nﬁlter the image. \\n(c) \\ng\\ny\\n, obtained \\nusing the kernel \\nin F\\nig. 10.14(g). \\n(d) The gradient \\nimage, \\ngg\\nxy\\n+\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   721\\n6/16/2017   2:12:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 722}),\n",
       " Document(page_content='722\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nFIGURE 10.17\\nGradient angle \\nimage computed \\nusing Eq. (10-18). \\nAreas of constant \\nintensity in this \\nimage indicate \\nthat the direction \\nof the gradient \\nvector is the same \\nat all the pixel \\nlocations in those \\nregions.\\nFigure 10.17 shows the gradient angle image computed using Eq. (10-18). In general, angle images are \\nnot as useful as gradient magnitude images for edge detection, but they do complement the information \\nextracted from an image using the magnitude of the gradient. For instance, the constant intensity areas \\nin Fig. 10.16(a), such as the front edge of the sloping roof and top horizontal bands of the front wall, \\nare constant in Fig. 10.17, indicating that the gradient vector direction at all the pixel locations in those \\nregions is the same. As we will show later in this section, angle information plays a key supporting role \\nin the implementation of the Canny edge detection algorithm, a widely used edge detection scheme.\\nThe original image in Fig. 10.16(a) is of reasonably high resolution, and at the \\ndistance the image was acquired, the contribution made to image detail by the wall \\nbricks is signiﬁcant. This level of ﬁne detail often is undesirable in edge detection \\nbecause it tends to act as noise, which is enhanced by derivative computations and \\nthus complicates detection of the principal edges. One way to reduce ﬁne detail is \\nto smooth the image prior to computing the edges. Figure 10.18 shows the same \\nsequence of images as in Fig. 10.16, but with the original image smoothed ﬁrst using \\na \\n55\\n×\\n averaging ﬁlter (see Section 3.5 regarding smoothing ﬁlters). The response \\nof each kernel now shows almost no contribution due to the bricks\\n, with the results \\nbeing dominated mostly by the principal edges in the image.\\nFigures 10.16 and 10.18 show that the horizontal and vertical Sobel kernels do \\nnot differentiate between edges in the \\n±°\\n45\\n directions. If it is important to empha-\\nsize edges oriented in particular diagonal directions\\n, then one of the Kirsch kernels \\nin Fig. 10.15 should be used. Figures 10.19(a) and (b) show the responses of the 45° \\n(NW) and \\n−\\n45\\n°\\n (SW) Kirsch kernels, respectively. The stronger diagonal selectivity \\nof these kernels is evident in these ﬁgures\\n. Both kernels have similar responses to \\nhorizontal and vertical edges, but the response in these directions is weaker.\\nCombining the Gradient with Thresholding\\nThe results in Fig. 10.18 show that edge detection can be made more selective by \\nsmoothing the image prior to computing the gradient. Another approach aimed \\nat achieving the same objective is to threshold the gradient image. For example, \\nFig. 10.20(a) shows the gradient image from Fig. 10.16(d), thresholded so that pix-\\nels with values greater than or equal to 33% of the maximum value of the gradi-\\nent image are shown in white, while pixels below the threshold value are shown in \\nThe threshold used to \\ngenerate Fig. 10.20(a) \\nwas selected so that most \\nof the small edges caused \\nby the bricks were \\neliminated. This was the \\nsame objective as when \\nthe image in Fig. 10.16(a) \\nwas smoothed prior to \\ncomputing the gradient.\\nDIP4E_GLOBAL_Print_Ready.indb   722\\n6/16/2017   2:12:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 723}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n723\\nb a\\nd c\\nFIGURE 10.18\\nSame sequence as \\nin Fig. 10.16, but \\nwith the original \\nimage smoothed \\nusing a \\n55\\n×\\n aver-\\naging kernel prior \\nto edge detection.\\nblack. Comparing this image with Fig. 10.16(d), we see that there are fewer edges \\nin the thresholded image, and that the edges in this image are much sharper (see, \\nfor example, the edges in the roof tile). On the other hand, numerous edges, such \\nas the sloping line defining the far edge of the roof (see arrow), are broken in the \\nthresholded image.\\nWhen interest lies both in highlighting the principal edges and on maintaining \\nas much connectivity as possible, it is common practice to use both smoothing and \\nthresholding. Figure 10.20(b) shows the result of thresholding Fig. 10.18(d), which is \\nthe gradient of the smoothed image. This result shows a reduced number of broken \\nedges;  for instance, compare the corresponding edges identiﬁed by the arrows in \\nFigs. 10.20(a) and (b). \\nb a\\nFIGURE 10.19\\nDiagonal edge  \\ndetection.  \\n(a) Result of using \\nthe Kirsch kernel in \\nFig. 10.15(c).  \\n(b) Result of using \\nthe kernel in Fig. \\n10.15(d). The input \\nimage in both cases \\nwas Fig. 10.18(a).\\nDIP4E_GLOBAL_Print_Ready.indb   723\\n6/16/2017   2:12:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 724}),\n",
       " Document(page_content='724\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nMORE ADVANCED TECHNIQUES FOR EDGE DETECTION\\nThe edge-detection methods discussed in the previous subsections are based on fil-\\ntering an image with one or more kernels, with no provisions made for edge char-\\nacteristics and noise content. In this section, we discuss more advanced techniques \\nthat attempt to improve on simple edge-detection methods by taking into account \\nfactors such as image noise and the nature of edges themselves.\\nThe Marr-Hildreth Edge Detector\\nOne of the earliest successful attempts at incorporating more sophisticated analy-\\nsis into the edge-finding process is attributed to Marr and Hildreth [1980]. Edge- \\ndetection methods in use at the time were based on small operators, such as the \\nSobel kernels discussed earlier. Marr and Hildreth argued (1) that intensity chang-\\nes are not independent of image scale, implying that their detection requires using \\noperators of different sizes; and (2) that a sudden intensity change will give rise to a \\npeak or trough in the first derivative or, equivalently, to a zero crossing in the second \\nderivative (as we saw in Fig. 10.10).\\nThese ideas suggest that an operator used for edge detection should have two \\nsalient features. First and foremost, it should be a differential operator capable of \\ncomputing a digital approximation of the ﬁrst or second derivative at every point in \\nthe image. Second, it should be capable of being “tuned” to act at any desired scale, \\nso that large operators can be used to detect blurry edges and small operators to \\ndetect sharply focused ﬁne detail.\\nMarr and Hildreth suggested that the most satisfactory operator fulﬁlling these \\nconditions is the ﬁlter \\n/H11612\\n2\\nG\\n where, as deﬁned in Section 3.6, \\n/H11612\\n2\\n is the Laplacian, and \\nG\\n is the 2-D Gaussian function\\n \\nGxy e\\nxy\\n(,)\\n=\\n−\\n+\\n22\\n2\\n2\\ns\\n \\n(10-27)\\nwith standard deviation \\ns\\n (sometimes \\ns\\n is called the \\nspace constant\\n in this context).\\n \\nWe find an expression for \\n/H11612\\n2\\nG\\n by applying the Laplacian to Eq. (10-27):\\nEquation (10-27) differs \\nfrom the deﬁnition of a \\nGaussian function by a \\nmultiplicative constant \\n[see Eq. (3-45)]. Here, \\nwe are interested only in \\nthe general shape of the \\nGaussian function.\\nb a\\nFIGURE 10.20\\n(a) Result of  \\nthresholding  \\nFig. 10.16(d), the  \\ngradient of the \\noriginal image.  \\n(b) Result of \\nthresholding  \\nFig. 10.18(d), the  \\ngradient of the \\nsmoothed image.\\nDIP4E_GLOBAL_Print_Ready.indb   724\\n6/16/2017   2:12:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 725}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n725\\n \\n∇=\\n∂\\n∂\\n+\\n∂\\n∂\\n=\\n∂\\n∂\\n−\\n+\\n∂\\n∂\\n−\\n−\\n+\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\nGxy\\nGxy\\nx\\nGxy\\ny\\nx\\nx\\ne\\ny\\nxy\\n(,)\\n(,) (,)\\nab a\\ns\\ns\\ny y\\ne\\nx\\ne\\ny\\ne\\nxy\\nxy\\nxy\\ns\\nss ss\\ns\\nss\\n2\\n2\\n2\\n42\\n2\\n2\\n42\\n2\\n22\\n2\\n22\\n2\\n22\\n2\\n11\\n−\\n+\\n−\\n+\\n−\\n+\\n=− + −\\nb\\nab ab\\n \\n(10-28)\\nCollecting terms, we obtain\\n \\n∇=\\n+−\\n−\\n+\\n2\\n22 2\\n4\\n2\\n2\\n22\\n2\\nGxy\\nxy\\ne\\nxy\\n(,)\\nab\\ns\\ns\\ns\\n \\n(10-29)\\nThis expression is called the \\nLaplacian of a Gaussian\\n (LoG).\\nF\\nigures 10.21(a) through (c) show a 3-D plot, image, and cross-section of the \\nnegative of the LoG function (note that the zero crossings of the LoG occur at \\nxy\\n22 2\\n2\\n+=\\ns\\n,\\n which deﬁnes a circle of radius \\n2\\ns\\n centered on the peak of the \\nGaussian function).\\n Because of the shape illustrated in Fig. 10.21(a), the LoG func-\\ntion sometimes is called the \\nMexican hat operator\\n. Figure 10.21(d) shows a \\n55\\n×\\n \\nkernel that approximates the shape in F\\nig. 10.21(a) (normally, we would use the neg-\\native of this kernel). This approximation is not unique. Its purpose is to capture the \\nessential shape of the LoG function; in terms of Fig. 10.21(a), this means a positive, \\ncentral term surrounded by an adjacent, negative region whose values decrease as a \\nfunction of distance from the origin, and a zero outer region. The coefﬁcients must \\nsum to zero so that the response of the kernel is zero in areas of constant intensity.\\nFilter kernels of arbitrary size (but ﬁxed \\ns\\n)\\n can be generated by sampling Eq. (10-29), \\nand scaling the coefﬁcients so that they sum to zero\\n. A more effective approach for \\ngenerating a LoG kernel is sampling Eq. (10-27) to the desired size, then convolving \\nthe resulting array with a Laplacian kernel, such as the kernel in Fig. 10.4(a). \\nBecause \\nconvolving an image with a kernel whose coefﬁcients sum to zero yields an image \\nwhose elements also sum to zero (see Problems 3.32 and 10.16), this approach auto-\\nmatically satisﬁes the requirement that the sum of the LoG kernel coefﬁcients be \\nzero. We will discuss size selection for LoG ﬁlter later in this section.\\nThere are two fundamental ideas behind the selection of the operator \\n∇\\n2\\nG\\n.\\n First, \\nthe Gaussian part of the operator blurs the image\\n, thus reducing the intensity of \\nstructures (including noise) at scales much smaller than \\ns\\n.\\n Unlike the averaging \\nﬁlter used in F\\nig. 10.18, the Gaussian function is smooth in both the spatial and \\nfrequency domains (see Section 4.8), and is thus less likely to introduce artifacts \\n(e.g., ringing) not present in the original image. The other idea concerns the second-\\nderivative properties of the Laplacian operator, \\n∇\\n2\\n.\\n Although ﬁrst derivatives can \\nbe used for detecting abrupt changes in intensity\\n, they are directional operators. The \\nLaplacian, on the other hand, has the important advantage of being isotropic (invari-\\nant to rotation), which not only corresponds to characteristics of the human visual \\nsystem (Marr [1982]) but also responds equally to changes in intensity in any kernel \\nDIP4E_GLOBAL_Print_Ready.indb   725\\n6/16/2017   2:12:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 726}),\n",
       " Document(page_content='726\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\ndirection, thus avoiding having to use multiple kernels to calculate the strongest \\nresponse at any point in the image.\\nThe Marr-Hildreth algorithm consists of convolving the LoG kernel with an input \\nimage,\\n \\ngxy Gxy\\nfx y\\n(,) (,)\\n(, )\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11612\\n2\\n/H22841\\n \\n(10-30)\\nand then finding the zero crossings of \\ngxy\\n(,\\n)\\n to determine the locations of edges in \\nfx y\\n(,\\n) .\\n Because the Laplacian and convolution are linear processes, we can write \\nEq.\\n (10-30) as\\n \\ngxy Gxy f xy\\n(,\\n) (,) (,)\\n=∇\\n[]\\n2\\n/H22841\\n \\n(10-31)\\nindicating that we can smooth the image first with a Gaussian filter and then com-\\npute the Laplacian of the result.\\n These two equations give identical results.\\nThe Marr-Hildreth edge-detection algorithm may be summarized as follows:\\n1. \\nFilter the input image with an \\nnn\\n×\\n Gaussian lowpass kernel obtained by sam-\\npling Eq.\\n (10-27).\\n2. \\nCompute the Laplacian of the image resulting from Step 1 using, for example, \\nthe \\n33\\n×\\n kernel in Fig. 10.4(a). [Steps 1 and 2 implement Eq. (10-31).]\\n3. \\nFind the zero crossings of the image from Step 2.\\nThis expression is  \\nimplemented in the \\nspatial domain using \\nEq. (3-35). It can be \\nimplemented also in the \\nfrequency domain using \\nEq. (4-104).\\n0\\n0\\n/H11002\\n1\\n0\\n0\\n0\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n0\\n/H11002\\n1\\n/H11002\\n2\\n16\\n/H11002\\n2\\n/H11002\\n1\\n0\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n0\\n0\\n0\\n/H11002\\n1\\n0\\n0\\nx\\ny\\n/H11612\\n2\\nG\\n/H11612\\n2\\nG\\nZero crossing\\nZero crossing\\n2\\ns\\n2\\nb a\\nd c\\nFIGURE 10.21\\n(a) 3-D plot of \\nthe \\nnegative\\n of the \\nLoG.  \\n(b) Negative of \\nthe LoG  \\ndisplayed as an \\nimage.  \\n(c) Cross section \\nof (a) showing \\nzero crossings. \\n(d) \\n55\\n×\\n kernel \\napproximation to \\nthe shape in (a).\\n \\nThe negative \\nof this kernel \\nwould be used in \\npractice.\\nDIP4E_GLOBAL_Print_Ready.indb   726\\n6/16/2017   2:12:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 727}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n727\\nTo specify the size of the Gaussian kernel, recall from our discussion of Fig. 3.35 that \\nthe values of a Gaussian function at a distance larger than \\n3\\ns\\n from the mean are \\nsmall enough so that they can be ignored.\\n As discussed in Section 3.5, this implies \\nusing a Gaussian kernel of size \\nLMLM\\n66\\nss\\n×\\n,\\n where \\nLM\\n6\\ns\\n denotes the ceiling of \\n6\\ns\\n;\\n that \\nis\\n, smallest integer not less than \\n6\\ns\\n.\\n Because we work with kernels of odd dimen-\\nsions\\n, we would use the smallest \\nodd\\n integer satisfying this condition. Using a kernel \\nsmaller than this will “truncate” the LoG function, with the degree of truncation \\nbeing inversely proportional to the size of the kernel. Using a larger kernel would \\nmake little difference in the result.\\nOne approach for ﬁnding the zero crossings at any pixel, \\np\\n,\\n of the ﬁltered image, \\ngxy\\n(,\\n) ,\\n is to use a \\n33\\n×\\n neighborhood centered at \\np\\n.\\n A zero crossing at \\np\\n implies \\nthat the signs of at least two of its opposing neighboring pixels must differ\\n. There are \\nfour cases to test: left/right, up/down, and the two diagonals. If the values of \\ngxy\\n(,\\n)\\n \\nare being compared against a threshold (a common approach),\\n then not only must \\nthe signs of opposing neighbors be different, but the absolute value of their numeri-\\ncal difference must also exceed the threshold before we can call \\np\\n a zero-crossing \\npixel.\\n We illustrate this approach in Example 10.7.\\nComputing zero crossings is the key feature of the Marr-Hildreth edge-detection \\nmethod. The approach discussed in the previous paragraph is attractive because of \\nits simplicity of implementation and because it generally gives good results. If the \\naccuracy of the zero-crossing locations found using this method is inadequate in a \\nparticular application, then a technique proposed by Huertas and Medioni [1986] \\nfor ﬁnding zero crossings with \\nsubpixel accuracy\\n can be employed.\\nEXAMPLE 10.7 :  Illustration of the Marr-Hildreth edge-detection method.\\nFigure 10.22(a) shows the building image used earlier and Fig. 10.22(b) is the result of Steps 1 and 2 of \\nthe Marr-Hildreth algorithm, using \\ns\\n=\\n4\\n (approximately 0.5% of the short dimension of the image) \\nand \\nn\\n=\\n25\\n to satisfy the size condition stated above. As in Fig. 10.5, the gray tones in this image are due \\nto scaling\\n. Figure 10.22(c) shows the zero crossings obtained using the \\n33\\n×\\n neighborhood approach just \\ndiscussed,\\n with a threshold of zero. Note that all the edges form closed loops. This so-called “spaghetti \\neffect” is a serious drawback of this method when a threshold value of zero is used (see Problem 10.17). \\nWe avoid closed-loop edges by using a positive threshold.\\nFigure 10.22(d) shows the result of using a threshold approximately equal to 4% of the maximum \\nvalue of the LoG image. The majority of the principal edges were readily detected, and “irrelevant” fea-\\ntures, such as the edges due to the bricks and the tile roof, were ﬁltered out. This type of performance \\nis virtually impossible to obtain using the gradient-based edge-detection techniques discussed earlier. \\nAnother important consequence of using zero crossings for edge detection is that the resulting edges are \\n1 pixel thick. This property simpliﬁes subsequent stages of processing, such as edge linking.\\nIt is possible to approximate the LoG function in Eq. (10-29) by a \\ndifference of \\nGaussians\\n (DoG):\\n \\nDx y e\\ne\\nG\\nxy\\nxy\\n(,)\\n=−\\n−\\n+\\n−\\n+\\n1\\n2\\n1\\n2\\n1\\n2\\n2\\n2\\n2\\n2\\n22\\n1\\n2\\n22\\n2\\n2\\nps\\nps\\ns\\ns\\n \\n(10-32)\\nAs explained in Section \\n3.5, \\n<\\n⋅\\n= \\nand \\n:\\n⋅\\n;\\n denote the \\nceiling\\n and \\nfloor\\n func-\\ntions. That is, the ceiling \\nand floor functions map \\na real number to the \\nsmallest following, or the \\nlargest previous, integer, \\nrespectively.\\nAttempts to ﬁnd zero \\ncrossings by ﬁnding the \\ncoordinates (\\nx\\n, \\ny\\n) where \\ng\\n(\\nx\\n, \\ny\\n) = 0 are impractical \\nbecause of noise and \\nother computational \\ninaccuracies.\\nDIP4E_GLOBAL_Print_Ready.indb   727\\n6/16/2017   2:12:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 728}),\n",
       " Document(page_content='728\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nb a\\nd c\\nFIGURE 10.22\\n(a) Image of size \\n834 1114\\n×\\n pixels, \\nwith intensity  \\nvalues scaled to the \\nrange [0,\\n 1].  \\n(b) Result of \\nSteps 1 and 2 of \\nthe Marr-Hildreth \\nalgorithm using \\ns\\n=\\n4 and \\nn\\n=\\n25. \\n(c) Zero cross-\\nings of (b) using \\na threshold of 0 \\n(note the closed-\\nloop edges).\\n  \\n(d) Zero cross-\\nings found using a \\nthreshold equal to \\n4% of the maxi-\\nmum value of the \\nimage in (b). Note \\nthe thin edges.\\nwith \\nss\\n12\\n>\\n.\\n Experimental results suggest that certain “channels” in the human \\nvision system are selective with respect to orientation and frequenc\\ny, and can be \\nmodeled using Eq. (10-32) with a ratio of standard deviations of 1.75:1. Using the \\nratio 1.6:1 preserves the basic characteristics of these observations and also pro-\\nvides a closer “engineering” approximation to the LoG function (Marr and Hil-\\ndreth [1980]). In order for the LoG and \\nDoG to have the same zero crossings, the \\nvalue of \\ns\\n for the LoG must be selected based on the following equation (see \\nProblem 10.19):\\n \\ns\\nss\\nss\\ns\\ns\\n2\\n1\\n2\\n2\\n2\\n1\\n2\\n2\\n2\\n1\\n2\\n2\\n2\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nln\\n \\n(10-33)\\n Although the zero crossings of the LoG and DoG will be the same when this value \\nof \\ns\\n is used, their amplitude scales will be different. We can make them compatible \\nby scaling both functions so that they have the same value at the origin.\\nT\\nhe proﬁles in Figs. 10.23(a) and (b) were generated with standard devia-\\ntion ratios of 1:1.75 and 1:1.6, respectively (by convention, the curves shown are \\ninverted, as in Fig. 10.21). The LoG proﬁles are the solid lines, and the DoG proﬁles \\nare dotted. The curves shown are intensity proﬁles through the center of the LoG \\nand DoG arrays, generated by sampling Eqs. (10-29) and (10-32), respectively. The \\namplitude of all curves at the origin were normalized to 1. As Fig. 10.23(b) shows, \\nthe ratio 1:1.6 yielded a slightly closer approximation of the LoG and DoG func-\\ntions (for example, compare the bottom lobes of the two ﬁgures).\\nDIP4E_GLOBAL_Print_Ready.indb   728\\n6/16/2017   2:12:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 729}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n729\\nb a\\nFIGURE 10.23\\n(a) Negatives of \\nthe LoG (solid) \\nand DoG  \\n(dotted) proﬁles \\nusing a \\ns\\n ratio of \\n1.75:1.\\n (b) Proﬁles \\nobtained using a \\nratio of 1.6:1.\\nGaussian kernels are separable (see Section 3.4). Therefore, both the LoG and \\nthe DoG ﬁltering operations can be implemented with 1-D convolutions instead of \\nusing 2-D convolutions directly (see Problem 10.19). For an image of size \\nMN\\n×\\n \\nand a kernel of size \\nnn\\n×\\n,\\n doing so reduces the number of multiplications and addi-\\ntions for each convolution from being proportional to \\nnM N\\n2\\n for 2-D convolutions \\nto being proportional to \\nnMN\\n for 1-D convolutions. This implementation difference \\nis signiﬁcant.\\n For example, if \\nn\\n=\\n25\\n,\\n a 1-D implementation will require on the order \\nof 12 times fewer multiplication and addition operations than using 2-D convolution.\\nThe Canny Edge Detector\\nAlthough the algorithm is more complex, the performance of the Canny edge detec-\\ntor (Canny [1986]) discussed in this section is superior in general to the edge detec-\\ntors discussed thus far. Canny’s approach is based on three basic objectives:\\n1. \\nLow error rate\\n.\\n All edges should be found, and there should be no spurious \\nresponses.\\n2. \\nEdge points should be well localized\\n.\\n The edges located must be as close as pos-\\nsible to the true edges. That is, the distance between a point marked as an edge \\nby the detector and the center of the true edge should be minimum.\\n3. \\nSingle edge point response.\\n \\nThe detector should return only one point for each \\ntrue edge point. That is, the number of local maxima around the true edge should \\nbe minimum. This means that the detector should not identify multiple edge pix-\\nels where only a single edge point exists.\\nThe essence of Canny’s work was in expressing the preceding three criteria math-\\nematically, and then attempting to find optimal solutions to these formulations. In \\ngeneral, it is difficult (or impossible) to find a closed-form solution that satisfies \\nall the preceding objectives. However, using numerical optimization with 1-D step \\nedges corrupted by additive white Gaussian noise\\n†\\n led to the conclusion that a good \\napproximation to the optimal step edge detector is the \\nfirst derivative of a Gaussian\\n,\\n \\nd\\ndx\\ne\\nx\\ne\\nx\\nx\\n−\\n−\\n=\\n−\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\ns\\ns\\ns\\n \\n(10-34)\\n \\n†\\n  Recall that \\nwhite noise\\n is noise having a frequency spectrum that is continuous and uniform over a speciﬁed \\nfrequency band. \\nWhite Gaussian noise\\n is white noise in which the distribution of amplitude values is Gaussian. \\nGaussian white noise is a good approximation of many real-world situations and generates mathematically \\ntractable models. It has the useful property that its values are statistically independent.\\nDIP4E_GLOBAL_Print_Ready.indb   729\\n6/16/2017   2:13:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 730}),\n",
       " Document(page_content='730\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nwhere the approximation was only about 20% worse that using the optimized \\nnumerical solution (a difference of this magnitude generally is visually impercep-\\ntible in most applications). \\nGeneralizing the preceding result to 2-D involves recognizing that the 1-D \\napproach still applies in the direction of the edge normal (see Fig. 10.12). Because \\nthe direction of the normal is unknown beforehand, this would require applying the \\n1-D edge detector in all possible directions. This task can be approximated by ﬁrst \\nsmoothing the image with a circular 2-D Gaussian function, computing the gradient \\nof the result, and then using the gradient magnitude and direction to estimate edge \\nstrength and direction at every point.\\nLet \\nfx y\\n(,\\n)\\n denote the input image and \\nGxy\\n(,\\n)\\n denote the Gaussian function:\\n \\nGxy e\\nxy\\n(,)\\n=\\n−\\n+\\n22\\n2\\n2\\ns\\n \\n(10-35)\\nWe form a smoothed image, \\nfx y\\ns\\n(,) ,\\n by convolving \\nf\\n and \\nG\\n:\\n \\nfx y G x y\\ns\\nfx y\\n(,) (,)\\n(, )\\n=\\n/H22841\\n \\n(10-36)\\nThis operation is followed by computing the gradient magnitude and direction \\n(angle),\\n as discussed earlier:\\n \\nM xy f xy g xy g xy\\nss x y\\n(,) (,) (,) (,)\\n==+\\n/H11612\\n22\\n \\n(10-37)\\nand\\n \\na\\n(,) t a n\\n(,)\\n(,)\\nxy\\ngx y\\ngx y\\ny\\nx\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n1\\n \\n(10-38)\\nwith \\ngx y fx y x\\nxs\\n(,) (,)\\n=∂ ∂\\n and \\ngx y fx y y\\nys\\n(,) (,) .\\n=∂ ∂\\n Any of the derivative fil-\\nter kernel pairs in F\\nig. 10.14 can be used to obtain \\ngx y\\nx\\n(,)\\n and \\ngx y\\ny\\n(,) .\\n Equation \\n(10-36) is implemented using an \\nnn\\n×\\n Gaussian kernel whose size is discussed below. \\nK\\neep in mind that \\n/H11612\\nfx y\\ns\\n(,)\\n and \\na\\n(,\\n)\\nxy\\n are arrays of the same size as the image \\nfrom which they are computed.\\nGradient image \\n/H11612\\nfx y\\ns\\n(,)\\n typically contains wide ridges around local maxima. \\nT\\nhe next step is to thin those ridges. One approach is to use \\nnonmaxima suppres-\\nsion\\n. The essence of this approach is to specify a number of discrete orientations of \\nthe edge normal (gradient vector). For example, in a \\n33\\n×\\n region we can deﬁne four \\norientations\\n†\\n for an edge passing through the center point of the region: horizontal, \\nvertical, \\n+° ,\\n45\\n and \\n−° .\\n45\\n Figure 10.24(a) shows the situation for the two possible \\norientations of a horizontal edge\\n. Because we have to quantize all possible edge \\ndirections into four ranges, we have to deﬁne a range of directions over which we \\nconsider an edge to be horizontal. We determine edge direction from the direction \\nof the edge normal, which we obtain directly from the image data using Eq. (10-38). \\nAs Fig. 10.24(b) shows, if the edge normal is in the range of directions from \\n−\\n22\\n5 .\\n°\\n to \\n†\\n  Every edge has two possible orientations. For example, an edge whose normal is oriented at 0° and an edge \\nwhose normal is oriented at 180° are the same \\nhorizontal\\n edge.\\nDIP4E_GLOBAL_Print_Ready.indb   730\\n6/16/2017   2:13:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 731}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n731\\n22 5 .\\n°\\n or from \\n−\\n157\\n5 .\\n°\\n to \\n157 5 .\\n°,\\n we call the edge a horizontal edge. Figure 10.24(c) \\nshows the angle ranges corresponding to the four directions under consideration.\\nLet \\nd\\n1\\n, \\nd\\n2\\n, \\nd\\n3\\n,\\nand \\nd\\n4\\n denote the four basic edge directions just discussed for \\na \\n33\\n×\\n region: horizontal, \\n−\\n45\\n°,\\n vertical, and \\n+\\n45\\n°,\\n respectively. We can formulate \\nthe following nonmaxima suppression scheme for a \\n33\\n×\\n region centered at an \\narbitrary point \\n(,)\\nxy\\n in \\na\\n:\\n1.\\n \\nFind the direction \\nd\\nk\\n that is closest to \\na\\n(,\\n) .\\nxy\\n2. \\nLet \\nK\\n denote the value of \\n/H11612\\nf\\ns\\n at \\n(,) .\\nxy\\n If \\nK\\n is less than the value of \\n/H11612\\nf\\ns\\n at one \\nor both of the neighbors of point \\n(,)\\nxy\\n along \\nd\\nk\\n,\\n let \\ngx y\\nN\\n(,)\\n=\\n0\\n (suppression); \\notherwise\\n, let \\ngx y K\\nN\\n(,) .\\n=\\nWhen repeated for all values of \\nx\\n and \\ny\\n,\\n this procedure yields a nonmaxima sup-\\npressed image \\ngx y\\nN\\n(,)\\n that is of the same size as \\nfx y\\ns\\n(,) .\\n For example, with reference \\nto F\\nig. 10.24(a), letting \\n(,)\\nxy\\n be at \\np\\n5\\n,\\n and assuming a horizontal edge through \\np\\n5\\n,\\nthe pixels of interest in Step 2 would be \\np\\n2\\n and \\np\\n8\\n.\\n Image \\ngx y\\nN\\n(,)\\n contains only the \\nthinned edges;\\n it is equal to image \\n/H11612\\nfx y\\ns\\n(,)\\n with the nonmaxima edge points sup-\\npressed.\\nT\\nhe ﬁnal operation is to threshold \\ngx y\\nN\\n(,)\\n to reduce false edge points. In the \\nMarr\\n-Hildreth algorithm we did this using a single threshold, in which all values \\nbelow the threshold were set to 0. If we set the threshold too low, there will still \\nbe some false edges (called \\nfalse positives\\n). If the threshold is set too high, then \\nvalid edge points will be eliminated (\\nfalse negatives\\n). Canny’s algorithm attempts to \\np\\n1\\np\\n2\\np\\n3\\np\\n4\\np\\n5\\np\\n6\\np\\n7\\nEdge normal\\np\\n8\\np\\n9\\np\\n1\\np\\n2\\np\\n3\\np\\n4\\np\\n5\\np\\n6\\np\\n7\\nEdge normal\\np\\n8\\np\\n9\\nEdge\\nEdge normal\\n(gradient vector)\\n/H11001\\n22.5\\n/H11034\\n/H11001\\n157.5\\n/H11034\\n/H11002\\n22.5\\n/H11034\\n/H11002\\n157.5\\n/H11034\\na\\nx\\ny\\nVertical edge\\nHorizontal edge\\n/H11001\\n157.5\\n/H11034\\n/H11001\\n112.5\\n/H11034\\n/H11001\\n67.5\\n/H11034\\n/H11001\\n22.5\\n/H11034\\n/H11001\\n45\\n/H11034\\nedge\\n/H11002\\n157.5\\n/H11034\\n/H11002\\n112.5\\n/H11034\\n/H11002\\n67.5\\n/H11034\\n/H11002\\n22.5\\n/H11034\\n0\\n/H11034\\n/H11002\\n45\\n/H11034\\nedge\\nb\\na\\nc\\nFIGURE 10.24\\n(a) Two possible \\norientations of a \\nhorizontal edge \\n(shaded) in a \\n33\\n×\\n \\nneighborhood.\\n  \\n(b) Range of values \\n(shaded) of \\na\\n, the \\ndirection angle of \\nthe edge normal \\nfor a horizontal \\nedge\\n. (c) The angle \\nranges of the edge \\nnormals for the \\nfour types of edge \\ndirections in a \\n33\\n×\\n \\nneighborhood.\\n \\nEach edge direc-\\ntion has two ranges, \\nshown in corre-\\nsponding shades.\\nDIP4E_GLOBAL_Print_Ready.indb   731\\n6/16/2017   2:13:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 732}),\n",
       " Document(page_content='732\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nimprove on this situation by using \\nhysteresis thresholding\\n which, as we will discuss \\nin Section 10.3, uses two thresholds: a low threshold, \\nT\\nL\\n and a high threshold, \\nT\\nH\\n. \\nExperimental evidence (Canny [1986]) suggests that the ratio of the high to low \\nthreshold should be in the range of 2:1 to 3:1.\\nW\\ne can visualize the thresholding operation as creating two additional images:\\n \\ngx yg x yT\\nNH\\nN\\nH\\n(,) (,)\\n=≥\\n \\n(10-39)\\nand\\n \\ngx yg x y T\\nNL\\nN\\nL\\n(,) (,)\\n=≥\\n \\n(10-40)\\nInitially, \\ngx y\\nNH\\n(,)\\n and \\ngx y\\nNL\\n(,)\\n are set to 0. After thresholding, \\ngx y\\nNH\\n(,)\\n will usu-\\nally have fewer nonzero pixels than \\ngx y\\nNL\\n(,) ,\\n but all the nonzero pixels in \\ngx y\\nNH\\n(,)\\n \\nwill be contained in \\ngx y\\nNL\\n(,)\\n because the latter image is formed with a lower thresh-\\nold.\\n We eliminate from \\ngx y\\nNL\\n(,)\\n all the nonzero pixels from \\ngx y\\nNH\\n(,)\\n by letting\\n \\ngx ygx y g x y\\nNL\\nNL\\nNH\\n(,) (,) (,)\\n=−\\n \\n(10-41)\\nThe nonzero pixels in \\ngx y\\nNH\\n(,)\\n and \\ngx y\\nNL\\n(,)\\n may be viewed as being “strong” \\nand \\n“weak” edge pixels, respectively. After the thresholding operations, all strong \\npixels in \\ngx y\\nNH\\n(,)\\n are assumed to be valid edge pixels, and are so marked imme-\\ndiately\\n. Depending on the value of \\nT\\nH\\n,\\n the edges in \\ngx y\\nNH\\n(,)\\n typically have gaps. \\nLonger edges are formed using the following procedure:\\n(a) \\nLocate the next unvisited edge pixel, \\np\\n, in \\ngx y\\nNH\\n(,) .\\n(b) \\nMark as valid edge pixels all the weak pixels in \\ngx y\\nNL\\n(,)\\n that are connected to \\np\\n using, say, 8-connectivity.\\n(c) \\nIf all nonzero pixels in \\ngx y\\nNH\\n(,)\\n have been visited go to Step (d). Else, return \\nto Step ( a).\\n(d) \\nSet to zero all pixels in \\ngx y\\nNL\\n(,)\\n that were not marked as valid edge pixels.\\nAt the end of this procedure\\n, the final image output by the Canny algorithm is \\nformed by appending to \\ngx y\\nNH\\n(,)\\n all the nonzero pixels from \\ngx y\\nNL\\n(,) .\\nWe used two additional images, \\ngx y\\nNH\\n(,)\\n and \\ngx y\\nNL\\n(,)\\n to simplify the discussion. \\nIn practice\\n, hysteresis thresholding can be implemented directly during nonmaxima \\nsuppression, and thresholding can be implemented directly on \\ngx y\\nN\\n(,)\\n by forming a \\nlist of strong pixels and the weak pixels connected to them.\\nSummarizing\\n, the Canny edge detection algorithm consists of the following steps:\\n1. \\nSmooth the input image with a Gaussian ﬁlter.\\n2. \\nCompute the gradient magnitude and angle images.\\n3. \\nApply nonmaxima suppression to the gradient magnitude image.\\n4. \\nUse double thresholding and connectivity analysis to detect and link edges. \\nAlthough the edges after nonmaxima suppression are thinner than raw gradient edg-\\nes\\n, the former can still be thicker than one pixel. To obtain edges one pixel thick, it is \\ntypical to follow Step 4 with one pass of an edge-thinning algorithm (see Section 9.5).\\nDIP4E_GLOBAL_Print_Ready.indb   732\\n6/16/2017   2:13:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 733}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n733\\nAs mentioned earlier, smoothing is accomplished by convolving the input image \\nwith a Gaussian kernel whose size, \\nnn\\n×\\n,\\n must be chosen. Once a value of \\ns\\n has \\nbeen speciﬁed,\\n we can use the approach discussed in connection with the Marr-Hil-\\ndreth algorithm to determine an odd value of \\nn\\n that provides the “full” smoothing \\ncapability of the Gaussian ﬁlter for the speciﬁed value of \\ns\\n.\\nSome ﬁnal comments on implementation: As noted earlier in the discussion of \\nthe Marr\\n-Hildreth edge detector, the 2-D Gaussian function in Eq. (10-35) is sepa-\\nrable into a product of two 1-D Gaussians. Thus, Step 1 of the Canny algorithm can \\nbe formulated as 1-D convolutions that operate on the rows (columns) of the image \\none at a time, and then work on the columns (rows) of the result. Furthermore, if \\nwe use the approximations in Eqs. (10-19) and (10-20), we can also implement the \\ngradient computations required for Step 2 as 1-D convolutions (see Problem 10.22).\\nEXAMPLE 10.8 : Illustration and comparison of the Canny edge-detection method.\\nFigure 10.25(a) shows the familiar building image. For comparison, Figs. 10.25(b) and (c) show, respec-\\ntively, the result in Fig. 10.20(b) obtained using the thresholded gradient, and Fig. 10.22(d) using the \\nMarr-Hildreth detector. Recall that the parameters used in generating those two images were selected \\nto detect the principal edges, while attempting to reduce “irrelevant” features, such as the edges of the \\nbricks and the roof tiles.\\nFigure 10.25(d) shows the result obtained with the Canny algorithm using the parameters \\nT\\nL\\n=\\n00 4\\n.,\\n \\nT\\nH\\n=\\n01 0\\n.\\n (2.5 times the value of the low threshold), \\ns\\n=\\n4,\\n and a kernel of size \\n25 25\\n×\\n,\\n which cor-\\nresponds to the smallest odd integer not less than \\n6\\ns\\n.\\n These parameters were chosen experimentally \\nUsually, selecting a \\nsuitable value of \\ns\\n \\nfor the ﬁrst time in an \\napplication requires \\nexperimentation.\\nb a\\nd c\\nFIGURE 10.25\\n(a) Original image \\nof size \\n834 1114\\n×\\n \\npixels\\n, with  \\nintensity values \\nscaled to the range \\n[, ] .\\n01\\n  \\n(b) Thresholded  \\ngradient of the \\nsmoothed image. \\n(c) Image obtained \\nusing the  \\nMarr-Hildreth  \\nalgorithm.  \\n(d) Image obtained \\nusing the Canny \\nalgorithm. Note the \\nsigniﬁcant  \\nimprovement of \\nthe Canny image \\ncompared to the \\nother two.\\nDIP4E_GLOBAL_Print_Ready.indb   733\\n6/16/2017   2:13:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 734}),\n",
       " Document(page_content='734\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nb a\\nd c\\nFIGURE 10.26\\n(a) Head CT image \\nof size \\n512 512\\n×\\n \\npixels\\n, with  \\nintensity values \\nscaled to the range \\n[, ] .\\n01\\n \\n(b) T\\nhresholded  \\ngradient of the \\nsmoothed image. \\n(c) Image obtained \\nusing the Marr-Hil-\\ndreth algorithm.  \\n(d) Image obtained \\nusing the Canny \\nalgorithm.  \\n(Original image \\ncourtesy of Dr. \\nDavid R. Pickens, \\nVanderbilt  \\nUniversity.)\\nto achieve the objectives stated in the previous paragraph for the gradient and Marr-Hildreth images. \\nComparing the Canny image with the other two images, we see in the Canny result signiﬁcant improve-\\nments in detail of the principal edges and, at the same time, more rejection of irrelevant features. For \\nexample, note that both edges of the concrete band lining the bricks in the upper section of the image \\nwere detected by the Canny algorithm, whereas the thresholded gradient lost both of these edges, and \\nthe Marr-Hildreth method detected only the upper one. In terms of ﬁltering out irrelevant detail, the \\nCanny image does not contain a single edge due to the roof tiles; this is not true in the other two images. \\nThe quality of the lines with regard to continuity, thinness, and straightness is also superior in the Canny \\nimage. Results such as these have made the Canny algorithm a tool of choice for edge detection.\\nEXAMPLE 10.9 :  Another illustration of the three principal edge-detection methods discussed in this section.\\nAs another comparison of the three principal edge-detection methods discussed in this section, consider \\nFig. 10.26(a), which shows a \\n512 512\\n×\\n head CT image. Our objective is to extract the edges of the outer \\ncontour of the brain (the gray region in the image),\\n the contour of the spinal region (shown directly \\nbehind the nose, toward the front of the brain), and the outer contour of the head. We wish to generate \\nthe thinnest, continuous contours possible, while eliminating edge details related to the gray content in \\nthe eyes and brain areas.\\nFigure 10.26(b) shows a thresholded gradient image that was ﬁrst smoothed using a \\n55\\n×\\n averaging \\nkernel.\\n The threshold required to achieve the result shown was 15% of the maximum value of the gradi-\\nent image. Figure 10.26(c) shows the result obtained with the Marr-Hildreth edge-detection algorithm \\nwith a threshold of 0.002, \\ns\\n=\\n3,\\n and a kernel of size \\n19 19\\n×\\n. Figure 10.26(d) was obtained using the \\nCanny algorithm with \\nT\\nL\\n=\\n00 5\\n.,\\nT\\nH\\n=\\n01 5\\n.\\n (3 times the value of the low threshold), \\ns\\n=\\n2,\\n and a kernel \\nof size 13 13\\n×\\n. \\nDIP4E_GLOBAL_Print_Ready.indb   734\\n6/16/2017   2:13:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 735}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n735\\nIn terms of edge quality and the ability to eliminate irrelevant detail, the results in Fig. 10.26 correspond \\nclosely to the results and conclusions in the previous example. Note also that the Canny algorithm was \\nthe only procedure capable of yielding a totally unbroken edge for the posterior boundary of the brain, \\nand the closest boundary of the spinal cord. It was also the only procedure capable of ﬁnding the cleanest \\ncontours, while eliminating all the edges associated with the gray brain matter in the original image.\\nThe price paid for the improved performance of the Canny algorithm is a sig-\\nniﬁcantly more complex implementation than the two approaches discussed earlier. \\nIn some applications, such as real-time industrial image processing, cost and speed \\nrequirements usually dictate the use of simpler techniques, principally the thresh-\\nolded gradient approach. When edge quality is the driving force, the Marr-Hildreth \\nand Canny algorithms, especially the latter, offer superior alternatives.\\nLINKING EDGE POINTS\\nIdeally, edge detection should yield sets of pixels lying only on edges. In practice, \\nthese pixels seldom characterize edges completely because of noise, breaks in the \\nedges caused by nonuniform illumination, and other effects that introduce disconti-\\nnuities in intensity values. Therefore, edge detection typically is followed by linking \\nalgorithms designed to assemble edge pixels into meaningful edges and/or region \\nboundaries. In this section, we discuss two fundamental approaches to edge linking \\nthat are representative of techniques used in practice. The first requires knowledge \\nabout edge points in a local region (e.g., a \\n33\\n×\\n neighborhood), and the second \\nis a global approach that works with an entire edge map\\n. As it turns out, linking \\npoints along the boundary of a region is also an important aspect of some of the \\nsegmentation methods discussed in the next chapter, and in extracting features from \\na segmented image, as we will do in Chapter 11. Thus, you will encounter additional \\nedge-point linking methods in the next two chapters. \\nLocal Processing\\nA simple approach for linking edge points is to analyze the characteristics of pixels \\nin a small neighborhood about every point \\n(,)\\nxy\\n that has been declared an edge \\npoint by one of the techniques discussed in the preceding sections\\n. All points that \\nare similar according to predefined criteria are linked, forming an edge of pixels that \\nshare common properties according to the specified criteria.\\nThe two principal properties used for establishing similarity of edge pixels in this \\nkind of local analysis are (1) the strength (magnitude) and (2) the direction of the \\ngradient vector. The ﬁrst property is based on Eq. (10-17). Let \\nS\\nxy\\n denote the set of \\ncoordinates of a neighborhood centered at point \\n(,)\\nxy\\n in an image. An edge pixel \\nwith coordinates \\n(,)\\nst\\n in \\nS\\nxy\\n is similar in \\nmagnitude\\n to the pixel at \\n(,)\\nxy\\n if\\n \\nMst Mxy E\\n(,) ( , )\\n−≤\\n \\n(10-42)\\nwhere \\nE\\n is a positive threshold.\\nDIP4E_GLOBAL_Print_Ready.indb   735\\n6/16/2017   2:13:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 736}),\n",
       " Document(page_content='736\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nThe direction angle of the gradient vector is given by Eq. (10-18). An edge pixel \\nwith coordinates \\n(,)\\nst\\n in \\nS\\nxy\\n has an \\nangle\\n similar to the pixel at \\n(,)\\nxy\\n if \\n \\naa\\n(,) ( , )\\nst xy A\\n−≤\\n \\n(10-43)\\nwhere \\nA\\n is a positive angle threshold. As noted earlier, the direction of the edge at \\n(,)\\nxy\\n is perpendicular to the direction of the gradient vector at that point. \\nA pixel with coordinates \\n(,)\\nst\\n in \\nS\\nxy\\n is considered to be linked to the pixel at \\n(,)\\nxy\\n \\nif both magnitude and direction criteria are satisﬁed.\\n This process is repeated for \\nevery edge pixel. As the center of the neighborhood is moved from pixel to pixel, a \\nrecord of linked points is kept. A simple bookkeeping procedure is to assign a dif-\\nferent intensity value to each set of linked edge pixels.\\nThe preceding formulation is computationally expensive because all neighbors of \\nevery point have to be examined. A simpliﬁcation particularly well suited for real \\ntime applications consists of the following steps:\\n1. \\nCompute the gradient magnitude and angle arrays, \\nMxy\\n(,\\n)\\n and \\na\\n(,\\n) ,\\nxy\\n of the \\ninput image\\n, \\nfx y\\n(,\\n) .\\n2. \\nForm a binary image, \\ngxy\\n(,\\n) ,\\n whose value at any point \\n(,)\\nxy\\n is given by:\\n \\ngxy\\nMx\\ny T xy A T\\nMA\\n(,)\\n(,)\\n(,)\\n=\\n>=\\n±\\n⎧\\n⎨\\n⎩\\n1\\n0\\nif\\nAND\\notherwise\\na\\nwhere \\nT\\nM\\n is a threshold, \\nA\\n is a speciﬁed angle direction, and \\n±\\nT\\nA\\n deﬁnes a \\n“band” of acceptable directions about \\nA\\n.\\n3. \\nScan the rows of \\ng\\n and ﬁll (set to 1) all gaps (sets of 0’s) in each row that do not \\nexceed a speciﬁed length,\\n \\nL\\n.\\n Note that, by deﬁnition, a gap is bounded at both \\nends by one or more 1’\\ns. The rows are processed individually, with no “memory” \\nkept between them.\\n4. \\nTo detect gaps in any other direction, \\nu\\n,\\n rotate \\ng\\n by this angle and apply the \\nhorizontal scanning procedure in Step 3.\\n Rotate the result back by \\n−\\nu\\n.\\nWhen interest lies in horizontal and vertical edge linking, Step 4 becomes a simple \\nprocedure in which \\ng\\n is rotated ninety degrees, the rows are scanned, and the result \\nis rotated back.\\n This is the application found most frequently in practice and, as the \\nfollowing example shows, this approach can yield good results. In general, image \\nrotation is an expensive computational process so, when linking in numerous angle \\ndirections is required, it is more practical to combine Steps 3 and 4 into a single, \\nradial scanning procedure.\\nEXAMPLE 10.10 :  Edge linking using local processing.\\nFigure 10.27(a) shows a \\n534 566\\n×\\n image of the rear of a vehicle. The objective of this example is to \\nillustrate the use of the preceding algorithm for ﬁnding rectangles whose sizes makes them suitable \\ncandidates for license plates\\n. The formation of these rectangles can be accomplished by detecting \\nDIP4E_GLOBAL_Print_Ready.indb   736\\n6/16/2017   2:13:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 737}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n737\\nstrong horizontal and vertical edges. Figure 10.27(b) shows the gradient magnitude image, \\nMxy\\n(,\\n) ,\\n and \\nF\\nigs. 10.27(c) and (d) show the result of Steps 3 and 4 of the algorithm, obtained by letting \\nT\\nM\\n equal \\nto 30% of the maximum gradient value, \\nA\\n=\\n90\\n°\\n, \\nT\\nA\\n=\\n45\\n°,\\n and ﬁlling all gaps of 25 or fewer pixels \\n(approximately 5% of the image width).\\n A large range of allowable angle directions was required to \\ndetect the rounded corners of the license plate enclosure, as well as the rear windows of the vehicle. \\nFigure 10.27(e) is the result of forming the logical OR of the two preceding images, and Fig. 10.27(f) \\nwas obtained by thinning 10.27(e) with the thinning procedure discussed in Section 9.5. As Fig. 10.27(f) \\nshows, the rectangle corresponding to the license plate was clearly detected in the image. It would be \\na simple matter to isolate the license plate from all the rectangles in the image, using the fact that the \\nwidth-to-height ratio of license plates have distinctive proportions (e.g., a 2:1 ratio in U.S. plates).\\nGlobal Processing Using the Hough Transform\\nThe method discussed in the previous section is applicable in situations in which \\nknowledge about pixels belonging to individual objects is available. Often, we have \\nto work in unstructured environments in which all we have is an edge map and no \\nknowledge about where objects of interest might be. In such situations, all pixels \\nare candidates for linking, and thus have to be accepted or eliminated based on pre-\\ndefined \\nglobal\\n properties. In this section, we develop an approach based on whether \\nsets of pixels lie on curves of a specified shape. Once detected, these curves form the \\nedges or region boundaries of interest.\\nGiven \\nn\\n points in an image, suppose that we want to ﬁnd subsets of these points \\nthat lie on straight lines\\n. One possible solution is to ﬁnd all lines determined by every \\npair of points, then ﬁnd all subsets of points that are close to particular lines. This \\napproach involves ﬁnding \\nnn n\\n−\\n()\\n12\\n2\\n∼\\n lines, then performing \\nnn n n\\n(\\n)\\n−\\n()\\n()\\n12\\n3\\n∼\\n \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.27\\n(a) Image of the rear \\nof a vehicle.  \\n(b) Gradient magni-\\ntude image.  \\n(c) Horizontally \\nconnected edge \\npixels.  \\n(d) Vertically con-\\nnected edge pixels. \\n(e) The logical OR \\nof (c) and (d).  \\n(f) Final result, \\nusing morphological \\nthinning. (Original \\nimage courtesy of \\nPerceptics  \\nCorporation.)\\nDIP4E_GLOBAL_Print_Ready.indb   737\\n6/16/2017   2:13:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 738}),\n",
       " Document(page_content='738\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\ncomparisons of every point to all lines. This is a computationally prohibitive task in \\nmost applications.\\nHough [1962] proposed an alternative approach, commonly referred to as the \\nHough transform\\n. Let \\n(,)\\nxy\\nii\\n denote a point in the \\nxy\\n-plane and consider the general \\nequation of a straight line in slope-intercept form: \\nya x b\\nii\\n=+\\n.\\n Inﬁnitely many lines \\npass through \\n(,) ,\\nxy\\nii\\n but they all satisfy the equation \\nya x b\\nii\\n=+\\n for varying val-\\nues of \\na\\n and \\nb\\n.\\n However, writing this equation as \\nbx a y\\nii\\n=− +\\n and considering the \\nab\\n-plane (also called \\nparameter space\\n) yields the equation of a \\nsingle\\n line for a ﬁxed \\npoint \\n(,) .\\nxy\\nii\\n Furthermore, a second point \\n(,)\\nxy\\njj\\n also has a single line in parameter \\nspace associated with it, which intersects the line associated with \\n(,)\\nxy\\nii\\n at some \\npoint \\n(, )\\nab\\n/H11032/H11032\\n in parameter space, where \\na\\n/H11032\\n is the slope and \\nb\\n/H11032\\n the intercept of the line \\ncontaining both \\n(,)\\nxy\\nii\\n and \\n(,)\\nxy\\njj\\n in the \\nxy\\n-plane (we are assuming, of course, that \\nthe lines are not parallel). In fact, \\nall\\n points on this line have lines in parameter space \\nthat intersect at \\n(, ) .\\nab\\n/H11032/H11032\\n Figure 10.28 illustrates these concepts.\\nIn principle\\n, the parameter space lines corresponding to all points \\n(,)\\nxy\\nkk\\n in the \\nxy\\n-plane could be plotted, and the principal lines in that plane could be found by \\nidentifying points in parameter space where large numbers of parameter-space lines \\nintersect. However, a difﬁculty with this approach is that \\na\\n,\\n (the slope of a line) \\napproaches inﬁnity as the line approaches the vertical direction.\\n One way around \\nthis difﬁculty is to use \\nthe normal representation\\n of a line: \\n \\nxy\\nco\\ns sin\\nuu r\\n+=\\n \\n(10-44)\\nFigure 10.29(a) illustrates the geometrical interpretation of the parameters \\nr\\n and \\nu\\n. \\nA horizontal line has \\nu\\n=°\\n0,\\n with \\nr\\n being equal to the positive \\nx\\n-intercept.\\n Simi-\\nlarly, a vertical line has \\nu\\n=°\\n90\\n,\\n with \\nr\\n being equal to the positive \\ny\\n-intercept,\\n or \\nu\\n=−\\n°\\n90 ,\\n with \\nr\\n being equal to the negative \\ny\\n-intercept (we limit the angle to the \\nrange \\n−° ≤≤ °\\n90\\n90\\nu\\n).\\n Each sinusoidal curve in Figure 10.29(b) represents the fam-\\nily of lines that pass through a particular point \\n(,)\\nxy\\nkk\\n in the \\nxy\\n-plane. The intersec-\\ntion point \\n(,)\\nru\\n/H11032/H11032\\n in Fig. 10.29(b) corresponds to the line that passes through both \\n(,)\\nxy\\nii\\n and \\n(,)\\nxy\\njj\\n in Fig. 10.29(a).\\nThe computational attractiveness of the Hough transform arises from subdividing \\nthe \\nru\\n parameter space into so-called \\naccumulator cells\\n,\\n as Fig. 10.29(c) illustrates, \\nwhere \\n(, )\\nmin max\\nrr\\n and \\nuu\\nmin max\\n,\\n()\\n are the expected ranges of the parameter values: \\nThe original formulation \\nof the Hough transform \\npresented here works \\nwith straight lines. For a \\ngeneralization to  \\narbitrary shapes, see  \\nBallard [1981].\\n(\\nx\\ni\\n,\\n y\\ni\\n)\\n(\\nx\\nj\\n,\\n y\\nj\\n)\\nx\\ny\\nb \\n/H11005 /H11002\\nx\\ni\\na \\n/H11001\\n y\\ni\\nb \\n/H11005 /H11002\\nx\\nj\\na \\n/H11001\\n y\\nj\\na\\nb\\nb\\n/H11032\\na\\n/H11032\\nb a\\nFIGURE 10.28\\n(a) \\nxy\\n-plane.  \\n(b) Parameter \\nspace.\\nDIP4E_GLOBAL_Print_Ready.indb   738\\n6/16/2017   2:13:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 739}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n739\\n−° ≤≤ °\\n90 90\\nu\\n and \\n−≤≤\\nDD\\nr\\n,\\n where \\nD\\n is the maximum distance between opposite \\ncorners in an image\\n. The cell at coordinates \\n(, )\\nij\\n with accumulator value \\nAi j\\n(,\\n)\\n cor-\\nresponds to the square associated with parameter\\n-space coordinates \\n(, ) .\\nru\\nij\\n Ini-\\ntially, these cells are set to zero. Then, for every non-background point \\n(,)\\nxy\\nkk\\n in \\nthe \\nxy\\n-plane, we let \\nu\\n equal each of the allowed subdivision values on the \\nu\\n-axis\\n \\nand solve for the corresponding \\nr\\n using the equation \\nruu\\n=+\\nxy\\nkk\\ncos sin .\\n The \\nresulting \\nr\\n values are then rounded off to the nearest allowed cell value along the \\nr\\n axis. If a choice of \\nu\\nq\\n results in the solution \\nr\\np\\n,\\n then we let \\nAp q Ap q\\n(,\\n) (,) .\\n=+\\n1  \\nAt the end of the procedure\\n, a value of \\nK\\n in a cell \\nAi j\\n(,\\n)\\n means that \\nK\\n points in the \\nxy\\n-plane lie on the line \\nxy\\njj i\\ncos sin .\\nuu r\\n+=\\n The number of subdivisions in the \\nru\\n-\\nplane\\n determines the accuracy of the colinearity of these points. It can be shown \\n(see Problem 10.27) that the number of computations in the method just discussed is \\nlinear with respect to \\nn\\n,\\n the number of non-background points in the \\nxy\\n-plane.\\nEXAMPLE 10.11 :  Some basic properties of the Hough transform.\\nFigure 10.30 illustrates the Hough transform based on Eq. (10-44). Figure 10.30(a) shows an image \\nof size \\nMM M\\n×\\n (\\n=\\n101\\n)\\n with ﬁve labeled white points, and Fig. 10.30(b) shows each of these points \\nmapped onto the \\nru\\n-\\nplane\\n using subdivisions of one unit for the \\nr\\n and \\nu\\n axes. The range of \\nu\\n values is \\n±°\\n90\\n,\\n and the range of \\nr\\n values is \\n±\\n2\\nM\\n.\\n As Fig. 10.30(b) shows, each curve has a different sinusoidal \\nshape\\n. The horizontal line resulting from the mapping of point 1 is a sinusoid of zero amplitude.\\nThe points labeled \\nA\\n (not to be confused with accumulator values) and \\nB\\n in Fig. 10.30(b) illustrate \\nthe colinearity detection property of the Hough transform. For example, point \\nB, \\nmarks the intersection \\nof the curves corresponding to points 2, 3, and 4 in the \\nxy\\n image plane. The location of point \\nA\\n indicates \\nthat these three points lie on a straight line passing through the origin \\n()\\nr\\n=\\n0\\n and oriented at \\n−\\n45\\n°\\n [see \\nF\\nig. 10.29(a)]. Similarly, the curves intersecting at point \\nB\\n in parameter space indicate that points 2, 3, \\nand 4 lie on a straight line oriented at \\n45\\n°\\n,\\n and whose distance from the origin is \\nr\\n=\\n71\\n (one-half the \\ndiagonal distance from the origin of the image to the opposite corner\\n, rounded to the nearest integer \\n(\\nx\\ni\\n,\\n y\\ni\\n)\\n(\\nx\\nj\\n,\\n y\\nj\\n)\\nx\\ny\\nu\\n/H11032\\nu\\nmin\\nr\\nmin\\nr\\nmax\\nu\\nmax\\nr\\n/H11032\\nr\\nu\\nx\\nj\\ncos\\nu\\n \\n/H11001 \\ny\\nj\\nsin\\nu\\n \\n/H11005 \\nr\\nx\\ni\\ncos\\nu\\n \\n/H11001 \\ny\\ni\\nsin\\nu\\n \\n/H11005 \\nr\\nr\\nu\\nr\\nu\\n0\\n0\\nb a\\nc\\nFIGURE 10.29\\n (a) \\n(,)\\nru\\n parameterization of a line in the \\nxy\\n-plane\\n. (b) Sinusoidal curves in the \\nru\\n-\\nplane;\\nthe point of \\nintersection \\n(,)\\nru\\n/H11032/H11032\\n corresponds to the line passing through points \\n(,)\\nxy\\nii\\n and \\n(,)\\nxy\\njj\\n in the \\nxy\\n-plane. (c) Division \\nof the \\nru\\n-\\nplane\\n into accumulator cells.\\nDIP4E_GLOBAL_Print_Ready.indb   739\\n6/16/2017   2:13:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 740}),\n",
       " Document(page_content='740\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nvalue). Finally, the points labeled \\nQ\\n, \\nR\\n, and \\nS\\n in Fig. 10.30(b) illustrate the fact that the Hough transform \\nexhibits a reﬂective adjacency relationship at the right and left edges of the parameter space. This prop-\\nerty is the result of the manner in which \\nr\\n and \\nu\\n change sign at the \\n±°\\n90  boundaries.\\nAlthough the focus thus far has been on straight lines\\n, the Hough transform is \\napplicable to any function of the form \\ng\\nvc\\n,,\\n()\\n=\\n0\\n where \\nv\\n is a vector of coordinates \\nand \\nc\\n is a vector of coefﬁcients. For example, points lying on the circle\\n \\nxc yc c\\n−\\n()\\n+−\\n()\\n=\\n1\\n2\\n2\\n2\\n3\\n2\\n \\n(10-45)\\ncan be detected by using the basic approach just discussed. The difference is the \\npresence of three parameters \\nc\\n1\\n, \\nc\\n2\\n,\\n and \\nc\\n3\\n that result in a 3-D parameter space with \\n/H11002\\n100\\n/H11002\\n50\\n0\\n50\\n100\\nr\\n80\\n60\\n40\\n20 0\\n/H11002\\n20\\n/H11002\\n40\\n/H11002\\n60\\n/H11002\\n80\\nu\\nQ\\nR\\nR\\nQ\\nS\\nS\\nA\\n3\\n5\\n4\\n2\\n1\\nB\\nb\\na\\nFIGURE 10.30\\n (a) Image of size \\n101 101\\n×\\n pixels, \\ncontaining ﬁve \\nwhite points (four \\nin the corners and \\none in the center).\\n  \\n(b) Corresponding \\nparameter space. \\nDIP4E_GLOBAL_Print_Ready.indb   740\\n6/16/2017   2:13:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 741}),\n",
       " Document(page_content='10.2\\n  \\nPoint, Line, and Edge Detection\\n    \\n741\\ncube-like cells, and accumulators of the form \\nAi jk\\n(,\\n, ) .\\n The procedure is to incre-\\nment \\nc\\n1\\n and \\nc\\n2\\n,\\n solve for the value of \\nc\\n3\\n that satisfies Eq. (10-45), and update the \\naccumulator cell associated with the triplet \\n(, , ) .\\ncc\\nc\\n123\\n Clearly, the complexity of the \\nHough transform depends on the number of coordinates and coefficients in a given \\nfunctional representation. As noted earlier, generalizations of the Hough transform \\nto detect curves with no simple analytic representations are possible, as is the appli-\\ncation of the transform to grayscale images.\\nReturning to the edge-linking problem, an approach based on the Hough trans-\\nform is as follows:\\n1. \\nObtain a binary edge map using any of the methods discussed earlier in this section.\\n2. \\nSpecify subdivisions in the \\nru\\n-\\nplane.\\n3. \\nExamine the counts of the accumulator cells for high pixel concentrations.\\n4. \\nExamine the relationship (principally for continuity) between pixels in a chosen \\ncell.\\nContinuity in this case usually is based on computing the distance between discon-\\nnected pixels corresponding to a given accumulator cell.\\n A gap in a line associated \\nwith a given cell is bridged if the length of the gap is less than a specified threshold. \\nBeing able to group lines based on direction is a global concept applicable over the \\nentire image, requiring only that we examine pixels associated with specific accumu-\\nlator cells. The following example illustrates these concepts.\\nEXAMPLE 10.12 :  Using the Hough transform for edge linking.\\nFigure 10.31(a) shows an aerial image of an airport. The objective of this example is to use the Hough \\ntransform to extract the two edges deﬁning the principal runway. A solution to such a problem might be \\nof interest, for instance, in applications involving autonomous air navigation.\\nThe ﬁrst step is to obtain an edge map. Figure 10.31(b) shows the edge map obtained using Canny’s \\nalgorithm with the same parameters and procedure used in Example 10.9. For the purpose of computing \\nthe Hough transform, similar results can be obtained using any of the other edge-detection techniques \\ndiscussed earlier. Figure 10.31(c) shows the Hough parameter space obtained using 1° increments for \\nu\\n, \\nand one-pixel increments for \\nr\\n.\\nThe runway of interest is oriented approximately \\n1\\n°\\n off the north direction, so we select the cells cor-\\nresponding to \\n±°\\n90\\n and containing the highest count because the runways are the longest lines oriented \\nin these directions\\n. The small boxes on the edges of Fig. 10.31(c) highlight these cells. As mentioned ear-\\nlier in connection with Fig. 10.30(b), the Hough transform exhibits adjacency at the edges. Another way \\nof interpreting this property is that a line oriented at \\n+°\\n90\\n and a line oriented at \\n−°\\n90\\n are equivalent (i.e., \\nthey are both vertical).\\n Figure 10.31(d) shows the lines corresponding to the two accumulator cells just \\ndiscussed, and Fig. 10.31(e) shows the lines superimposed on the original image. The lines were obtained \\nby joining all gaps not exceeding 20% (approximately 100 pixels) of the image height. These lines clearly \\ncorrespond to the edges of the runway of interest.\\nNote that the only information needed to solve this problem was the orientation of the runway and \\nthe observer’s position relative to it. In other words, a vehicle navigating autonomously would know \\nthat if the runway of interest faces north, and the vehicle’s direction of travel also is north, the runway \\nshould appear vertically in the image. Other relative orientations are handled in a similar manner. The \\nDIP4E_GLOBAL_Print_Ready.indb   741\\n6/16/2017   2:13:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 742}),\n",
       " Document(page_content='742\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\norientations of runways throughout the world are available in ﬂight charts, and the direction of travel \\nis easily obtainable using GPS (Global Positioning System) information. This information also could be \\nused to compute the distance between the vehicle and the runway, thus allowing estimates of param-\\neters such as expected length of lines relative to image size, as we did in this example.\\n10.3  THRESHOLDING  \\nBecause of its intuitive properties, simplicity of implementation, and computational \\nspeed, image thresholding enjoys a central position in applications of image segmen-\\ntation. Thresholding was introduced in Section 3.1, and we have used it in various \\ndiscussions since then. In this section, we discuss thresholding in a more formal way, \\nand develop techniques that are considerably more general than what has been pre-\\nsented thus far.\\nFOUNDATION\\nIn the previous section, regions were identified by first finding edge segments, \\nthen attempting to link the segments into boundaries. In this section, we discuss \\n10.3\\nb a\\nc\\ne\\nd\\nFIGURE 10.31\\n (a) A \\n502 564\\n×\\n aerial image of an airport. (b) Edge map obtained using Canny’s algorithm. (c) Hough \\nparameter space (the boxes highlight the points associated with long vertical lines).\\n (d) Lines in the image plane \\ncorresponding to the points highlighted by the boxes. (e) Lines superimposed on the original image.\\nDIP4E_GLOBAL_Print_Ready.indb   742\\n6/16/2017   2:13:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 743}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n743\\ntechniques for partitioning images directly into regions based on intensity values \\nand/or properties of these values.\\nThe Basics of Intensity Thresholding\\nSuppose that the intensity histogram in Fig. 10.32(a) corresponds to an image, \\nfx y\\n(,\\n) ,\\n \\ncomposed of light objects on a dark background,\\n in such a way that object and back-\\nground pixels have intensity values grouped into two dominant modes. One obvious \\nway to extract the objects from the background is to select a threshold, \\nT\\n, that sepa-\\nrates these modes. Then, any point \\n(,)\\nxy\\n in the image at which \\nfx y T\\n(,\\n)\\n>\\n is called \\nan \\nobject point\\n.\\n Otherwise, the point is called a \\nbackground\\n point. In other words, \\nthe segmented image, denoted by \\ngxy\\n(,\\n) ,\\n is given by\\n \\ngxy\\nfx\\ny T\\nfx y T\\n(,)\\n(,)\\n(,)\\n=\\n>\\n⎧\\n⎨\\n⎩\\n1\\n0\\nif \\nif \\n≤\\n \\n(10-46)\\nWhen \\nT\\n is a constant applicable over an entire image\\n, the process given in this equa-\\ntion is referred to as \\nglobal thresholding\\n. When the value of \\nT\\n changes over an image, \\nwe use the term \\nvariable thresholding\\n. The terms \\nlocal\\n or \\nregional \\nthresholding are \\nused sometimes to denote variable thresholding in which the value of \\nT\\n at any point \\n(,)\\nxy\\n in an image depends on properties of a neighborhood of \\n(,)\\nxy\\n (for example, \\nthe average intensity of the pixels in the neighborhood).\\n If \\nT\\n depends on the spa-\\ntial coordinates \\n(,)\\nxy\\n themselves, then variable thresholding is often referred to as \\ndynamic\\n or \\nadaptive\\n thresholding\\n. Use of these terms is not universal.\\nFigure 10.32(b) shows a more difﬁcult thresholding problem involving a histo-\\ngram with three dominant modes corresponding, for example, to two types of light \\nobjects on a dark background. Here, \\nmultiple thresholding\\n classiﬁes a point \\n(,)\\nxy\\n as \\nbelonging to the background if \\nfx y T\\n(,\\n) ,\\n≤\\n1\\n to one object class if \\nTf x y T\\n12\\n<\\n(,) ,\\n≤\\n \\nand to the other object class if \\nfx y T\\n(,\\n) .\\n>\\n2\\n That is, the segmented image is given by\\n \\ngx y\\naf\\nx y T\\nbT f x y T\\ncf x y T\\n,\\n(,)\\n(,)\\n(,)\\n()\\n=\\n>\\n<\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\nif \\nif \\nif \\n2\\n12\\n1\\n≤\\n≤\\n \\n(10-47)\\nRemember, \\nf\\n(\\nx\\n, y)  \\ndenotes the intensity of \\nf\\n \\nat coordinates (\\nx\\n, \\ny\\n).\\nAlthough we follow \\nconvention in using 0 \\nintensity for the back-\\nground and 1 for object \\npixels, any two distinct \\nvalues can be used in  \\nEq. (10-46).\\nT\\nT\\n1\\nT\\n2\\nb a\\nFIGURE 10.32\\nIntensity  \\nhistograms that \\ncan be partitioned \\n(a) by a single \\nthreshold, and  \\n(b) by dual \\nthresholds.\\nDIP4E_GLOBAL_Print_Ready.indb   743\\n6/16/2017   2:13:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 744}),\n",
       " Document(page_content='744\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nwhere \\na,\\n \\nb, \\nand\\n c\\n are any three distinct intensity values. We will discuss dual threshold-\\ning later in this section. Segmentation problems requiring more than two thresholds \\nare difficult (or often impossible) to solve, and better results usually are obtained \\nusing \\nother methods, such as variable thresholding, as will be discussed later in this section, \\nor region growing, as we will discuss in Section 10.4.\\nBased on the preceding discussion, we may infer intuitively that the success of \\nintensity thresholding is related directly to the width and depth of the valley(s) sepa-\\nrating the histogram modes. In turn, the key factors affecting the properties of the \\nvalley(s) are: (1) the separation between peaks (the further apart the peaks are, the \\nbetter the chances of separating the modes); (2) the noise content in the image (the \\nmodes broaden as noise increases); (3) the relative sizes of objects and background; \\n(4) the uniformity of the illumination source; and (5) the uniformity of the reﬂectance \\nproperties of the image.\\nThe Role of Noise in Image Thresholding\\nThe simple synthetic image in Fig. 10.33(a) is free of noise, so its histogram con-\\nsists of two “spike” modes, as Fig. 10.33(d) shows. Segmenting this image into two \\nregions is a trivial task: we just select a threshold anywhere between the two modes. \\nFigure 10.33(b) shows the original image corrupted by Gaussian noise of zero \\nmean and a standard deviation of 10 intensity levels. The modes are broader now \\n191\\n255\\n63\\n127\\n0\\n191\\n191\\n255\\n63\\n127\\n0\\n255\\n63 127\\n0\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.33\\n (a) Noiseless 8-bit image. (b) Image with additive Gaussian noise of mean 0 and standard deviation of \\n10 intensity levels. (c) Image with additive Gaussian noise of mean 0 and standard deviation of 50 intensity levels. \\n(d) through (f) Corresponding histograms.\\nDIP4E_GLOBAL_Print_Ready.indb   744\\n6/16/2017   2:13:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 745}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n745\\n[see Fig. 10.33(e)], but their separation is enough so that the depth of the valley \\nbetween them is sufficient to make the modes easy to separate. A threshold placed \\nmidway between the two peaks would do the job. Figure 10.33(c) shows the result \\nof corrupting the image with Gaussian noise of zero mean and a standard deviation \\nof 50 intensity levels. As the histogram in Fig. 10.33(f) shows, the situation is much \\nmore serious now, as there is no way to differentiate between the two modes. With-\\nout additional processing (such as the methods discussed later in this section) we \\nhave little hope of finding a suitable threshold for segmenting this image.\\nThe Role of Illumination and Reﬂectance in Image Thresholding\\nFigure 10.34 illustrates the effect that illumination can have on the histogram of \\nan image. Figure 10.34(a) is the noisy image from Fig. 10.33(b), and Fig. 10.34(d) \\nshows its histogram. As before, this image is easily segmentable with a single thresh-\\nold. With reference to the image formation model discussed in Section 2.3, suppose \\nthat we multiply the image in Fig. 10.34(a) by a nonuniform intensity function, such \\nas the intensity ramp in Fig. 10.37(b), whose histogram is shown in Fig. 10.34(e). \\nFigure 10.34(c) shows the product of these two images, and Fig. 10.34(f) is the result-\\ning histogram. The deep valley between peaks was corrupted to the point where sep-\\naration of the modes without additional processing (to be discussed later in this sec-\\ntion) is no longer possible. Similar results would be obtained if the illumination was \\nIn theory, the histogram \\nof a ramp image is \\nuniform. In practice, the \\ndegree of uniformity \\ndepends on the size of \\nthe image and number of \\nintensity levels. \\n0 63 127 191 255\\n0 0.2 0.4 0.6 0.8 1\\n0 63 127 191 255\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.34\\n (a) Noisy image. (b) Intensity ramp in the range [0.2, 0.6]. (c) Product of (a) and (b). (d) through (f) \\nCorresponding histograms.\\nDIP4E_GLOBAL_Print_Ready.indb   745\\n6/16/2017   2:13:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 746}),\n",
       " Document(page_content='746\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nperfectly uniform, but the reflectance of the image was not, as a results, for example, \\nof natural reflectivity variations in the surface of objects and/or background.\\nThe important point is that illumination and reﬂectance play a central role in the \\nsuccess of image segmentation using thresholding or other segmentation techniques. \\nTherefore, controlling these factors when possible should be the ﬁrst step consid-\\nered in the solution of a segmentation problem. There are three basic approaches \\nto the problem when control over these factors is not possible. The ﬁrst is to correct \\nthe shading pattern directly. For example, nonuniform (but ﬁxed) illumination can \\nbe corrected by multiplying the image by the inverse of the pattern, which can be \\nobtained by imaging a ﬂat surface of constant intensity. The second is to attempt \\nto correct the global shading pattern via processing using, for example, the top-hat \\ntransformation introduced in Section 9.8. The third approach is to “work around” \\nnonuniformities using variable thresholding, as discussed later in this section.\\nBASIC GLOBAL THRESHOLDING\\nWhen the intensity distributions of objects and background pixels are sufficiently \\ndistinct, it is possible to use a single (\\nglobal\\n) threshold applicable over the entire \\nimage. In most applications, there is usually enough variability between images that, \\neven if global thresholding is a suitable approach, an algorithm capable of estimat-\\ning the threshold value for each image is required. The following iterative algorithm \\ncan be used for this purpose:\\n1. \\nSelect an initial estimate for the global threshold, \\nT\\n.\\n2.\\n \\nSegment the image using \\nT\\n in Eq.\\n (10-46). This will produce two groups of \\npixels: \\nG\\n1\\n,\\n consisting of pixels with intensity values \\n>\\nT\\n;\\n and \\nG\\n2\\n,\\n consisting of \\npixels with values \\n≤\\nT\\n.\\n3.\\n \\nCompute the average (mean) intensity values \\nm\\n1\\n and \\nm\\n2\\n for the pixels in \\nG\\n1\\n \\nand \\nG\\n2\\n, respectively.\\n4. \\nCompute a new threshold value midway between \\nm\\n1\\nand \\nm\\n2\\n:\\n \\nTm m\\n=+\\n()\\n1\\n2\\n12\\n5. \\nRepeat Steps 2 through 4 until the difference between values of \\nT\\n in successive \\niterations is smaller than a predeﬁned value\\n, \\n/H9004\\nT\\n. \\nT\\nhe algorithm is stated here in terms of successively thresholding the input image \\nand calculating the means at each step, because it is more intuitive to introduce \\nit in this manner. However, it is possible to develop an equivalent (and more efﬁ-\\ncient) procedure by expressing all computations in the terms of the image histogram, \\nwhich has to be computed only once (see Problem 10.29).\\nThe preceding algorithm works well in situations where there is a reasonably \\nclear valley between the modes of the histogram related to objects and background. \\nParameter \\n/H9004\\nT\\n is used to stop iterating when the changes in threshold values is small. \\nT\\nhe initial threshold must be chosen greater than the minimum and less than the \\nmaximum intensity level in the image (the average intensity of the image is a good \\nDIP4E_GLOBAL_Print_Ready.indb   746\\n6/16/2017   2:13:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 747}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n747\\ninitial choice for \\nT \\n). If this condition is met, the algorithm converges in a ﬁnite num-\\nber of steps, whether or not the modes are separable  (see Problem 10.30).\\nEXAMPLE 10.13 :  Global thresholding.\\nFigure 10.35 shows an example of segmentation using the preceding iterative algorithm. Figure 10.35(a) \\nis the original image and Fig. 10.35(b) is the image histogram, showing a distinct valley. Application \\nof the basic global algorithm resulted in the threshold \\nT\\n=\\n125\\n4 .\\n after three iterations, starting with \\nT\\n \\nequal to the average intensity of the image\\n, and using \\n/H9004\\nT\\n=\\n0.\\n Figure 10.35(c) shows the result obtained \\nusing \\nT\\n=\\n125\\n to segment the original image. As expected from the clear separation of modes in the \\nhistogram,\\n the segmentation between object and background was perfect.\\nOPTIMUM GLOBAL THRESHOLDING USING OTSU’S METHOD\\nThresholding may be viewed as a statistical-decision theory problem whose objec-\\ntive is to minimize the average error incurred in assigning pixels to two or more \\ngroups (also called \\nclasses\\n). This problem is known to have an elegant closed-form \\nsolution known as the \\nBayes decision function\\n (see Section 12.4). The solution is \\nbased on only two parameters: the probability density function (PDF) of the inten-\\nsity levels of each class, and the probability that each class occurs in a given applica-\\ntion. Unfortunately, estimating PDFs is not a trivial matter, so the problem usually \\nis simpliﬁed by making workable assumptions about the form of the PDFs, such as \\nassuming that they are Gaussian functions. Even with simpliﬁcations, the process \\nof implementing solutions using these assumptions can be complex and not always \\nwell-suited for real-time applications.\\nThe approach in the following discussion, called \\nOtsu’s method\\n (Otsu [1979]), is \\nan attractive alternative. The method is optimum in the sense that it maximizes the \\n0\\n63 127\\n191 255\\nb a\\nc\\nFIGURE 10.35\\n (a) Noisy ﬁngerprint. (b) Histogram. (c) Segmented result using a global threshold (thin image border \\nadded for clarity). (Original image courtesy of the National Institute of Standards and Technology.).\\nDIP4E_GLOBAL_Print_Ready.indb   747\\n6/16/2017   2:13:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 748}),\n",
       " Document(page_content='748\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nbetween-class variance\\n, a well-known measure used in statistical discriminant analy-\\nsis. The basic idea is that properly thresholded classes should be distinct with respect \\nto the intensity values of their pixels and, conversely, that a threshold giving the \\nbest separation between classes in terms of their intensity values would be the best \\n(optimum) threshold. In addition to its optimality, Otsu’s method has the important \\nproperty that it is based entirely on computations performed on the histogram of an \\nimage, an easily obtainable 1-D array (see Section 3.3).\\nLet \\n012 1\\n,,\\n, ,\\n…\\nL\\n−\\n{}\\n denote the set of \\nL\\n distinct integer intensity levels in a digi-\\ntal image of size \\nMN\\n×\\n pixels, and let \\nn\\ni\\n denote the number of pixels with intensity \\ni\\n. \\nT\\nhe total number, \\nMN\\n,\\n of pixels in the image is \\nMN n n n n\\nL\\n=+++ +\\n−\\n012 1\\n/midhorizellipsis\\n.\\n The \\nnormalized histogram (see Section 3.3) has components \\npn M N\\nii\\n=\\n,\\n from which it \\nfollows that\\n \\npp\\nii\\ni\\nL\\n=≥\\n=\\n−\\n∑\\n10\\n0\\n1\\n \\n(10-48)\\nNow, suppose that we select a threshold \\nTk k k L\\n()\\n,\\n,\\n=< < −\\n01\\n and use it to thresh-\\nold the input image into two classes\\n, \\nc\\n1\\n and \\nc\\n2\\n,\\n where \\nc\\n1\\n consists of all the pixels in \\nthe image with intensity values in the range \\n[, ]\\n0\\nk\\n and \\nc\\n2\\n consists of the pixels with \\nvalues in the range \\n[, ] .\\nkL\\n+−\\n11\\n Using this threshold, the probability, \\nPk\\n1\\n() ,\\n that a \\npixel is assigned to (i.e\\n., thresholded into) class \\nc\\n1\\n is given by the cumulative sum\\n \\nPk p\\ni\\ni\\nk\\n1\\n0\\n()\\n=\\n=\\n∑\\n \\n(10-49)\\nViewed another way, this is the probability of class \\nc\\n1\\n occurring. For example, if we \\nset \\nk\\n=\\n0,\\n the probability of class \\nc\\n1\\n having any pixels assigned to it is zero. Similarly, \\nthe probability of class \\nc\\n2\\n occurring is\\n \\nPk p Pk\\ni\\nik\\nL\\n21\\n1\\n1\\n1\\n()\\n()\\n== −\\n=+\\n−\\n∑\\n \\n(10-50)\\nFrom Eq. (3-25), the \\nmean intensity\\n value of the pixels in \\nc\\n1\\n is \\n \\nm k iP i c iP c i P i P c\\nPk\\nip\\ni\\nk\\ni\\nk\\ni\\ni\\nk\\n11\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n()\\n=\\n()\\n=\\n()\\n()\\n()\\n=\\n()\\n==\\n=\\n∑∑\\n∑\\n \\n(10-51)\\nwhere \\nPk\\n1\\n()\\n is given by Eq. (10-49). The term \\nPic\\n1\\n()\\n in Eq. (10-51) is the probability \\nof intensity value \\ni\\n,\\n given that \\ni\\n comes from class \\nc\\n1\\n.\\n The rightmost term in the first \\nline of the equation follows from Bayes’ formula:\\n \\nPA B PB A PA PB\\n()\\n=\\n() ( ) ( )\\nThe second line follows from the fact that \\nPc i\\n1\\n()\\n,\\n the probability of \\nc\\n1\\n given \\ni\\n,\\n is 1 \\nbecause we are dealing only with values of \\ni\\n from class \\nc\\n1\\n.\\n Also, \\nPi\\n()\\n is the probabil-\\nity of the \\ni\\nth value\\n, which is the \\ni\\nth component of the histogram, \\np\\ni\\n.\\n Finally, \\nPc\\n()\\n1\\n is \\nthe probability of class \\nc\\n1\\n which, from Eq. (10-49), is equal to \\nPk\\n1\\n() .\\nDIP4E_GLOBAL_Print_Ready.indb   748\\n6/16/2017   2:13:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 749}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n749\\nSimilarly, the \\nmean intensity\\n value of the pixels assigned to class \\nc\\n2\\n is\\n \\nmk i P i c\\nPk\\nip\\nik\\nL\\ni\\nik\\nL\\n22\\n1\\n1\\n2\\n1\\n1\\n1\\n()\\n()\\n=\\n()\\n=\\n=+\\n−\\n=+\\n−\\n∑\\n∑\\n \\n(10-52)\\nThe \\ncumulative mean\\n (average intensity) up to level \\nk\\n is given by\\n \\nmk i p\\ni\\ni\\nk\\n()\\n=\\n=\\n∑\\n0\\n \\n(10-53)\\nand the average intensity of the entire image (i.e., the \\nglobal\\n mean) is given by\\n \\nmi p\\nGi\\ni\\nL\\n=\\n=\\n−\\n∑\\n0\\n1\\n \\n(10-54)\\nThe validity of the following two equations can be verified by direct substitution of \\nthe preceding results:\\n \\nPm Pm m\\nG\\n11 22\\n+=\\n \\n(10-55)\\nand\\n \\nPP\\n12\\n1\\n+=\\n \\n(10-56)\\nwhere we have omitted the \\nk\\ns temporarily in favor of notational clarity\\n.\\nIn order to evaluate the effectiveness of the threshold at level \\nk\\n,\\n we use the nor-\\nmalized,\\n dimensionless measure\\n \\nh\\ns\\ns\\n=\\nB\\nG\\n2\\n2\\n \\n(10-57)\\nwhere \\ns\\nG\\n2\\n is the \\nglobal variance\\n [i.e., the intensity variance of all the pixels in the \\nimage, as given in Eq. (3-26)],\\n \\ns\\nGG\\ni\\ni\\nL\\nim p\\n2\\n2\\n0\\n1\\n=−\\n()\\n=\\n−\\n∑\\n \\n(10-58)\\nand \\ns\\nB\\n2\\n is the \\nbetween-class variance\\n, defined as\\n \\ns\\nBG G\\nPm m Pm m\\n2\\n11\\n2\\n22\\n2\\n=−\\n()\\n+−\\n()\\n \\n(10-59)\\nThis expression can also be written as\\n \\ns\\nB\\nG\\nPP m m\\nmP m\\nPP\\n2\\n12\\n12\\n2\\n1\\n2\\n11\\n1\\n=−\\n()\\n=\\n−\\n()\\n−\\n()\\n \\n(10-60)\\nThe second step in this \\nequation makes sense \\nonly if \\nP\\n1\\n is greater than \\n0 and less than 1, which, \\nin view of Eq. (10-56), \\nimplies that \\nP\\n2\\n must  \\nsatisfy the same  \\ncondition.\\nDIP4E_GLOBAL_Print_Ready.indb   749\\n6/16/2017   2:13:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 750}),\n",
       " Document(page_content='750\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nThe first line of this equation follows from Eqs. (10-55), (10-56), and (10-59). The \\nsecond line follows from Eqs. (10-50) through (10-54). This form is slightly more \\nefficient computationally because the global mean, \\nm\\nG\\n,\\n is computed only once, so \\nonly two parameters\\n, \\nm\\n1\\n and \\nP\\n1\\n, need to be computed for any value of \\nk\\n.\\nThe ﬁrst line in Eq. (10-60) indicates that the farther the two means \\nm\\n1\\n and \\nm\\n2\\n are \\nfrom each other, the larger \\ns\\nB\\n2\\n will be, implying that the between-class variance is a \\nmeasure of separability between classes. Because \\ns\\nG\\n2\\n is a constant, it follows that \\nh\\n \\nalso is a measure of separability, and maximizing this metric is equivalent to maximiz-\\ning \\ns\\nB\\n2\\n.\\n The objective, then, is to determine the threshold value, \\nk\\n,\\n that maximizes the \\nbetween-class variance\\n, as stated earlier. Note that Eq. (10-57) assumes implicitly that \\ns\\nG\\n2\\n0\\n>\\n.\\n This variance can be zero only when all the intensity levels in the image are \\nthe same\\n, which implies the existence of only one class of pixels. This in turn means \\nthat \\nh\\n=\\n0\\n for a constant image because the separability of a single class from itself \\nis zero\\n.\\nReintroducing \\nk\\n, we have the ﬁnal results:\\n \\nh\\ns\\ns\\nk\\nk\\nB\\nG\\n()\\n=\\n2\\n2\\n()\\n \\n(10-61)\\nand\\n \\ns\\nB\\nG\\nk\\nmPk m k\\nPk Pk\\n2\\n1\\n2\\n11\\n1\\n()\\n=\\n−\\n[]\\n−\\n[]\\n() ()\\n() ()\\n \\n(10-62)\\nThen, the optimum threshold is the value, \\nk\\n*\\n, that maximizes \\ns\\nB\\nk\\n2\\n() :\\n \\nss\\nB\\nkL\\nB\\nkk\\n2\\n01\\n2\\n*\\nmax ( )\\n(\\n)\\n=\\n≤≤−\\n \\n(10-63)\\nTo find \\nk\\n*\\n  we simply evaluate this equation for all \\ninteger\\n values of \\nk\\n (subject to the \\ncondition \\n01\\n1\\n<<\\nPk\\n() )\\n and select the value of \\nk\\n that yielded the maximum \\ns\\nB\\nk\\n2\\n() .\\n \\nIf the maximum exists for more than one value of \\nk\\n,\\n it is customary to average the \\nvarious values of \\nk\\n for which \\ns\\nB\\nk\\n2\\n()\\n is maximum. It can be shown (see Problem \\n10.36) that a maximum always exists\\n, subject to the condition \\n01\\n1\\n<<\\nPk\\n() .\\n Evaluat-\\ning Eqs\\n. (10-62) and (10-63) for all values of \\nk\\n is a relatively inexpensive computa-\\ntional procedure\\n, because the maximum number of integer values that \\nk\\n can have \\nis \\nL\\n, which is only 256 for 8-bit images.\\nOnce \\nk\\n*\\n has been obtained, input image \\nfx y\\n(,\\n)\\n is segmented as before:\\n \\ngxy\\nfx\\ny k\\nfx y k\\n(,)\\n(,)\\n(,)\\n*\\n*\\n=\\n>\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n0\\nif \\nif \\n≤\\n \\n(10-64)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n Note that all the quantities needed \\nto evaluate Eq.\\n (10-62) are obtained using only the histogram of \\nfx y\\n(,\\n) .\\n In addition \\nto the optimum threshold,\\n other information regarding the segmented image can be \\nextracted from the histogram. For example, \\nPk\\n1\\n()\\n*\\n and \\nP\\nk\\n2\\n() ,\\n*\\n the class probabilities \\nevaluated at the optimum threshold, indicate the portions of the areas occupied by \\nthe classes (groups of pixels) in the thresholded image. Similarly, the means \\nmk\\n1\\n()\\n*\\n \\nand \\nmk\\n2\\n()\\n*\\n are estimates of the average intensity of the classes in the original image.\\nDIP4E_GLOBAL_Print_Ready.indb   750\\n6/16/2017   2:13:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 751}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n751\\nIn general, the measure in Eq.(10-61) has values in the range\\n \\n01\\n≤≤\\nh\\n()\\nk\\n \\n(10-65)\\nfor values of \\nk\\n in the range \\n01\\n,.\\nL\\n−\\n[]\\n When evaluated at the optimum threshold \\nk\\n*\\n,\\n  this measure is a quantitative estimate of the separability of classes, which in \\nturn gives us an idea of the accurac\\ny of thresholding a given image with \\nk\\n*\\n.\\n The \\nlower bound in Eq.\\n (10-65) is attainable only by images with a single, constant inten-\\nsity level. The upper bound is attainable only by two-valued images with intensities \\nequal to 0 and \\nL\\n−\\n1 (see Problem 10.37).\\nOtsu’\\ns algorithm may be summarized as follows:\\n1. \\nCompute the normalized histogram of the input image. Denote the components \\nof the histogram by \\npi L\\ni\\n,, , , , .\\n=−\\n012 1\\n…\\n2. \\nCompute the cumulative sums, \\nPk\\n1\\n() ,\\n for \\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n using Eq. (10-49).\\n3. \\nCompute the cumulative means, \\nmk\\n()\\n,\\n for \\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n using Eq. (10-53).\\n4. \\nCompute the global mean, \\nm\\nG\\n, using Eq. (10-54).\\n5. \\nCompute the between-class variance term, \\ns\\nB\\nk\\n2\\n() ,\\n for \\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n using \\nEq.\\n (10-62).\\n6. \\nObtain the Otsu threshold, \\nk\\n*\\n,\\n  as the value of \\nk\\n for which \\ns\\nB\\nk\\n2\\n()\\n is maximum. If \\nthe maximum is not unique\\n, obtain \\nk\\n*\\n  by averaging the values of \\nk\\n  correspond-\\ning to the various maxima detected.\\n7. \\nCompute the global variance, \\ns\\nG\\n2\\n,\\n using Eq. (10-58), and then obtain the separa-\\nbility measure\\n, \\nh\\n*\\n, by evaluating Eq. (10-61) with \\nkk\\n=\\n*\\n.\\nThe following example illustrates the use of this algorithm.\\nEXAMPLE 10.14 :  Optimum global thresholding using Otsu’s method.\\nFigure 10.36(a) shows an optical microscope image of polymersome cells. These are cells artiﬁcially engi-\\nneered using polymers. They are invisible to the human immune system and can be used, for example, \\nto deliver medication to targeted regions of the body. Figure 10.36(b) shows the image histogram. The \\nobjective of this example is to segment the molecules from the background. Figure 10.36(c) is the result \\nof using the basic global thresholding algorithm discussed earlier. Because the histogram has no distinct \\nvalleys and the intensity difference between the background and objects is small, the algorithm failed to \\nachieve the desired segmentation. Figure 10.36(d) shows the result obtained using Otsu’s method. This \\nresult obviously is superior to Fig. 10.36(c). The threshold value computed by the basic algorithm was \\n169, while the threshold computed by Otsu’s method was 182, which is closer to the lighter areas in the \\nimage deﬁning the cells. The separability measure \\nh\\n*\\n was 0.467.\\nAs a point of interest, applying Otsu’s method to the ﬁngerprint image in Example 10.13 yielded a \\nthreshold of 125 and a separability measure of 0.944. The threshold is identical to the value (rounded to \\nthe nearest integer) obtained with the basic algorithm. This is not unexpected, given the nature of the \\nhistogram. In fact, the separability measure is high because of the relatively large separation between \\nmodes and the deep valley between them.\\nDIP4E_GLOBAL_Print_Ready.indb   751\\n6/16/2017   2:13:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 752}),\n",
       " Document(page_content='752\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nUSING IMAGE SMOOTHING TO IMPROVE GLOBAL THRESHOLDING\\nAs illustrated in Fig. 10.33, noise can turn a simple thresholding problem into an \\nunsolvable one. When noise cannot be reduced at the source, and thresholding is the \\npreferred segmentation method, a technique that often enhances performance is to \\nsmooth the image prior to thresholding. We illustrate this approach with an example.\\nFigure 10.37(a) is the image from Fig. 10.33(c), Fig. 10.37(b) shows its histogram, \\nand Fig. 10.37(c) is the image thresholded using Otsu’s method. Every black point \\nin the white region and every white point in the black region is a thresholding error, \\nso the segmentation was highly unsuccessful. Figure 10.37(d) shows the result of \\nsmoothing the noisy image with an averaging kernel of size \\n55\\n×\\n (the image is of size \\n651 814\\n×\\n pixels), and Fig. 10.37(e) is its histogram. The improvement in the shape \\nof the histogram as a result of smoothing is evident,\\n and we would expect threshold-\\ning of the smoothed image to be nearly perfect. Figure 10.37(f) shows this to be the \\ncase. The slight distortion of the boundary between object and background in the \\nsegmented, smoothed image was caused by the blurring of the boundary. In fact, the \\nmore aggressively we smooth an image, the more boundary errors we should antici-\\npate in the segmented result.\\n0\\n63\\n127\\n191 255\\nb a\\nd c\\nFIGURE 10.36\\n(a) Original  \\nimage.  \\n(b) Histogram \\n(high peaks \\nwere clipped to \\nhighlight details in \\nthe lower values). \\n(c) Segmenta-\\ntion result using \\nthe basic global \\nalgorithm from \\nSection 10.3.  \\n(d) Result using \\nOtsu’s method. \\n(Original image \\ncourtesy of \\nProfessor Daniel \\nA. Hammer, the \\nUniversity of \\nPennsylvania.)\\nDIP4E_GLOBAL_Print_Ready.indb   752\\n6/16/2017   2:13:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 753}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n753\\nNext, we investigate the effect of severely reducing the size of the foreground \\nregion with respect to the background. Figure 10.38(a) shows the result. The noise in \\nthis image is additive Gaussian noise with zero mean and a standard deviation of 10 \\nintensity levels (as opposed to 50 in the previous example). As Fig. 10.38(b) shows, \\nthe histogram has no clear valley, so we would expect segmentation to fail, a fact that \\nis conﬁrmed by the result in Fig. 10.38(c). Figure 10.38(d) shows the image smoothed \\nwith an averaging kernel of size \\n55\\n×\\n,\\n and Fig. 10.38(e) is the corresponding histo-\\ngram.\\n As expected, the net effect was to reduce the spread of the histogram, but the \\ndistribution still is unimodal. As Fig. 10.38(f) shows, segmentation failed again. The \\nreason for the failure can be traced to the fact that the region is so small that its con-\\ntribution to the histogram is insigniﬁcant compared to the intensity spread caused \\nby noise. In situations such as this, the approach discussed in the following section is \\nmore likely to succeed.\\nUSING EDGES TO IMPROVE GLOBAL THRESHOLDING\\nBased on the discussion thus far, we conclude that the chances of finding a “good” \\nthreshold are enhanced considerably if the histogram peaks are tall, narrow, sym-\\nmetric, and separated by deep valleys. One approach for improving the shape of \\nhistograms is to consider only those pixels that lie on or near the edges between \\n0 63 127 191\\n255\\n0 63 127 191\\n255\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.37\\n (a) Noisy image from Fig. 10.33(c) and (b) its histogram. (c) Result obtained using Otsu’s method.  \\n(d) Noisy image smoothed using a \\n55\\n×\\n averaging kernel and (e) its histogram. (f) Result of thresholding using \\nOtsu’\\ns method.\\nDIP4E_GLOBAL_Print_Ready.indb   753\\n6/16/2017   2:13:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 754}),\n",
       " Document(page_content='754\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nobjects and the background. An immediate and obvious improvement is that his-\\ntograms should be less dependent on the relative sizes of objects and background. \\nFor instance, the histogram of an image composed of a small object on a large back-\\nground area (or vice versa) would be dominated by a large peak because of the high \\nconcentration of one type of pixels. We saw in Fig. 10.38 that this can lead to failure \\nin thresholding.\\nIf only the pixels on or near the edges between objects and background were \\nused, the resulting histogram would have peaks of approximately the same height. In \\naddition, the probability that any of those pixels lies on an object would be approxi-\\nmately equal to the probability that it lies on the background, thus improving the \\nsymmetry of the histogram modes. Finally, as indicated in the following paragraph, \\nusing pixels that satisfy some simple measures based on gradient and Laplacian \\noperators has a tendency to deepen the valley between histogram peaks.\\nThe approach just discussed assumes that the edges between objects and back-\\nground are known. This information clearly is not available during segmentation, \\nas ﬁnding a division between objects and background is precisely what segmenta-\\ntion aims to do. However, an indication of whether a pixel is on an edge may be \\nobtained by computing its gradient or Laplacian. For example, the average value \\nof the Laplacian is 0 at the transition of an edge (see Fig. 10.10), so the valleys of \\n0 63 127 191\\n255\\n0 63 127 191\\n255\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.38\\n (a) Noisy image and (b) its histogram. (c) Result obtained using Otsu’s method. (d) Noisy image \\nsmoothed using a \\n55\\n×\\n averaging kernel and (e) its histogram. (f) Result of thresholding using Otsu’s method. \\nT\\nhresholding failed in both cases to extract the object of interest. (See Fig. 10.39 for a better solution.)\\nDIP4E_GLOBAL_Print_Ready.indb   754\\n6/16/2017   2:13:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 755}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n755\\nhistograms formed from the pixels selected by a Laplacian criterion can be expected \\nto be sparsely populated. This property tends to produce the desirable deep valleys \\ndiscussed above. In practice, comparable results typically are obtained using either \\nthe gradient or Laplacian images, with the latter being favored because it is compu-\\ntationally more attractive and is also created using an isotropic edge detector.\\nThe preceding discussion is summarized in the following algorithm, where \\nfx y\\n(,\\n)\\n \\nis the input image:\\n1. \\nCompute an edge image as either the magnitude of the gradient, or absolute \\nvalue of the Laplacian,\\n of \\nfx y\\n(,\\n)\\n using any of the methods in Section 10.2.\\n2. \\nSpecify a threshold value, \\nT\\n.\\n3. \\nThreshold the image from Step 1 using \\nT\\n from Step 2 to produce a binary image\\n, \\ngx y\\nT\\n(,) .\\n This image is used as a mask image in the following step to select pixels \\nfrom \\nfx y\\n(,\\n)\\n corresponding to “strong” edge pixels in the mask.\\n4. \\nCompute a histogram using only the pixels in \\nfx y\\n(,\\n)\\n that correspond to the \\nlocations of the 1-valued pixels in \\ngx y\\nT\\n(,) .\\n5. \\nUse the histogram from Step 4 to segment \\nfx y\\n(,\\n)\\n globally using, for example, \\nOtsu’\\ns method.\\nIf \\nT\\n is set to any value less than the minimum value of the edge image then, accord-\\ning to Eq.\\n (10-46), \\ngx y\\nT\\n(,)\\n will consist of all 1’s, implying that all pixels of \\nfx y\\n(,\\n)\\n \\nwill be used to compute the image histogram.\\n In this case, the preceding algorithm \\nbecomes global thresholding using the histogram of the original image. It is custom-\\nary to specify the value of \\nT\\n to correspond to a percentile, which typically is set \\nhigh (e\\n.g., in the high 90’s) so that few pixels in the gradient/Laplacian image will \\nbe used in the computation. The following examples illustrate the concepts just dis-\\ncussed. The first example uses the gradient, and the second uses the Laplacian. Simi-\\nlar results can be obtained in both examples using either approach. The important \\nissue is to generate a suitable derivative image.\\nEXAMPLE 10.15 :  Using edge information based on the gradient to improve global thresholding.\\nFigures 10.39(a) and (b) show the image and histogram from Fig. 10.38. You saw that this image could \\nnot be segmented by smoothing followed by thresholding. The objective of this example is to solve the \\nproblem using edge information. Figure 10.39(c) is the mask image, \\ngx y\\nT\\n(,) ,\\n formed as gradient mag-\\nnitude image thresholded at the 99.7 percentile\\n. Figure 10.39(d) is the image formed by multiplying the \\nmask by the input image. Figure 10.39(e) is the histogram of the nonzero elements in Fig. 10.39(d). Note \\nthat this histogram has the important features discussed earlier; that is, it has reasonably symmetrical \\nmodes separated by a deep valley. Thus, while the histogram of the original noisy image offered no hope \\nfor successful thresholding, the histogram in Fig. 10.39(e) indicates that thresholding of the small object \\nfrom the background is indeed possible. The result in Fig. 10.39(f) shows that this is the case. This image \\nwas generated using Otsu’s method [to obtain a threshold based on the histogram in Fig. 10.42(e)], and \\nthen applying the Otsu threshold globally to the noisy image in Fig. 10.39(a). The result is nearly perfect.\\nIt is possible to modify \\nthis algorithm so that \\nboth the magnitude of \\nthe gradient and the \\nabsolute value of the \\nLaplacian images are \\nused. In this case, we \\nwould specify a threshold \\nfor each image and form \\nthe logical OR of the \\ntwo results to obtain \\nthe marker image. This \\napproach is useful when \\nmore control is desired \\nover the points deemed \\nto be valid edge points.\\nThe \\nn\\nth percentile is \\nthe smallest number \\nthat is greater than \\nn\\n% \\nof the numbers in a \\ngiven set. For example, \\nif you received a 95 in a \\ntest and this score was \\ngreater than 85% of all \\nthe students taking the \\ntest, then you would be \\nin the 85th percentile \\nwith respect to the test \\nscores.\\nDIP4E_GLOBAL_Print_Ready.indb   755\\n6/16/2017   2:13:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 756}),\n",
       " Document(page_content='756\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nEXAMPLE 10.16 :  Using edge information based on the Laplacian to improve global thresholding.\\nIn this example, we consider a more complex thresholding problem. Figure 10.40(a) shows an 8-bit \\nimage of yeast cells for which we want to use global thresholding to obtain the regions corresponding \\nto the bright spots. As a starting point, Fig. 10.40(b) shows the image histogram, and Fig. 10.40(c) is \\nthe result obtained using Otsu’s method directly on the image, based on the histogram shown. We see \\nthat Otsu’s method failed to achieve the original objective of detecting the bright spots. Although the \\nmethod was able to isolate some of the cell regions themselves, several of the segmented regions on the \\nright were actually joined. The threshold computed by the Otsu method was 42, and the separability \\nmeasure was 0.636.\\nFigure 10.40(d) shows the mask image \\ngx y\\nT\\n(,)\\n obtained by computing the absolute value of the \\nLaplacian image\\n, then thresholding it with \\nT\\n set to 115 on an intensity scale in the range \\n[, ] .\\n0\\n255\\n This \\nvalue of \\nT\\n corresponds approximately to the 99.5 percentile of the values in the absolute Laplacian \\nimage\\n, so thresholding at this level results in a sparse set of pixels, as Fig. 10.40(d) shows. Note in this \\nimage how the points cluster near the edges of the bright spots, as expected from the preceding dis-\\ncussion. Figure 10.40(e) is the histogram of the nonzero pixels in the product of (a) and (d). Finally, \\nFig. 10.40(f) shows the result of globally segmenting the original image using Otsu’s method based on \\nthe histogram in Fig. 10.40(e). This result agrees with the locations of the bright spots in the image. The \\nthreshold computed by the Otsu method was 115, and the separability measure was 0.762, both of which \\nare higher than the values obtained by using the original histogram.\\n0 63 127 191 255\\n63\\n0 127 191 255\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.39\\n (a) Noisy image from Fig. 10.38(a) and (b) its histogram. (c) Mask image formed as the gradient mag-\\nnitude image thresholded at the 99.7 percentile. (d) Image formed as the product of (a) and (c). (e) Histogram of \\nthe nonzero pixels in the image in (d). (f) Result of segmenting image (a) with the Otsu threshold based on the \\nhistogram in (e). The threshold was 134, which is approximately midway between the peaks in this histogram.\\nDIP4E_GLOBAL_Print_Ready.indb   756\\n6/16/2017   2:13:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 757}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n757\\n0 63 127 191 255\\n0 63 127 191 255\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 10.40\\n (a) Image of yeast cells. (b) Histogram of (a). (c) Segmentation of (a) with Otsu’s method using the  \\nhistogram in (b). (d) Mask image formed by thresholding the absolute Laplacian image. (e) Histogram of the non-\\nzero pixels in the product of (a) and (d). (f) Original image thresholded using Otsu’s method based on the histogram \\nin (e). (Original image courtesy of Professor Susan L. Forsburg, University of Southern California.)\\nBy varying the percentile at which the threshold is set, we can even improve the segmentation of the \\ncomplete cell regions. For example, Fig. 10.41 shows the result obtained using the same procedure as in \\nthe previous paragraph, but with the threshold set at 55, which is approximately 5% of the maximum \\nvalue of the absolute Laplacian image. This value is at the 53.9 percentile of the values in that image. \\nThis result clearly is superior to the result in Fig. 10.40(c) obtained using Otsu’s method with the histo-\\ngram of the original image.\\nMULTIPLE THRESHOLDS\\nThus far, we have focused attention on image segmentation using a single global \\nthreshold. Otsu’s method can be extended to an arbitrary number of thresholds \\nDIP4E_GLOBAL_Print_Ready.indb   757\\n6/16/2017   2:13:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 758}),\n",
       " Document(page_content='758\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nbecause the separability measure on which it is based also extends to an arbitrary \\nnumber of classes (Fukunaga [1972]). In the case of \\nK\\n classes, \\ncc c\\nK\\n12\\n,,, ,\\n…\\n the \\nbetween-class variance generalizes to the expression\\n \\ns\\nBk k G\\nk\\nK\\nPm m\\n2\\n2\\n1\\n=−\\n()\\n=\\n∑\\n \\n(10-66)\\nwhere\\n \\nPp\\nki\\nic\\nk\\n=\\n∈\\n∑\\n \\n(10-67)\\nand\\n \\nm\\nP\\nip\\nk\\nk\\ni\\nic\\nk\\n=\\n∈\\n∑\\n1\\n \\n(10-68)\\nAs before, \\nm\\nG\\n is the global mean given in Eq. (10-54). The \\nK\\n classes are separated \\nby \\nK\\n−\\n1\\n thresholds whose values, \\nkk k\\nK\\n12 1\\n∗∗\\n−\\n∗\\n,,, ,\\n…\\n are the values that maximize Eq. \\n(10-66):\\n \\nss\\nBK\\nkk k L\\nBK\\nkk k\\nkk k\\nK\\n2\\n12 1\\n01\\n2\\n12 1\\n12\\n∗∗\\n−\\n∗\\n<<< < −\\n−\\n(\\n)\\n=\\n()\\n,,, m a x ,,\\n……\\n…\\n \\n(10-69)\\nAlthough this result is applicable to an arbitrary number of classes, it begins to lose \\nmeaning as the number of classes increases because we are dealing with only one \\nvariable (intensity).\\n In fact, the between-class variance usually is cast in terms of \\nmultiple variables expressed as vectors (Fukunaga [1972]). In practice, using mul-\\ntiple global thresholding is considered a viable approach when there is reason to \\nbelieve that the problem can be solved effectively with two thresholds. Applications \\nthat require more than two thresholds generally are solved using more than just \\nintensity values. Instead, the approach is to use additional descriptors (e.g., color) \\nand the application is cast as a pattern recognition problem, as you will learn shortly \\nin the discussion on multivariable thresholding.\\nIn applications involving \\nmore than one variable \\n(for example the RGB \\ncomponents of a color \\nimage), thresholding can \\nbe implemented using a \\ndistance measure, such \\nas the \\nEuclidean distance\\n, \\nor \\nMahalanobis distance\\n \\ndiscussed in Section 6.7 \\n(see Eqs. (6-48), (6-49), \\nand Example 6.15).\\nFIGURE 10.41\\nImage in Fig. \\n10.40(a) segmented \\nusing the same \\nprocedure as \\nexplained in Figs. \\n10.40(d) through \\n(f), but using a \\nlower value to \\nthreshold the \\nabsolute Laplacian \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   758\\n6/16/2017   2:13:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 759}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n759\\nFor three classes consisting of three intensity intervals (which are separated by \\ntwo thresholds), the between-class variance is given by:\\n \\ns\\nBG G G\\nPm m Pm m Pm m\\n2\\n11\\n2\\n22\\n2\\n33\\n2\\n=−\\n()\\n+−\\n()\\n+−\\n()\\n \\n(10-70)\\nwhere\\n \\nPp\\nPp\\nPp\\ni\\ni\\nk\\ni\\nik\\nk\\ni\\nik\\nL\\n1\\n0\\n2\\n1\\n3\\n1\\n1\\n1\\n1\\n2\\n2\\n=\\n=\\n=\\n=\\n=+\\n=+\\n−\\n∑\\n∑\\n∑\\n \\n(10-71)\\nand\\n \\nm\\nP\\nip\\nm\\nP\\nip\\nm\\nP\\nip\\ni\\ni\\nk\\ni\\nik\\nk\\ni\\nik\\nL\\n1\\n1\\n0\\n2\\n2\\n1\\n3\\n3\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n=\\n=\\n=\\n=\\n=+\\n=+\\n−\\n∑\\n∑\\n∑\\n \\n(10-72)\\nAs in Eqs. (10-55) and (10-56), the following relationships hold:\\n \\nPm Pm Pm m\\nG\\n11 22 33\\n++=\\n \\n(10-73)\\nand\\n \\nPPP\\n123\\n1\\n++=\\n \\n(10-74)\\nWe see from Eqs. (10-71) and (10-72) that \\nP\\n and \\nm,\\n and therefore \\ns\\nB\\n2\\n,\\n are functions \\nof \\nk\\n1\\n and \\nk\\n2\\n.\\n The two optimum threshold values, \\nk\\n1\\n*\\n and \\nk\\n2\\n*\\n,\\n are the values that maxi-\\nmize \\ns\\nB\\nkk\\n2\\n12\\n(, ) .\\n That is, as indicated in Eq. (10-69), we find the optimum thresholds \\nby finding\\n \\nss\\nB\\nkkL\\nB\\nkk\\nkk\\n2\\n12\\n01\\n2\\n12\\n12\\n∗∗\\n<<< −\\n()\\n=\\n()\\n, max ,\\n \\n(10-75)\\nThe procedure starts by selecting the first value of \\nk\\n1\\n (that value is 1 because look-\\ning for a threshold at 0 intensity makes no sense; also, keep in mind that the incre-\\nment values are integers because we are dealing with integer intensity values). \\nNext, \\nk\\n2\\n is incremented through all its values greater than \\nk\\n1\\n and less than \\nL\\n−\\n1 \\n(i.e\\n., \\nkk L\\n21\\n12\\n=+ −\\n,, ) .\\n…\\n Then, \\nk\\n1\\n is incremented to its next value and \\nk\\n2\\n is incre-\\nmented again through all its values greater than \\nk\\n1\\n.\\n This procedure is repeated \\nuntil \\nkL\\n1\\n3\\n=−\\n.\\n The result of this procedure is a 2-D array, \\ns\\nB\\nkk\\n2\\n12\\n,,\\n()\\n and the last \\nstep is to look for the maximum value in this array. The values of \\nk\\n1\\n and \\nk\\n2\\n cor-\\nresponding to that maximum in the array are the optimum thresholds, \\nk\\n1\\n*\\n and \\nk\\n2\\n*\\n. \\nRecall from the  \\ndiscussion of the \\nCanny edge detec-\\ntor that thresholding \\nwith two thresholds is \\nreferred to as \\nhysteresis \\nthresholding\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   759\\n6/16/2017   2:13:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 760}),\n",
       " Document(page_content='760\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nIf there are several maxima, the corresponding values of \\nk\\n1\\n and \\nk\\n2\\n are averaged to \\nobtain the final thresholds. The thresholded image is then given by\\n \\ngxy\\naf\\nx y k\\nbk f x y k\\ncf x y k\\n(,)\\n(,)\\n(,)\\n(,)\\n*\\n**\\n*\\n=<\\n>\\n⎧\\n⎨\\n⎪\\n⎪\\n⎩\\nif \\nif \\nif \\n≤\\n≤\\n1\\n12\\n2\\n⎪ ⎪\\n⎪\\n \\n(10-76)\\nwhere \\na\\n, \\nb\\n,\\n and \\nc\\n are any three distinct intensity values.\\nFinally, the separability measure deﬁned earlier for one threshold extends direct-\\nly to multiple thresholds:\\n \\nh\\ns\\ns\\nkk\\nkk\\nB\\nG\\n12\\n2\\n12\\n2\\n∗∗\\n∗∗\\n()\\n=\\n()\\n,\\n,\\n \\n(10-77)\\nwhere \\ns\\nG\\n2\\n is the total image variance from Eq. (10-58).\\nEXAMPLE 10.17 :  Multiple global thresholding.\\nFigure 10.42(a) shows an image of an iceberg. The objective of this example is to segment the image into \\nthree regions: the dark background, the illuminated area of the iceberg, and the area in shadows. It is \\nevident from the image histogram in Fig. 10.42(b) that two thresholds are required to solve this problem. \\nThe procedure discussed above resulted in the thresholds \\nk\\n1\\n80\\n∗\\n=\\n and \\nk\\n2\\n177\\n∗\\n=\\n,\\n which we note from \\nF\\nig. 10.45(b) are near the centers of the two histogram valleys. Figure 10.42(c) is the segmentation that \\nresulted using these two thresholds in Eq. (10-76). The separability measure was 0.954. The principal \\nreason this example worked out so well can be traced to the histogram having three distinct modes \\nseparated by reasonably wide, deep valleys. But we can do even better using superpixels, as you will see \\nin Section 10.5.\\n0 63 127 191 255\\nb a\\nc\\nFIGURE 10.42\\n (a) Image of an iceberg. (b) Histogram. (c) Image segmented into three regions using dual Otsu thresholds. \\n(Original image courtesy of NOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   760\\n6/16/2017   2:13:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 761}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n761\\nVARIABLE THRESHOLDING\\nAs discussed earlier in this section, factors such as noise and nonuniform illumina-\\ntion play a major role in the performance of a thresholding algorithm. We showed \\nthat image smoothing and the use of edge information can help significantly. How-\\never, sometimes this type of preprocessing is either impractical or ineffective in \\nimproving the situation, to the point where the problem cannot be solved by any \\nof the thresholding methods discussed thus far. In such situations, the next level of \\nthresholding complexity involves variable thresholding, as we will illustrate in the \\nfollowing discussion.\\nVariable Thresholding Based on Local Image Properties\\nA basic approach to variable thresholding is to compute a threshold at every point, \\n(,) ,\\nxy\\n in the image based on one or more specified properties in a neighborhood \\nof \\n(,) .\\nxy\\n Although this may seem like a laborious process, modern algorithms and \\nhardware allow for fast neighborhood processing\\n, especially for common functions \\nsuch as logical and arithmetic operations.\\nWe illustrate the approach using the mean and standard deviation of the pixel \\nvalues in a neighborhood of every point in an image. These two quantities are use-\\nful for determining local thresholds because, as you know from\\n \\nChapter 3, they are \\ndescriptors of average intensity and contrast.\\n Let \\nm\\nxy\\n and \\ns\\nxy\\n denote the mean and \\nstandard deviation of the set of pixel values in a neighborhood, \\nS\\nxy\\n,\\n centered at \\ncoordinates \\n(,)\\nxy\\n in an image (see Section 3.3 regarding computation of the local \\nmean and standard deviation).\\n The following are common forms of variable thresh-\\nolds based on the local image properties:\\n \\nTa b m\\nxy xy xy\\n=+\\ns\\n \\n(10-78)\\nwhere \\na\\n and \\nb\\n are nonnegative constants, and\\n \\nTa b m\\nxy xy G\\n=+\\ns\\n \\n(10-79)\\nwhere \\nm\\nG\\n is the global image mean. The segmented image is computed as\\n \\ngxy\\nfx\\nyT\\nfx\\ny T\\nxy\\nxy\\n(,)\\n(,)\\n(,)\\n=\\n>\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n0\\nif  \\nif  \\n≤\\n \\n(10-80)\\nwhere \\nfx y\\n(,\\n)\\n is the input image. This equation is evaluated for all pixel locations \\nin the image\\n, and a different threshold is computed at each location \\n(,)\\nxy\\n using the \\npixels in the neighborhood \\nS\\nxy\\n.\\nSigniﬁcant power (with a modest increase in computation) can be added to vari-\\nable thresholding by using predicates based on the parameters computed in the neigh-\\nborhood of a point \\n(,) :\\nxy\\n \\ngxy\\nQ\\nQ\\n(,\\n)\\n=\\n1\\n0\\nif (local parameters) is TRUE\\nif (local parameter\\nrs) is FALSE\\n⎧\\n⎨\\n⎩\\n \\n(10-81)\\nWe simpliﬁed the nota-\\ntion slightly from the \\nform we used in  \\nEqs. (3-27) and (3-28) by  \\nletting \\nxy\\n imply a  \\nneighborhood \\nS\\n, centered \\nat coordinates (\\nx\\n, \\ny\\n).\\nNote that \\nT\\nxy\\n is a \\nthreshold \\narray\\n of the \\nsame size as the image \\nfrom which it was \\nobtained. The threshold \\nat a location (\\nx\\n, \\ny\\n) in the \\narray is used to segment \\nthe value of an image at \\nthat location.\\nDIP4E_GLOBAL_Print_Ready.indb   761\\n6/16/2017   2:13:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 762}),\n",
       " Document(page_content='762\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nwhere \\nQ\\n is a \\npredicate\\n based on parameters computed using the pixels in neighbor\\n-\\nhood \\nS\\nxy\\n.\\n For example, consider the following predicate, \\nQm\\nxy xy\\ns\\n,,\\n()\\n based on the \\nlocal mean and standard deviation:\\n \\nQm\\nfx\\ny a fx\\nyb m\\nxy xy\\nxy xy\\ns\\ns\\n,\\n(,\\n)(\\n,\\n)\\n()\\n=\\n>>\\nTRUE if  AND  \\nFALSE otherwise\\ne\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(10-82)\\nNote that Eq. (10-80) is a special case of Eq. (10-81), obtained by letting \\nQ\\n be TRUE \\nif \\nfx y T\\nxy\\n(,)\\n>\\n and FALSE otherwise. In this case, the predicate is based simply on \\nthe intensity at a point.\\nEXAMPLE 10.18 : Variable thresholding based on local image properties.\\nFigure 10.43(a) shows the yeast image from Example 10.16. This image has three predominant inten-\\nsity levels, so it is reasonable to assume that perhaps dual thresholding could be a good segmentation \\napproach. Figure 10.43(b) is the result of using the dual thresholding method summarized in Eq. (10-76). \\nAs the ﬁgure shows, it was possible to isolate the bright areas from the background, but the mid-gray \\nregions on the right side of the image were not segmented (i.e., separated) properly. To illustrate the use \\nb a\\nd c\\nFIGURE 10.43\\n(a) Image from \\nFig. 10.40.  \\n(b) Image  \\nsegmented using \\nthe dual  \\nthresholding  \\napproach given \\nby Eq. (10-76). \\n(c) Image of local \\nstandard  \\ndeviations.  \\n(d) Result  \\nobtained using  \\nlocal thresholding.\\nDIP4E_GLOBAL_Print_Ready.indb   762\\n6/16/2017   2:13:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 763}),\n",
       " Document(page_content='10.3\\n  \\nThresholding\\n    \\n763\\nof local thresholding, we computed the local standard deviation \\ns\\nxy\\n for all \\n(,)\\nxy\\n in the input image using \\na neighborhood of size \\n33\\n×\\n.\\n Figure 10.43(c) shows the result. Note how the faint outer lines correctly \\ndelineate the boundaries of the cells\\n. Next, we formed a predicate of the form shown in Eq. (10-82), but \\nusing the global mean instead of \\nm\\nxy\\n.\\n Choosing the global mean generally gives better results when the \\nbackground is nearly constant and all the object intensities are above or below the background intensity\\n. \\nThe values \\na\\n=\\n30\\n and \\nb\\n=\\n15\\n.\\n were used to complete the speciﬁcation of the predicate (these values \\nwere determined experimentally\\n, as is usually the case in applications such as this). The image was then \\nsegmented using Eq. (10-82). As Fig. 10.43(d) shows, the segmentation was quite successful. Note in par-\\nticular that all the outer regions were segmented properly, and that most of the inner, brighter regions \\nwere isolated correctly.\\nVariable Thresholding Based on Moving Averages\\nA special case of the variable thresholding method discussed in the previous sec-\\ntion is based on computing a moving average along scan lines of an image. This \\nimplementation is useful in applications such as document processing, where speed \\nis a fundamental requirement. The scanning typically is carried out line by line in a \\nzigzag pattern to reduce illumination bias. Let \\nz\\nk\\n+\\n1\\n denote the intensity of the point \\nencountered in the scanning sequence at step \\nk\\n+\\n1.\\n The moving average (mean \\nintensity) at this new point is given by\\n \\nmk\\nn\\nzk n\\nmk\\nn\\nzz k n\\ni\\nik n\\nk\\nkk n\\n()\\n()\\n+=\\n−\\n=+ −\\n(\\n)\\n+\\n=+−\\n+\\n+−\\n∑\\n1\\n1\\n1\\n1\\n2\\n1\\n1\\nfor \\nfor \\n≥\\n≥\\n1\\n1 \\n \\n(10-83)\\nwhere \\nn\\n is the number of points used in computing the average\\n, and \\nmz\\n()\\n.\\n1\\n1\\n=\\n The \\nconditions imposed on \\nk\\n are so that all subscripts on \\nz\\nk\\n are positive. All this means \\nis that \\nn\\n points must be available for computing the average. When \\nk\\n is less than the \\nlimits shown (this happens near the image borders) the averages are formed with \\nthe available image points. Because a moving average is computed for every point \\nin the image, segmentation is implemented using Eq. (10-80) with \\nTc m\\nxy xy\\n=\\n,\\n where \\nc\\n is positive scalar\\n, and \\nm\\nxy\\n is the moving average from Eq. (10-83) at point \\n(,)\\nxy\\n in \\nthe input image\\n. \\nEXAMPLE 10.19 :  Document thresholding using moving averages.\\nFigure 10.44(a) shows an image of handwritten text shaded by a spot intensity pattern. This form of \\nintensity shading is typical of images obtained using spot illumination (such as a photographic ﬂash). \\nFigure 10.44(b) is the result of segmentation using the Otsu global thresholding method. It is not unex-\\npected that global thresholding could not overcome the intensity variation because the method gener-\\nally performs poorly when the areas of interest are embedded in a nonuniform illumination ﬁeld. Figure \\n10.44(c) shows successful segmentation with local thresholding using moving averages. For images of \\nwritten material, a rule of thumb is to let \\nn\\n equal ﬁve times the average stroke width. In this case, the \\naverage width was 4 pixels, so we let \\nn\\n=\\n20 in Eq. (10-83) and used \\nc\\n=\\n05\\n..\\nDIP4E_GLOBAL_Print_Ready.indb   763\\n6/16/2017   2:13:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 764}),\n",
       " Document(page_content='764\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nAs another illustration of the effectiveness of this segmentation approach, we used the same param-\\neters as in the previous paragraph to segment the image in Fig. 10.45(a), which is corrupted by a sinu-\\nsoidal intensity variation typical of the variations that may occur when the power supply in a document \\nscanner is not properly grounded. As Figs. 10.45(b) and (c) show, the segmentation results are compa-\\nrable to those in Fig. 10.44.\\nNote that successful segmentation results were obtained in both cases using the same values for \\nn\\n \\nand \\nc\\n, which shows the relative ruggedness of the approach. In general, thresholding based on moving \\naverages works well when the objects of interest are small (or thin) with respect to the image size, a \\ncondition satisﬁed by images of typed or handwritten text.\\n10.4   SEGMENTATION BY REGION GROWING AND BY REGION \\nSPLITTING AND MERGING \\nAs we discussed in Section 10.1, the objective of segmentation is to partition an \\nimage into regions. In Section 10.2, we approached this problem by attempting to \\nfind boundaries between regions based on discontinuities in intensity levels, where-\\nas in Section 10.3, segmentation was accomplished via thresholds based on the dis-\\ntribution of pixel properties, such as intensity values or color. In this section and in \\nSections 10.5 and 10.6, we discuss segmentation techniques that find the regions \\ndirectly. In Section 10.7,\\n \\nwe will discuss a method that finds the regions and their \\nboundaries simultaneously\\n. \\nREGION GROWING\\nAs its name implies, \\nregion growing\\n is a procedure that groups pixels or subregions \\ninto larger regions based on predefined criteria for growth. The basic approach is to \\nstart with a set of “seed” points, and from these grow regions by appending to each \\nseed those neighboring pixels that have predefined properties similar to the seed \\n(such as ranges of intensity or color).\\nSelecting a set of one or more starting points can often be based on the nature of \\nthe problem, as we show later in Example 10.20. When a priori information is not \\n10.4\\nYou should review the \\nterminology introduced \\nin Section 10.1 before \\nproceeding.\\nb a\\nc\\nFIGURE 10.44\\n (a) Text image corrupted by spot shading. (b) Result of global thresholding using Otsu’s method. \\n \\n(c) Result of local thresholding using moving averages.\\nDIP4E_GLOBAL_Print_Ready.indb   764\\n6/16/2017   2:13:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 765}),\n",
       " Document(page_content='10.4\\n  \\nSegmentation by Region Growing and by Region Splitting and Merging\\n    \\n765\\navailable, the procedure is to compute at every pixel the same set of properties that \\nultimately will be used to assign pixels to regions during the growing process. If the \\nresult of these computations shows clusters of values, the pixels whose properties \\nplace them near the centroid of these clusters can be used as seeds.\\nThe selection of similarity criteria depends not only on the problem under con-\\nsideration, but also on the type of image data available. For example, the analysis of \\nland-use satellite imagery depends heavily on the use of color. This problem would \\nbe signiﬁcantly more difﬁcult, or even impossible, to solve without the inherent infor-\\nmation available in color images. When the images are monochrome, region analysis \\nmust be carried out with a set of descriptors based on intensity levels and spatial \\nproperties (such as moments or texture). We will discuss descriptors useful for region \\ncharacterization in Chapter 11.\\nDescriptors alone can yield misleading results if connectivity properties are not \\nused in the region-growing process. For example, visualize a random arrangement of \\npixels that have three distinct intensity values. Grouping pixels with the same inten-\\nsity value to form a “region,” without paying attention to connectivity, would yield a \\nsegmentation result that is meaningless in the context of this discussion.\\nAnother problem in region growing is the formulation of a stopping rule. Region \\ngrowth should stop when no more pixels satisfy the criteria for inclusion in that \\nregion. Criteria such as intensity values, texture, and color are local in nature and \\ndo not take into account the “history” of region growth. Additional criteria that can \\nincrease the power of a region-growing algorithm utilize the concept of size, like-\\nness between a candidate pixel and the pixels grown so far (such as a comparison of \\nthe intensity of a candidate and the average intensity of the grown region), and the \\nshape of the region being grown. The use of these types of descriptors is based on \\nthe assumption that a model of expected results is at least partially available.\\nLet: \\nfx y\\n(,\\n)\\n denote an input image; \\nSxy\\n(,\\n)\\n denote a \\nseed\\n array containing 1’\\ns \\nat the locations of seed points and 0’s elsewhere; and \\nQ\\n denote a \\npredicate\\n to be \\napplied at each location \\n(,) .\\nxy\\n Arrays \\nf\\n and \\nS\\n are assumed to be of the same size. \\nA basic region-growing algorithm based on 8-connectivity may be stated as follows\\n.\\nb a\\nc\\nFIGURE 10.45\\n (a) Text image corrupted by sinusoidal shading. (b) Result of global thresholding using Otsu’s method. \\n(c) Result of local thresholding using moving averages..\\nDIP4E_GLOBAL_Print_Ready.indb   765\\n6/16/2017   2:13:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 766}),\n",
       " Document(page_content='766\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n1. \\nFind all connected components in \\nSxy\\n(,\\n)\\n and reduce each connected component \\nto one pixel;\\n label all such pixels found as 1. All other pixels in \\nS\\n are labeled 0.\\n2. \\nForm an image \\nf\\nQ\\n such that, at each point \\n(,) ,\\nxy\\n \\nfx y\\nQ\\n(,)\\n=\\n1\\n if the input image \\nsatisﬁes a given predicate\\n, \\nQ\\n, at those coordinates, and \\nfx y\\nQ\\n(,)\\n=\\n0 otherwise.\\n3. \\nLet \\ng\\n be an image formed by appending to each seed point in \\nS\\n all the 1-valued \\npoints in \\nf\\nQ\\n that are 8-connected to that seed point.\\n4. \\nLabel each connected component in \\ng\\n with a different region label (e.g.,integers \\nor letters).\\n This is the segmented image obtained by region growing.\\nThe following example illustrates the mechanics of this algorithm.\\nEXAMPLE 10.20 :  Segmentation by region growing.\\nFigure 10.46(a) shows an 8-bit X-ray image of a weld (the horizontal dark region) containing several \\ncracks and porosities (the bright regions running horizontally through the center of the image). We illus-\\ntrate the use of region growing by segmenting the defective weld regions. These regions could be used \\nin applications such as weld inspection, for inclusion in a database of historical studies, or for controlling \\nan automated welding system.\\nThe ﬁrst thing we do is determine the seed points. From the physics of the problem, we know that \\ncracks and porosities will attenuate X-rays considerably less than solid welds, so we expect the regions \\ncontaining these types of defects to be signiﬁcantly brighter than other parts of the X-ray image. We \\ncan extract the seed points by thresholding the original image, using a threshold set at a high percen-\\ntile. Figure 10.46(b) shows the histogram of the image, and Fig. 10.46(c) shows the thresholded result \\nobtained with a threshold equal to the 99.9 percentile of intensity values in the image, which in this case \\nwas 254 (see Section 10.3 regarding percentiles). Figure 10.46(d) shows the result of morphologically \\neroding each connected component in Fig. 10.46(c) to a single point.\\nNext, we have to specify a predicate. In this example, we are interested in appending to each seed \\nall the pixels that (a) are 8-connected to that seed, and (b) are “similar” to it. Using absolute intensity \\ndifferences as a measure of similarity, our predicate applied at each location \\n(,)\\nxy\\n is\\n \\nQ\\n=\\nTRUE\\nif the absolute difference of intensities\\nbetween the \\n seed and the pixel at  is \\nFALSE otherwise\\n(,)\\nxy T\\n≤\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\nwhere \\nT\\n is a speciﬁed threshold. Although this predicate is based on intensity differences and uses a \\nsingle threshold,\\n we could specify more complex schemes in which a different threshold is applied to \\neach pixel, and properties other than differences are used. In this case, the preceding predicate is suf-\\nﬁcient to solve the problem, as the rest of this example shows.\\nFrom the previous paragraph, we know that all seed values are 255 because the image was thresh-\\nolded with a threshold of 254. Figure 10.46(e) shows the difference between the seed value (255) and \\nFig. 10.46(a). The image in Fig. 10.46(e) contains all the differences needed to compute the predicate at \\neach location \\n(,) .\\nxy\\n Figure 10.46(f) shows the corresponding histogram. We need a threshold to use in \\nthe predicate to establish similarity\\n. The histogram has three principal modes, so we can start by apply-\\ning to the difference image the dual thresholding technique discussed in Section 10.3. The resulting two \\nthresholds in this case were \\nT\\n1\\n68\\n=\\n and \\nT\\n2\\n126\\n=\\n,\\n which we see correspond closely to the valleys of \\nthe histogram.\\n (As a brief digression, we segmented the image using these two thresholds. The result in \\nSee Sections 2.5 and 9.5 \\nregarding connected \\ncomponents, and  \\nSection 9.2 regarding \\nerosion.\\nDIP4E_GLOBAL_Print_Ready.indb   766\\n6/16/2017   2:13:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 767}),\n",
       " Document(page_content='10.4\\n  \\nSegmentation by Region Growing and by Region Splitting and Merging\\n    \\n767\\nFig. 10.46(g) shows that segmenting the defects cannot be accomplished using dual thresholds, despite \\nthe fact that the thresholds are in the deep valleys of the histogram.) \\nFigure 10.46(h) shows the result of thresholding the difference image with only \\nT\\n1\\n.\\n The black points \\nare the pixels for which the predicate was \\nTRUE; the others failed the predicate. The important result \\nhere is that the points in the good regions of the weld failed the predicate, so they will not be included \\nin the ﬁnal result. The points in the outer region will be considered by the region-growing algorithm as \\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\n \\nFigure \\n10.46\\n (a) X-ray image of a defective weld. (b) Histogram. (c) Initial seed image. (d) Final seed image (the \\npoints were enlarged for clarity). (e) Absolute value of the difference between the seed value (255) and (a). \\n \\n(f) Histogram of (e). (g) Difference image thresholded using dual thresholds. (h) Difference image thresholded with \\nthe smallest of the dual thresholds. (i) Segmentation result obtained by region growing. (Original image courtesy \\nof X-TEK Systems, Ltd.)\\n191 255\\n0\\n63 127\\n0 63 127 191 255\\nDIP4E_GLOBAL_Print_Ready.indb   767\\n6/16/2017   2:13:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 768}),\n",
       " Document(page_content='768\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\ncandidates. However, Step 3 will reject the outer points because they are not 8-connected to the seeds. \\nIn fact, as Fig. 10.46(i) shows, this step resulted in the correct segmentation, indicating that the use of \\nconnectivity was a fundamental requirement in this case. Finally, note that in Step 4 we used the same \\nvalue for all the regions found by the algorithm. In this case, it was visually preferable to do so because \\nall those regions have the same physical meaning in this application—they all represent porosities. \\nREGION SPLITTING AND MERGING\\nThe procedure just discussed grows regions from seed points. An alternative is to sub-\\ndivide an image initially into a set of disjoint regions and then merge and/or split the \\nregions in an attempt to satisfy the conditions of segmentation stated in Section 10.1. \\nThe basics of region splitting and merging are discussed next.\\nLet \\nR\\n represent the entire image region and select a predicate \\nQ\\n.\\n One approach \\nfor segmenting \\nR\\n is to subdivide it successively into smaller and smaller quadrant \\nregions so that,\\n for any region \\nRQ R\\nii\\n,() .\\n=\\nTRUE\\n We start with the entire region, \\nR\\n.\\nIf \\nQR\\n()\\n=\\nFALSE,\\n we divide the image into quadrants. If \\nQ\\n is FALSE for any \\nquadrant,\\n we subdivide that quadrant into sub-quadrants, and so on. This splitting \\ntechnique has a convenient representation in the form of so-called \\nquadtrees\\n; that \\nis, trees in which each node has exactly four descendants, as Fig. 10.47 shows (the \\nimages corresponding to the nodes of a quadtree sometimes are called \\nquadregions\\n \\nor \\nquadimages\\n). Note that the root of the tree corresponds to the entire image, and \\nthat each node corresponds to the subdivision of a node into four descendant nodes. \\nIn this case, only \\nR\\n4\\n was subdivided further.\\nIf only splitting is used, the ﬁnal partition normally contains adjacent regions with \\nidentical properties. This drawback can be remedied by allowing \\nmerging\\n as well as \\nsplitting. Satisfying the constraints of segmentation outlined in Section 10.1 requires \\nmerging only adjacent regions whose combined pixels satisfy the predicate \\nQ\\n.\\n That \\nis\\n, two adjacent regions \\nR\\nj\\n and \\nR\\nk\\n are merged only if \\nQR R\\njk\\n/H20668\\n()\\n=\\nTRUE.\\nThe preceding discussion can be summarized by the following procedure in which, \\nat any step\\n, we\\n1. \\nSplit into four disjoint quadrants any region \\nR\\ni\\n for which \\nQR\\ni\\n()\\n=\\nFALSE.\\n2. \\nWhen no further splitting is possible, merge any adjacent regions \\nR\\nj\\n and \\nR\\nk\\n for \\nwhich \\nQR R\\njk\\n/H20668\\n()\\n=\\nTRUE.\\nSee Section 2.5  \\nregarding region  \\nadjacency.\\nR\\n1\\nR\\n3\\nR\\n41\\nR\\n42\\nR\\n43\\nR\\n44\\nR\\n2\\nR\\n1\\nR\\n2\\nR\\n3\\nR\\nR\\n4\\nR\\n41\\nR\\n42\\nR\\n43\\nR\\n44\\nR\\nb a\\nFIGURE 10.47\\n(a) Partitioned \\nimage.  \\n(b) Corresponding \\nquadtree.  \\nR\\n represents \\nthe entire image \\nregion.\\nDIP4E_GLOBAL_Print_Ready.indb   768\\n6/16/2017   2:13:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 769}),\n",
       " Document(page_content='10.4\\n  \\nSegmentation by Region Growing and by Region Splitting and Merging\\n    \\n769\\n3. \\nStop when no further merging is possible.\\nNumerous variations of this basic theme are possible\\n. For example, a significant \\nsimplification results if in Step 2 we allow merging of any two adjacent regions \\nR\\nj\\n \\nand \\nR\\nk\\n if each one satisfies the predicate individually. This results in a much sim-\\npler (and faster) algorithm, because testing of the predicate is limited to individual \\nquadregions. As the following example shows, this simplification is still capable of \\nyielding good segmentation results.\\nEXAMPLE 10.21 :  Segmentation by region splitting and merging.\\nFigure 10.48(a) shows a \\n566 566\\n×\\n X-ray image of the Cygnus Loop supernova. The objective of this \\nexample is to segment (extract from the image) the \\n“ring” of less dense matter surrounding the dense \\ninner region. The region of interest has some obvious characteristics that should help in its segmenta-\\ntion. First, we note that the data in this region has a random nature, indicating that its standard devia-\\ntion should be greater than the standard deviation of the background (which is near 0) and of the large \\ncentral region, which is smooth. Similarly, the mean value (average intensity) of a region containing \\ndata from the outer ring should be greater than the mean of the darker background and less than the \\nmean of the lighter central region. Thus, we should be able to segment the region of interest using the \\nfollowing predicate:\\nb a\\nd c\\nFIGURE 10.48\\n(a) Image of the \\nCygnus Loop  \\nsupernova, taken \\nin the X-ray band \\nby NASA’s \\nHubble Telescope. \\n(b) through (d) \\nResults of limit-\\ning the smallest \\nallowed  \\nquadregion to be \\nof sizes of \\n32 32\\n×\\n, \\n16 16\\n×\\n, and \\n88\\n×\\n \\npixels\\n,  \\nrespectively. \\n(Original image \\ncourtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   769\\n6/16/2017   2:13:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 770}),\n",
       " Document(page_content='770\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n \\nQR\\nam\\nb\\nRR\\n()\\n=\\n>< <\\n⎧\\n⎨\\n⎩\\nTRUE if AND\\nFALSE otherwise\\ns\\n0\\nwhere \\ns\\nR\\n and \\nm\\nR\\nare the standard deviation and mean of the region being processed, and \\na\\n and \\nb\\n are \\nnonnegative constants\\n.\\nAnalysis of several regions in the outer area of interest revealed that the mean intensity of pixels \\nin those regions did not exceed 125, and the standard deviation was always greater than 10. Figures \\n10.48(b) through (d) show the results obtained using these values for \\na\\n and \\nb\\n,\\n and varying the minimum \\nsize allowed for the quadregions from 32 to 8.\\n The pixels in a quadregion that satisﬁed the predicate \\nwere set to white; all others in that region were set to black. The best result in terms of capturing the \\nshape of the outer region was obtained using quadregions of size \\n16 16\\n×\\n.\\n The small black squares in \\nF\\nig. 10.48(d) are quadregions of size \\n88\\n×\\n whose pixels did not satisfy the predicate. Using smaller \\nquadregions would result in increasing numbers of such black regions\\n. Using regions larger than the one \\nillustrated here would result in a more “block-like” segmentation. Note that in all cases the segmented \\nregion (white pixels) was a connected region that completely separates the inner, smoother region from \\nthe background. Thus, the segmentation effectively partitioned the image into three distinct areas that \\ncorrespond to the three principal features in the image: background, a dense region, and a sparse region. \\nUsing any of the white regions in Fig. 10.48 as a mask would make it a relatively simple task to extract \\nthese regions from the original image (see Problem 10.43). As in Example 10.20, these results could not \\nhave been obtained using edge- or threshold-based segmentation.\\nAs used in the preceding example, properties based on the mean and standard \\ndeviation of pixel intensities in a region attempt to quantify the texture of the region \\n(see Section 11.3 for a discussion on texture). The concept of texture segmentation \\nis based on using measures of texture in the predicates. In other words, we can per-\\nform texture segmentation by any of the methods discussed in this section simply by \\nspecifying predicates based on texture content.\\n10.5  REGION SEGMENTATION USING CLUSTERING AND  \\nSUPERPIXELS  \\nIn this section, we discuss two related approaches to region segmentation. The first \\nis a classical approach based on seeking clusters in data, related to such variables as \\nintensity and color. The second approach is significantly more modern, and is based \\non using clustering to extract “superpixels” from an image.\\nREGION SEGMENTATION USING K-MEANS CLUSTERING\\nThe basic idea behind the clustering approach used in this chapter is to partition a \\nset, \\nQ\\n, of observations into a specified number, \\nk\\n, of clusters. In \\nk\\n-means clustering, \\neach observation is assigned to the cluster with the nearest mean (hence the name \\nof the method), and each mean is called the \\nprototype\\n of its cluster. A \\nk-means algo-\\nrithm\\n is an iterative procedure that successively refines the means until convergence \\nis achieved.\\nLet \\n{, , , }\\nzz\\nz\\n12\\n…\\nQ\\n be set of vector observations (samples). These vectors have \\nthe form\\n10.5\\nA more general form of \\nclustering is  \\nunsupervised clustering\\n, \\nin which a clustering \\nalgorithm attempts to \\nﬁnd a meaningful set of \\nclusters in a given set \\nof samples. We do not \\naddress this topic, as \\nour focus in this brief \\nintroduction is only to \\nillustrate how \\nsupervised \\nclustering\\n is used for \\nimage segmentation.\\nDIP4E_GLOBAL_Print_Ready.indb   770\\n6/16/2017   2:13:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 771}),\n",
       " Document(page_content='10.5\\n  \\nRegion Segmentation Using Clustering and Superpixels\\n    \\n771\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nz\\nz\\nz\\nn\\n1\\n2\\n/vertellipsis\\n \\n(10-84)\\nIn image segmentation, each component of a vector \\nz\\n represents a numerical pixel \\nattribute\\n. For example, if segmentation is based on just grayscale intensity, then \\nz\\n=\\nz\\n \\nis a scalar representing the intensity of a pixel.\\n If we are segmenting RGB color \\nimages, \\nz\\n typically is a 3-D vector, each component of which is the intensity of a pixel \\nin one of the three primary color images, as we discussed in Chapter 6. The objec-\\ntive of \\nk\\n-means clustering is to partition the set \\nQ\\n of observations into \\nkk Q\\n()\\n≤\\n \\ndisjoint cluster sets \\nCC C C\\nk\\n=\\n{, , , } ,\\n12\\n…\\n so that the following criterion of optimality \\nis satisfied:\\n†\\n \\narg min\\nC\\ni\\nC i\\nk\\ni\\nab\\nzm\\nz\\n−\\n∈ =\\n∑ ∑\\n2\\n1\\n \\n(10-85)\\nwhere \\nm\\ni\\n is the \\nmean vector\\n (or \\ncentroid\\n) of the samples in set \\nC\\ni\\n and \\narg\\n is the vec-\\ntor norm of the argument.\\n Typically, the Euclidean norm is used, so the term \\nzm\\n−\\ni\\n \\nis the familiar \\nEuclidean distance\\n from a sample in \\nC\\ni\\n to mean \\nm\\ni\\n.\\n In words, this \\nequation says that we are interested in finding the sets \\nCC C C\\nk\\n=\\n{, , , }\\n12\\n…\\n such that \\nthe \\nsum of the distances\\n from each point in a set to the mean of that set is minimum.\\nUnfortunately\\n, ﬁnding this minimum is an NP-hard problem for which no practi-\\ncal solution is known. As a result, a number of heuristic methods that attempt to ﬁnd \\napproximations to the minimum have been proposed over the years. In this section, \\nwe discuss what is generally considered to be the “standard” \\nk\\n-means algorithm, \\nwhich is based on the Euclidean distance (see Section 2.6). Given a set \\n{, , , }\\nzz\\nz\\n12\\n…\\nQ\\n \\nof vector observation and a speciﬁed value of \\nk\\n, the algorithm is as follows:\\n1. \\nInitialize the algorithm:\\n Specify an initial set of means\\n, \\nm\\ni\\n() ,\\n1  \\nik\\n=\\n12\\n,,\\n,.\\n…\\n2. \\nAssign samples to clusters:\\n \\nAssign each sample to the cluster set whose mean \\nis the closest (ties are resolved arbitrarily, but samples are assigned to only \\none\\n \\ncluster):\\n      \\nzz m z m\\nqi q i q j\\nCj\\nk\\nj\\ni\\nq\\nQ\\n→− < − = =\\nif\\n/H20648/H20648 /H20648/H20648\\n22\\n12\\n12\\n,, , ( ) ; ,, ,\\n……\\n≠\\n3. \\nUpdate the cluster centers (means)\\n:\\n   \\nmz\\nz\\ni\\ni\\nC\\nC\\nik\\ni\\n==\\n∈\\n∑\\n1\\n12\\n,, ,\\n…\\n \\nwhere \\nC\\ni\\n is the number of samples in cluster set \\nC\\ni\\n.\\n4. \\nTest for completion:\\n Compute the Euclidean norms of the differences between \\nthe mean vectors in the current and previous steps\\n. Compute the residual error, \\nE\\n, as the sum of the \\nk\\n norms. Stop if \\nET\\n≤\\n,\\n where \\nT\\n a speciﬁed,\\n nonnegative \\nthreshold. Else, go back to Step 2.\\n†\\n   Remember, \\nmin ( )\\nx\\nhx\\n(\\n)\\n is the minimum of \\nh\\n with respected to \\nx\\n, whereas \\narg min ( )\\nx\\nhx\\n(\\n)\\n is the value (or values) \\nof \\nx\\n at which \\nh\\n is minimum.\\nThese initial means are \\nthe initial cluster centers. \\nThey are also called \\nseeds.\\nDIP4E_GLOBAL_Print_Ready.indb   771\\n6/16/2017   2:13:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 772}),\n",
       " Document(page_content='772\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nWhen \\nT\\n=\\n0,\\n this algorithm is known to converge in a finite number of iterations \\nto a local minimum.\\n It is not guaranteed to yield the global minimum required to \\nminimize Eq. (10-85). The result at convergence does depend on the initial values \\nchosen for \\nm\\ni\\n.\\n An approach used frequently in data analysis is to specify the initial \\nmeans as \\nk\\n randomly chosen samples from the given sample set,\\n and to run the \\nalgorithm several times, with a new random set of initial samples each time. This is \\nto test the “stability” of the solution. In image segmentation, the important issue is \\nthe value selected for \\nk\\n because this determines the number of segmented regions; \\nthus, multiple passes are rarely used.\\nEXAMPLE 10.22 :  Using k-means clustering for segmentation.\\nFigure 10.49(a) shows an image of size \\n688 688\\n×\\n pixels, and Fig. 10.49(b) is the segmentation obtained \\nusing the \\nk\\n-means algorithm with \\nk\\n=\\n3.\\n As you can see, the algorithm was able to extract all the mean-\\ningful regions of this image with high accurac\\ny. For example, compare the quality of the characters in \\nboth images. It is important to realize that the entire segmentation was done by clustering of a single \\nvariable (intensity). Because \\nk\\n-means works with vector observations in general, its power to discrimi-\\nnate between regions increases as the number of components of vector \\nz\\n in Eq. (10-84) increases.\\nREGION SEGMENTATION USING SUPERPIXELS\\nThe idea behind \\nsuperpixels\\n is to replace the standard pixel grid by grouping pixels \\ninto primitive regions that are more perceptually meaningful than individual pixels. \\nThe objectives are to lessen computational load, and to improve the performance of \\nsegmentation algorithms by reducing irrelevant detail. A simple example will help \\nexplain the basic approach of superpixel representations.\\nFigure 10.50(a) shows an image of size \\n600 800\\n×\\n (480,000) pixels containing \\nvarious levels of detail that could be described verbally as:\\n “This is an image of two \\nlarge carved ﬁgures in the foreground, and at least three, much smaller, carved ﬁg-\\nures resting on a fence behind the large ﬁgures. The ﬁgures are on a beach, with \\nb a\\nFIGURE 10.49\\n(a) Image of size \\n688 688\\n×\\n pixels.  \\n(b) Image  \\nsegmented using \\nthe \\nk\\n-means  \\nalgorithm with \\nk\\n=\\n3.\\nDIP4E_GLOBAL_Print_Ready.indb   772\\n6/16/2017   2:13:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 773}),\n",
       " Document(page_content='10.5\\n  \\nRegion Segmentation Using Clustering and Superpixels\\n    \\n773\\nthe ocean and sky in the background.” Figure 10.50(b) shows the same image rep-\\nresented by 4,000 superpixels and their boundaries (the boundaries are shown for \\nreference—they are not part of the data), and Fig. 10.50(c) shows the superpixel \\nimage. One could argue that the level of detail in the superpixel image would lead \\nto the same description as the original, but the former contains only 4,000 primitive \\nunits, as opposed to 480,000 in the original. Whether the superpixel representation \\nis “adequate” depends on the application. If the objective is to describe the image \\nat the level of detail mentioned above, then the answer is yes. On the other hand, if \\nthe objective is to detect imperfections at pixel-level resolutions, then the answer \\nobviously is no. And there are application, such as computerized medical diagnosis, \\nin which approximate representations of any kind are not acceptable. Nevertheless, \\nnumerous application areas, such as image-database queries, autonomous naviga-\\ntion, and certain branches of robotics, in which economy of implementation and \\npotential improvements in segmentation performance far outweigh any appreciable \\nloss of image detail.\\nOne important requirement of any superpixel representation is \\nadherence to bound-\\naries\\n. This means that boundaries between regions of interest must be preserved \\nin a superpixel image. We can see that this indeed is the case with the image in \\nFig. 10.50(c). Note, for example, how clear the boundaries between the ﬁgures and \\nthe background are. The same is true of the boundaries between the beach and the \\nocean, and between the ocean and the sky. Other important characteristics are the \\npreservations of topological properties and, of course, computational efﬁciency. The \\nsuperpixel algorithm discussed in this section meets these requirements.\\nAs another illustration, we show the results of severely decreasing the number of \\nsuperpixels to 1,000, 500, and 250. The results in Fig. 10.51, show a signiﬁcant loss of \\ndetail compared to Fig. 10.50(a), but the ﬁrst two images contain most of the detail \\nrelevant to the image description discussed earlier. A notable difference is that two \\nof the three small carvings on the fence in the back were eliminated. The 250-ele-\\nment superpixel image even lost the third. However, the boundaries between the \\nprincipal regions, as well as the basic topology of the images, were preserved.\\nFigures 10.50(b) and (c) \\nwere obtained using a \\nmethod to be discussed \\nlater in this section.\\nb a\\nc\\nFIGURE 10.50\\n (a) Image of size \\n600 480\\n×\\n (480,000) pixels. (b) Image composed of 4,000 superpixels (the boundaries \\nbetween superpixels (in white) are superimposed on the superpixel image for reference—the boundaries are not \\npart of the data).\\n (c) Superpixel image. (Original image courtesy of the U.S. National Park Services.).\\nDIP4E_GLOBAL_Print_Ready.indb   773\\n6/16/2017   2:13:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 774}),\n",
       " Document(page_content='774\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nSLIC Superpixel Algorithm\\nIn this section we discuss an algorithm for generating superpixels, called \\nsimple lin-\\near iterative clustering\\n (SLIC). This algorithm, developed by Achanta et al. [2012], \\nis conceptually simple, and has computational and other performance advantages \\nover other superpixels techniques. SLIC is a modification of the \\nk\\n-means algorithm \\ndiscussed in the previous section. SLIC observations typically use (but are not lim-\\nited to) 5-dimensional vectors containing three color components and two spatial \\ncoordinates. For example, if we are using the RGB color system, the 5-dimensional \\nvector associated with an image pixel has the form\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\nr\\ng\\nb\\nx\\ny\\n \\n(10-86)\\nwhere \\n(, , )\\nrg\\nb\\n are the three color components of a pixel, and \\n(,)\\nxy\\n are its two spatial \\ncoordinates\\n. Let \\nn\\nsp\\n denote the desired number of superpixels and let \\nn\\ntp\\n denote the \\ntotal number of pixels in the image. \\nThe initial superpixel centers, \\nm\\nii i i i i\\nT\\nrgbxy\\n=\\n[]\\n, \\nin\\nsp\\n=\\n12\\n,, , ,\\n…\\n are obtained by sampling the image on a regular grid spaced \\ns\\n units \\napart.\\n To generate superpixels approximately equal in size (i.e., area), the grid spac-\\nAs you will learn in \\nChapter 11, vectors  \\ncontaining image  \\nattributes are called \\nfeature vectors\\n.\\nFIGURE 10.51\\n Top row: Results of using 1,000, 500, and 250 superpixels in the representation of Fig. 10.50(a). As before, \\nthe boundaries between superpixels are superimposed on the images for reference. Bottom row: Superpixel images.\\nDIP4E_GLOBAL_Print_Ready.indb   774\\n6/16/2017   2:13:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 775}),\n",
       " Document(page_content='10.5\\n  \\nRegion Segmentation Using Clustering and Superpixels\\n    \\n775\\ning interval is selected as \\nsn n\\ntp sp\\n=\\n[] .\\n12\\n To prevent centering a superpixel on the \\nedge of the image, and to reduce the chances of starting at a noisy point, the initial \\ncluster centers are moved to the lowest gradient position in the \\n33\\n×\\n neighborhood \\nabout each center\\n.\\nThe SLIC superpixel algorithm consists of the following steps. Keep in mind that \\nsuperpixels are vectors in general. When we refer to a “pixel” in the algorithm, we \\nare referring to the \\n(,)\\nxy\\n \\nlocation\\n of the superpixel relative to the image\\n. \\n1. \\nInitialize the algorithm:\\n Compute the initial superpixel cluster centers\\n, \\n \\nm\\nii i i i i\\nT\\nsp\\nrgbxy i n\\n=\\n[]\\n=\\n,, , ,\\n12\\n…\\nby sampling the image at regular grid steps, \\ns\\n.\\n Move the cluster centers to the \\nlowest gradient position in a \\n33\\n×\\n neighborhood. For each pixel location, \\np\\n,\\n in \\nthe image, set a label \\nLp\\n()\\n=−\\n1 and a distance \\ndp\\n()\\n.\\n=\\n/H11009\\n \\n2. \\nAssign samples to cluster centers:\\n F\\nor each cluster center \\nm\\ni\\n, \\nin\\nsp\\n=\\n12\\n,, , ,\\n…\\n \\ncompute the distance\\n, \\nDp\\ni\\n()\\n between \\nm\\ni\\n and \\neach\\n pixel \\np\\n in a \\n22\\nss\\n×\\n neighbor-\\nhood about \\nm\\ni\\n.\\n Then, for each \\np\\n and \\nin\\nsp\\n=\\n12\\n,, , ,\\n…\\n if \\nD\\ndp\\ni\\n<\\n() ,\\n let \\ndp D\\ni\\n()\\n=\\n \\nand \\nLp i\\n()\\n.\\n=\\n3. \\nUpdate the cluster centers:\\n Let \\nC\\ni\\n denote the set of pixels in the image with \\nlabel \\nLp i\\n()\\n.\\n=\\n Update \\nm\\ni\\n:\\n \\nmz\\nz\\ni\\ni\\nC\\nsp\\nC\\nin\\ni\\n==\\n∈\\n∑\\n1\\n12\\n,, ,\\n…\\nwhere \\nC\\ni\\n is the number of pixels in set \\nC\\ni\\n, and the \\nz\\n’\\ns are given by Eq. (10-86).\\n4. \\nTest for convergence:\\n Compute the Euclidean norms of the differences between \\nthe mean vectors in the current and previous steps\\n. Compute the residual error, \\nE\\n, as the sum of the \\nn\\nsp\\n norms. If \\nET\\n<\\n,\\n where \\nT\\n a speciﬁed nonnegative thresh-\\nold,\\n go to Step 5. Else, go back to Step 2.\\n5. \\nPost-process the superpixel regions:\\n Replace all the superpixels in each region,\\n \\nC\\ni\\n, by their average value, \\nm\\ni\\n.\\nNote in Step 5 that superpixels end up as contiguous regions of constant value. The \\naverage value is not the only way to compute this constant,\\n but it is the most widely \\nused. For graylevel images, the average is just the average intensity of all the pixels \\nin the region spanned by the superpixel. This algorithm is similar to the \\nk\\n-means \\nalgorithm in the previous section, with the exceptions that the distances, \\nD\\ni\\n,\\n are not \\nspecified as Euclidean distances (see below),\\n and that these distances are computed \\nfor regions of size \\n22\\nss\\n×\\n,\\n rather than for all the pixels in the image, thus reduc-\\ning computation time significantly\\n. In practice, SLIC convergence with respect to \\nE\\n can be achieved with fairly large values of \\nT\\n. For example, all results reported by \\nAchanta et al. [2012] were obtained using \\nT\\n=\\n10.\\nDIP4E_GLOBAL_Print_Ready.indb   775\\n6/16/2017   2:14:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 776}),\n",
       " Document(page_content='776\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nSpecifying the Distance Measure\\nSLIC superpixels correspond to clusters in a space whose coordinates are colors \\nand spatial variables. It would be senseless to use a single Euclidean distance in this \\ncase, because the scales in the axes of this coordinate system are different and unre-\\nlated. In other words, spatial and color distances must be treated separately. This is \\naccomplished by normalizing the distance of the various components, then combin-\\ning them into a single measure. Let \\nd\\nc\\n and \\nd\\ns\\n denote the color and spatial Euclidean \\ndistances between two points in a cluster, respectively:\\n \\ndr r g g b b\\ncj i j i j i\\n=−+− +−\\n⎡\\n⎣\\n⎤\\n⎦\\n() ( ) ( )\\n22 2\\n12\\n \\n(10-87)\\nand\\n \\ndx x y y\\nsj i j i\\n=−+ −\\n⎡\\n⎣\\n⎤\\n⎦\\n() ()\\n22\\n12\\n \\n(10-88)\\nWe then define \\nD\\n as the \\ncomposite\\n distance\\n \\nD\\nd\\nd\\nd\\nd\\nc\\ncm\\ns\\nsm\\n=+\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nabab\\n2\\n2\\n12\\n \\n(10-89)\\nwhere \\nd\\ncm\\n and \\nd\\nsm\\n are the maximum expected values of \\nd\\nc\\n and \\nd\\ns\\n.\\n The maximum spa-\\ntial distance should correspond to the sampling interval;\\n that is, \\nds n n\\nsm\\ntp sp\\n==\\n[] .\\n12\\n \\nDetermining the maximum color distance is not as straightforward, because these \\ndistances can vary significantly from cluster to cluster, and from image to image. A \\nsolution is to set \\nd\\ncm\\n to a constant \\nc\\n so that Eq. (10-89) becomes\\n \\nD\\nd\\nc\\nd\\ns\\ncs\\n=+\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nab ab\\n2\\n2\\n12\\n \\n(10-90)\\nWe can write this equation as\\n \\nDd\\nd\\ns\\nc\\nc\\ns\\n=+\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n2\\n2\\n2\\n12\\nab\\n \\n(10-91)\\nThis is the distance measure used for each cluster in the algorithm. Constant \\nc\\n can be \\nused to weigh the relative importance between color similarity and spatial proximity\\n. \\nWhen \\nc\\n is large, spatial proximity is more important, and the resulting superpixels \\nare more compact. When \\nc\\n is small, the resulting superpixels adhere more tightly to \\nimage boundaries, but have less regular size and shape.\\nFor grayscale images, as in Example 10.23 below, we use\\n \\ndl l\\ncj i\\n=−\\n⎡\\n⎣\\n⎤\\n⎦\\n()\\n2\\n12\\n \\n(10-92)\\nDIP4E_GLOBAL_Print_Ready.indb   776\\n6/16/2017   2:14:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 777}),\n",
       " Document(page_content='10.6\\n  \\nRegion Segmentation Using Graph Cuts\\n    \\n777\\nin Eq. (10-91), where the \\nl’\\ns are intensity levels of the points for which the distance \\nis being computed. \\nIn 3-D, superpixels become \\nsupervoxels\\n, which are handled by deﬁning\\n \\ndx x y y z z\\nsj i j i j i\\n=−+ −+ −\\n⎡\\n⎣\\n⎤\\n⎦\\n() () ( )\\n22 2\\n12\\n \\n(10-93)\\nwhere the \\nz’\\ns are the coordinates of the third spatial dimension.\\n We must also add \\nthe third spatial variable, \\nz\\n, to the vector in Eq. (10-86).\\nBecause no provision is made in the algorithm to enforce connectivity, it is pos-\\nsible for isolated pixels to remain after convergence. These are assigned the label \\nof the nearest cluster using a connected components algorithm (see Section 9.6). \\nAlthough we explained the algorithm in the context of RGB color components, the \\nmethod is equally applicable to other colors systems. In fact, other components of \\nvector \\nz\\n in Eq. (10-86) (with the exception of the spatial variables) could be other \\nreal-valued feature values, provided that a meaningful distance measure can be \\ndeﬁned for them.\\nEXAMPLE 10.23 :  Using superpixels for image segmentation.\\nFigure 10.52(a) shows an image of an iceberg, and Fig. 10.52(b) shows the result of segmenting this \\nimage using the \\nk\\n-means algorithm developed in the last section, with \\nk\\n=\\n3.\\n Although the main regions \\nof the image were segmented,\\n there are numerous segmentation errors in both regions of the iceberg, \\nand also on the boundary separating it from the background. Errors are visible as isolated pixels (and \\nalso as small groups of pixels) with the wrong shade (e.g., black pixels within a white region). Figure \\n10.52(c) shows a 100-superpixel representation of the image with the superpixel boundaries superim-\\nposed for reference, and Fig. 10.52(d) shows the same image without the boundaries. Figure 10.52(e) is \\nthe segmentation of (d) using the \\nk\\n-means algorithm with \\nk\\n=\\n3\\n as before. Note the signiﬁcant improve-\\nment over the result in (b),\\n indicating that the original image has considerably more (irrelevant) detail \\nthan is needed for a proper segmentation. In terms of computational advantage, consider that generat-\\ning Fig. 10.52(b) required individual processing of over 300K pixels, while (e) required processing of 100 \\npixels with considerably fewer shades of gray.\\n10.6  REGION SEGMENTATION USING GRAPH CUTS  \\nIn this section, we discuss an approach for partitioning an image into regions by \\nexpressing the pixels of the image as nodes of a graph, and then finding an optimum \\npartition (\\ncut\\n) of the graph into groups of nodes. Optimality is based on criteria whose \\nvalues are high for members within a group (i.e., a region) and low across members of \\ndifferent groups. As you will see later in this section, graph-cut segmentation is capa-\\nble in some cases of results that can be superior to the results achievable by any of the \\nsegmentation methods studied thus far. The price of this potential benefit is added \\ncomplexity in implementation, which generally translates into slower execution.\\n10.6\\nDIP4E_GLOBAL_Print_Ready.indb   777\\n6/16/2017   2:14:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 778}),\n",
       " Document(page_content='778\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nIMAGES AS GRAPHS\\nA graph, \\nG\\n, is a mathematical structure consisting of a set \\nV\\n of \\nnodes\\n and a set \\nE\\n of \\nedges\\n connecting those vertices:\\n \\nGV E\\n=\\n(,\\n)\\n \\n(10-94)\\nwhere \\nV\\n is a set and \\n \\nEV V\\n8\\n×\\n \\n(10-95)\\nis a set of ordered pairs of elements from \\nV\\n.\\n If \\n(,)\\nuv\\n∈\\nE\\n implies that \\n(, ) ,\\nvu\\n∈\\nE\\n and \\nvice versa,\\n the graph is said to be \\nundirected\\n; otherwise the graph is \\ndirected\\n. For \\nexample, we may consider a street map as a graph in which the nodes are street \\nintersections, and the edges are the streets connecting those intersections. If all \\nstreets are bidirectional, the graph is undirected (meaning that we can travel both \\nways from any two intersections). Otherwise, if at least one street is a one-way street, \\nthe graph is directed. \\nNodes and edges are also \\nreferred to as \\nvertices\\n \\nand \\nlinks, \\nrespectively.\\nSee Section 2.5\\n \\nfor an \\nexplanation of the  \\nCartesian product \\nV \\n×  \\nV \\nand for a review of the \\nset symbols used in this \\nsection\\n.\\nb a\\nc\\ne\\nd\\nFIGURE 10.52\\n (a) Image of size \\n533 566\\n×\\n (301,678) pixels. (b) Image segmented using the \\nk\\n-means algorithm.\\n \\n \\n(c) 100-element superpixel image showing boundaries for reference. (d) Same image without boundaries. (e) Super-\\npixel image (d) segmented using the \\nk\\n-means algorithm. (Original image courtesy of NOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   778\\n6/16/2017   2:14:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 779}),\n",
       " Document(page_content='10.6\\n  \\nRegion Segmentation Using Graph Cuts\\n    \\n779\\nThe types of graphs in which we are interested are undirected graphs whose \\nedges are further characterized by a matrix, \\nW\\n, whose element \\nw\\n(,\\n)\\nij\\n is a weight \\nassociated with the edge that connects nodes \\ni\\n and \\nj\\n.\\n Because the graph is undirected, \\nww\\n(,\\n) ( ,) ,\\nij ji\\n=\\n which means that \\nW\\n is a symmetric matrix.\\n The weights are selected \\nto be proportional to one or more similarity measures between all pairs of nodes. A \\ngraph whose edges are associated with weights is called a \\nweighted graph\\n. \\nThe essence of the material in this section is to represent an image to be seg-\\nmented as a weighted, undirected graph, where the nodes of the graph are the pixels \\nin the image, and an edge is formed between every pair of nodes. The weight, \\nw\\n(,\\n) ,\\nij\\n \\nof each edge is a function of the similarity between nodes \\ni\\n and \\nj\\n.\\n We then seek to \\npartition the nodes of the graph into disjoint subsets \\nVV V\\nK\\n12\\n,,,\\n…\\n where, by some \\nmeasure\\n, the similarity among the nodes within a subset is high, and the similarity \\nacross the nodes of different subsets is low. The nodes of the partitioned subsets \\ncorrespond to the regions in the segmented image.\\nSet \\nV\\n is partitioned into subsets by cutting the graph. A \\ncut\\n of a graph is a parti-\\ntion of \\nV\\n into two subsets \\nA\\n and \\nB\\n such that\\n \\nAB V AB\\n´¨\\n==\\n∅\\nand\\n \\n(10-96)\\n \\nwhere the cut is implemented by removing the edges connecting subgraphs \\nA\\n and \\nB\\n. \\nT\\nhere are two key aspects of using graph cuts for image segmentation: (1) how to \\nassociate a graph with an image; and (2) how to cut the graph in a way that makes \\nsense in terms of partitioning the image into background and foreground (object) \\npixels. We address these two questions next.\\nFigure 10.53 shows a simpliﬁed approach for generating a graph from an image. \\nThe nodes of the graph correspond to the pixels in the image and, to keep the expla-\\nnation simple, we allow edges only between adjacent pixels using 4-connectivity, \\nwhich means that there are no diagonal edges linking the pixels. But, keep in mind \\nthat, in general, edges are speciﬁed between every pair of pixels. The weights for the \\nedges typically are formed from spatial relationships (for example, distance from the \\nvertex pixel) and intensity measures (for example, texture and color), consistent with \\nexhibiting similarity between pixels. In this simple example, we deﬁne the degree \\nof similarity between two pixels as the inverse of the difference in their intensities. \\nThat is, for two nodes (pixels) \\nn\\ni\\n and \\nn\\nj\\n,\\n the weight of the edge between them is \\nw\\n(,\\n) ( ) ( ) ,\\nij In In c\\nij\\n=− +\\n1\\n/H20907/H20919 /H20919\\nAB\\n where \\nIn\\ni\\n()\\n and \\nIn\\nj\\n() ,\\n are the intensities of the two \\nnodes (pixels) and \\nc\\n is a constant included to prevent division by 0.\\n Thus, the closer \\nthe values of intensity between adjacent pixels is, the larger the value of \\nw\\n will be.\\nF\\nor illustrative purposes, the thickness of each edge in Fig. 10.53 is shown propor-\\ntional to the degree of similarity between the pixels that it connects (see Problem \\n10.44). As you can see in the ﬁgure, the edges between the dark pixels are stronger \\nthan the edges between dark and light pixels, and vice versa. Conceptually, segmen-\\ntation is achieved by cutting the graph along its weak edges, as illustrated by the \\ndashed line in Fig. 10.53(d). Figure 10.53(c) shows the segmented image. \\nAlthough the basic structure in Fig. 10.53 is the focus of the discussion in this \\nsection, we mention for completeness another common approach for constructing \\nSuperpixels are also well \\nsuited for use as graph \\nnodes. Thus, when we \\nrefer in this section to \\n“pixels” in an image, we \\nare, by implication,  \\nalso referring to super-\\npixels.\\nDIP4E_GLOBAL_Print_Ready.indb   779\\n6/16/2017   2:14:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 780}),\n",
       " Document(page_content='780\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nimage graphs. Figure 10.54 shows the same graph as the one we just discussed, but \\nhere you see two additional nodes called the \\nsource\\n and \\nsink\\n \\nterminal\\n nodes, respec-\\ntively, each connected to all nodes in the graph via unidirectional links called \\nt-links\\n. \\nThe terminal nodes are not part of the image; their role, for example, is to associate \\nwith each pixel a probability that it is a background or foreground (object) pixel. \\nThe probabilities are the weights of the t-links. In Figs. 10.54(c) and (d), the thickness \\nof each t-link is proportional to the value of the probability that the graph node to \\nwhich it is connected is a foreground or background pixel (the thicknesses shown \\nare so that the segmentation result would be the same as in Fig. 10.53). Which of the \\ntwo nodes we call background or foreground is arbitrary. \\nMINIMUM GRAPH CUTS\\nOnce an image has been expressed as a graph, the next step is to cut the graph into \\ntwo or more subgraphs. The nodes (pixels) in each resulting subgraph correspond \\nto a region in the segmented image. Approaches based on Fig. 10.54 rely on inter-\\npreting the graph as a flow network (of pipes, for example) and obtaining what is \\ncommonly referred to as a \\nminimum graph cut\\n. This formulation is based on the \\nso-called \\nMax-Flow, Min-Cut Theorem\\n. This theorem states that, in a flow network, \\nthe maximum amount of flow passing from the source to the sink is equal to the \\nminimum cut\\n. This minimum cut is defined as the smallest \\ntotal\\n weight of the edges \\nthat, if removed, would disconnect the sink from the source:\\n \\ncut A B\\nu\\nAB\\n(,) ( , )\\n,\\n=\\n∈∈\\n∑\\nwv\\nuv\\n \\n(10-97)\\nCut\\n⇓\\n⇓\\n⇓\\nImage\\nGraph\\nSegmentation\\nEdge\\nNode\\nb a\\nd c\\n \\nFIGURE 10.53\\n(a) A \\n33\\n×\\n image. \\n(c) A \\ncorresponding \\ngraph.  \\n(d) Graph cut.  \\n(c) Segmented  \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   780\\n6/16/2017   2:14:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 781}),\n",
       " Document(page_content='10.6\\n  \\nRegion Segmentation Using Graph Cuts\\n    \\n781\\nwhere \\nA\\n and \\nB\\n satisfy Eq. (10-96). The optimum partition of a graph is the one that \\nminimizes this cut value. There is an exponential number of such partitions, which \\nwould present us with an intractable computational problem. However, efficient \\nalgorithms that run in polynomial time have been developed for solving max-flow \\nproblems. Therefore, based on the Max-Flow, Min-Cut Theorem, we can apply these \\nalgorithms to image segmentation, provided that we cast segmentation as a flow \\nproblem and select the weights for the edges and t-links such that minimum graph \\ncuts will result in meaningful segmentations.\\nAlthough the min-cut approach offers an elegant solution, it can result in group-\\nings that favor cutting small sets of isolated nodes in a graph, leading to improper \\nsegmentations. Figure 10.55 shows an example, in which the two regions of interest \\nare characterized by the tightness of the pixel groupings. Meaningful edge weights \\nthat reﬂect this property would be inversely proportional to the distance between \\npairs of points. But this would lead to weights that would be smaller for isolated \\npoints, resulting in min cuts such as the example in Fig. 10.55. In fact, any cut that \\npartitions out individual points on the left of the ﬁgure will have a smaller cut value \\nin Eq. (10-4) than a cut that properly partitions the points into two groups based on \\nb a\\nd c\\nFIGURE 10.54\\n(a) Same image \\nas in Fig. 10.53(a). \\n(c) Corresponding \\ngraph and terminal \\nnodes. (d) Graph \\ncut. (b) Segmented \\nimage. \\nCut\\n⇓\\n⇓\\n⇓\\nImage\\nGraph\\nSegmentation\\n(Background)\\n(Foreground)\\nSource Terminal\\nSink Terminal\\n(Background)\\n(Foreground)\\nSource Terminal\\nSink Terminal\\nDIP4E_GLOBAL_Print_Ready.indb   781\\n6/16/2017   2:14:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 782}),\n",
       " Document(page_content='782\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\ntheir proximity, such as the partition shown in Fig. 10.55. The approach presented in \\nthis section, proposed by Shi and Malik [2000] (see also Hochbaum [2010]), is aimed \\nat avoiding this type of behavior by redeﬁning the concept of a cut.\\nInstead of looking at the total weight value of the edges that connect two parti-\\ntions, the idea is to work with a measure of “disassociation” that computes the cost \\nas a fraction of the total edge connections to all nodes in the graph. This measure, \\ncalled the \\nnormalized cut\\n (\\nNcut\\n), is deﬁned as\\n \\nNcut A B\\ncut A B\\nassoc A V\\ncut A B\\nassoc B V\\n(,)\\n(,)\\n(,)\\n(,)\\n(, )\\n=+\\n \\n(10-98)\\nwhere \\ncut A B\\n(,\\n)\\n is given by Eq. (10-97) and \\n \\nassoc A V\\nu z\\nuA z V\\n(,) ( , )\\n,\\n=\\n∈∈\\n∑\\nw\\n \\n(10-99)\\nis the sum of the weights of all the edges from the nodes of subgraph \\nA\\n to the nodes \\nof the entire graph.\\n Similarly,\\n \\nassoc B V\\nz\\nBz V\\n(, ) ( , )\\n,\\n=\\n∈∈\\n∑\\nwv\\nv\\n \\n(10-100)\\nis the sum of the weights of the edges from all the edges in \\nB\\n to the entire graph.\\n As \\nyou can see, \\nassoc A V\\n(,\\n)\\n is simply the cut of \\nA\\n from the rest of the graph,\\n and simi-\\nlarly for \\nassoc B V\\n(,\\n) .\\nBy using \\nNcut A B\\n(,\\n)\\n instead of \\ncut A B\\n(,\\n) ,\\n the cut that partitions isolated points \\nwill no longer have small values\\n. You can see this, for example, by noting in Fig. 10.55 \\nthat if \\nA\\n is the single node shown, \\ncut A B\\n(,\\n)\\n and \\nassoc A V\\n(,\\n)\\n will have the same val-\\nue\\n. Thus, independently of how small \\ncut A B\\n(,\\n)\\n is, \\nNcut A B\\n(,\\n)\\n will always be greater \\nthan or equal to 1,\\n thus providing normalization for “pathological” cases such as this.\\nBased on similar concepts, we can deﬁne a measure for total \\nnormalized associa-\\ntion\\n within graph partitions as\\nA more meaningful cut\\nA min cut\\nFIGURE 10.55\\nAn example \\nshowing how a \\nmin cut can lead \\nto a meaningless \\nsegmentation. In \\nthis example, the \\nsimilarity between \\npixels is deﬁned \\nas their spatial \\nproximity, which \\nresults in two \\ndistinct regions.\\nDIP4E_GLOBAL_Print_Ready.indb   782\\n6/16/2017   2:14:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 783}),\n",
       " Document(page_content='10.6\\n  \\nRegion Segmentation Using Graph Cuts\\n    \\n783\\n \\nNassoc A B\\nassoc A A\\nassoc A V\\nassoc B B\\nassoc B V\\n(,)\\n(,)\\n(,)\\n(,)\\n(, )\\n=+\\n \\n(10-101)\\nwhere \\nassoc A A\\n(,\\n)\\n and \\nassoc B B\\n(,\\n)\\n are the total weights connecting the nodes within \\nA\\n and within \\nB\\n,\\n respectively. It is not difficult to show (see Problem 10.46) that\\n \\nNcut A B Nassoc A B\\n(,\\n)\\n(,)\\n=−\\n2\\n \\n(10-102)\\nwhich implies that minimizing \\nNcut A B\\n(,\\n)\\n simultaneously maximizes \\nNassoc A B\\n(,\\n) .\\nBased on the preceding discussion, image segmentation using graph cuts is now \\nbased on ﬁnding a partition that minimizes \\nNcut A B\\n(,\\n) .\\n Unfortunately, minimizing \\nthis quantity exactly is an NP-complete computational task,\\n and we can no longer \\nrely on the solutions available for max ﬂow because the approach being followed \\nnow is based on the concepts explained in connection with Fig. 10.53. However, Shi \\nand Malik [2000] (see also Hochbaum [2010]) were able to ﬁnd an approximate dis-\\ncrete solution to minimizing \\nNcut A B\\n(,\\n)\\n by formulating minimization as a general-\\nized eigenvalue problem,\\n for which numerous implementations exist.\\nCOMPUTING MINIMAL GRAPH CUTS\\nAs above, let \\nV\\n denote the nodes of a graph \\nG\\n, and let \\nA\\n and \\nB\\n be two subsets \\nof \\nV\\n satisfying Eq. (10-96). Let \\nK\\n denote the number of nodes in \\nV\\n and deﬁne a \\nK\\n-dimensional \\nindicator\\n vector, \\nx\\n, whose element \\nx\\ni\\n has the property \\nx\\ni\\n=\\n1\\n if node \\nn\\ni\\n of \\nV\\n is in \\nA\\n and \\nx\\ni\\n=−\\n1 if it is in \\nB\\n.\\n Let\\n \\ndi j\\ni\\nj\\n=\\n∑\\nw\\n(, )\\n \\n(10-103)\\nbe the sum of the weights from node \\nn\\ni\\n to all other nodes in \\nV\\n. Using these defini-\\ntions, we can write Eq. (10-98) as\\n \\nNcut A B\\ncut A B\\ncut A V\\ncut A B\\ncut B V\\nijx x\\nij\\nx\\ni\\n(,)\\n(,)\\n(,)\\n(,)\\n(, )\\n(, )\\n=+\\n=\\n−\\n>\\nw\\n0\\n00\\n0\\n00\\n0\\n,,\\n(, )\\nx\\ni\\nx\\nij\\nxx\\ni\\nx\\nj\\ni\\nij\\ni\\nd\\nijx x\\nd\\n<\\n>\\n<>\\n<\\n∑\\n∑\\n∑\\n∑\\n+\\n−\\nw\\n \\n(10-104)\\nThe objective is to find a vector, \\nx\\n,\\n that minimizes \\nNcut A B\\n(,\\n) .\\n A closed-form solu-\\ntion that minimizes Eq.\\n (10-104) can be found, but only if the elements of \\nx\\n are \\nallowed to be real, continuous numbers instead of being constrained to be \\n±\\n1.\\n The \\nsolution derived by Shi and Malik [2000] is given by solving the generalized eigen-\\nsystem expression\\n \\n()\\nDW\\ny D y\\n−=\\nl\\n \\n(10-105)\\nwhere \\nD\\n is a \\nKK\\n×\\n diagonal matrix with main-diagonal elements \\nd\\ni\\n, \\niK\\n=\\n12\\n,,\\n, ,\\n…\\n \\nand \\nW\\n is a \\nKK\\n×\\n weight matrix with elements \\nw\\n(,\\n) ,\\nij\\n as defined earlier. Solving \\nIf the nodes of graph \\nG\\n are the pixels in an \\nimage, then \\nK\\n = \\nM \\n× \\nN\\n, \\nwhere \\nM\\n and \\nN\\n are the \\nnumber of rows and \\ncolumns in the image.\\nDIP4E_GLOBAL_Print_Ready.indb   783\\n6/16/2017   2:14:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 784}),\n",
       " Document(page_content='784\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nEq. (10-105) gives \\nK\\n eigenvalues and \\nK\\n eigenvectors, each corresponding to one \\neigenvalue. The solution to our problem is the eigenvector corresponding the \\nsecond\\n \\nsmallest eigenvalue.\\nWe can convert the preceding generalized eigenvalue formulation into a standard \\neigenvalue problem by writing Eq. (10-105) as (see Problem 10.45):\\n \\nAz z\\n=\\nl\\n \\n(10-106)\\nwhere\\n \\nADD W D\\n=−\\n−−\\n1\\n2\\n1\\n2\\n()\\n \\n(10-107)\\nand\\n \\nzD y\\n=\\n1\\n2\\n \\n(10-108)\\nfrom which it follows that\\n \\nyD z\\n=\\n−\\n1\\n2\\n \\n(10-109)\\nThus, we can find the (continuous-valued) eigenvector corresponding to the second \\nsmallest eigenvalue using either a generalized or a standard eigenvalue solver\\n. The \\ndesired (discrete) vector \\nx\\n can be generated from the resulting, continuous valued \\nsolution vector by finding a splitting point that divides the values of the continuous \\neigenvector elements into two parts. We do this by finding the splitting point that \\nyields the smallest value of \\nNcut A B\\n(,\\n) ,\\n since this is the quantity we are trying to \\nminimize\\n. To simplify the search, we divide the range of values in the continuous \\nvector into \\nQ\\n evenly spaced values, evaluate Eq. (10-104) for each value, and choose \\nthe splitting point that yields the smallest value of \\nNcut A B\\n(,\\n) .\\n Then, all values of the \\neigenvector with values above the split point are assigned the value 1;\\n all others are \\nassigned the value \\n−\\n1.\\n The result is the desired vector \\nx\\n.\\n Then, partition \\nA\\n is the set \\nnodes in \\nV\\n corresponding to 1’s in \\nx\\n; the remaining nodes correspond to partition \\nB\\n. \\nThis partitioning is carried out only if the stability criterion discussed in the follow-\\ning paragraph is met.\\nSearching for a splitting point implies computing a total of \\nQ\\n values of \\nNcut A B\\n(,\\n)\\n \\nand selecting the smallest one\\n. A region that is not clearly segmentable into two \\nsubregions using the speciﬁed weights will usually result in many splitting points \\nwith similar values of \\nNcut A B\\n(,\\n) .\\n Trying to segment such a region is likely to result \\nin a meaningless partition.\\n To avoid this behavior, a region (i.e., subgraph) is split \\nonly if it satisﬁes a \\nstability\\n \\ncriterion\\n, obtained by ﬁrst computing the histogram of \\nthe eigenvector values, then forming the ratio of the minimum to the maximum bin \\ncounts. In an “uncertain” eigenvector, the values in the histogram will stay relatively \\nthe same, and the ratio will be relatively high. Shi and Malik [2000] found experi-\\nmentally that thresholding the ratio at 0.06 was a effective criterion for not splitting \\nthe region in question.\\nDIP4E_GLOBAL_Print_Ready.indb   784\\n6/16/2017   2:14:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 785}),\n",
       " Document(page_content='10.6\\n  \\nRegion Segmentation Using Graph Cuts\\n    \\n785\\nGRAPH CUT SEGMENTATION ALGORITHM\\nIn the preceding discussion, we illustrated two ways in which edge weights can be \\ngenerated from an image. In Figs. 10.53 and 10.54, we looked at weights generated \\nusing image intensity values, and in Fig. 10.55 we considered weights based on the \\ndistance between pixels. But these are just two examples of the many ways that \\nwe can generate a graph and corresponding weights from an image. For example, \\nwe could use color, texture, statistical moments about a region, and other types of \\nfeatures to be discussed in Chapter 11. In general, then, graphs can be constructed \\nfrom image \\nfeatures\\n, of which pixel intensities are a special case. With this concept \\nas background, we can summarize the discussion thus far in this section as the fol-\\nlowing algorithm:\\n1. \\nGiven a set of features, specify a weighted graph, \\nGV E\\n=\\n(,\\n)\\n in which \\nV\\n contains \\nthe points in the feature space\\n, and \\nE\\n contains the edges of the graph. Compute \\nthe edge weights and use them to construct matrices \\nW\\n and \\nD\\n. Let \\nK\\n denote the \\ndesired number of partitions of the graph.\\n2. \\nSolve the eigenvalue system \\n()\\nDW\\ny D y\\n−=\\nl\\n to ﬁnd the eigenvector with the \\nsecond smallest eigenvalue\\n.\\n3. \\nUse the eigenvector from Step 2 to bipartition the graph by ﬁnding the splitting \\npoint such that \\nNcut A B\\n(,\\n)\\n is minimized.\\n4. \\nIf the number of cuts has not reached \\nK\\n,\\n decide if the current partition should \\nbe subdivided by checking the stability of the cut.\\n5. \\nRecursively repartition the segmented parts if necessary.\\nNote that the algorithm works by recursively generating two-way cuts\\n. The number of \\ngroups (e.g., regions) in the segmented image is controlled by \\nK\\n. Other criteria, such \\nas the maximum size allowed for each cut, can further refine the final segmentation. \\nFor example, when using pixels and their intensities as the basis for constructing the \\ngraph, we can specify the maximum and/or minimum size allowed for each region.\\nEXAMPLE 10.24 :  Specifying weights for graph cut segmentation.\\nIn Fig. 10.53, we illustrated how to generate graph weights using intensity values, and in Fig. 10.55 we \\ndiscussed brieﬂy how to generate weights based on the distance between pixels. In this example, we give \\na more practical approach for generating weights that include both intensity and distance from a pixel, \\nthus introducing the concept of a neighborhood in graph segmentation.\\nLet \\nn\\ni\\n and \\nn\\nj\\n denote two nodes (image pixels). As mentioned earlier in this section, weights are sup-\\nposed to reﬂect the similarity between nodes in a graph. When considering segmentation, one of the \\nprincipal ways to establish how likely two pixels in an image are to be a part of the same region or object \\nis to determine the difference in their intensity values, and how close the pixels are to each other. The \\nweight value of the edge between two pixels should be large when the pixels are very close in intensity \\nand proximity (i.e., when the pixels are “similar), and should decrease as their intensity difference and \\ndistance from each other increases. That is, the weight value should be a function of how similar the \\npixels are in intensity and distance. These two concepts can be embedded into a single weight function \\nusing the following expression:\\nDIP4E_GLOBAL_Print_Ready.indb   785\\n6/16/2017   2:14:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 786}),\n",
       " Document(page_content='786\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n \\nw\\n(, )\\n(, )\\n[( ) ( ) ]\\n(, )\\nij\\ne e\\ndist n n r\\nIn In\\ndist n n\\nij\\nij\\nI\\nij\\nd\\n=\\n<\\n−\\n−\\n−\\n2\\n2\\n2\\n0\\ns\\ns\\nif\\no\\notherwise\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\nwhere \\nIn\\ni\\n()\\n is the intensity of node \\nn\\ni\\n, \\ns\\nI\\n2\\n and \\ns\\nd\\n2\\n are constants determining the spread of the two \\nGaussian-like functions, \\ndist n n\\nij\\n(, )\\n is the distance (e.g., the Euclidean distance) between the two nodes, \\nand \\nr\\n is a radial constant that establishes how far away we are willing to consider similarity\\n. The expo-\\nnential terms decrease as a function of dissimilarity in intensity and as function of distance between the \\nnodes, as required of our measure of similarity in this case.\\nEXAMPLE 10.25 :  Segmentation using graph cuts.\\nGraph cuts are ideally suited for obtaining a rough segmentation of the principal regions in an image. \\nFigure 10.56 shows a typical result. Figure 10.56(a) is the familiar building image. Consistent with the \\nidea of extracting the principal regions of an image, Fig. 10.56(b) shows the image smoothed with a \\nsimple 25 25\\n×\\n box kernel. Observe how the ﬁne detail is smoothed out, leaving only major regional \\nfeatures such as the facade and sky\\n. Figure 10.56(c) is the result of segmentation using the graph cut \\nalgorithm just developed, with weights of the form discussed in the previous example, and allowing only \\ntwo partitions. Note how well the region corresponding to the building was extracted, with none of the \\ndetails characteristic of the methods discussed earlier in this chapter. In fact, it would have been nearly \\nimpossible to obtain comparable results using any of the methods we have discussed thus far without \\nsigniﬁcant additional processing. This type of result is ideal for tasks such as providing broad cues for \\nautonomous navigation, for searching image databases, and for low-level image analysis.\\n10.7  SEGMENTATION USING MORPHOLOGICAL WATERSHEDS  \\nThus far, we have discussed segmentation based on three principal concepts: edge \\ndetection, thresholding, and region extraction. Each of these approaches was found \\nto have advantages (for example, speed in the case of global thresholding) and dis-\\nadvantages (for example, the need for post-processing, such as edge linking, in edge-\\nbased segmentation). In this section, we discuss an approach based on the concept of \\nso-called \\nmorphological watersheds\\n. Segmentation by watersheds embodies many of \\nthe concepts of the other three approaches and, as such, often produces more stable \\nsegmentation results, including connected segmentation boundaries. This approach \\nalso provides a simple framework for incorporating knowledge-based constraints \\n(see Fig. 1.23) in the segmentation process, as we discuss at the end of this section.\\nBACKGROUND\\nThe concept of a watershed is based on visualizing an image in three dimensions, \\ntwo spatial coordinates versus intensity, as in Fig. 2.18(a). In such a “topographic” \\ninterpretation, we consider three types of points: (1) points belonging to a regional \\nminimum; (2) points at which a drop of water, if placed at the location of any of those \\n10.7\\nDIP4E_GLOBAL_Print_Ready.indb   786\\n6/16/2017   2:14:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 787}),\n",
       " Document(page_content='10.7\\n  \\nSegmentation Using Morphological Watersheds\\n    \\n787\\npoints, would fall with certainty to a single minimum; and (3) points at which water \\nwould be equally likely to fall to more than one such minimum. For a particular \\nregional minimum, the set of points satisfying condition (2) is called the \\ncatchment \\nbasin\\n or \\nwatershed\\n of that minimum. The points satisfying condition (3) form crest \\nlines on the topographic surface, and are referred to as \\ndivide lines\\n or \\nwatershed lines\\n.\\nThe principal objective of segmentation algorithms based on these concepts is to \\nﬁnd the watershed lines. The method for doing this can be explained with the aid of \\nFig. 10.57. Figure 10.57(a) shows a gray-scale image and Fig. 10.57(b) is a topograph-\\nic view, in which the height of the “mountains” is proportional to intensity values in \\nthe input image. For ease of interpretation, the backsides of structures are shaded. \\nThis is not to be confused with intensity values; only the general topography of the \\nthree-dimensional representation is of interest. In order to prevent the rising water \\nfrom spilling out through the edges of the image, we imagine the perimeter of the \\nentire topography (image) being enclosed by dams that are higher than the highest \\npossible mountain, whose value is determined by the highest possible intensity value \\nin the input image.\\nSuppose that a hole is punched in each regional minimum [shown as dark areas in \\nFig. 10.57(b)] and that the entire topography is ﬂooded from below by letting water \\nrise through the holes at a uniform rate. Figure 10.57(c) shows the ﬁrst stage of ﬂood-\\ning, where the “water,” shown in light gray, has covered only areas that correspond \\nto the black \\nbackground\\n in the image. In Figs. 10.57(d) and (e) we see that the water \\nnow has risen into the ﬁrst and second catchment basins, respectively. As the water \\ncontinues to rise, it will eventually overﬂow from one catchment basin into another. \\nThe ﬁrst indication of this is shown in 10.57(f). Here, water from the lower part of \\nthe left basin overﬂowed into the basin on the right, and a short “dam” (consisting of \\nsingle pixels) was built to prevent water from merging at that level of ﬂooding (the \\nmathematical details of dam building are discussed in the following section). The \\nBecause of neighboring \\ncontrast, the leftmost \\nbasin in Fig. 10.57(c) \\nappears black, but it is a \\nfew shades lighter than \\nthe black background. \\nThe mid-gray in the \\nsecond basin is a natural \\ngray from the image \\nin (a).\\nb a\\nc\\nFIGURE 10.56\\n (a) Image of size \\n600 600\\n×\\n pixels. (b) Image smoothed with a \\n25 25\\n×\\n box kernel. (c) Graph cut segmen-\\ntation obtained by specifying two regions\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   787\\n6/16/2017   2:14:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 788}),\n",
       " Document(page_content='788\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\neffect is more pronounced as water continues to rise, as shown in Fig. 10.57(g). This \\nﬁgure shows a longer dam between the two catchment basins and another dam in \\nthe top part of the right basin. The latter dam was built to prevent merging of water \\nfrom that basin with water from areas corresponding to the background. This pro-\\ncess is continued until the maximum level of ﬂooding (corresponding to the highest \\nintensity value in the image) is reached. The ﬁnal dams correspond to the \\nwatershed \\nlines\\n, which are the desired segmentation boundaries. The result for this example is \\nshown in Fig. 10.57(h) as dark, one-pixel-thick paths superimposed on the original \\nimage. Note the important property that the watershed lines form connected paths, \\nthus giving continuous boundaries between regions.\\nOne of the principal applications of watershed segmentation is in the extraction \\nof nearly uniform (blob-like) objects from the background. Regions characterized \\nby small variations in intensity have small gradient values. Thus, in practice, we often \\nsee watershed segmentation applied to the gradient of an image, rather than to the \\nimage itself. In this formulation, the regional minima of catchment basins correlate \\nnicely with the small value of the gradient corresponding to the objects of interest.\\nWater\\nWater\\nWater\\nb\\na\\nd\\nc\\nFIGURE 10.57\\n(a) Original  \\nimage.  \\n(b) Topographic \\nview. Only the \\nbackground is \\nblack\\n. The basin \\non the left is \\nslightly lighter \\nthan black. \\n(c) and (d) Two \\nstages of ﬂooding. \\nAll constant dark \\nvalues of gray are \\nintensities in the \\noriginal image. \\nOnly constant \\nlight gray\\n repre-\\nsents “water.” \\n(Courtesy of Dr. \\nS. Beucher, CMM/\\nEcole des Mines \\nde Paris.) \\n(\\nContinued on \\nnext page\\n.)\\nDIP4E_GLOBAL_Print_Ready.indb   788\\n6/16/2017   2:14:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 789}),\n",
       " Document(page_content='10.7\\n  \\nSegmentation Using Morphological Watersheds\\n    \\n789\\nDAM CONSTRUCTION\\nDam construction is based on binary images, which are members of 2-D integer \\nspace \\nZ\\n2\\n (see Sections 2.4 and 2.6). The simplest way to construct dams separating \\nsets of binary points is to use morphological dilation (see Section 9.2).\\nFigure 10.58 illustrates the basics of dam construction using dilation. Part (a) \\nshows portions of two catchment basins at ﬂooding step \\nn\\n−\\n1,\\n and Fig. 10.58(b) \\nshows the result at the next ﬂooding step\\n, \\nn\\n.\\n The water has spilled from one basin \\nto the another and,\\n therefore, a dam must be built to keep this from happening. In \\norder to be consistent with notation to be introduced shortly, let \\nM\\n1\\n and \\nM\\n2\\n denote \\nthe sets of coordinates of points in two regional minima. Then let the set of coordi-\\nnates of points \\nin the catchment basin\\n associated with these two minima at stage \\nn\\n−\\n1 \\nof ﬂooding be denoted by \\nCM\\nn\\n−\\n11\\n()\\n and \\nCM\\nn\\n−\\n12\\n() ,\\n respectively. These are the two \\ngray regions in F\\nig. 10.58(a).\\nLet \\nCn\\n−\\n[]\\n1\\n denote the union of these two sets. There are two connected com-\\nponents in F\\nig. 10.58(a), and only one component in Fig. 10.58(b). This connected \\nSee Sections 2.5\\n \\nand 9.5\\n \\nregarding connected \\ncomponents.\\n \\nFIGURE 10.57\\n \\n(Continued)\\n \\n(e) Result of \\nfurther ﬂooding. \\n(f) Beginning of \\nmerging of water \\nfrom two  \\ncatchment basins \\n(a short dam was \\nbuilt between \\nthem).  \\n(g) Longer dams. \\n(h) Final water-\\nshed (segmenta-\\ntion) lines super-\\nimposed on the \\noriginal image.  \\n(Courtesy of Dr. \\nS. Beucher, CMM/\\nEcole des Mines \\nde Paris.)\\nf e\\nh\\ng\\nDIP4E_GLOBAL_Print_Ready.indb   789\\n6/16/2017   2:14:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 790}),\n",
       " Document(page_content='790\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nFirst\\nSecond dilation\\nDam points\\n1\\n1\\n11\\n11\\n111\\nOrigin\\ndilation\\nb\\na\\nd\\nc\\nFIGURE 10.58\\n (a) Two partially ﬂooded catchment basins at stage \\nn\\n−\\n1\\n of ﬂooding. (b) Flooding at stage \\nn\\n,\\n showing \\nthat water has spilled between basins\\n. (c) Structuring element used for dilation. (d) Result of dilation and dam \\nconstruction.\\nDIP4E_GLOBAL_Print_Ready.indb   790\\n6/16/2017   2:14:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 791}),\n",
       " Document(page_content='10.7\\n  \\nSegmentation Using Morphological Watersheds\\n    \\n791\\ncomponent encompasses the earlier two components, which are shown dashed. \\nTwo connected components having become a \\nsingle\\n component indicates that \\nwater between the two catchment basins has merged at ﬂooding step \\nn\\n.\\n Let this \\nconnected component be denoted by \\nq\\n.\\n Note that the two components from step \\nn\\n−\\n1\\n can be extracted from \\nq\\n by performing a logical AND operation, \\nqC n\\n/H20669\\n−\\n[]\\n1.\\n \\nObserve also that all points belonging to an individual catchment basin form a \\nsingle connected component.\\nSuppose that each of the connected components in F\\nig. 10.58(a) is dilated by \\nthe structuring element in Fig. 10.58(c), subject to two conditions: (1) The dilation \\nhas to be constrained to \\nq\\n (this means that the center of the structuring element \\ncan be located only at points in \\nq\\n during dilation); and (2) the dilation cannot be \\nperformed on points that would cause the sets being dilated to merge (i.e\\n., become \\na single connected component). Figure 10.58(d) shows that a ﬁrst dilation pass (in \\nlight gray) expanded the boundary of each original connected component. Note that \\ncondition (1) was satisﬁed by every point during dilation, and that condition (2) did \\nnot apply to any point during the dilation process; thus, the boundary of each region \\nwas expanded uniformly.\\nIn the second dilation, shown in black in 10.58(d), several points failed condition \\n(1) while meeting condition (2), resulting in the broken perimeter shown in the ﬁgure. \\nIt is evident that the only points in \\nq\\n that satisfy the two conditions under consid-\\neration describe the one-pixel-thick connected path shown crossed-hatched in F\\nig. \\n10.58(d). This path is the desired separating dam at stage \\nn\\n of ﬂooding. Construction \\nof the dam at this level of ﬂooding is completed by setting all the points in the path \\njust determined to a value greater than the maximum possible intensity value of the \\nimage (e\\n.g., greater than 255 for an 8-bit image). This will prevent water from cross-\\ning over the part of the completed dam as the level of ﬂooding is increased. As noted \\nearlier, dams built by this procedure, which are the desired segmentation boundaries, \\nare connected components. In other words, this method eliminates the problems of \\nbroken segmentation lines.\\nAlthough the procedure just described is based on a simple example, the method \\nused for more complex situations is exactly the same, including the use of the \\n33\\n×\\n \\nsymmetric structuring element in F\\nig. 10.58(c).\\nWATERSHED SEGMENTATION ALGORITHM\\nLet \\nMM M\\nR\\n12\\n,, ,\\n…\\n be sets denoting the \\ncoor\\ndinates\\n of the points in the regional \\nminima of an image, \\ngxy\\n(,\\n) .\\n As mentioned earlier, this typically will be a gradient \\nimage\\n. Let \\nCM\\ni\\n()\\n be a set denoting the coordinates of the points in the catchment \\nbasin associated with regional minimum \\nM\\ni\\n (recall that the points in any catchment \\nbasin form a connected component). The notation min and max will be used to \\ndenote the minimum and maximum values of \\ngxy\\n(,\\n) .\\n Finally, let \\nTn\\n[]\\n represent the \\nset of coordinates \\n(,)\\nst\\n for which \\ngst n\\n(,\\n) .\\n<\\n That is,\\n \\nTn s t gs t n\\n[]\\n=\\n(\\n)\\n(\\n)\\n<\\n{}\\n,,\\n \\n(10-110)\\nDIP4E_GLOBAL_Print_Ready.indb   791\\n6/16/2017   2:14:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 792}),\n",
       " Document(page_content='792\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nGeometrically, \\nTn\\n[]\\n is the set of coordinates of points in \\ngxy\\n(,\\n)\\n lying below the \\nplane \\ngxy n\\n(,\\n) .\\n=\\nThe topography will be ﬂooded in \\ninteger\\n ﬂood increments\\n, from \\nn\\n=+\\nmi\\nn 1\\n to \\nn\\n=+\\nma\\nx .\\n1\\n At any step \\nn\\n of the ﬂooding process, the algorithm needs to know \\nthe number of points below the ﬂood depth.\\n Conceptually, suppose that the coordi-\\nnates in \\nTn\\n[]\\n that are below the plane \\ngxy n\\n(,\\n)\\n=\\n are “marked” black, and all other \\ncoordinates are marked white\\n. Then when we look “down” on the \\nxy\\n-plane at any \\nincrement \\nn\\n of ﬂooding, we will see a binary image in which black points correspond \\nto points in the function that are below the plane \\ngxy n\\n(,\\n) .\\n=\\n This interpretation is \\nquite useful,\\n and will make it easier to understand the following discussion.\\nLet \\nCM\\nni\\n()\\n denote the set of coordinates of points in the catchment basin associ-\\nated with minimum \\nM\\ni\\n that are ﬂooded at stage \\nn\\n.\\n With reference to the discussion \\nin the previous paragraph,\\n we may view \\nCM\\nni\\n()\\n as a binary image given by\\n \\nCM C M T n\\nni i\\n()\\n=\\n()\\n[]\\n/H20669\\n \\n(10-111)\\nIn other words, \\nCM\\nni\\n()\\n=\\n1\\n at location \\n(,)\\nxy\\n if \\n(,)\\nxy\\nC M\\ni\\n∈\\n()\\n AND \\n(,) ;\\nxy\\nT n\\n∈\\n[]\\n \\notherwise \\nCM\\nni\\n()\\n=\\n0.\\n The geometrical interpretation of this result is straightfor-\\nward.\\n We are simply using the AND operator to isolate at stage \\nn\\n of flooding the \\nportion of the binary image in \\nTn\\n[]\\n that is associated with regional minimum \\nM\\ni\\n.\\nNext, let \\nB\\n denote the number of number of flooded catchment basins at stage \\nn\\n, \\nand let \\nCn\\n[]\\n denote the union of these basins at stage \\nn\\n:\\n \\nCn C M\\nni\\ni\\nB\\n[]\\n=\\n()\\n=\\n1\\n∪\\n \\n(10-112)\\nThen \\nC\\n[ma\\nx ]\\n+\\n1  is the union of all catchment basins:\\n \\nCC\\nM\\ni\\ni\\nB\\nmax\\n+\\n[]\\n=\\n()\\n=\\n1\\n1\\n∪\\n \\n(10-113)\\nIt can be shown (see Problem 10.47) that the elements in both \\nCM\\nni\\n()\\n and \\nTn\\n[]\\n are \\nnever replaced during execution of the algorithm,\\n and that the number of elements \\nin these two sets either increases or remains the same as \\nn\\n increases. Thus, it fol-\\nlows that \\nCn\\n[]\\n−\\n1\\n is a subset of \\nCn\\n[]\\n.\\n According to Eqs. (10-112) and (10-113), \\nCn\\n[]\\n \\nis a subset of \\nTn\\n[]\\n,\\n so it follows that \\nCn\\n[]\\n−\\n1\\n is also a subset of \\nT\\nn\\n[] .\\n From this we \\nhave the important result that each connected component of \\nCn\\n[]\\n−\\n1\\n is contained \\nin exactly one connected component of \\nTn\\n[]\\n.\\nThe algorithm  \\nfor ﬁnding \\nthe watershed lines is initialized by letting \\nC\\n[mi\\nn ]\\n+=\\n1\\nT\\n[min ].\\n+\\n1\\n The procedure then proceeds recursively, successively computing \\nCn\\n[]\\n \\nfrom \\nCn\\n[]\\n,\\n−\\n1\\n using the following approach. Let \\nQ\\n denote the set of connected com-\\nponents in \\nTn\\n[]\\n.\\n Then, for each connected component \\nqQ n\\n∈\\n[]\\n,\\n there are three pos-\\nsibilities:\\n1. \\nqC\\nn\\n/H20669\\n[]\\n−\\n1  is empty.\\nDIP4E_GLOBAL_Print_Ready.indb   792\\n6/16/2017   2:14:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 793}),\n",
       " Document(page_content='10.7\\n  \\nSegmentation Using Morphological Watersheds\\n    \\n793\\n2. \\nqC n\\n/H20669\\n[]\\n−\\n1  contains one connected component of \\nCn\\n[]\\n.\\n−\\n1\\n3. \\n[]\\nqC n\\n/H20669\\n−\\n1  contains more than one connected component of \\nCn\\n[]\\n.\\n−\\n1\\nThe construction of \\nCn\\n[]\\n from \\nCn\\n[]\\n−\\n1\\n depends on which of these three conditions \\nholds\\n. Condition 1 occurs when a new minimum is encountered, in which case con-\\nnected component \\nq\\n is incorporated into \\nCn\\n[]\\n−\\n1  to form \\nCn\\n[]\\n.\\n Condition 2 occurs \\nwhen \\nq\\n lies within the catchment basin of some regional minimum, in which case \\nq\\n is incorporated into \\nCn\\n[]\\n−\\n1\\n to form \\nCn\\n[]\\n.\\n Condition 3 occurs when all (or part) \\nof a ridge separating two or more catchment basins is encountered.\\n Further flood-\\ning would cause the water level in these catchment basins to merge. Thus, a dam (or \\ndams if more than two catchment basins are involved) must be built within \\nq\\n to pre-\\nvent overflow between the catchment basins\\n. As explained earlier, a one-pixel-thick \\ndam can be constructed when needed by dilating \\nqC n\\n/H20669\\n[]\\n−\\n1\\n with a \\n33\\n×\\n structur-\\ning element of 1’\\ns, and constraining the dilation to \\nq\\n.\\nAlgorithm efﬁciency is improved by using only values of \\nn\\n that correspond to \\nexisting intensity values in \\ngxy\\n(,\\n) .\\n We can determine these values, as well as the \\nvalues of min and max,\\n from the histogram of \\ngxy\\n(,\\n) .\\nEXAMPLE 10.26 :  Illustration of the watershed segmentation algorithm.\\nConsider the image and its gradient in Figs. 10.59(a) and (b), respectively. Application of the watershed \\nalgorithm just described yielded the watershed lines (white paths) shown superimposed on the gradient \\nimage in Fig. 10.59(c). These segmentation boundaries are shown superimposed on the original image in \\nFig. 10.59(d). As noted at the beginning of this section, the segmentation boundaries have the important \\nproperty of being connected paths.\\nTHE USE OF MARKERS\\nDirect application of the watershed segmentation algorithm in the form discussed \\nin the previous section generally leads to over-segmentation, caused by noise and \\nother local irregularities of the gradient. As Fig. 10.60 illustrates, over-segmentation \\ncan be serious enough to render the result of the algorithm virtually useless. In this \\ncase, this means a large number of segmented regions. A practical solution to this \\nproblem is to limit the number of allowable regions by incorporating a preprocess-\\ning stage designed to bring additional knowledge into the segmentation procedure.\\nAn approach used to control over-segmentation is based on the concept of mark-\\ners. A \\nmarker\\n is a connected component belonging to an image. We have \\ninternal \\nmarkers\\n, associated with objects of interest, and \\nexternal markers\\n, associated with \\nthe background. A procedure for marker selection typically will consist of two prin-\\ncipal steps: (1) preprocessing; and (2) deﬁnition of a set of criteria that markers \\nmust satisfy. To illustrate, consider Fig. 10.60(a) again. Part of the problem that led \\nto the over-segmented result in Fig. 10.60(b) is the large number of potential min-\\nima. Because of their size, many of these minima are irrelevant detail. As has been \\npointed out several times in earlier discussions, an effective method for minimizing \\nthe effect of small spatial detail is to ﬁlter the image with a smoothing ﬁlter. This is \\nan appropriate preprocessing scheme in this case also.\\nDIP4E_GLOBAL_Print_Ready.indb   793\\n6/16/2017   2:14:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 794}),\n",
       " Document(page_content='794\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nb a\\nd c\\nFIGURE 10.59\\n(a) Image of blobs. \\n(b) Image gradient.  \\n(c) Watershed lines, \\nsuperimposed on \\nthe gradient image.\\n(d) Watershed lines \\nsuperimposed on \\nthe original image. \\n(Courtesy of Dr. \\nS. Beucher, CMM/\\nEcole des Mines de \\nParis.)\\nb a\\nFIGURE 10.60\\n(a) Electrophoresis \\nimage.  \\n(b) Result of apply-\\ning the watershed \\nsegmentation algo-\\nrithm to the gradient \\nimage.  \\nOver-segmentation \\nis evident.  \\n(Courtesy of Dr. \\nS. Beucher, CMM/\\nEcole des Mines de \\nParis.)\\nDIP4E_GLOBAL_Print_Ready.indb   794\\n6/16/2017   2:14:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 795}),\n",
       " Document(page_content='10.7\\n  \\nSegmentation Using Morphological Watersheds\\n    \\n795\\nSuppose that we deﬁne an internal marker as (1) a region that is surrounded by \\npoints of higher “altitude”; (2) such that the points in the region form a connected \\ncomponent; and (3) in which all the points in the connected component have the \\nsame intensity value. After the image was smoothed, the internal markers resulting \\nfrom this deﬁnition are shown as light gray, blob-like regions in Fig. 10.61(a). Next, \\nthe watershed algorithm was applied to the smoothed image, under the restriction \\nthat these internal markers be the only allowed regional minima. Figure 10.61(a) \\nshows the resulting watershed lines. These watershed lines are deﬁned as the exter-\\nnal markers. Note that the points along the watershed line pass along the highest \\npoints between neighboring markers.\\nThe external markers in Fig. 10.61(a) effectively partition the image into regions, \\nwith each region containing a single internal marker and part of the background. \\nThe problem is thus reduced to partitioning each of these regions into two: a single \\nobject, and its background. We can bring to bear on this simpliﬁed problem many of \\nthe segmentation techniques discussed earlier in this chapter. Another approach is \\nsimply to apply the watershed segmentation algorithm to each individual region. In \\nother words, we simply take the gradient of the smoothed image [as in Fig. 10.59(b)] \\nand restrict the algorithm to operate on a single watershed that contains the marker \\nin that particular region. Figure 10.61(b) shows the result obtained using this \\napproach. The improvement over the image in 10.60(b) is evident.\\nMarker selection can range from simple procedures based on intensity values \\nand connectivity, as we just illustrated, to more complex descriptions involving size, \\nshape, location, relative distances, texture content, and so on (see Chapter 11 regard-\\ning feature descriptors). The point is that using markers brings a priori knowledge \\nto bear on the segmentation problem. Keep in mind that humans often aid segmen-\\ntation and higher-level tasks in everyday vision by using a priori knowledge, one \\nof the most familiar being the use of context. Thus, the fact that segmentation by \\nwatersheds offers a framework that can make effective use of this type of knowledge \\nis a signiﬁcant advantage of this method.\\nb a\\nFIGURE 10.61\\n(a) Image showing \\ninternal markers \\n(light gray regions) \\nand external  \\nmarkers (watershed \\nlines).  \\n(b) Result of \\nsegmentation. Note \\nthe improvement \\nover Fig. 10.60(b). \\n(Courtesy of Dr. \\nS. Beucher, CMM/\\nEcole des Mines de \\nParis.)\\nDIP4E_GLOBAL_Print_Ready.indb   795\\n6/16/2017   2:14:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 796}),\n",
       " Document(page_content='796\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n10.8 THE USE OF MOTION IN SEGMENTATION  \\nMotion is a powerful cue used by humans and many animals to extract objects or \\nregions of interest from a background of irrelevant detail. In imaging applications, \\nmotion arises from a relative displacement between the sensing system and the \\nscene being viewed, such as in robotic applications, autonomous navigation, and \\ndynamic scene analysis. In the following discussion we consider the use of motion in \\nsegmentation both spatially and in the frequency domain.\\nSPATIAL TECHNIQUES\\nIn what follows, we will consider two approaches for detecting motion, working direct-\\nly in the spatial domain. The key objective is to give you an idea how to measure \\nchanges in digital images using some straightforward techniques. \\nA Basic Approach\\nOne of the simplest approaches for detecting changes between two image frames \\nfx y t\\ni\\n(,, )\\n and \\nfx y t\\nj\\n(,, )\\n taken at times \\nt\\ni\\n and \\nt\\nj\\n,\\n respectively, is to compare the two \\nimages pixel by pixel.\\n One procedure for doing this is to form a difference image. \\nSuppose that we have a \\nreference image\\n containing only stationary components. \\nComparing this image against a subsequent image of the same scene, but including \\none or more moving objects, results in the difference of the two images canceling the \\nstationary elements, leaving only nonzero entries that correspond to the nonstation-\\nary image components.\\nA \\ndifference image\\n of two images (of the same size) taken at times \\nt\\ni\\n and \\nt\\nj\\n may \\nbe deﬁned as\\n \\ndx y\\nfx y t fx y t T\\nij\\nij\\n(,)\\n(,, ) (,, )\\n=\\n−>\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n0\\nif  \\notherwise\\n \\n(10-114)\\nwhere \\nT\\n is a nonnegative threshold. Note that \\ndx y\\nij\\n(,)\\n has a value of 1 at spatial coor-\\ndinates \\n(,)\\nxy\\n only if the intensity difference between the two images is appreciably \\ndifferent at those coordinates\\n, as determined by \\nT\\n.\\n Note also that coordinates \\n(,)\\nxy\\n \\nin Eq.\\n (10-114) span the dimensions of the two images, so the difference image is of \\nthe same size as the images in the sequence.\\nIn the discussion that follows, all pixels in \\ndx y\\nij\\n(,)\\n that have value 1 are consid-\\nered the result of object motion.\\n This approach is applicable only if the two imag-\\nes are registered spatially, and if the illumination is relatively constant within the \\nbounds established by \\nT\\n.\\n In practice, 1-valued entries in \\ndx y\\nij\\n(,)\\n may arise as a result \\nof noise also\\n. Typically, these entries are isolated points in the difference image, and \\na simple approach to their removal is to form 4- or 8-connected regions of 1’s in \\nimage \\ndx y\\nij\\n(,) ,\\n then ignore any region that has less than a predetermined number of \\nelements\\n. Although it may result in ignoring small and/or slow-moving objects, this \\napproach improves the chances that the remaining entries in the difference image \\nactually are the result of motion, and not noise.\\n10.8\\nDIP4E_GLOBAL_Print_Ready.indb   796\\n6/16/2017   2:14:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 797}),\n",
       " Document(page_content='10.8\\n  \\nThe Use of Motion in Segmentation\\n    \\n797\\nAlthough the method just described is simple, it is used frequently as the basis of \\nimaging systems designed to detect changes in controlled environments, such as in \\nsurveillance of parking facilities, buildings, and similar ﬁxed locales.\\nAccumulative Differences\\nConsider a sequence of image frames denoted by \\nfx y t fx y t fx y t\\nn\\n( ,,) ,( ,, ) , ,( ,, ) ,\\n12\\n…\\n \\nand let \\nfx y t\\n(,\\n, )\\n1\\n be the reference imag\\ne\\n. An \\naccumulative difference image \\n(ADI) \\nis formed by comparing this reference image with every subsequent image in the \\nsequence. A counter for each pixel location in the accumulative image is increment-\\ned every time a difference occurs at that pixel location between the reference and an \\nimage in the sequence. Thus, when the \\nk\\nth frame is being compared with the refer-\\nence, the entry in a given pixel of the accumulative image gives the number of times \\nthe intensity at that position was different [as determined by \\nT\\n in Eq. (10-114)] from \\nthe corresponding pixel value in the reference image\\n.\\nAssuming that the intensity values of the moving objects are greater than the \\nbackground, we consider three types of ADIs. Let \\nRxy\\n(,\\n)\\n denote the reference \\nimage and,\\n to simplify the notation, let \\nk\\n denote \\nt\\nk\\n so that \\nfx y k fx y t\\nk\\n(,, ) (,, ) .\\n=\\n We \\nassume that \\nRxy f xy\\n(,\\n) (,,) .\\n=\\n1\\n Then, for any \\nk\\n>\\n1,\\n and keeping in mind that the \\nvalues of the \\nADIs are counts, we deﬁne the following accumulative differences for \\nall relevant values of \\n(,) :\\nxy\\n \\nAx y\\nAx y R x y f x y k T\\nAx y\\nk\\nk\\nk\\n(,)\\n(,) (,) (,, )\\n(,)\\n=\\n+− >\\n−\\n−\\n1\\n1\\n1i f   \\notherwise\\n⎧ ⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(10-115)\\n \\nPx y\\nPx y R x y f x y k T\\nPx y\\nk\\nk\\nk\\n(,)\\n(,) (,) (,, )\\n(,)\\n=\\n+− >\\n−\\n−\\n1\\n1\\n1i f   \\notherwise\\n⎧ ⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(10-116)\\nand\\n \\nNx y\\nNx y R x y f x y k T\\nNx y\\nk\\nk\\nk\\n(,)\\n(,) (,) (,, )\\n(,)\\n=\\n+− <\\n−\\n−\\n−\\n1\\n1\\n1i f   \\notherwise\\ne\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(10-117)\\nwhere \\nAx y\\nk\\n(,) ,\\n \\nP\\nxy\\nk\\n(,) ,\\n and \\nNx y\\nk\\n(,)\\n are the \\nabsolute\\n, \\npositive\\n,\\n and \\nnegative\\n ADIs, \\nrespectively, computed using the \\nk\\nth image in the sequence. All three ADIs start \\nout with zero counts and are of the same size as the images in the sequence. The \\norder of the inequalities and signs of the thresholds in Eqs. (10-116) and (10-117) are \\nreversed if the intensity values of the background pixels are greater than the values \\nof the moving objects.\\nEXAMPLE 10.27 :  Computation of the absolute, positive, and negative accumulative difference images.\\nFigure 10.62 shows the three ADIs displayed as intensity images for a rectangular object of dimension \\n75 50\\n×\\n pixels that is moving in a southeasterly direction at a speed of \\n52\\n pixels per frame. The images \\nDIP4E_GLOBAL_Print_Ready.indb   797\\n6/16/2017   2:14:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 798}),\n",
       " Document(page_content='798\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nare of size \\n256 256\\n×\\n pixels. We note the following: (1) The nonzero area of the positive ADI is equal \\nto the size of the moving object;\\n (2) the location of the positive ADI corresponds to the location of the \\nmoving object in the reference frame; (3) the number of counts in the positive ADI stops increasing \\nwhen the moving object is displaced completely with respect to the same object in the reference frame; \\n(4) the absolute ADI contains the regions of the positive and negative ADI; and (5) the direction and \\nspeed of the moving object can be determined from the entries in the absolute and negative ADIs.\\nEstablishing a Reference Image\\nA key to the success of the techniques just discussed is having a reference image \\nagainst which subsequent comparisons can be made. The difference between two \\nimages in a dynamic imaging problem has the tendency to cancel all stationary com-\\nponents, leaving only image elements that correspond to noise and to the moving \\nobjects.\\nObtaining a reference image with only stationary elements is not always pos-\\nsible, and building a reference from a set of images containing one or more moving \\nobjects becomes necessary. This applies particularly to situations describing busy \\nscenes or in cases where frequent updating is required. One procedure for generat-\\ning a reference image is as follows. Consider the ﬁrst image in a sequence to be the \\nreference image. When a nonstationary component has moved completely out of \\nits position in the reference frame, the corresponding background in the present \\nframe can be duplicated in the location originally occupied by the object in the ref-\\nerence frame. When all moving objects have moved completely out of their original \\npositions, a reference image containing only stationary components will have been \\ncreated. Object displacement can be established by monitoring the changes in the \\npositive ADI, as indicated earlier. The following example illustrates how to build a \\nreference frame using the approach just described.\\nb a\\nc\\nFIGURE 10.62\\n ADIs of a rectangular object moving in a southeasterly direction. (a) Absolute ADI. (b) Positive ADI. \\n(c) Negative ADI.\\nDIP4E_GLOBAL_Print_Ready.indb   798\\n6/16/2017   2:14:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 799}),\n",
       " Document(page_content='10.8\\n  \\nThe Use of Motion in Segmentation\\n    \\n799\\nEXAMPLE 10.28 :  Building a reference image.\\nFigures 10.63(a) and (b) show two image frames of a trafﬁc intersection. The ﬁrst image is considered \\nthe reference, and the second depicts the same scene some time later. The objective is to remove the \\nprincipal moving objects in the reference image in order to create a static image. Although there are \\nother smaller moving objects, the principal moving feature is the automobile at the intersection mov-\\ning from left to right. For illustrative purposes we focus on this object. By monitoring the changes in \\nthe positive ADI, it is possible to determine the initial position of a moving object, as explained above. \\nOnce the area occupied by this object is identiﬁed, the object can be removed from the image by sub-\\ntraction. By looking at the frame in the sequence at which the positive ADI stopped changing, we can \\ncopy from this image the area previously occupied by the moving object in the initial frame. This area \\nthen is pasted onto the image from which the object was cut out, thus restoring the background of that \\narea. If this is done for all moving objects, the result is a reference image with only static components \\nagainst which we can compare subsequent frames for motion detection. The reference image resulting \\nfrom removing the east-bound moving vehicle and restoring the background is shown in Fig. 10.63(c).\\nFREQUENCY DOMAIN TECHNIQUES\\nIn this section, we consider the problem of determining motion via a Fourier trans-\\nform formulation. Consider a sequence \\nfx y t t K\\n(,\\n,) , ,, , , ,\\n=−\\n012 1\\n…\\n of \\nK\\n digital \\nimage frames of size \\nMN\\n×\\n pixels, generated by a stationary camera. We begin the \\ndevelopment by assuming that all frames have a homogeneous background of zero \\nintensity\\n. The exception is a single, 1-pixel object of unit intensity that is moving \\nwith constant velocity. Suppose that for frame one \\n() ,\\nt\\n=\\n0\\n the object is at location \\n(,)\\nxy\\n/H11032/H11032\\n and the image plane is projected onto the \\nx\\n-axis;\\n that is, the pixel intensities \\nare summed (for each row) across the columns in the image. This operation yields \\na 1-D array with \\nM\\n entries that are zero, except at \\nx\\n/H11032\\n,\\n which is the \\nx\\n-coordinate of \\nthe single-point object.\\n If we now multiply all the components of the 1-D array by \\nthe quantity \\nexp\\nja\\nx t\\n2\\n1\\np\\nΔ\\n[]\\n for \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and add the results, we obtain \\nthe single term \\nexp\\nja\\nx t\\n2\\n1\\np\\n′Δ\\n[]\\n because there is only one nonzero point in the array. \\nIn this notation, \\na\\n1\\n is a positive integer, and \\n/H9004\\nt\\n is the time interval between frames.\\nb a\\nc\\nFIGURE 10.63\\n Building a static reference image. (a) and (b) Two frames in a sequence. (c) Eastbound automobile sub-\\ntracted from (a), and the background restored from the corresponding area in (b). (Jain and Jain.)\\nDIP4E_GLOBAL_Print_Ready.indb   799\\n6/16/2017   2:14:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 800}),\n",
       " Document(page_content='800\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nSuppose that in frame two \\n() ,\\nt\\n=\\n1\\n the object has moved to coordinates \\n(, ) ;\\nxy\\n/H11032/H11032\\n+\\n1  \\nthat is\\n, it has moved 1 pixel parallel to the \\nx\\n-axis. Then, repeating the projection pro-\\ncedure discussed in the previous paragraph yields the sum \\nexp\\n.\\nja\\nx t\\n21\\n1\\np\\n′+\\n()\\nΔ\\n⎡\\n⎣\\n⎤\\n⎦\\n If \\nthe object continues to move 1 pixel location per frame then,\\n at any integer instant \\nof time, \\nt\\n,\\n the result will be \\nexp\\n,\\nja\\nxtt\\n2\\n1\\np\\n′+\\n()\\nΔ\\n⎡\\n⎣\\n⎤\\n⎦\\n which, using Euler’s formula, may \\nbe expressed as\\n \\nea\\nx\\nt\\nt\\nj\\na\\nx\\nt\\nt\\nja x t t\\n2\\n11\\n1\\n22\\np\\npp\\n′\\n′′\\n+\\n()\\nΔ\\n=+\\n(\\n)\\nΔ\\n⎡\\n⎣\\n⎤\\n⎦\\n++\\n(\\n)\\nΔ\\n⎡\\n⎣\\n⎤\\n⎦\\ncos\\nsin\\n \\n(10-118)\\nfor \\ntK\\n=−\\n012\\n1\\n,,, , .\\n…\\n In other words, this procedure yields a complex sinusoid \\nwith frequenc\\ny \\na\\n1\\n.\\n If the object were moving \\nV\\n1\\n pixels (in the \\nx\\n-direction) between \\nframes, the sinusoid would have frequency \\nVa\\n11\\n.\\n Because \\nt\\n varies between 0 and \\nK\\n−\\n1\\n in integer increments, restricting \\na\\n1\\n to have integer values causes the discrete \\nFourier transform of the complex sinusoid to have two peaks—one located at fre-\\nquency \\nVa\\n11\\n and the other at \\nKV a\\n−\\n11\\n.\\n This latter peak is the result of symmetry in \\nthe discrete F\\nourier transform, as discussed in Section 4.6, and may be ignored. Thus \\na peak search in the Fourier spectrum would yield one peak with value \\nVa\\n11\\n.\\n Divid-\\ning this quantity by \\na\\n1\\n yields \\nV\\n1\\n,\\n which is the velocity component in the \\nx\\n-direction,\\n \\nas the frame rate is assumed to be known. A similar analysis would yield \\nV\\n2\\n,\\n the \\ncomponent of velocity in the \\ny\\n-direction.\\nA sequence of frames in which no motion takes place produces identical exponen-\\ntial terms\\n, whose Fourier transform would consist of a single peak at a frequency of 0 \\n(a single dc term). Therefore, because the operations discussed so far are linear, the \\ngeneral case involving one or more moving objects in an arbitrary static background \\nwould have a Fourier transform with a peak at dc corresponding to static image \\ncomponents, and peaks at locations proportional to the velocities of the objects.\\nThese concepts may be summarized as follows. For a sequence of \\nk\\n digital images \\nof size \\nMN\\n×\\n pixels, the sum of the weighted projections onto the \\nx\\n-axis at any inte-\\nger instant of time is\\n \\ngt a f x y t e t K\\nx\\ny\\nN\\nx\\nM\\nja x t\\n(, ) ( , ,)\\n, , ,\\n1\\n0\\n1\\n0\\n1\\n2\\n1\\n01 1\\n==\\n−\\n=\\n−\\n=\\n−\\nΔ\\n∑ ∑\\np\\n…\\n \\n(10-119)\\nSimilarly, the sum of the projections onto the \\ny\\n-axis is\\n \\ngt a f x y t e t K\\ny\\nx\\nM\\ny\\nN\\nja y t\\n(, ) ( , ,)\\n, , ,\\n2\\n0\\n1\\n0\\n1\\n2\\n2\\n01 1\\n==\\n−\\n=\\n−\\n=\\n−\\nΔ\\n∑ ∑\\np\\n…\\n \\n(10-120)\\nwhere, as noted earlier, \\na\\n1\\n and \\na\\n2\\n are positive integers.\\nThe 1D Fourier transforms of Eqs. (10-119) and (10-120), respectively, are\\n \\nGua gt ae u K\\nxx\\nt\\nK\\nju t K\\n(,) ( ,)\\n, ,,\\n11\\n1\\n0\\n1\\n2\\n1\\n1\\n01 1\\n== −\\n=\\n−\\n−\\n∑\\np\\n…\\n \\n(10-121)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   800\\n6/16/2017   2:14:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 801}),\n",
       " Document(page_content='10.8\\n  \\nThe Use of Motion in Segmentation\\n    \\n801\\n \\nGua gt ae u K\\nyy\\nt\\nK\\nju t K\\n(,) ( ,)\\n, ,,\\n22\\n2\\n0\\n1\\n2\\n2\\n2\\n01 1\\n== −\\n=\\n−\\n−\\n∑\\np\\n…\\n \\n(10-122)\\nThese transforms are computed using an FFT algorithm, as discussed in Section 4.11.\\nT\\nhe frequency-velocity relationship is\\n \\nua V\\n11 1\\n=\\n \\n(10-123)\\nand\\n \\nua V\\n22 2\\n=\\n \\n(10-124)\\nIn the preceding formulation, the unit of velocity is in pixels per total frame time. \\nF\\nor example, \\nV\\n1\\n10\\n=\\n indicates motion of 10 pixels in \\nK\\n frames. For frames that \\nare taken uniformly\\n, the actual physical speed depends on the frame rate and the \\ndistance between pixels. Thus, if \\nV\\n1\\n10\\n=\\n,\\n and \\nK\\n=\\n30\\n,\\n the frame rate is two images \\nper second,\\n and the distance between pixels is 0.5 m, then the actual physical speed \\nin the \\nx\\n-direction is\\n \\nV\\n1\\n10 0 5 2 30\\n=\\n() ( ) ( ) ( )\\npixels m pixel frames s frames\\n.\\nThe sign of the \\nx\\n-component of the velocity is obtained by computing\\n \\nS\\ndg t a\\ndt\\nx\\nx\\ntn\\n1\\n2\\n1\\n2\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\nRe ,\\n \\n(10-125)\\nand\\n \\nS\\ndg t a\\ndt\\nx\\nx\\ntn\\n2\\n2\\n1\\n2\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\nIm ,\\n \\n(10-126)\\nBecause \\ng\\nx\\n is sinusoidal, it can be shown (see Problem 10.53) that \\nS\\nx\\n1\\n and \\nS\\nx\\n2\\n will \\nhave the same sign at an arbitrary point in time, \\nn\\n,\\n if the velocity component \\nV\\n1\\n \\nis positive. Conversely, opposite signs in \\nS\\nx\\n1\\n and \\nS\\nx\\n2\\n indicate a negative velocity \\ncomponent. If either \\nS\\nx\\n1\\n or \\nS\\nx\\n2\\n is zero, we consider the next closest point in time, \\ntn t\\n=±\\n/H9004\\n. Similar comments apply to computing the sign of \\nV\\n2\\n.\\nEXAMPLE 10.29 :  Detection of a small moving object via frequency-domain analysis.\\nFigures 10.64 through 10.66 illustrate the effectiveness of the approach just developed. Figure 10.64 \\nshows one of a 32-frame sequence of LANDSAT images generated by adding white noise to a reference \\nimage. The sequence contains a superimposed target moving at 0.5 pixel per frame in the \\nx\\n-direction \\nand 1 pixel per frame in the \\ny\\n-direction. The target, shown circled in Fig. 10.65, has a Gaussian intensity \\ndistribution spread over a small (9-pixel) area, and is not easily discernible by eye. Figure 10.66 shows \\nDIP4E_GLOBAL_Print_Ready.indb   801\\n6/16/2017   2:14:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 802}),\n",
       " Document(page_content='802\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nthe results of computing Eqs. (10-121) and (10-122) with \\na\\n1\\n6\\n=\\n and \\na\\n2\\n4\\n=\\n,\\n respectively. The peak at \\nu\\n1\\n3\\n=\\n in Fig. 10.66(a) yields \\nV\\n1\\n05\\n=\\n.\\n from Eq. (10-123). Similarly, the peak at \\nu\\n2\\n4\\n=\\n in Fig. 10.66(b) \\nyields \\nV\\n2\\n10\\n=\\n.  from Eq. (10-124).\\nGuidelines for selecting \\na\\n1\\n and \\na\\n2\\n can be explained with the aid of Fig. 10.66. For \\ninstance, suppose that we had used \\na\\n2\\n15\\n=\\n instead of \\na\\n2\\n4\\n=\\n.\\n In that case, the peaks in \\nF\\nig. 10.66(b) would now be at \\nu\\n2\\n15\\n=\\n and 17 because \\nV\\n2\\n10\\n=\\n..\\n This would be a seri-\\nously aliased result.\\n As discussed in Section 4.5, aliasing is caused by under-sampling \\n(too few frames in the present discussion, as the range of \\nu\\n is determined by \\nK\\n).\\n \\nBecause \\nua V\\n=\\n,\\n one possibility is to select \\na\\n as the integer closest to \\nau V\\n=\\nmax max\\n, \\nFIGURE 10.64\\nLANDSAT \\nframe. (Cowart, \\nSnyder, and \\nRuedger.)\\ny\\nx\\nFIGURE 10.65\\nIntensity plot of \\nthe image in  \\nFig. 10.64, with \\nthe target circled. \\n(Rajala, Riddle, \\nand Snyder.)\\nDIP4E_GLOBAL_Print_Ready.indb   802\\n6/16/2017   2:14:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 803}),\n",
       " Document(page_content='10.8\\n  \\nThe Use of Motion in Segmentation\\n    \\n803\\nb a\\nFIGURE 10.66\\n (a) Spectrum of Eq. (10-121) showing a peak at \\nu\\n1\\n3\\n=\\n.\\n (b) Spectrum of Eq. (10-122) showing a peak at \\nu\\n2\\n4\\n=\\n. (Rajala, Riddle, and Snyder.)\\n0 4 8 12 16 20 24 28 32 36 40\\nFrequency\\nMagnitude ( \\n/H11003\\n 10)\\n0\\n80\\n160\\n240\\n320\\n400\\n480\\n560\\n640\\n0 4 8 12 16 20 24 28 32 36 40\\nFrequency\\n0\\n20\\n40\\n60\\n80\\n100\\nMagnitude ( \\n/H11003\\n 10\\n2\\n)\\nSummary, References, and Further Reading\\n \\nBecause of its central role in autonomous image processing, segmentation is a topic covered in most books dealing \\nwith image processing, image analysis, and computer vision. The following books provide complementary and/or \\nsupplementary reading for our coverage of this topic: Umbaugh [2010]; Prince [2012]; Nixon and Aguado, A [2012]; \\nPratt [2014]; and Petrou and Petrou [2010].\\nWork dealing with the use of kernels to detect intensity discontinuities (see Section 10.2) has a long history. \\nNumerous kernels have been proposed over the years: Roberts [1965]; Prewitt [1970]; and Kirsh [1971]. The Sobel \\noperators are from [Sobel]; see also Danielsson and Seger [1990]. Our presentation of the zero-crossing properties of \\nthe Laplacian is based on Marr [1982]. The Canny edge detector discussed in Section 10.2 is due to Canny [1986]. The \\nbasic reference for the Hough transform is Hough [1962]. See Ballard [1981], for a generalization to arbitrary shapes.\\nOther approaches used to deal with the effects of illumination and reﬂectance on thresholding are illustrated by \\nthe work of Perez and Gonzalez [1987], Drew et al. [1999], and Toro and Funt [2007]. The optimum thresholding \\napproach due to Otsu [1979] has gained considerable acceptance because it combines excellent performance with \\nsimplicity of implementation, requiring only estimation of image histograms. The basic idea of using preprocessing \\nto improve thresholding dates back to an early paper by White and Rohrer [1983]), which combined thresholding, \\nthe gradient, and the Laplacian in the solution of a difﬁcult segmentation problem.\\nSee Fu and Mui [1981] for an early survey on the topic of region-oriented segmentation. The work of Haddon \\nand Boyce [1990] and of Pavlidis and Liow [1990] are among the earliest efforts to integrate region and boundary \\ninformation for the purpose of segmentation. Region growing is still an active area of research in image processing, \\nas exempliﬁed by Liangjia et al. [2013]. The basic reference on the \\nk\\n-means algorithm presented in Section 10.5 \\ngoes way back several decades to an obscure 1957 Bell Labs report by Lloyd, who subsequenty published in Lloyd \\n[1982]. This algorithm was already being in used in areas such as pattern recognition in the 1960s and ’70s (Tou and \\nwhere \\nu\\nmax\\n is the aliasing frequency limitation established by \\nK\\n,\\n and \\nV\\nmax\\n is the \\nmaximum expected object velocity.\\nDIP4E_GLOBAL_Print_Ready.indb   803\\n6/16/2017   2:14:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 804}),\n",
       " Document(page_content='804\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nGonzalez [1974]). The superpixel algorithm presented in Section 10.5 is from Achanta et al. [2012]. See their paper \\nfor a listing and comparison of other superpixel approaches\\n. The material on graph cuts is based on the paper by \\nShi and Malik [2000]. See Hochbaum [2010] for an example of faster implementations.\\nSegmentation by watersheds was shown in Section 10.7 to be a powerful concept. Early references dealing with \\nsegmentation by watersheds are Serra [1988], and Beucher and Meyer [1992]. As indicated in our discussion in Sec-\\ntion 10.7, one of the key issues with watersheds is the problem of over-segmentation. The papers by Bleau and Leon \\n[2000] and by Gaetano et al. [2015] are illustrative of approaches for dealing with this problem. \\nThe material in Section 10.8 dealing with accumulative differences is from Jain, R. [1981]. See also Jain, Kasturi, \\nand Schunck [1995]. The material dealing with motion via Fourier techniques is from Rajala, Riddle, and Snyder \\n[1983]. The books by Snyder and Qi [2004], and by \\nChakrabarti et al. [2015]\\n, provide additional reading on \\nmotion estimation. For details on the software aspects of many of the examples in this chapter, see Gonzalez, Woods, \\nand Eddins [2009].\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n.\\n10.1 * \\nIn a Taylor series approximation, the \\nremainder\\n \\n(also called the \\ntruncation error\\n) consists of all \\nthe terms not used in the approximation.\\n The \\nﬁrst term in the remainder of a ﬁnite difference \\napproximation is indicative of the error in the \\napproximation. The higher the derivative order \\nof that term is, the lower the error will be in the \\napproximation. All three approximations to the \\nﬁrst derivative given in Eqs. (10-4)-(10-6) are \\ncomputed using the same number of sample \\npoints. However, the error of the central differ-\\nence approximation is less than the other two. \\nShow that this is true.\\n10.2 \\nDo the following:\\n(a) * \\nShow how Eq. (10-8) was obtained.\\n(b) \\nShow how Eq. (10-9) was obtained.\\n10.3 \\nA binary image contains straight lines oriented \\nhorizontally\\n, vertically, at 45°, and at \\n−\\n45\\n°.\\n Give \\na set of \\n33\\n×\\n kernels that can be used to detect \\none-pixel breaks in these lines\\n. Assume that the \\nintensities of the lines and background are 1 and \\n0, respectively.\\n10.4 \\nPropose a technique for detecting gaps of length \\nranging between 1 and \\nK\\n pixels in line segments \\nof a binary image\\n. Assume that the lines are one \\npixel thick. Base your technique on 8-neighbor \\nconnectivity analysis, rather than attempting to \\nconstruct kernels for detecting the gaps.\\n10.5 * \\nWith reference to Fig. 10.6, what are the angles \\n(measured with respect to the \\nx\\n-axis of the book \\naxis convention in F\\nig. 2.19) of the horizontal and \\nvertical lines to which the kernels in Figs. 10.6(a) \\nand (c) are most responsive?\\n10.6 \\nRefer to Fig. 10.7 in answering the following ques- \\ntions\\n.\\n(a) * \\nSome of the lines joining the pads and center \\nelement in F\\nig. 10.7(e) are single lines, while \\nothers are double lines. Explain why.\\n(b) \\nPropose a method for eliminating the com-\\nponents in F\\nig. 10.7(f) that are not part of \\nthe line oriented at \\n−\\n45\\n°\\n.\\n(c) \\n10.7 \\nWith reference to the edge models in Fig. 10.8, \\nanswer the following without generating the gra-\\ndient and angle images\\n. Simply provide sketches \\nof the proﬁles that show what you would expect \\nthe proﬁles of the magnitude and angle images \\nto look like.\\n(a) * \\nSuppose that we compute the gradient mag-\\nnitude of each of these models using the \\nPrewitt kernels in F\\nig. 10.14. Sketch what a \\nhorizontal proﬁle through the center of each \\ngradient image would look like.\\n(b) \\nSketch a horizontal proﬁle for each corre-\\nsponding angle image\\n.\\n10.8 \\nConsider a horizontal intensity proﬁle through \\nDIP4E_GLOBAL_Print_Ready.indb   804\\n6/16/2017   2:14:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 805}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n805\\nthe middle of a binary image that contains a ver-\\ntical step edge through the center of the image. \\nDraw what the proﬁle would look like after the \\nimage has been blurred by an averaging kernel \\nof size \\nnn\\n×\\n with coefﬁcients equal to \\n1\\n2\\nn\\n.\\n For \\nsimplicity\\n, assume that the image was scaled so \\nthat its intensity levels are 0 on the left of the \\nedge and 1 on its right. Also, assume that the size \\nof the kernel is much smaller than the image, so \\nthat image border effects are not a concern near \\nthe center of the image.\\n10.9 * \\nSuppose that we had used the edge models in the \\nfollowing image\\n, instead of the ramp in Fig. 10.10. \\nSketch the gradient and Laplacian of each proﬁle.\\nImage\\nProfile of a\\nhorizontal line\\n10.10 \\nDo the following:\\n(a) * \\nShow that the \\ndirection\\n of steepest (maxi-\\nmum) ascent of a function \\nf\\n at point \\n(,)\\nxy\\n \\nis given by the vector \\n/H11612\\nfx\\ny\\n(,)\\n in Eq. (10-16), \\nand that the rate of that descent is \\n/H11612\\nfx y\\n(,) ,\\n \\ndeﬁned in Eq.\\n (10-17).\\n(b) \\nShow that the \\ndirection\\n of \\nsteepest descent\\n is \\ngiven by the vector \\n−\\n/H11612\\nfx\\ny\\n(,) ,\\n and that the \\nrate\\n of the steepest descent is \\n/H11612\\nfx y\\n(,) .\\n(c) \\nGive the description of an image whose gra-\\ndient magnitude image would be the same\\n, \\nwhether we computed it using Eq. (10-17) or \\n(10-26). A constant image is not acceptable \\nanswer.\\n10.11 \\nDo the following.\\n(a) \\nHow would you modify the Sobel and \\nPrewitt kernels in F\\nig. 10.14 so that they give \\ntheir strongest gradient response for edges \\noriented at \\n±°\\n45\\n?\\n(b) * \\nShow that the Sobel and Prewitt kernels \\nin F\\nig. 10.14, and in (a) above, and give iso-\\ntropic results only for horizontal and verti-\\ncal edges, and for edges oriented at \\n±°\\n45\\n,\\n \\nrespectively\\n.\\n10.12 \\nThe results obtained by a single pass through an \\nimage of some 2-D kernels can be achieved also \\nby two passes using 1-D kernels\\n. For example, \\nthe same result of using a \\n33\\n×\\n smoothing kernel \\nwith coefﬁcients \\n19\\n can be obtained by a pass \\nof the kernel \\n[]\\n111\\n through an image, followed \\nby a pass of the result with the kernel \\n[] .\\n111\\nT\\n \\nThe ﬁnal result is then scaled by \\n19 .\\n Show that \\nthe response of Sobel kernels (F\\nig. 10.14) can \\nbe implemented similarly by one pass of the \\ndifferencing\\n kernel \\n[]\\n−\\n10\\n1\\n (or its vertical coun-\\nterpart) followed by the smoothing kernel \\n[]\\n121\\n \\n(or its vertical counterpart).\\n10.13 \\nA popular variation of the compass kernels\\n \\nshown\\n \\nin Fig. 10.15 is based on using coefﬁcients \\nwith values 0,\\n 1, and \\n−\\n1.\\n(a) * \\nGive the form of the eight compass kernels \\nusing these coefﬁcients\\n. As in Fig. 10.15, let N, \\nNW, . . . denote the direction of the edge that \\ngives the strongest response.\\n(b) \\nSpecify the gradient vector direction of the \\nedges detected by each kernel in (a).\\n10.14 \\nThe rectangle in the following binary image is of \\nsize \\nmn\\n×\\n pixels.\\n(a) * \\nWhat would the magnitude of the gradient \\nof this image look like based on using the \\napproximation in Eq.\\n (10-26)? Assume that\\ng\\nx\\n and \\ng\\ny\\n are obtained using the Sobel ker-\\nnels. Show all relevant different pixel values \\nin the gradient image.\\n(b) \\nWith reference to Eq. (10-18) and Fig. 10.12, \\nDIP4E_GLOBAL_Print_Ready.indb   805\\n6/16/2017   2:14:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 806}),\n",
       " Document(page_content='806\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nsketch the histogram of edge \\ndirections\\n.\\n Be \\nprecise in labeling the height of each compo-\\nnent of the histogram.\\n(c) \\nWhat would the Laplacian of this image look \\nlike based on using Eq.\\n (10-14)? Show all \\nrelevant different pixel values in the Lapla-\\ncian image.\\n10.15 \\nSuppose that an image \\nfx y\\n(,\\n)\\n is convolved with \\na kernel of size \\nnn\\n×\\n (with coefﬁcients \\n1\\n2\\nn\\n)\\n to \\nproduce a smoothed image \\nfx y\\n(,) .\\n(a) * \\nDerive an expression for edge strength \\n(edge magnitude) as a function of \\nn\\n.\\n Assume \\nthat \\nn\\n is odd and that the partial derivatives \\nare computed using Eqs. (10-19) and (10-20).\\n(b) \\nShow that the ratio of the maximum edge \\nstrength of the smoothed image to the maxi-\\nmum edge strength of the original image is \\n1\\nn\\n.\\n In other words, edge strength is inversely \\nproportional to the size of the smoothing \\nkernel,\\n as one would expect.\\n10.16 \\nWith reference to Eq. (10-29),\\n(a) * \\nShow that the average value of the LoG \\noperator\\n, \\n/H11612\\n2\\nGxy\\n(,) ,\\n is zero.\\n(b) \\nShow that the average value of any image \\nconvolved with this operator also is zero\\n. \\n(\\nHint:\\n Consider solving this problem in the \\nfrequency domain, using the convolution \\ntheorem and the fact that the average value \\nof a function is proportional to its Fourier \\ntransform evaluated at the origin.)\\n(c) \\nSuppose that we: (1) used the kernel in Fig. \\n10.4(a) to approximate the Laplacian of a \\nGaussian,\\n and (2) convolved this result with \\nany image. What would be true in general of \\nthe values of the resulting image? Explain. \\n(\\nHint:\\n Take a look at Problem 3.32.)\\n10.17 \\nRefer to Fig. 10.22(c).\\n(a) \\nExplain why the edges form closed contours.\\n(b) * \\nDoes the zero-crossing method for ﬁnding \\nedge location always result in closed con-\\ntours? Explain.\\n10.18 \\nOne often ﬁnds in the literature a derivation of \\nthe Laplacian of a Gaussian (LoG) that starts \\nwith the expression\\nGr e\\nr\\n()\\n=\\n−\\n22\\n2\\ns\\nwhere \\nrx y\\n22 2\\n=+\\n.\\n The LoG is then derived by \\ntaking the second partial derivative with respect \\nto \\nr\\n: \\n/H11612\\n22 2\\nGr Gr r\\n() () .\\n=∂ ∂\\n Finally, \\nxy\\n22\\n+\\n is sub-\\nstituted for \\nr\\n2\\n to get the ﬁnal (\\nincorrect\\n) result:\\n \\n/H11612\\n22\\n2\\n2\\n4\\n22 2\\n2\\nGx y x y\\nxy\\n,\\nexp\\n(\\n)\\n=+ −\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\nss\\ns\\nDerive this result and explain the reason for the \\ndifference between this expression and Eq.\\n (10-29).\\n10.19 \\nDo the following:\\n(a) * \\nDerive Eq. (10-33).\\n(b) \\nLet \\nk\\n=\\nss\\n12\\n denote the standard \\ndeviation \\nratio discussed in connection with the DoG \\nfunction, and express Eq. (10-33) in terms of \\nk\\n and \\ns\\n2\\n.\\n10.20 \\nIn the following, assume that \\nG\\n and \\nf\\n are discrete \\narrays of size \\nnn\\n×\\n and \\nMN\\n×\\n, respectively.\\n(a) \\nShow that the 2-D convolution of the Gauss-\\nian function \\nGxy\\n(,\\n)\\n in Eq. (10-27) with an \\nimage \\nfx y\\n(,\\n)\\n can be expressed as a 1-D con-\\nvolution along the rows (columns) of \\nfx y\\n(,\\n) ,\\n \\nfollowed by a 1-D convolution along the col-\\numns (rows) of the result.\\n (\\nHints:\\n See Sec-\\ntion 3.4\\n \\nregarding discrete convolution and \\nseparability).\\n(b) * \\nDerive an expression for the computa-\\ntional advantage using the 1-D convolution \\napproach in (a) as opposed to implementing \\nthe 2-D convolution directly\\n. Assume that \\nGxy\\n(,\\n)\\n is sampled to produce an array of size \\nnn\\n×\\n and that \\nfx y\\n(,\\n)\\n is of size \\nMN\\n×\\n.\\n The \\ncomputational advantage is the ratio of the \\nnumber of multiplications required for 2-D \\nconvolution to the number required for 1-D \\nconvolution.\\n (\\nHint:\\n Review the subsection \\non separable kernels in Section 3.4.)\\n10.21 \\nDo the following.\\n(a) \\nShow that Steps 1 and 2 of the Marr-Hildreth \\nalgorithm can be implemented using four \\n1-D convolutions\\n. (\\nHints:\\n Refer to Problem \\n10.20(a) and express the Laplacian operator \\nas the sum of two partial derivatives, given \\nDIP4E_GLOBAL_Print_Ready.indb   806\\n6/16/2017   2:14:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 807}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n807\\nby Eqs. (10-10) and (10-11), and implement \\neach derivative using a 1-D kernel, as in \\nProblem 10.12.)\\n(b) \\nDerive an expression for the computational \\nadvantage of using the 1-D convolution \\napproach in (a) as opposed to implementing \\nthe 2-D convolution directly\\n. Assume that \\nGxy\\n(,\\n)\\n is sampled to produce an array of \\nsize \\nnn\\n×\\n and that \\nfx y\\n(,\\n)\\n is of size \\nMN\\n×\\n. \\nT\\nhe computational advantage is the ratio of \\nthe number of multiplications required for \\n2-D convolution to the number required for \\n1-D convolution (see Problem 10.20).\\n10.22 \\nDo the following.\\n(a) * \\nFormulate Step 1 and the gradient mag-\\nnitude image computation in Step 2 of the \\nCanny algorithm using 1-D instead of 2-D \\nconvolutions\\n.\\n(b) \\nWhat is the computational advantage of \\nusing the 1-D convolution approach as \\nopposed to implementing a 2-D convolu-\\ntion.\\n Assume that the 2-D Gaussian ﬁlter in \\nStep 1 is sampled into an array of size \\nnn\\n×\\n \\nand that the input image is of size \\nMN\\n×\\n. \\nExpress the computational advantage as \\nthe ratio of the number of multiplications \\nrequired by each method.\\n10.23 \\nWith reference to the three vertical edge models \\nand corresponding proﬁles in F\\nig. 10.8 provide \\nsketches of the proﬁles that would result from \\neach of the following methods. You may sketch \\nthe proﬁles manually.\\n(a) * \\nSuppose that we compute the gradient \\nmagnitude of each of the three edge model \\nimages using the Sobel kernels\\n. Sketch the \\nhorizontal intensity proﬁles of the three \\nresulting gradient images.\\n(b) \\nSketch the horizontal intensity proﬁles that \\nwould result from using the \\n33\\n×\\n Laplacian \\nkernel in F\\nig. 10.10.4(a).\\n(c) * \\nRepeat (b) using only the ﬁrst two steps of \\nthe Marr\\n-Hildreth edge detector.\\n(d) \\nRepeat (b) using the ﬁrst two steps of the \\nCanny edge detector\\n. You may ignore the \\nangle images.\\n(e) \\nSketch the horizontal proﬁles of the angle \\nimages resulting from using the Canny edge \\ndetector\\n.\\n10.24 \\nIn Example 10.9, we used a smoothing kernel of \\nsize \\n19 19\\n×\\n to generate Fig. 10.26(c) and a kernel \\nof size \\n13 13\\n×\\n to generate Fig. 10.26(d). What was \\nthe rationale that led to choosing these values? \\n(\\nHint:\\n Observe that both are Gaussian kernels\\n, \\nand refer to the discussion of lowpass Gaussian \\nkernels in Section 3.5.)\\n10.25 \\nRefer to the Hough transform in Section 10.2.\\n(a) \\nPropose a general procedure for obtaining \\nthe normal representation of a line from its \\nslope-intercept form,\\n \\nya x b\\n=+\\n.\\n(b) * \\nFind the normal representation of the line \\nyx\\n=−\\n+\\n21 .\\n10.26 \\nRefer to the Hough transform in Section 10.2.\\n(a) * \\nExplain why the Hough mapping of the point \\nlabeled 1 in F\\nig. 10.30(a) is a straight line in \\nFig. 10.30(b).\\n(b) * \\nIs this the only point that would produce that \\nresult? Explain.\\n(c) \\nExplain the reﬂective adjacency relationship \\nillustrated by\\n, for example, the curve labeled \\nQ\\n in Fig. 10.30(b).\\n10.27 \\nShow that the number of operations required to \\nimplement the accumulator\\n-cell approach dis-\\ncussed in Section 10.2 is linear in \\nn\\n, the number \\nof non-background points in the image plane (i.e., \\nthe \\nxy\\n-plane).\\n10.28 \\nAn important application of image segmentation \\nis in processing images resulting from so-called \\nbub\\nble chamber\\n events. These images arise from \\nexperiments in high-energy physics in which a \\nbeam of particles of known properties is directed \\nonto a target of known nuclei. A typical event con-\\nsists of incoming tracks, any one of which, upon \\na collision, branches out into secondary tracks of \\nparticles emanating from the point of collision. \\nPropose a segmentation approach for detecting \\nall tracks angled at any of the following six direc-\\ntions off the horizontal: \\n±° ,\\n25  \\n±°\\n50\\n,\\n and \\n±° .\\n75  \\nT\\nhe estimation error allowed in any of these six \\ndirections is \\n±5° .\\n For a track to be valid it must \\nbe at least 100 pixels long and have no more than \\nthree gaps, each not exceeding 10 pixels. You may \\nDIP4E_GLOBAL_Print_Ready.indb   807\\n6/16/2017   2:14:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 808}),\n",
       " Document(page_content='808\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\nassume that the images have been preprocessed \\nso that they are binary and that all tracks are 1 \\nthick,\\n except at the point of collision from which \\nthey emanate. Your procedure should be able to \\ndifferentiate between tracks that have the same \\ndirection but different origins. (\\nHint:\\n Base your \\nsolution on the Hough transform.)\\n10.29 * \\nRestate the basic global thresholding algorithm \\nin Section 10.3 so that it uses the histogram of an \\nimage instead of the image itself\\n.\\n10.30 * \\nProve that the basic global thresholding algo-\\nrithm in Section 10.3 converges in a ﬁnite number \\nof steps\\n. (\\nHint:\\n Use the histogram formulation \\nfrom Problem 10.29.)\\n10.31 \\nGive an explanation why the initial threshold in \\nthe basic global thresholding algorithm in Sec-\\ntion 10.3 must be between the minimum and \\nmaximum values in the image\\n. (\\nHint:\\n Construct \\nan example that shows the algorithm failing for a \\nthreshold value selected outside this range.)\\n10.32 * \\nAssume that the initial threshold in the basic \\nglobal thresholding algorithm in Section 10.3 is \\nselected as a value between the minimum and \\nmaximum intensity values in an image\\n. Do you \\nthink the ﬁnal value of the threshold at conver-\\ngence depends on the speciﬁc initial value used? \\nExplain. (You can use a simple image example to \\nsupport your conclusion.)\\n10.33 \\nYou may assume in both of the following cases \\nthat the initial threshold is in the open interval \\n(, ) .\\n01\\nL\\n−\\n(a) * \\nShow that if the histogram of an image is \\nuniform over all possible intensity levels\\n, \\nthe basic global thresholding algorithm con-\\nverges to the average intensity of the image. \\n(b) \\nShow that if the histogram of an image is \\nbimodal,\\n with identical modes that are sym-\\nmetric about their means, then the basic \\nglobal thresholding algorithm will converge \\nto the point halfway between the means of \\nthe modes.\\n10.34 \\nRefer to the basic global thresholding algorithm in \\nSection 10.3.\\n Assume that in a given problem, the \\nhistogram is bimodal with modes that are Gauss-\\nian curves of the form \\nAz m\\n11\\n2\\n1\\n2\\n2\\nexp[ ( ) ]\\n−−\\ns\\n \\nand\\nAz m\\n22\\n2\\n2\\n2\\n2\\nexp[ ( ) ].\\n−−\\ns\\n Assume that \\nm\\n1\\n is \\ngreater than \\nm\\n2\\n,\\n and that the initial \\nT\\n is between\\n \\nthe max and min image intensities. Give conditions \\n(in terms of the parameters of these curves) for the \\nfollowing to be true when the algorithm converges:\\n(a) * \\nThe threshold is equal to \\n() .\\nmm\\n12\\n2\\n+\\n(b) * \\nThe threshold is to the left of \\nm\\n2\\n.\\n(c) \\nThe threshold is in the interval given by the \\nequation \\n() .\\nmm T m\\n12 1\\n2\\n+< <\\n10.35 \\nDo the following:\\n(a) * \\nShow how the ﬁrst line in Eq. (10-60) fol-\\nlows from Eqs\\n. (10-55), (10-56), and (10-59).\\n(b) \\nShow how the second line in Eq. (10-60) \\nfollows from the ﬁrst.\\n10.36 \\nShow that a maximum value for Eq. (10-63) \\nalways exists for \\nk\\n in the range \\n01\\n≤≤\\n−\\nkL\\n.\\n10.37 * \\nWith reference to Eq. (10-65), advance an \\nargument that establishes that \\n01\\n≤≤\\nh\\n()\\nk\\n for \\nk\\n \\nin the range \\n01\\n≤≤\\n−\\nkL\\n,\\n where the minimum \\nis achievable only by images with constant inten-\\nsity\\n, and the maximum occurs only for 2-valued \\nimages with values 0 and \\n() .\\nL\\n−\\n1\\n10.38 \\nDo the following:\\n(a) * \\nSuppose that the intensities of a digital \\nimage \\nfx y\\n(,\\n)\\n are in the range \\n[, ]\\n01\\n and that \\na threshold,\\n \\nT\\n, successfully segmented the \\nimage into objects and background. Show \\nthat the threshold \\nTT\\n′=\\n−\\n1\\n will success-\\nfully segment the \\nnegative of \\nfx y\\n(,\\n)\\n into the \\nsame regions\\n. The term negative is used here \\nin the sense deﬁned in Section 3.2.\\n(b) \\nThe intensity transformation function in \\n(a) that maps an image into its negative is \\na linear function with negative slope\\n. State \\nthe conditions that an arbitrary intensity \\ntransformation function must satisfy for the \\nsegmentability of the original image with \\nrespect to a threshold, \\nT\\n, to be preserved. \\nWhat would be the value of the threshold \\nafter the intensity transformation?\\n10.39 \\nThe objects and background in the image below \\nhave a mean intensity of 170 and 60,\\n respectively, \\non a [0, 255] scale. The image is corrupted by \\nGaussian noise with 0 mean and a standard devia-\\ntion of 10 intensity levels. Propose a thresholding \\nDIP4E_GLOBAL_Print_Ready.indb   808\\n6/16/2017   2:14:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 809}),\n",
       " Document(page_content='  \\n  \\nProblems\\n    \\n809\\nmethod capable of a correct segmentation rate of \\n90% or higher. (Recall that 99.7% of the area of \\na Gaussian curve lies in a \\n±\\n3\\ns\\n interval about the \\nmean,\\n where \\ns\\n is the standard deviation.)\\n10.40 \\nRefer to the intensity ramp image in Fig. 10.34(b) \\nand the moving-average algorithm discussed in \\nSection 10.3.\\n Assume that the image is of size \\n500 700\\n×\\n pixels and that its minimum and maxi-\\nmum values are 0 and 1,\\n where 0’s are contained \\nonly in the ﬁrst column.\\n(a) * \\nWhat would be the result of segmenting this \\nimage with the moving-average algorithm \\nusing \\nb\\n=\\n0\\n and an arbitrary value for \\nn\\n.\\nExplain what the segmented image would \\nlook like\\n.\\n(b) \\nNow reverse the direction of the ramp so \\nthat its leftmost value is 1 and the rightmost \\nvalue is 0 and repeat (a).\\n(c) \\nRepeat (a) but with \\nb\\n=\\n1 and \\nn\\n=\\n2.\\n(d) \\nRepeat (a) but with \\nb\\n=\\n1 and \\nn\\n=\\n100.\\n10.41 \\nPropose a region-growing algorithm to segment \\nthe image in Problem 10.39.\\n10.42 * \\nSegment the image shown by using the split and \\nmerge procedure discussed in Section 10.4.\\n Let \\nQR\\ni\\n()\\n=\\nTRUE\\n if all pixels in \\nR\\ni\\n have the same \\nintensity. Show the quadtree corresponding to \\nyour segmentation.\\nN\\nN\\n10.43 \\nConsider the region of 1’s resulting from the \\nsegmentation of the sparse regions in the image \\nof the Cygnus Loop in Example 10.21.\\n Propose \\na technique for using this region as a mask to \\nisolate the three main components of the image: \\n(1) background; (2) dense inner region; and (3) \\nsparse outer region.\\n10.44 \\nLet the pixels in the ﬁrst row of a \\n33\\n×\\n image, like \\nthe one in F\\nig. 10.53(a), be labeled as 1, 2, 3, and \\nthe pixels in the second and third rows be labeled \\nas 4, 5, 6 and 7, 8, 9, respectively. Let the inten-\\nsity of these pixels be [90, 80, 30; 70, 5, 20; 80 20 \\n30] where, for example, the intensity of pixel 2 is \\n80 and of pixel 4 it is 70. Compute the weights \\nfor the edges for the graph in Fig. 10.53(c), using \\nthe formula \\nw\\n(,\\n) [ ( ) ( ) ]\\nij In In c\\nij\\n=− +\\n30 1\\n/H20907/H20919\\n/H20919\\nAB\\nexplained in the text in connection with that \\nﬁgure (we scaled the formula by 30 to make the \\nnumerical results easier to interpret).\\n Let \\nc\\n=\\n0\\nin this case.\\n10.45 * \\nShow how Eqs. (10-106) through (10-108) follow \\nfrom Eq.\\n (10-105).\\n10.46 \\nDemonstrate the validity of Eq. (10-102).\\n10.47 \\nRefer to the discussion in Section 10.7.\\n(a) * \\nShow that the elements of \\nCM\\nni\\n()\\n and \\nTn\\n[]\\nare never replaced during execution of the \\nwatershed segmentation algorithm.\\n(b) \\nShow that the number of elements in sets \\nCM\\nni\\n()\\n and \\nTn\\n[]\\n either increases or remains \\nthe same as \\nn\\n increases\\n.\\n10.48 \\nYou saw in Section 10.7 that the boundaries \\nobtained using the watershed segmentation algo-\\nrithm form closed loops (for example\\n, see Figs. \\n10.59 and 10.61). Advance an argument that estab-\\nlishes whether or not closed boundaries \\nalways\\n \\nresult from application of this algorithm.\\n10.49 * \\nGive a step-by-step implementation of the dam-\\nbuilding procedure for the one-dimensional inten-\\nsity cross section shown below\\n. Show a drawing \\nof the cross section at each step, showing “water” \\nlevels and dams constructed.\\n123456789 1 0 1 1 1 2 1 3 1 4 1 5\\nx\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nDIP4E_GLOBAL_Print_Ready.indb   809\\n6/16/2017   2:14:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 810}),\n",
       " Document(page_content='810\\n    \\nChapter\\n \\n10\\n  \\nImage Segmentation\\n10.50 \\nWhat would the negative ADI image shown \\nin F\\nig. 10.62(c) look like if we tested against \\nT\\n \\n(instead of testing against \\n−\\nT\\n) in Eq. (10-117)?\\n10.51 \\nAre the following statements true or false? Ex-\\nplain the reason for your answer in each.\\n(a) * \\nThe nonzero entries in the absolute ADI \\ncontinue to grow in dimension,\\n provided \\nthat the object is moving.\\n(b) \\nThe nonzero entries in the positive ADI  \\nalways occupy the same area,\\n regardless of \\nthe motion undergone by the object.\\n(c) \\nThe nonzero entries in the negative ADI \\ncontinue to grow in dimension,\\n provided \\nthat the object is moving.\\n10.52 \\nSuppose that in Example 10.29 motion along the \\nx\\n-axis is set to zero\\n. The object now moves only \\nalong the \\ny\\n-axis at 1 pixel per frame for 32 frames \\nand then (instantaneously) reverses direction \\nand moves in exactly the opposite direction for  \\nanother 32 frames. What would Figs. 10.66(a)  \\nand (b) look like under these conditions?\\n10.53 * \\nAdvance an argument that demonstrates that \\nwhen the signs of \\nS\\nx\\n1\\n and \\nS\\nx\\n2\\n in Eqs. (10-125) \\nand (10-126) are the same, velocity component \\nV\\n1\\n is positive.\\n10.54 \\nAn automated pharmaceutical plant uses image \\nprocessing to measure the shapes of medication \\ntablets for the purpose of quality control.\\n The \\nsegmentation stage of the system is based on \\nOtsu’s method. The speed of the inspection lines \\nis so high that a very high rate ﬂash illumina-\\ntion is required to “stop” motion. When new, the  \\nillumination lamps project a uniform pattern of \\nlight. However, as the lamps age, the illumination  \\npattern deteriorates as a function of time and \\nspatial coordinates according to the equation\\n \\nixy At te\\nxM yN\\n(,) ( )\\n[( ) ( ) ]\\n=−\\n−− +−\\n22 2\\n22\\nwhere \\nMN\\n22\\n,\\n(\\n)\\n is the center of the viewing \\narea and \\nt\\n is time measured in increments of \\nmonths. The lamps are still experimental and \\nthe behavior of \\nAt\\n()\\n is not fully understood by \\nthe manufacturer\\n. All that is known is that, dur-\\ning the life of the lamps, \\nAt\\n()\\n is always greater \\nthan the negative component in the preceding \\nequation because illumination cannot be nega-\\ntive\\n. It has been observed that Otsu’s algorithm \\nworks well when the lamps are new, and their  \\npattern of illumination is nearly constant over the  \\nentire image. \\nHowever, segmentation perfor-\\nmance deteriorates with time. Being experimental, \\nthe lamps are exceptionally expensive, so you are \\nemployed as a consultant to help solve \\nthe prob-\\nlem using digital image processing techniques to \\ncompensate for the changes in illumination, and \\nthus extend the useful life of the lamps. You are \\ngiven ﬂexibility to install any special markers or \\nother visual cues in the viewing area of the imag-\\ning cameras. Propose a solution in sufﬁcient detail \\nthat the engineering plant manager can under-\\nstand your approach. (\\nHint:\\n Review the image \\nmodel discussed in Section 2.3 and consider using \\none or more targets of known reﬂectivity.)\\n10.55 \\nThe speed of a bullet in ﬂight is to be estimated by \\nusing high-speed imaging techniques\\n. The method \\nof choice involves the use of a CCD camera and \\nﬂash that exposes the scene for \\nK\\n seconds. The bul-\\nlet is 2.5 cm long, 1 cm wide, and its range of speed \\nis \\n750 250\\n±\\nms .\\n The camera optics produce an  \\nimage in which the bullet occupies 10% of the \\nhorizontal resolution of a \\n256 256\\n×\\n digital image.\\n(a) * \\nDetermine the maximum value of \\nK\\n that \\nwill guarantee that the blur from motion \\ndoes not exceed 1 pixel.\\n(b) \\nDetermine the minimum number of frames \\nper second that would have to be acquired \\nin order to guarantee that at least two com-\\nplete images of the bullet are obtained dur\\n-\\ning its path through the ﬁeld of view of the \\ncamera.\\n(c) * \\nPropose a segmentation procedure for \\nautomatically extracting the bullet from a \\nsequence of frames\\n.\\n(d) \\nPropose a method for automatically deter-\\nmining the speed of the bullet.\\nDIP4E_GLOBAL_Print_Ready.indb   810\\n6/16/2017   2:14:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 811}),\n",
       " Document(page_content='81111\\nFeature Extraction \\nPreview\\nAfter an image has been segmented into regions or their boundaries using methods such as those in \\nChapters 10 and 11, the resulting sets of segmented pixels usually have to be converted into a form suit-\\nable for further computer processing. Typically, the step after segmentation is \\nfeature extraction\\n, which \\nconsists of feature detection and feature description. \\nFeature detection\\n refers to ﬁnding the features \\nin an image, region, or boundary. \\nFeature description\\n assigns quantitative attributes to the detected \\nfeatures. For example, we might \\ndetect\\n corners in a region boundary, and \\ndescribe \\nthose corners by \\ntheir orientation and location, both of which are quantitative attributes. Feature processing methods \\ndiscussed in this chapter are subdivided into three principal categories, depending on whether they are \\napplicable to boundaries, regions, or whole images. Some features are applicable to more than one cat-\\negory. Feature descriptors should be as insensitive as possible to variations in parameters such as scale, \\ntranslation, rotation, illumination, and viewpoint. The descriptors discussed in this chapter are either \\ninsensitive to, or can be normalized to compensate for, variations in one or more of these parameters. \\nUpon completion of this chapter, readers should:\\n Understand the meaning and applicability of \\na broad class of features suitable for image \\nprocessing.\\n Understand the concepts of feature vectors \\nand feature space, and how to relate them \\nto the various descriptors developed in this \\nchapter.\\n Be skilled in the mathematical tools used in \\nfeature extraction algorithms.\\n Be familiar with the limitations of the various \\nfeature extraction methods discussed.\\n Understand the principal steps used in the \\nsolution of feature extraction problems. \\n Be able to formulate feature extraction algo-\\nrithms.\\n Have a “feel” for the types of features that \\nhave a good chance of success in a given \\napplication.\\nWell, but reflect; have we not several times  \\nacknowledged that names rightly given are the  \\nlikenesses and images of the things which they name?\\nSocrates\\nDIP4E_GLOBAL_Print_Ready.indb   811\\n6/16/2017   2:14:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 812}),\n",
       " Document(page_content='812\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\n11.1  BACKGROUND  \\nAlthough there is no universally accepted, formal definition of what constitutes an \\nimage feature\\n, there is little argument that, intuitively, we generally think of a fea-\\nture as a distinctive attribute or description of “something” we want to label or \\ndifferentiate. For our purposes, the key words here are \\nlabel\\n and \\ndifferentiate\\n. The \\n“something” of interest in this chapter refers either to individual image objects, or \\neven to entire images or sets of images. Thus, we think of features as attributes that \\nare going to help us assign unique labels to objects in an image or, more gener-\\nally, are going to be of value in differentiating between entire images or families of \\nimages.\\nThere are two principal aspects of image \\nfeature extraction\\n: \\nfeature detection\\n, and \\nfeature description\\n. That is, when we refer to feature extraction, we are referring \\nto both detecting the features and then describing them. To be useful, the extrac-\\ntion process must encompass both. The terminology you are likely to encounter in \\nimage processing and analysis to describe feature detection and description varies, \\nbut a simple example will help clarify our use of these term. Suppose that we use \\nobject corners as features for some image processing task. In this chapter, detection \\nrefers to \\nﬁnding\\n the corners in a region or image. Description, on the other hand, \\nrefers to \\nassigning\\n \\nquantitative\\n (or sometimes \\nqualitative\\n) \\nattributes\\n to the detected \\nfeatures, such as corner orientation, and location with respect to other corners. In \\nother words, knowing that there are corners in an image has limited use without \\nadditional information that can help us differentiate between objects in an image, \\nor between images, based on corners and their attributes.\\nGiven that we want to use features for purposes of differentiation, the next ques-\\ntion is: What are the important characteristics that these features must possess in \\nthe realm of digital image processing? You are already familiar with some of these \\ncharacteristics. In general, features should be independent of location, rotation, and \\nscale. Other factors, such as independence of illumination levels and changes caused \\nby the viewpoint between the imaging sensor(s) and the scene, also are impor-\\ntant. Whenever possible, preprocessing should be used to normalize input images \\nbefore feature extraction. For example, in situations where changes in illumination \\nare severe enough to cause difﬁculties in feature detection, it would make sense to \\npreprocess an image to compensate for those changes. Histogram equalization or \\nspeciﬁcation come to mind as automatic techniques that we know are helpful in \\nthis regard. The idea is to use as much a priori information as possible to preprocess \\nimages in order to improve the chances of accurate feature extraction.\\nWhen used in the context of a feature, the word “independent” usually has one of \\ntwo meanings: invariant or covariant. A feature descriptor is \\ninvariant\\n with respect \\nto a set of transformations if its value remains unchanged after the application (to \\nthe entity being described) of any transformation from the family. A feature descrip-\\ntor is \\ncovariant\\n with respect to a set of transformations if applying to the entity any \\ntransformation from the set produces the same result in the descriptor. For example, \\nconsider this set of afﬁne transformations: {\\ntranslation\\n, \\nreﬂection\\n, \\nrotation\\n}, and sup-\\npose that we have an elliptical region to which we assign the feature descriptor \\narea\\n. \\nClearly, applying any of these transformations to the region does not change its area. \\n11.1\\nSee Table 2.3 regarding \\nafﬁne transformations.\\nDIP4E_GLOBAL_Print_Ready.indb   812\\n6/16/2017   2:14:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 813}),\n",
       " Document(page_content='11.1\\n  \\nBackground\\n    \\n813\\nTherefore, \\narea\\n is an invariant feature descriptor with respect to the given family of \\ntransformations. However, if we add the afﬁne transformation \\nscaling\\n to the fam-\\nily, descriptor \\narea\\n ceases to be invariant with respect to the extended family. The \\ndescriptor is now covariant with respect to the family, because scaling the area of the \\nregion by any factor scales the value of the descriptor by the same factor. Similarly, \\nthe descriptor \\ndirection\\n (of the principal axis of the region) is covariant because \\nrotating the region by any angle has the same effect on the value of the descriptor. \\nMost of the feature descriptors we use in this chapter are covariant in general, in \\nthe sense that they may be invariant to some transformations of interest, but not to \\nothers that may be equally as important. As you will see shortly, it is good practice to \\nnormalize as many relevant invariances as possible out of covariances. For instance, \\nwe can compensate for changes in direction of a region by computing its actual \\ndirection and rotating the region so that its principal axis points in a predeﬁned \\ndirection. If we do this for every region detected in an image, \\nrotation\\n will cease to \\nbe covariant.\\nAnother major classiﬁcation of features is \\nlocal\\n vs. \\nglobal\\n. You are likely to see \\nmany different attempts to classify features as belonging to one of these two catego-\\nries. What makes this difﬁcult is that a feature may belong to both, depending on the \\napplication. For example, consider the descriptor \\narea\\n again, and suppose that we \\nare applying it to the task of inspecting the degree to which bottles moving past an \\nimaging sensor on a production line are full of liquid. The sensor and its accompany-\\ning software are capable of generating images of ten bottles at once, in which liquid \\nin each bottle appears as a bright region, and the rest of the image appears as dark \\nbackground. The area of a region in this ﬁxed geometry is directly proportional to \\nthe amount of liquid in a bottle and, if detected and measured reliably, \\narea\\n is the \\nonly feature we need to solve the inspection problem. Each image has ten regions, so \\nwe consider area to be a local feature, in the sense that it is applicable to \\nindividual\\n \\nelements (regions) of an image. If the problem were to detect the \\ntotal\\n amount (area) \\nof liquid in an image, we would now consider area to be a global descriptor. But the \\nstory does not end there. Suppose that the liquid inspection task is redeﬁned so that \\nit calculates the entire amount of liquid per day passing by the imaging station. We \\nno longer care about the area of individual regions per se. Our units now are images. \\nIf we know the total area in an image, and we know the number of images, calculat-\\ning the total amount of liquid in a day is trivial. Now the area of an entire image is a \\nlocal feature, and the area of the total at the end of the day is global. Obviously, we \\ncould redeﬁne the task so that the area at the end of a day becomes a local feature \\ndescriptor, and the area for all assembly lines becomes a global measure. And so on, \\nendlessly. In this chapter, we call a feature \\nlocal\\n if it is applies to a member of a set, \\nand \\nglobal\\n if it applies to the entire set, where “member” and “set” are determined \\nby the application.\\nFeatures by themselves are seldom generated for human consumption, except in \\napplications such as interactive image processing, topics that are not in the main-\\nstream of this book. In fact, as you will see later, some feature extraction meth-\\nods generate tens, hundreds, or even thousands of descriptor values that would \\nappear meaningless if examined visually. Instead, feature description typically is \\nused as a preprocessing step for higher-level tasks, such as image registration, object \\nDIP4E_GLOBAL_Print_Ready.indb   813\\n6/16/2017   2:14:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 814}),\n",
       " Document(page_content='814\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nrecognition for automated inspection, searching for patterns (e.g., individual faces \\nand/or ﬁngerprints) in image databases, and autonomous applications, such as robot \\nand vehicle navigation. For these applications, numerical features usually are “pack-\\naged” in the form of a \\nfeature vector\\n, (i.e., a \\n1\\n×\\nn\\n or \\nn\\n×\\n1\\n matrix) whose elements are \\nthe descriptors\\n. An RGB image is one of the simplest examples. As you know from \\nChapter 6, each pixel of an RGB image can be expressed as 3-D vector,\\n \\nx\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\nx\\nx\\n1\\n2\\n3\\nin which \\nx\\n1\\n is the intensity value of the red image at a point, and the other com-\\nponents are the intensity values of the green and blue images at the same point. If \\ncolor is used as a feature, then a region in an RGB image would be represented as \\na set of feature vectors (points) in 3-D space. When \\nn\\n descriptors are used, feature \\nvectors become \\nn\\n-dimensional, and the space containing them is referred to as an \\nn-dimensional feature space\\n. You may “visualize” a set of \\nn\\n-dimensional feature vec-\\ntors as a “hypercloud” of points in \\nn\\n-dimensional Euclidean space.\\nIn this chapter, we group features into three principal categories: \\nboundary\\n, \\nregion\\n, and \\nwhole image\\n features. This subsidivision is not based on the applicabil-\\nity of the methods we are about to discuss; rather, it is based on the fact that some \\ncategories make more sense than others when considered in the context of what is \\nbeing described. For example, it is implied that when we refer to the “length of a \\nboundary” we are referring to the “length of the boundary of a region,” but it makes \\nno sense to refer to the “length” of an image. It will become clear that many of the \\nfeatures we will be discussing are applicable to boundaries and regions, and some \\napply to whole images as well.\\n11.2  BOUNDARY PREPROCESSING  \\nThe segmentation techniques discussed in the previous two chapters yield raw data \\nin the form of pixels along a boundary or pixels contained in a region. It is standard \\npractice to use schemes that compact the segmented data into representations that \\nfacilitate the computation of descriptors. In this section, we discuss various bound-\\nary preprocessing approaches suitable for this purpose. \\nBOUNDARY FOLLOWING (TRACING)\\nSeveral of the algorithms discussed in this chapter require that the points in the \\nboundary of a region be ordered in a clockwise or counterclockwise direction. Con-\\nsequently, we begin our discussion by introducing a boundary-following algorithm \\nwhose output is an \\nordered\\n sequence of points. We assume (1) that we are work-\\ning with binary images in which object and background points are labeled 1 and 0, \\nrespectively; and (2) that images are padded with a border of 0’s to eliminate the \\npossibility of an object merging with the image border. For clarity, we limit the dis-\\ncussion to single regions. The approach is extended to multiple, disjoint regions by \\nprocessing the regions individually.\\n11.2\\nYou will ﬁnd it helpful to \\nreview the discussion in \\nSections 2.5\\n \\non neighbor-\\nhoods, adjacency and \\nconnectivity, and the \\ndiscussion in Section 9.6\\n \\ndealing with connected \\ncomponents.\\nDIP4E_GLOBAL_Print_Ready.indb   814\\n6/16/2017   2:14:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 815}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n815\\nThe following algorithm traces the boundary of a 1-valued region, \\nR\\n, in a binary \\nimage.\\n1. \\nLet the starting point, \\nb\\n0\\n,\\n be the \\nuppermost\\n-\\nleftmost\\n point\\n†\\n in the image that is \\nlabeled 1. Denote by \\nc\\n0\\n the \\nwest\\n neighbor of \\nb\\n0\\n [see Fig. 11.1(b)]. Clearly, \\nc\\n0\\n is \\nalways a background point. Examine the 8-neighbors of \\nb\\n0\\n,\\n starting at \\nc\\n0\\n and \\nproceeding in a clockwise direction. Let \\nb\\n1\\n denote the \\nﬁrst\\n neighbor encountered \\nwhose value is 1, and let \\nc\\n1\\n be the (background) point immediately preceding \\nb\\n1\\n \\nin the sequence. Store the locations of \\nb\\n0\\n for use in Step 5.\\n2. \\nLet \\nbb\\n=\\n0\\n and \\ncc\\n=\\n0\\n.\\n3. \\nLet the 8-neighbors of \\nb\\n,\\n starting at \\nc\\n and proceeding in a clockwise direction, \\nbe denoted by \\nnn n\\n12 8\\n,,,.\\n…\\n Find the ﬁrst neighbor labeled 1 and denote it by \\nn\\nk\\n.\\n4. \\nLet \\nbn\\nk\\n=\\n and \\ncn\\nk\\n=\\n–\\n.\\n1\\n5. \\nRepeat Steps 3 and 4 until \\nbb\\n=\\n0\\n.\\n The sequence of \\nb\\n points found when the \\nalgorithm stops is the set of ordered boundary points\\n.\\nNote that \\nc\\n in Step 4 is always a background point because \\nn\\nk\\n is the first 1-valued \\npoint found in the clockwise scan. This algorithm is referred to as the \\nMoore bound-\\nary tracing algorithm\\n after Edward F. Moore, a pioneer in cellular automata theory. \\nFigure 11.1 illustrates the ﬁrst few steps of the algorithm. It is easily veriﬁed (see \\nProblem 11.1) that continuing with this procedure will yield the correct boundary, \\nshown in Fig. 11.1(f), whose points are ordered in a clockwise sequence. The algo-\\nrithm works equally well with more complex boundaries, such as the boundary with \\nan attached branch in Fig. 11.2(a) or the self-intersecting boundary in Fig. 11.2(b). \\nMultiple boundaries [Fig. 11.2(c)] are handled by processing one boundary at a time.\\nIf we start with a binary region instead of a boundary, the algorithm extracts the \\nouter boundary\\n of the region. Typically, the resulting boundary will be one pixel \\nthick, but not always [see Problem 11.1(b)]. If the objective is to ﬁnd the boundaries \\nof holes in a region (these are called the \\ninner \\nor\\n interior boundaries\\n of the region), \\n†\\n As you will see later in this chapter and in Problem 11.8, the uppermost-leftmost point in a 1-valued boundary \\nhas the important property that a polygonal approximation to the boundary has a convex vertex at that location. \\nAlso, the left and north neighbors of the point are guaranteed to be background points. These properties make \\nit a good “standard” point at which to start boundary-following algorithms. \\nSee Section 2.5 for the \\ndeﬁnition of 4-neigh-\\nbors, 8-neighbors, and \\nm\\n-neighbors of a point,\\nb a\\nc\\ne d\\nf\\nFIGURE 11.1\\n Illustration of the ﬁrst few steps in the boundary-following algorithm. The point to be processed next is \\nlabeled in bold, black; the points yet to be processed are gray; and the points found by the algorithm are shaded. \\nSquares without labels are considered background (0) values.\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n111\\n1\\n1\\n1\\n1\\n1 1 1 1\\n1\\n1\\n1\\nc\\n0\\nb\\n0\\n1\\n11\\n1\\n1\\n1 1 1 1\\n1\\n1\\n1\\n1\\nc\\nb\\n1\\n1\\n1\\n1 1 1 1\\n1\\n1\\n1 1\\n1\\nc\\nb\\n. . .\\n1\\n1\\n1 1 1 1\\n1\\n1\\nc\\nb\\n1\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   815\\n6/16/2017   2:14:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 816}),\n",
       " Document(page_content='816\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\na straightforward approach is to extract the holes (see Section 9.6) and treat them \\nas 1-valued regions on a background of 0’s. Applying the boundary-following algo-\\nrithm to these regions will yield the inner boundaries of the original region.\\nWe could have stated the algorithm just as easily based on following a boundary \\nin the counterclockwise direction but you will ﬁnd it easier to have just one algo-\\nrithm and then reverse the order of the result to obtain a sequence in the opposite \\ndirection. We use both directions interchangeably (but consistently) in the following \\nsections to help you become familiar with both approaches.\\nCHAIN CODES\\nChain codes are used to represent a boundary by a connected sequence of straight-\\nline segments of specified length and direction. We assume in this section that all \\ncurves are closed, simple curves (i.e., curves that are closed and not self intersecting).\\nFreeman Chain Codes \\nTypically, a chain code representation is based on 4- or 8-connectivity of the seg-\\nments. The \\ndirection\\n of each segment is coded by using a numbering scheme, as in Fig. \\n11.3. A boundary code formed as a sequence of such directional numbers is referred \\nto as a \\nFreeman chain code\\n.\\nDigital images usually are acquired and processed in a grid format with equal \\nspacing in the \\nx\\n- and \\ny\\n-directions, so a chain code could be generated by following a \\nboundary in, say, a clockwise direction and assigning a direction to the segments con-\\nnecting every pair of pixels. This level of detail generally is not used for two principal \\nreasons: (1) The resulting chain would be quite long and (2) any small disturbances \\nalong the boundary due to noise or imperfect segmentation would cause changes \\nin the code that may not be related to the principal shape features of the boundary.\\nAn approach used to address these problems is to resample the boundary by \\nselecting a larger grid spacing, as in Fig. 11.4(a). Then, as the boundary is traversed, a \\nboundary point is assigned to a node of the coarser grid, depending on the proximity \\nof the original boundary point to that node, as in Fig. 11.4(b). The resampled bound-\\nary obtained in this way can be represented by a 4- or 8-code. Figure 11.4(c) shows \\nthe coarser boundary points represented by an 8-directional chain code. It is a simple \\nmatter to convert from an 8-code to a 4-code and vice versa (see Problems 2.15, 9.27, \\nb a\\nc\\nFIGURE 11.2\\n Examples of boundaries that can be processed by the boundary-following algo-\\nrithm. (a) Closed boundary with a branch. (b) Self-intersecting boundary. (c) Multiple bound-\\naries (processed one at a time). \\n1\\n1\\n1\\n1\\n1\\n111\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1 1\\nDIP4E_GLOBAL_Print_Ready.indb   816\\n6/16/2017   2:14:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 817}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n817\\nand\\n \\n9.29). For the same reason mentioned when discussing boundary tracing earlier \\nin this section,\\n we chose the starting point in Fig. 11.4(c) as the uppermost-leftmost \\npoint of the boundary, which gives the chain code \\n0766 1212\\n…\\n.\\n As you might suspect, \\nthe spacing of the resampling grid is determined by the application in which the \\nchain code is used.\\n \\nIf the sampling grid used to obtain a connected digital curve is a uniform quad-\\nrilateral (see Fig. 2.19) all points of a Freeman code based on Fig. 11.3 are guaran-\\nteed to coincide with the points of the curve. The same is true if a digital curve is \\nsubsampled using the same type of sampling grid, as in Fig. 11.4(b). This is because \\nthe samples of curves produced using such grids have the same arrangement as in \\nFig. 11.3, so all points are reachable as we traverse a curve from one point to the next \\nto generate the code. \\nThe numerical value of a chain code depends on the starting point. However, the \\ncode can be normalized with respect to the starting point by a straightforward pro-\\ncedure: We simply treat the chain code as a circular sequence of direction numbers \\nand redeﬁne the starting point so that the resulting sequence of numbers forms an \\ninteger of minimum magnitude. We can normalize also for rotation (in angles that \\nare integer multiples of the directions in Fig. 11.3) by using the \\nﬁrst difference\\n of the \\nchain code instead of the code itself. This difference is obtained by counting the num-\\nber of direction changes (in a counterclockwise direction in Fig. 11.3) that separate \\ntwo adjacent elements of the code. If we treat the code as a circular sequence to nor-\\nmalize it with respect to the starting point, then the ﬁrst element of the difference is \\ncomputed by using the transition between the last and ﬁrst components of the chain. \\n0\\n7\\n6\\n6\\n6\\n6\\n6\\n4\\n5\\n3\\n3\\n2\\n1\\n1\\n2\\n2\\nb a\\nc\\nFIGURE 11.4\\n(a) Digital \\nboundary with \\nresampling grid \\nsuperimposed.  \\n(b) Result of \\nresampling.  \\n(c) 8-directional \\nchain-coded \\nboundary.\\n1\\n0\\n3\\n2\\n1\\n0\\n6\\n7\\n5\\n4\\n3\\n2\\nb a\\nFIGURE 11.3\\nDirection  \\nnumbers for  \\n(a) 4-directional \\nchain code, and \\n(b) 8-directional \\nchain code.\\nDIP4E_GLOBAL_Print_Ready.indb   817\\n6/16/2017   2:14:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 818}),\n",
       " Document(page_content='818\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nFor instance, the ﬁrst difference of the 4-directional chain code 10103322 is 3133030. \\nSize normalization can be achieved by altering the spacing of the resampling grid.\\nThe normalizations just discussed are exact only if the boundaries themselves \\nare invariant to rotation (again, in angles that are integer multiples of the directions \\nin Fig. 11.3) and scale change, which seldom is the case in practice. For instance, \\nthe same object digitized in two different orientations will have different bound-\\nary shapes in general, with the degree of dissimilarity being proportional to image \\nresolution. This effect can be reduced by selecting chain elements that are long in \\nproportion to the distance between pixels in the digitized image, and/or by orienting \\nthe resampling grid along the principal axes of the object to be coded, as discussed \\nin Section 11.3, or along its eigen axes, as discussed in Section 11.5.\\nEXAMPLE 11.1 : Freeman chain code and some of its variations.\\nFigure 11.5(a) shows a \\n570 570\\n×\\n-p\\nixel,\\n 8-bit gray-scale image of a circular stroke embedded in small, \\nrandomly distributed specular fragments\\n. The objective of this example is to obtain a Freeman chain \\ncode, the corresponding integer of minimum magnitude, and the ﬁrst difference of the outer boundary \\nof the stroke. Because the object of interest is embedded in small fragments, extracting its boundary \\nwould result in a noisy curve that would not be descriptive of the general shape of the object. As you \\nknow, smoothing is a routine process when working with noisy boundaries. Figure 11.5(b) shows the \\noriginal image smoothed using a box kernel of size \\n99\\n×\\n pixels (see Section 3.5 for a discussion of spa-\\ntial smoothing),\\n and Fig. 11.5(c) is the result of thresholding this image with a global threshold obtained \\nusing Otsu’s method. Note that the number of regions has been reduced to two (one of which is a dot), \\nsigniﬁcantly simplifying the problem.\\nFigure 11.5(d) is the outer boundary of the region in Fig. 11.5(c). Obtaining the chain code of this \\nboundary directly would result in a long sequence with small variations that are not representative \\nof the global shape of the boundary, so we resample it before obtaining its chain code.  This reduces \\ninsigniﬁcant variability. Figure 11.5(e) is the result of using a resampling grid with nodes 50 pixels apart \\n(approximately 10% of the image width) and Fig. 11.5(f) is the result of joining the sample points by \\nstraight lines. This simpler approximation retained the principal features of the original boundary.\\nThe 8-directional Freeman chain code of the simpliﬁed boundary is\\n \\n0000606666666\\n6444444\\n24\\n2222202\\n                             2\\n202\\n  \\n \\nT\\nhe starting point of the boundary is at coordinates (2, 5) in the subsampled grid (remember from \\nFig. 2.19 that the origin of an image is at its top, left). This is the uppermost-leftmost point in Fig. 11.5(f). \\nThe integer of minimum magnitude of the code happens in this case to be the same as the chain code:\\n \\n0000606666666\\n6444444\\n24\\n2222202\\n                             2\\n202\\n  \\n \\nT\\nhe ﬁrst difference of the code is\\n \\n00062600000006000006260000620\\n   \\n                          6\\n626\\n  \\n \\nUsing this code to represent the boundary results in a signiﬁcant reduction in the amount of data \\nneeded to store the boundary\\n. In addition, working with code numbers offers a uniﬁed way to analyze \\nthe shape of a boundary, as we discuss in Section 11.3. Finally, keep in mind that the subsampled bound-\\nary can be recovered from any of the preceding codes.\\nDIP4E_GLOBAL_Print_Ready.indb   818\\n6/16/2017   2:14:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 819}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n819\\nSlope Chain Codes\\nUsing Freeman chain codes generally requires resampling a boundary to smooth \\nsmall variations, a process that implies defining a grid and subsequently assigning \\nall boundary points to their closest neighbors in the grid. An alternative to this \\napproach is to use \\nslope chain codes\\n (SCCs) (Bribiesca [1992, 2013]). The SCC of a \\n2-D curve is obtained by placing straight-line segments of equal length around the \\ncurve, with the end points of the segments touching the curve. \\nObtaining an SSC requires calculating the \\nslope changes\\n between contiguous line \\nsegments, and normalizing the changes to the \\ncontinuou\\ns (open) interval \\n(, ) .\\n−\\n11\\n \\nT\\nhis approach requires deﬁning the length of the line segments, as opposed to Free-\\nman codes, which require deﬁning a grid and assigning curve points to it—a much \\nmore elaborate procedure. Like Freeman codes, SCCs are independent of rotation, \\nbut a larger range of possible slope changes provides a more accurate representa-\\ntion under rotation than the rotational independence of the Freeman codes, which is \\nlimited to the eight directions in Fig. 11.3(b). As with Freeman codes, SCCs are inde-\\npendent of translation, and can be normalized for scale changes (see Problem 11.8).\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.5\\n (a) Noisy image of size \\n570 570\\n×\\n pixels. (b) Image smoothed with a \\n99\\n×\\n box kernel. (c) Smoothed \\nimage\\n, thresholded using Otsu’s method. (d) Longest outer boundary of (c). (e) Subsampled boundary (the points \\nare shown enlarged for clarity). (f) Connected points from (e).\\nDIP4E_GLOBAL_Print_Ready.indb   819\\n6/16/2017   2:14:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 820}),\n",
       " Document(page_content='820\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nFigure 11.6 illustrates how an SCC is generated. The ﬁrst step is to select the \\nlength of the line segment to use in generating the code [see Fig. 11.6(b)]. Next, a \\nstarting point (the origin) is speciﬁed (for an open curve, the logical starting point is \\none of its end points). As Fig. 11.6(c) shows, once the origin has been selected, one \\nend of a line segment is placed at the origin and the other end of the segment is set \\nto coincide with the curve. This point becomes the starting point of the next line seg-\\nment, and we repeat this procedure until the starting point (or end point in the case \\nof an open curve) is reached. As the ﬁgure illustrates, you can think of this process as \\na sequence of identical circles (with radius equal to the length of the line segment) \\ntraversing the curve. The intersections of the circles and the curve determine the \\nnodes of the straight-line approximation to the curve. \\nOnce the intersections of the circles are known, we determine the slope changes \\nbetween contiguous line segments. Positive and zero slope changes are normalized \\nto the open half interval \\n[, ) ,\\n01\\n while negative slope changes are normalized to the \\nopen interval \\n(, ) .\\n−\\n10\\n Not allowing slope changes of \\n±\\n1\\n eliminates the implementa-\\ntion issues that result from having to deal with the fact that such changes result in \\nthe same line segment with opposite directions\\n. \\nThe sequence of slope changes is the chain that deﬁnes the SCC approximation \\nto the original curve. For example, the code for the curve in Fig. 11.6(e) is \\n01 2\\n.,\\n \\n02 0\\n.,\\n \\n02 1\\n.,\\n \\n01 1\\n.,\\n \\n−\\n01\\n1\\n.,\\n \\n−\\n01\\n2\\n.,\\n \\n−\\n02\\n1\\n.,\\n \\n−\\n02\\n2\\n.,\\n \\n−\\n02\\n4\\n.,\\n \\n−\\n02\\n8\\n.,\\n \\n−\\n02\\n8\\n.,\\n \\n−\\n03\\n1\\n.,\\n \\n−\\n03\\n0\\n..\\n The accu-\\nrac\\ny of the slope changes deﬁned in Fig. 11.6(d) is \\n10\\n2\\n−\\n,\\n resulting in an “alphabet” \\nof 199 possible symbols (slope changes).\\n The accuracy can be changed, of course. For \\ninstance, and accuracy of \\n10\\n1\\n−\\n produces an alphabet of 19 symbols (see Problem 11.6). \\nUnlike a Freeman code, there is no guarantee that the last point of the coded curve \\nwill coincide with the last point of the curve itself. However, shortening the line \\nLine segment\\nb a\\nc\\ne d\\nFIGURE 11.6\\n (a) An open curve. (b) A straight-line segment. (c) Traversing the curve using circumferences to deter-\\nmine slope changes; the dot is the origin (starting point). (d) Range of slope changes in the open interval \\n(, )\\n−\\n11\\n \\n(the arrow in the center of the chart indicates direction of travel).\\n There can be ten subintervals between the slope \\nnumbers shown.(e) Resulting coded curve showing its corresponding numerical sequence of slope changes. (Cour-\\ntesy of Professor Ernesto Bribiesca, IIMAS-UNAM, Mexico.)\\nDIP4E_GLOBAL_Print_Ready.indb   820\\n6/16/2017   2:14:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 821}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n821\\nlength and/or increasing angle resolution often resolves the problem, because the \\nresults of computations are rounded to the nearest integer (remember we work with \\ninteger coordinates).\\nThe \\ninverse\\n of an SCC is another chain of the same length, obtained by reversing \\nthe order of the symbols and their signs. The \\nmirror image\\n of a chain is obtained by \\nstarting at the origin and reversing the signs of the symbols. Finally, we point out \\nthat the preceding discussion is directly applicable to closed curves. Curve following \\nwould start at an arbitrary point (for example, the uppermost-leftmost point of the \\ncurve) and proceed in a clockwise or counterclockwise direction, stopping when the \\nstarting point is reached. We will illustrate an use of SSCs in\\n \\nExample 11.6.\\nBOUNDARY APPROXIMATIONS USING MINIMUM-PERIMETER  \\nPOLYGONS\\nA digital boundary can be approximated with arbitrary accuracy by a polygon. For a \\nclosed curve, the approximation becomes exact when the number of segments of the \\npolygon is equal to the number of points in the boundary, so each pair of adjacent \\npoints defines a segment of the polygon. The goal of a polygonal approximation \\nis to capture the essence of the shape in a given boundary using the fewest pos-\\nsible number of segments. Generally, this problem is not trivial, and can turn into \\na time-consuming iterative search. However, approximation techniques of modest \\ncomplexity are well suited for image-processing tasks. Among these, one of the most \\npowerful is representing a boundary by a \\nminimum-perimeter polygon\\n (MPP), as \\ndefined in the following discussion.\\nFoundation\\nAn intuitive approach for computing MPPs is to enclose a boundary [see Fig. 11.7(a)] \\nby a set of concatenated cells, as in Fig. 11.7(b). Think of the boundary as a rubber \\nband contained in the gray cells in Fig. 11.7(b). As it is allowed to shrink, the rubber \\nband will be constrained by the vertices of the inner and outer walls of the region \\nof the gray cells. Ultimately, this shrinking produces the shape of a polygon of mini-\\nmum perimeter (with respect to this geometrical arrangement) that circumscribes \\nthe region enclosed by the cell strip, as in Fig. 11.7(c). Note in this figure that all the \\nvertices of the MPP coincide with corners of either the inner or the outer wall.\\nThe size of the cells determines the accuracy of the polygonal approximation. \\nIn the limit, if the size of each (square) cell corresponds to a pixel in the boundary, \\nthe maximum error in each cell between the boundary and the MPP approxima-\\ntion would be \\n2\\nd\\n,\\n where \\nd\\n is the minimum possible distance between pixels (i.e\\n., \\nthe distance between pixels established by the resolution of the original sampled \\nboundary). This error can be reduced in half by forcing each cell in the polygonal \\napproximation to be centered on its corresponding pixel in the original boundary. \\nThe objective is to use the largest possible cell size acceptable in a given application, \\nthus producing MPPs with the fewest number of vertices. Our objective in this sec-\\ntion is to formulate a procedure for ﬁnding these MPP vertices.\\nThe cellular approach just described reduces the shape of the object enclosed \\nby the original boundary, to the area circumscribed by the gray walls in Fig. 11.7(b). \\nFor an open curve, the \\nnumber of segments \\nof an exact polygonal \\napproximation is equal \\nto the number of points \\nminus 1.\\nDIP4E_GLOBAL_Print_Ready.indb   821\\n6/16/2017   2:14:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 822}),\n",
       " Document(page_content='822\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nFigure 11.8(a) shows this shape in dark gray. Suppose that we traverse the bound-\\nary of the dark gray region in a \\ncounterclockwise\\n direction. Every turn encountered \\nin the traversal will be either a convex or a concave vertex (the angle of a vertex is \\ndeﬁned as an \\ninterior\\n angle of the boundary at that vertex). Convex and concave \\nvertices are shown, respectively, as white and blue dots in Fig. 11.8(b). Note that \\nthese vertices are the vertices of the inner wall of the light-gray bounding region in \\nFig. 11.8(b), and that every concave (blue) vertex in the dark gray region has a corre-\\nsponding concave “mirror” vertex in the light gray wall, located diagonally opposite \\nthe vertex. Figure 11.8(c) shows the mirrors of all the concave vertices, with the MPP \\nfrom Fig. 11.7(c) superimposed for reference. We see that the vertices of the MPP \\ncoincide either with convex vertices in the inner wall (white dots) or with the mir-\\nrors of the concave vertices (blue dots) in the outer wall. Only convex vertices of the \\ninner wall and concave vertices of the outer wall can be vertices of the MPP .  Thus, \\nour algorithm needs to focus attention only on those vertices.\\nMPP Algorithm\\nThe set of cells enclosing a digital boundary [e.g., the gray cells in Fig. 11.7(b)] is \\ncalled a \\ncellular complex\\n. We assume the cellular complexes to be \\nsimply connected\\n, \\nin the sense the boundaries they enclose are not self-intersecting. Based on this \\nassumption, and letting \\nwhite\\n (\\nW\\n) denote convex vertices, and \\nblue\\n (\\nB\\n) denote mir-\\nrored concave vertices, we state the following observations:\\n1. \\nThe MPP bounded by a simply connected cellular complex is not self-intersecting.\\n2. \\nEvery \\nconvex\\n vertex of the MPP is a \\nW\\n vertex,\\n but not every \\nW\\n vertex of a bound-\\nary is a vertex of the MPP .\\nA convex vertex is the \\ncenter point of a triplet \\nof points that deﬁne an \\nangle in the range  \\n0° < \\nu\\n < 180°. Similarly, \\nangles of a concave  \\nvertex are in the range  \\n180° < \\nu\\n < 360°. An \\nangle of 180° deﬁnes a \\ndegenerate vertex\\n (i.e., \\nsegment of a straight \\nline), which cannot be an \\nMPP-vertex.\\nb a\\nc\\nFIGURE 11.7\\n (a) An object boundary. (b) Boundary enclosed by cells (shaded). (c) Minimum-perimeter polygon \\nobtained by allowing the boundary to shrink. The vertices of the polygon are created by the corners of the inner \\nand outer walls of the gray region.\\nDIP4E_GLOBAL_Print_Ready.indb   822\\n6/16/2017   2:14:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 823}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n823\\n3. \\nEvery \\nmirrored concave\\n vertex of the MPP is a \\nB\\n vertex,\\n but not every \\nB\\n vertex \\nof a boundary is a vertex of the MPP .\\n4. \\nAll \\nB\\n vertices are on or outside the MPP\\n, and all \\nW\\n vertices are on or inside the \\nMPP .\\n5. \\nThe uppermost-leftmost vertex in a sequence of vertices contained in a cellular \\ncomplex is always a \\nW\\n vertex of the MPP (see Problem 11.8).\\nT\\nhese assertions can be proved formally (Sklansky et al. [1972], Sloboda et al. [1998], \\nand Klette and Rosenfeld [2004]). However, their correctness is evident for our pur-\\nposes (see Fig. 11.8), so we do not dwell on the proofs here. Unlike the angles of the \\nvertices of the dark gray region in Fig. 11.8, the angles sustained by the vertices of \\nthe MPP are not necessarily multiples of 90°.\\nIn the discussion that follows, we will need to calculate the orientation of triplets \\nof points. Consider a triplet of points, \\nabc\\n,,\\n,\\n()\\n and let the coordinates of these points \\nbe \\naa a\\nxy\\n=\\n(,) ,\\n \\nbb b\\nxy\\n=\\n(,) ,\\n and \\ncc c\\nxy\\n=\\n(,) .\\n If we arrange these points as the rows of \\nthe matrix\\n \\nA\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\naa\\nbb\\ncc\\nxy\\nxy\\nxy\\n1\\n1\\n1\\n \\n(11-1)\\nDirection of travel\\nb a\\nc\\nFIGURE 11.8\\n (a) Region (dark gray) resulting from enclosing the original boundary by cells (see Fig. 11.7). (b) Convex \\n(white dots) and concave (blue dots) vertices obtained by following the boundary of the dark gray region in the \\ncounterclockwise direction. (c) Concave vertices (blue dots) displaced to their diagonal mirror locations in the \\nouter wall of the bounding region; the convex vertices are not changed. The MPP (solid boundary) is superimposed \\nfor reference.\\nDIP4E_GLOBAL_Print_Ready.indb   823\\n6/16/2017   2:14:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 824}),\n",
       " Document(page_content='824\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nThen, it follows from matrix analysis that\\n \\ndet( )\\n(,\\n,)\\nA\\n=\\n>\\n0\\n0\\nif  is a counterclockwise sequence\\nif the \\nabc\\np\\npoints \\nare colinear\\nif  is a clockwise sequence\\n<\\n⎧\\n⎨\\n0( , , )\\nabc\\n⎪ ⎪\\n⎩\\n⎪\\n \\n(11-2)\\nwhere \\ndet( )\\nA\\n is the determinant of \\nA\\n.\\n In terms of this equation, movement in a \\ncounterclockwise or clockwise direction is with respect to a right-handed coordinate \\nsystem (see the footnote in the discussion of F\\nig. 2.19). For example, using the image \\ncoordinate system from Fig. 2.19 (in which the origin is at the top left, the positive \\nx\\n-axis extends vertically downward, and the positive \\ny\\n-axis extends horizontally to \\nthe right), the sequence \\na\\n=\\n(,\\n) ,\\n34\\n \\nb\\n=\\n(,\\n) ,\\n23\\n and \\nc\\n=\\n(,\\n)\\n32\\n is in the counterclockwise \\ndirection.\\n This would give \\ndet( )\\nA\\n>\\n0\\n when substituted into Eq. (11-2). It is conve-\\nnient when describing the algorithm to define\\n \\nsgn( , , ) det( )\\nab\\nc\\n≡\\nA\\n \\n(11-3)\\nso that \\nsgn( , , )\\nab\\nc\\n>\\n0\\n for a counterclockwise sequence, \\nsgn( , , )\\nab\\nc\\n<\\n0\\n for a clock-\\nwise sequence\\n, and \\nsgn( , , )\\nab\\nc\\n=\\n0\\n when the points are collinear. Geometrically, \\nsgn( , , )\\nab\\nc\\n>\\n0\\n indicates that point \\nc\\n lies on the positive side of pair \\n(, )\\nab\\n (i.e., \\nc\\n lies on\\n \\nthe positive side of the line passing through points \\na\\n and \\nb\\n). Similarly, if \\nsgn( , , ) ,\\nab\\nc\\n<\\n0  \\npoint \\nc\\n lies on the negative side of the line\\n. Equations (11-2) and (11-3) give the same \\nresult if the sequence \\n(, , )\\nca\\nb\\n or \\n(,, )\\nbc\\na\\n is used because the direction of travel in the \\nsequence is the same as for \\n(, , ) .\\nab\\nc\\n However, the geometrical interpretation is differ-\\nent.\\n For example, \\nsgn( , , )\\nca\\nb\\n>\\n0\\n indicates that point \\nb\\n lies on the positive side of the\\n \\nline through points \\nc\\n and \\na\\n.\\nTo prepare the data for the MPP algorithm, we form a list of triplets consisting \\nof a vertex label (e.g., \\nV\\n0\\n, \\nV\\n1\\n,\\n etc.); the coordinates of each vertex; and an additional \\nelement denoting whether the vertex is \\nW\\n or \\nB\\n.\\n It is important that the concave ver-\\ntices be mirrored, as in Fig. 11.8(c), that the vertices be in sequential order,\\n†\\n and that \\nthe ﬁrst vertex be the uppermost-leftmost vertex, which we know from property 5 \\nis a \\nW\\n vertex of the MPP .  Let \\nV\\n0\\n denote this vertex. We assume that the vertices are \\narranged in the counterclockwise direction. The algorithm for ﬁnding MPPs uses \\ntwo “crawler” points: a white crawler \\n()\\nW\\nC\\n and a blue crawler \\n() .\\nB\\nC\\n \\nW\\nC\\n crawls along \\nthe convex (\\nW\\n) vertices, and \\nB\\nC\\n crawls along the concave (\\nB\\n) vertices. These two \\ncrawler points, the last MPP vertex found, and the vertex being examined are all that \\nis necessary to implement the algorithm.\\nThe algorithm starts by setting \\nWBV\\nCC\\n==\\n0\\n (recall that \\nV\\n0\\n is an MPP-vertex). \\nThen, at any step in the algorithm, let \\nV\\nL\\n denote the last MPP vertex found, and let \\nV\\nk\\n denote the current vertex being examined. One of the following three conditions \\ncan exist between \\nV\\nL\\n, \\nV\\nk\\n, and the two crawler points:\\n† \\n Vertices of a boundary can be ordered by tracking the boundary using the boundary-following algorithm \\ndiscussed earlier. \\nDIP4E_GLOBAL_Print_Ready.indb   824\\n6/16/2017   2:14:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 825}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n825\\n(a) \\nV\\nk\\n is on the positive side of the line through the pair of points \\n(, ) ,\\nVW\\nLC\\n in which \\ncase \\nsgn , , .\\nVW\\nV\\nLC k\\n()\\n>\\n0\\n(b) \\nV\\nk\\n is on the negative side of the line though pair \\nVW\\nLC\\n,\\n()\\n or is collinear with \\nit; that is \\nsgn , , .\\nVW\\nV\\nLC k\\n()\\n≤\\n0\\n Simultaneously, \\nV\\nk\\n lies to the positive side of the \\nline through \\nVB\\nLC\\n,\\n()\\n or is collinear with it; that is, \\nsgn , , .\\nVB\\nV\\nLCk\\n()\\n≥\\n0\\n(c) \\nV\\nk\\n is on the negative side of the line though pair \\nVB\\nLC\\n,,\\n()\\n in which case \\nsgn , , .\\nVB\\nV\\nLCk\\n()\\n<\\n0\\nIf condition (a) holds, the next MPP vertex is \\nW\\nC\\n,\\n and we let \\nVW\\nLC\\n=\\n;\\n then we \\nreinitialize the algorithm by setting \\nWBV\\nCCL\\n==\\n,\\n and start with the next vertex \\nafter the newly changed \\nV\\nL\\n.\\nIf condition (b) holds, \\nV\\nk\\n becomes a \\ncandidate\\n MPP vertex. In this case, we set \\nWV\\nCk\\n=\\n if \\nV\\nk\\n is convex (i.e., it is a \\nW\\n vertex); otherwise we set \\nBV\\nCk\\n=\\n.\\n We then \\ncontinue with the next vertex in the list.\\nIf condition (c) holds\\n, the next MPP vertex is \\nB\\nC\\n and we let \\nVB\\nLC\\n=\\n;\\n then we \\nreinitialize the algorithm by setting \\nWBV\\nCCL\\n==\\n and start with the next vertex \\nafter the newly changed \\nV\\nL\\n.\\nThe algorithm stops when it reaches the ﬁrst vertex again, and thus has processed \\nall the vertices in the polygon.\\n The \\nV\\nL\\n vertices found by the algorithm are the ver-\\ntices of the MPP . Klette and Rosenfeld [2004] have proved that this algorithm ﬁnds \\nall the MPP vertices of a polygon enclosed by a simply connected cellular complex.\\nEXAMPLE 11.2 :  A numerical example showing the details of how the MPP algorithm works.\\nA simple example in which we can follow the algorithm step-by-step will help clarify the preceding con-\\ncepts. Consider the vertices in Fig. 11.8(c). In our image coordinate system, the top-left point of the grid \\nis at coordinates \\n(,) .\\n00\\n Assuming unit grid spacing, the ﬁrst few (counterclockwise) vertices are:\\n \\nVW VB VW VB VW VW V\\n01 23 456\\n14 23 33 32 41 71 8\\n(, ) (, ) (, ) (, ) (,) (,) (\\n⏐⏐ ⏐⏐ ⏐ ⏐\\n,, ) ( , )\\n29 2\\n7\\nBV B\\n⏐\\nwhere the triplets are separated by vertical lines, and the \\nB\\n vertices are mirrored,\\n as required by the \\nalgorithm.\\nThe uppermost-leftmost vertex is always the ﬁrst vertex of the MPP , so we start by letting \\nV\\nL\\n and \\nV\\n0\\n \\nbe equal, \\nVV\\nL\\n==\\n0\\n14\\n(, ) ,\\n and initializing the other variables: \\nWBV\\nCCL\\n===\\n()\\n14\\n,.\\n \\nT\\nhe next vertex is \\nV\\n1\\n23\\n=\\n()\\n,.\\n In this case we have \\nsgn , ,\\nVW\\nV\\nLC\\n1\\n0\\n()\\n=\\n and \\nsgn , , ,\\nVB\\nV\\nLC\\n1\\n0\\n()\\n=\\n so \\ncondition (b) holds\\n. Because \\nV\\n1\\n is a \\nB\\n (concave) vertex, we update the blue crawler: \\nBV\\nC\\n==\\n()\\n1\\n23\\n,.\\n At \\nthis stage\\n, we have \\nV\\nL\\n=\\n(, ) ,\\n14\\n \\nW\\nC\\n=\\n(, ) ,\\n14\\n and \\nB\\nC\\n=\\n(,) .\\n23\\n \\nNext,\\n we look at \\nV\\n2\\n33\\n=\\n()\\n,.\\n In this case, \\nsgn , , ,\\nVW\\nV\\nLC\\n2\\n0\\n()\\n=\\n and \\nsgn , , ,\\nVB\\nV\\nLC\\n2\\n1\\n()\\n=\\n so condition (b) \\nholds\\n. Because \\nV\\n2\\n is \\nW\\n, we update the white crawler: \\nW\\nC\\n=\\n(,) .\\n33\\nThe next vertex is \\nV\\n3\\n=\\n()\\n32\\n,.\\n At this junction we have \\nV\\nL\\n=\\n(, ) ,\\n14\\n \\nW\\nC\\n=\\n(,) ,\\n33\\n and \\nB\\nC\\n=\\n(,) .\\n23\\n Then, \\nsgn , ,\\nVW\\nV\\nLC\\n3\\n2\\n()\\n=−\\n and \\nsgn , , ,\\nVB\\nV\\nLC\\n3\\n0\\n()\\n=\\n so condition (b) holds again. Because \\nV\\n3\\n is \\nB\\n, we let \\nBV\\nC\\n==\\n3\\n43\\n(,)\\n and look at the next vertex.\\nT\\nhe next vertex is \\nV\\n4\\n41\\n=\\n()\\n,.\\n We are working with \\nV\\nL\\n=\\n(, ) ,\\n14\\nW\\nC\\n=\\n(,) ,\\n33\\n and \\nB\\nC\\n=\\n(, ) .\\n32\\n The values \\nof sgn are \\nsgn( , , )\\nVW\\nV\\nLC\\n4\\n3\\n=−\\n and \\nsgn( , , ) .\\nVB\\nV\\nLC\\n4\\n0\\n=\\n So, condition (b) holds yet again, and we let \\nWV\\nC\\n==\\n4\\n41\\n(,)\\n because \\nV\\n4\\n is a \\nW \\nvertex.\\nDIP4E_GLOBAL_Print_Ready.indb   825\\n6/16/2017   2:15:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 826}),\n",
       " Document(page_content='826\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nThe next vertex is \\nV\\n5\\n71\\n=\\n(,) .\\n Using the values from the previous step we obtain \\nsgn( , , ) ,\\nVW\\nV\\nLC\\n5\\n9\\n=\\n \\nso condition (a) is satisﬁed.\\n Therefore, we let \\nVW\\nLC\\n==\\n(,)\\n41\\n (this is \\nV\\n4\\n)\\n and reinitialize: \\nBWV\\nCC L\\n== =\\n(,) .\\n41\\n Note that once we knew that \\nsgn( , , )\\nVW\\nV\\nLC\\n5\\n0\\n>\\n we did not bother to compute \\nthe other sgn expression.\\n Also, reinitialization means that we start fresh again by examining the next \\nvertex following the newly found MPP vertex. In this case, that next vertex is \\nV\\n5\\n,\\n so we visit it again.\\nW\\nith \\nV\\n5\\n71\\n=\\n()\\n,,\\n and using the new values of \\nV\\nL\\n, \\nW\\nC\\n,\\n and \\nB\\nC\\n,\\n it follows that \\nsgn , ,\\nVW\\nV\\nLC\\n5\\n0\\n()\\n=\\n and \\nsgn , , ,\\nVB\\nV\\nLC\\n5\\n0\\n()\\n=\\n so condition (b) holds. Therefore, we let \\nWV\\nC\\n==\\n()\\n5\\n71\\n,\\n because \\nV\\n5\\n is a \\nW \\nvertex\\n.\\nThe next vertex is \\nV\\n6\\n82\\n=\\n()\\n,\\n and \\nsgn , , ,\\nVW\\nV\\nLC\\n6\\n3\\n()\\n=\\n so condition (a) holds. Thus, we let \\nVW\\nLC\\n==\\n()\\n71\\n,  and reinitialize the algorithm by setting \\nWBV\\nCCL\\n==\\n.\\nBecause the algorithm was reinitialized at \\nV\\n5\\n,\\n the next vertex is \\nV\\n6\\n82\\n=\\n(, )\\n again. Using the results \\nfrom the previous step gives us \\nsgn( , , )\\nVW\\nV\\nLC\\n6\\n0\\n=\\n and \\nsgn( , , ) ,\\nVB\\nV\\nLC\\n6\\n0\\n=\\n so condition (b) holds this \\ntime\\n. Because \\nV\\n6\\n is \\nB\\n we let \\nBV\\nC\\n==\\n6\\n82\\n(, ) .\\nSummarizing, we have found three vertices of the MPP up to this point: \\nV\\n1\\n14\\n=\\n(, ) ,\\n \\nV\\n4\\n41\\n=\\n(,) ,\\n and \\nV\\n5\\n71\\n=\\n(,) .\\n Continuing as above with the remaining vertices results in the MPP vertices in Fig. 11.8(c) \\n(see Problem 11.9).\\n The mirrored \\nB\\n vertices at (2, 3), (3, 2), and on the lower-right side at (13, 10), are on \\nthe boundary of the MPP\\n. However, they are collinear and thus are not considered vertices of the MPP . \\nAppropriately, the algorithm did not detect them as such.\\nEXAMPLE 11.3 :  Applying the MPP algorithm.\\nFigure 11.9(a) is a \\n566 566\\n×\\n binary image of a maple leaf, and Fig. 11.9(b) is its 8-connected boundary. \\nT\\nhe sequence in Figs. 11.9(c) through (h) shows MMP representations of this boundary using square \\ncellular complex cells of sizes 2, 4, 6, 8, 16, and 32, respectively (the vertices in each ﬁgure were con-\\nnected with straight lines to form a closed boundary). The leaf has two major features: a stem and three \\nmain lobes. The stem begins to be lost for cell sizes greater than \\n44\\n×\\n,\\n as Fig. 11.9(e) shows. The three \\nmain lobes are preserved reasonably well,\\n even for a cell size of \\n16 16\\n×\\n,\\n as Fig. 11.9(g) shows. However, \\nwe see in F\\nig. 11.8(h) that by the time the cell size is increased to \\n32 32\\n×\\n,\\n this distinctive feature has \\nbeen nearly lost.\\nT\\nhe number of points in the original boundary [Fig. 11.9(b)] is 1900. The numbers of vertices in \\nFigs. 11.9(c) through (h) are 206, 127, 92, 66, 32, and 13, respectively. Figure 11.9(e), which has 127 ver-\\ntices, retained all the major features of the original boundary while achieving a data reduction of over \\n90%. So here we see a signiﬁcant advantage of MMPs for representing a boundary. Another important \\nadvantage is that MPPs perform boundary smoothing. As explained in the previous section, this is a \\nusual requirement when representing a boundary by a chain code.\\nSIGNATURES\\nA signature is a 1-D functional representation of a 2-D boundary and may be gener-\\nated in various ways. One of the simplest is to plot the distance from the centroid \\nto the boundary as a function of angle, as illustrated in Fig. 11.10. The basic idea of \\nusing signatures is to reduce the boundary representation to a 1-D function that \\npresumably is easier to describe than the original 2-D boundary.\\nBased on the assumptions of uniformity in scaling with respect to both axes, and \\nthat sampling is taken at equal intervals of \\nu\\n,\\n changes in the size of a shape result \\nin changes in the amplitude values of the corresponding signature\\n. One way to \\nDIP4E_GLOBAL_Print_Ready.indb   826\\n6/16/2017   2:15:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 827}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n827\\nnormalize for this is to scale all functions so that they always span the same range of \\nvalues, e.g., \\n[,] .\\n01\\n The main advantage of this method is simplicity, but it has the dis-\\nadvantage that scaling of the entire function depends on only two values:\\n the mini-\\nmum and maximum. If the shapes are noisy, this can be a source of signiﬁcant error \\nfrom object to object. A more rugged (but also more computationally intensive) \\napproach is to divide each sample by the variance of the signature, assuming that \\nthe variance is not zero—as in the case of Fig. 11.10(a)—or so small that it creates \\ncomputational difﬁculties. Using the variance yields a variable scaling factor that \\nis inversely proportional to changes in size and works much as automatic volume \\ncontrol does. Whatever the method used, the central idea is to remove dependency \\non size while preserving the fundamental shape of the waveforms.\\nDistance versus angle is not the only way to generate a signature. For example, \\nanother way is to traverse the boundary and, corresponding to each point on the \\nboundary, plot the angle between a line tangent to the boundary at that point and a \\nreference line. The resulting signature, although quite different from the \\nr\\n()\\nu\\n curves \\nin F\\nig. 11.10, carries information about basic shape characteristics. For instance, \\nhorizontal segments in the curve correspond to straight lines along the boundary \\nbecause the tangent angle is constant there. A variation of this approach is to use \\nthe so-called \\nslope density function\\n as a signature. This function is a histogram of \\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 11.9\\n (a) \\n566 566\\n×\\n binary image. (b) 8-connected boundary. (c) through (h), MMPs obtained using square cells \\nof sizes 2,\\n 4, 6, 8, 16, and 32, respectively (the vertices were joined by straight-line segments for display). The number \\nof boundary points in (b) is 1900. The numbers of vertices in (c) through (h) are 206, 127, 92, 66, 32, and 13, respec-\\ntively. Images (b) through (h) are shown as negatives to make the boundaries easier to see.\\nDIP4E_GLOBAL_Print_Ready.indb   827\\n6/16/2017   2:15:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 828}),\n",
       " Document(page_content='828\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\ntangent-angle values. Because a histogram is a measure of the concentration of val-\\nues, the slope density function responds strongly to sections of the boundary with \\nconstant tangent angles (straight or nearly straight segments) and has deep valleys \\nin sections producing rapidly varying angles (corners or other sharp inﬂections).\\nEXAMPLE 11.4 :  Signatures of two regions.\\nFigures 11.11(a) and (d) show two binary objects, and Figs. 11.11(b) and (e) are their boundaries. The \\ncorresponding \\nr\\n()\\nu\\n signatures in Figs. 11.11(c) and (f) range from 0° to 360° in increments of 1°. The \\nnumber of prominent peaks in the signatures is sufﬁcient to differentiate between the shapes of the two \\nobjects\\n.\\nSKELETONS, MEDIAL AXES, AND DISTANCE TRANSFORMS\\nLike boundaries, skeletons are related to the shape of a region. Skeletons can be \\ncomputed from a boundary by filling the area enclosed by the boundary with fore-\\nground values, and treating the result as a binary region. In other words, a skeleton is \\ncomputed using the coordinates of points in the entire region, including its boundary. \\nThe idea is to reduce a region to a tree or graph by computing its skeleton. As we \\nexplained in\\n \\nSection 9.5 (see Fig. 9.25), the \\nskeleton\\n of a region is the set of points in \\nthe region that are equidistant from the border of the region.\\n \\nThe skeleton is obtained using one of two principal approaches: (1) by succes-\\nsively thinning the region (e.g., using morphological erosion) while preserving end \\npoints and line connectivity (this is called \\ntopology-preserving\\n thinning); or (2) \\nby computing the \\nmedial axis\\n of the region via an efﬁcient implementation of the \\nmedial axis transform\\n (MAT) proposed by Blum [1967]. We discussed thinning in \\nSection 9.5. The MAT of a region \\nR\\n with border \\nB\\n is as follows: For each point \\np\\n in \\nR\\n, we ﬁnd its closest neighbor in \\nB\\n. If \\np\\n has more than one such neighbor, it is said \\nAs is true of thinning, \\nthe MAT is highly \\nsusceptible to boundary \\nand internal region \\nirregularities, so smooth-\\ning and other preprocess-\\ning steps generally are \\nrequired to obtain a \\nclean a binary image.\\nA\\n0\\n0\\nr\\n(\\nu\\n)\\nA\\nr\\nu\\nA\\nr\\nu\\nu\\np\\n4\\np\\n2\\n3\\np\\n4\\n5\\np\\n4\\n3\\np\\n2\\n7\\np\\n4\\np\\n2\\np\\nA\\nr\\n(\\nu\\n)\\nu\\np\\n4\\np\\n2\\n3\\np\\n4\\n5\\np\\n4\\n3\\np\\n2\\n7\\np\\n4\\np\\n2\\np\\n2\\nA\\nb a\\nFIGURE 11.10\\nDistance-versus-\\nangle signatures. \\nIn (a), \\nr\\n()\\nu\\n is  \\nconstant.\\n In (b), \\nthe signature  \\nconsists of \\nrepetitions of \\nthe pattern \\nrA\\nuu\\n(\\n)\\n=\\nsec\\n for \\n04\\n≤≤\\nup\\n, and \\nrA\\nuu\\n(\\n)\\n=\\ncsc\\n for \\npu p\\n42\\n<≤\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   828\\n6/16/2017   2:15:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 829}),\n",
       " Document(page_content='11.2\\n  \\nBoundary Preprocessing\\n    \\n829\\nto belong to the medial axis of \\nR\\n. The concept of “closest” (and thus the resulting \\nMAT) depends on the deﬁnition of a distance metric (see Section 2.5). Figure 11.12 \\nshows some examples using the Euclidean distance. If the Euclidean distance is used, \\nthe resulting skeleton is the same as what would be obtained by using the maximum \\ndisks from Section 9.5. The skeleton of a region is \\ndeﬁned\\n as its medial axis.\\nThe MAT of a region has an intuitive interpretation based on the “prairie ﬁre” \\nconcept discussed in Section 11.3\\n \\n(see Fig. 11.15). Consider an image region as a \\nprairie of uniform,\\n dry grass, and suppose that a ﬁre is lit simultaneously along all \\nthe points on its border. All ﬁre fronts will advance into the region at the same speed. \\nThe MAT of the region is the set of points reached by more than one ﬁre front at \\nthe same time.\\nIn general, the MAT comes considerably closer than thinning to producing skel-\\netons that “make sense.” However, computing the MAT of a region requires cal-\\nculating the distance from every interior point to every point on the border of the \\nregion—an impractical endeavor in most applications. Instead, the approach is to \\nobtain the skeleton equivalently from the \\ndistance transform\\n, for which numerous \\nefﬁcient algorithms exist.\\n The distance transform of a region of foreground pixels in a background of zeros \\nis the distance from every pixel to the \\nnearest\\n nonzero valued pixel. Figure 11.13(a) \\nshows a small binary image, and Fig. 11.13(b) is its distance transform. Observe that \\nevery 1-valued pixel has a distance transform value of 0 because its closest nonzero \\nvalued pixel is itself. For the purpose of ﬁnding skeletons equivalent to the MAT, \\nwe are interested in the distance from the pixels of a region of foreground (white) \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.11\\n(a) and (d) Two \\nbinary regions,  \\n(b) and (e) their  \\nexternal  \\nboundaries, and \\n(c) and (f) their \\ncorresponding \\nr\\n()\\nu\\n \\nsignatures\\n. The \\nhorizontal axes \\nin (c) and (f) cor-\\nrespond to angles \\nfrom 0° to 360°, in \\nincrements of 1°.\\nDIP4E_GLOBAL_Print_Ready.indb   829\\n6/16/2017   2:15:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 830}),\n",
       " Document(page_content='830\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\npixels to their nearest background (zero) pixels, which constitute the region bound-\\nary. Thus, we compute the distance transform of the \\ncomplement\\n of the image, as \\nFigs. 11.13(c) and (d) illustrate. By comparing Figs. 11.13(d) and 11.12(a), we see \\nin the former that the MAT (skeleton) is equivalent to the \\nridge\\n of the distance \\ntransform [i.e., the ridge in the image in Fig. 11.13(d)]. This ridge is the set of \\nlocal \\nmaxima\\n [shown bold in Fig. 11.13(d)]. Figures 11.13(e) and (f) show the same effect \\non a larger \\n()\\n414\\n708\\n×\\n binary image. \\nF\\ninding approaches for computing the distance transform efﬁciently has been a \\ntopic of research for many years. Numerous approaches exist that can compute the \\ndistance transform with linear time complexity, \\nOK\\n()\\n,\\n for a binary image with \\nK\\n \\npixels\\n. For example, the algorithm by Maurer et al. [2003] not only can compute the \\ndistance transform in \\nOK\\n()\\n,\\n it can compute it in \\nOKP\\n()\\n using \\nP\\n processors\\n.\\n1.41   1    1    1   1.41\\n   1      0    0    0     1\\n   1      0    0    0     1\\n1.41   1    1    1    1.41\\n0     0     0     0     0     0     0     0     0\\n0     \\n1\\n     1     1     1     1     1     \\n1\\n     0\\n0     1     \\n2\\n     2     2     2     \\n2\\n     1     0\\n0     1     2     \\n3\\n     \\n3     3\\n \\n    2     1     0\\n0     1     \\n2\\n     2     2     2     \\n2\\n     1     0\\n0     \\n1\\n     1     1     1     1     1     \\n1\\n     0\\n0     0     0     0     0     0     0     0     0\\n0      0      0      0      0 \\n0      1      1      1      0 \\n0      1      1      1      0 \\n0      0      0      0      0 \\n \\n0     0     0     0     0     0     0     0     0\\n 0     1     1     1     1     1     1     1     0\\n 0     1     1     1     1     1     1     1     0\\n 0     1     1     1     1     1     1     1     0\\n 0     1     1     1     1     1     1     1     0\\n 0     1     1     1     1     1     1     1     0\\n 0     0     0     0     0     0     0     0     0\\nb a\\nd c\\nf e\\nFIGURE 11.13\\n(a) A small  \\nimage and (b) its \\ndistance  \\ntransform. Note \\nthat all 1-valued \\npixels in (a) have \\ncorresponding \\n0’s in (b). (c) A \\nsmall image, and \\n(d) the distance \\ntransform of its \\ncomplement\\n. (e) A \\nlarger image, and \\n(f) the distance \\ntransform of its \\ncomplement. The \\nEuclidian distance \\nwas used through-\\nout.\\nb a\\nc\\n \\nFIGURE 11.12\\nMedial axes \\n(dashed) of three \\nsimple regions.\\nDIP4E_GLOBAL_Print_Ready.indb   830\\n6/16/2017   2:15:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 831}),\n",
       " Document(page_content='11.3\\n  \\nBoundary Feature Descriptors\\n    \\n831\\nEXAMPLE 11.5 :  Skeletons obtained using thinning and pruning vs. the distance transform.\\nFigure 11.14(a) shows a segmented image of blood vessels, and Fig. 11.14(b) shows the skeleton obtained \\nusing morphological thinning. As we discussed in Chapter 9, thinning is characteristically accompanied \\nby spurs, which certainly is the case here. Figure 11.14(c) shows the result of forty passes of spur removal. \\nWith the exception of the few small spurs visible on the bottom left of the image, pruning did a reason-\\nable job of cleaning up the skeleton. One drawback of thinning is the loss of potentially important \\nfeatures. This was not the case here, except the pruned skeleton does not cover the full expanse of the \\nimage. Figure 11.14(c) shows the skeleton obtained using distance transform computations based on fast \\nmarching (see Lee et al. [2005] and Shi and Karl [2008]). The way the algorithm we used implements \\nbranch generation handles ambiguities such as spurs automatically. \\nThe result in Fig. 11.14(d) is slightly superior to the result in Fig. 11.14(c), but both skeletons certainly \\ncapture the important features of the image in this case. A key advantage of the thinning approach \\nis simplicity of implementation, which can be important in dedicated applications. Overall, distance-\\ntransform formulations tend to produce skeletons less prone to discontinuities, but overcoming the \\ncomputational burden of the distance transform results in implementations that are considerably more \\ncomplex than thinning.\\n11.3  BOUNDARY FEATURE DESCRIPTORS  \\nWe begin our discussion of feature descriptors by considering several fundamental \\napproaches for describing region boundaries.\\n \\n11.3\\nb a\\nd c\\n \\nFIGURE 11.14\\n  \\n(a) Thresholded \\nimage of blood \\nvessels.  \\n(b) Skeleton \\nobtained by  \\nthinning, shown  \\nsuperimposed \\non the image \\n(note the spurs). \\n(c) Result of 40 \\npasses of spur \\nremoval.  \\n(d) Skeleton \\nobtained using the \\ndistance  \\ntransform.\\nDIP4E_GLOBAL_Print_Ready.indb   831\\n6/16/2017   2:15:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 832}),\n",
       " Document(page_content='832\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nSOME BASIC BOUNDARY DESCRIPTORS \\nThe \\nlength\\n of a boundary is one of its simplest descriptors. The number of pixels \\nalong a boundary is an approximation of its length. For a chain-coded curve with \\nunit spacing in both directions, the number of vertical and horizontal components \\nplus \\n2\\n multiplied by the number of diagonal components gives its exact length. If \\nthe boundary is represented by a polygonal curve\\n, the length is equal to the sum of \\nthe lengths of the polygonal segments. \\nThe \\ndiameter\\n of a boundary \\nB\\n is deﬁned as\\n \\ndiam\\neter\\nB D p p\\nij\\nij\\n() m a x ,\\n,\\n=\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n \\n(11-4)\\nwhere \\nD\\n is a distance measure (see Section 2.5) and \\np\\ni\\n and \\np\\nj\\n are points on the \\nboundary. The value of the diameter and the orientation of a line segment connect-\\ning the two extreme points that comprise the diameter is called the \\nmajor axis\\n (or \\nlongest chord\\n) of the boundary. That is, if the major axis is defined by points \\n(, )\\nxy\\n11\\n \\nand \\n(, ) ,\\nxy\\n22\\n then the length and orientation of the major axis are given by\\n \\nlength x x y y\\nm\\n=−+ −\\n⎡\\n⎣\\n⎤\\n⎦\\n() ()\\n21\\n2\\n21\\n2\\n12\\n \\n(11-5)\\nand\\n \\nangle\\nyy\\nxx\\nm\\n=\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\ntan\\n1\\n21\\n21\\nThe \\nminor axis\\n (also called the \\nlongest perpendicular chord\\n) of a boundary is defined \\nas the line perpendicular to the major axis, and of such length that a box passing \\nthrough the outer four points of intersection of the boundary with the two axes com-\\npletely encloses the boundary. The box just described is called the \\nbasic rectangle\\n or \\nbounding box\\n, and the ratio of the major to the minor axis is called the \\neccentricity\\n \\nof the boundary. We give some examples of this descriptor in Section 11.4. \\nThe \\ncurvature\\n of a boundary is deﬁned as the rate of change of slope. In general, \\nobtaining reliable measures of curvature at a point of a raw digital boundary is dif-\\nﬁcult because these boundaries tend to be locally “ragged.” Smoothing can help, but \\na more rugged measure of curvature is to use the difference between the slopes of \\nadjacent boundary segments that have been represented as straight lines. Polygonal \\napproximations are well-suited for this approach [see Fig. 11.8(c)], in which case we \\nare concerned only with curvature at the vertices. As we traverse the polygon in the \\nclockwise direction, a vertex point \\np\\n is said to be \\nconvex\\n if the change in slope at \\np\\n \\nis nonnegative; otherwise, \\np\\n is said to be \\nconcave\\n. The description can be reﬁned \\nfurther by using ranges for the changes of slope. For instance, \\np\\n could be labeled as \\npart of a nearly straight line segment if the absolute change of slope at that point is \\nless than 10°, or it could be labeled as “corner-like” point if the absolute change is \\nin the range 90°, \\n±°\\n30\\n.\\n \\nDescriptors based on changes of slope can be formulated easily by expressing a \\nboundary in the form of a slope chain code (SSC),\\n as discussed earlier (see Fig. 11.6). \\nA particularly useful boundary descriptor that is easily implemented using SSCs is \\ntortuosity,\\n a measure of the twists and turns of a curve. The tortuosity, \\nt\\n,\\n of a curve \\nThe major and minor \\naxes are used also as \\nregional descriptors.\\nWe will discuss corners \\nin detail later in this \\nchapter.\\nDIP4E_GLOBAL_Print_Ready.indb   832\\n6/16/2017   2:15:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 833}),\n",
       " Document(page_content='11.3\\n  \\nBoundary Feature Descriptors\\n    \\n833\\nrepresented by an SCC is deﬁned as the sum of the absolute values of the chain ele-\\nments:\\n \\nt\\n=\\n=\\n∑\\na\\ni\\ni\\nn\\n1\\n \\n(11-6)\\nwhere \\nn\\n is the number of elements in the SCC\\n, and \\na\\ni\\n are the values (slope changes)\\n \\nof the elements in the code. The next example illustrates one use of this descriptor\\nEXAMPLE 11.6 :  Using slope chain codes to describe tortuosity.\\nAn important measures of blood vessel morphology is its tortuosity. This metric can assist in the computer-   \\naided diagnosis of Retinopathy of Prematurity (ROP), an eye disease that affects babies born prema-\\nturely (Bribiesca [2013]). ROP causes abnormal blood vessels to grow in the retina (see Section 2.1). This \\ngrowth can cause the retina to detach from the back of the eye, potentially leading to blindness.\\nFigure 11.15(a) shows an image of the retina (called a \\nfundus\\n image) from a newborn baby. Ophthal-\\nmologists diagnose and make decisions about the initial treatment of ROP based on the appearance of \\nretinal blood vessels. Dilatation and increased tortuosity of the retinal vessels are signs of highly prob-\\nable ROP . Blood vessels denoted A, B, and C in Fig. 11.15 were selected to demonstrate the discrimi-\\nnative potential of SCCs for quantifying tortuosity (each vessel shown is a long, thin \\nregion\\n, not a line \\nsegment).\\nThe border of each vessel was extracted and its length (number of pixels), \\nP\\n, was calculated. To make \\nSCC comparisons meaningful, the three boundaries were normalized so that each would have the same \\nnumber, \\nm\\n, of straight-line segments. The length, \\nL\\n, of the line segment was then computed as \\nLm P\\n=\\n. \\nIt follows that the number of elements of each SCC is \\nm\\n−\\n1.\\n The tortuosity, \\nt\\n,\\n of a curve represented by \\nan SCC is deﬁned as the sum of the absolute values of the chain elements\\n, as noted in Eq. (11-6).\\nThe table in Fig. 11.15(b) shows values of \\nt\\n for vessels A, B, and C based on 51 straight-line segments \\n(as noted above\\n, \\nnm\\n=−\\n1)\\n.\\n The values of tortuosity are in agreement with our visual analysis of the \\nthree vessels\\n, showing B as being slightly “busier” than A, and C as having the fewest twists and turns.\\nb a\\n \\nFIGURE 11.15\\n(a) Fundus image \\nfrom a prematurely \\nborn baby with ROP . \\n(b) Tortuosity of \\nvessels A, B, and C. \\n(Courtesy of  \\nProfessor Ernesto \\nBribiesca, IIMAS-\\nUNAM, Mexico.)\\nA\\nB\\nC\\nCurve\\nn\\nT\\n2.3770\\n2.5132\\n1.6285\\n50\\n50\\n50\\nDIP4E_GLOBAL_Print_Ready.indb   833\\n6/16/2017   2:15:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 834}),\n",
       " Document(page_content='834\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nSHAPE NUMBERS\\nThe shape  number of a Freeman chain-coded boundary, based on the 4-directional \\ncode of Fig. 11.3(a), is defined as the first difference of smallest magnitude. The \\norder\\n, \\nn\\n, of a shape number is defined as the number of digits in its representation. More-\\nover, \\nn\\n is even for a closed boundary, and its value limits the number of possible \\ndifferent shapes. Figure 11.16 shows all the shapes of order 4, 6, and 8, along with \\ntheir chain-code representations, first differences, and corresponding shape numbers. \\nAlthough the first difference of a 4-directional chain code is independent of rotation \\n(in increments of 90°), the coded boundary in general depends on the orientation of \\nthe grid. One way to normalize the grid orientation is by aligning the chain-code grid \\nwith the sides of the basic rectangle defined in the previous section.\\nIn practice, for a desired shape order, we ﬁnd the rectangle of order \\nn\\n whose \\neccentricity (deﬁned in Section 11.4) best approximates that of the basic rectangle, \\nand use this new rectangle to establish the grid size. For example, if \\nn\\n=\\n12\\n,\\n all the \\nrectangles of order 12 (that is\\n, those whose perimeter length is 12) are of sizes \\n24\\n×\\n, \\n33\\n×\\n,\\n and \\n15\\n×\\n.\\n If the eccentricity of the \\n24\\n×\\n rectangle best matches the \\neccentricity of the basic rectangle for a given boundary\\n, we establish a \\n24\\n×\\n grid \\ncentered on the basic rectangle and use the procedure outlined in Section 11.2 to \\nobtain the F\\nreeman chain code. The shape number follows from the ﬁrst differ-\\nence of this code. Although the order of the resulting shape number usually equals \\nn\\n because of the way the grid spacing was selected, boundaries with depressions \\ncomparable to this spacing sometimes yield shape numbers of order greater than \\nn\\n. \\nIn this case, we specify a rectangle of order lower than \\nn,\\n and repeat the procedure \\nuntil the resulting shape number is of order \\nn\\n. The order of a shape number starts \\nat 4 and is always even because we are working with 4-connectivity and require that \\nboundaries be closed.\\nAs explained  \\nSection 11.2, the ﬁrst dif-\\nference of smallest mag-\\nnitude makes a Freeman \\nchain code independent \\nof the starting point, and \\nis insensitive to rotation \\nin increments of 90° if a \\n4-directional code is used.\\nOrder 4\\nChain code: 0  3  2  1\\nDifference: 3  3  3  3\\nShape no.:\\n3  3  3  3\\nOrder 6\\n0  0  3  2  2  1\\n3  0  3  3  0  3\\n0  3  3  0  3  3\\nOrder 8\\nChain code: 0  0  3  3  2  2  1  1 0  3  0  3  2  2  1  1 0  0  0  3  2  2  2  1\\nDifference: 3  0  3  0  3  0  3  0 3  3  1  3  3  0  3  0 3  0  0  3  3  0  0  3\\nShape no.:\\n0  3  0  3  3  1  3  3 0  0  3  3  0  0  3  3\\n0  3  0  3  0  3  0  3\\nFIGURE 11.16\\nAll shapes of \\norder 4, 6, and 8. \\nThe directions are \\nfrom Fig. 11.3(a), \\nand the dot  \\nindicates the  \\nstarting point.\\nDIP4E_GLOBAL_Print_Ready.indb   834\\n6/16/2017   2:15:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 835}),\n",
       " Document(page_content='11.3\\n  \\nBoundary Feature Descriptors\\n    \\n835\\nEXAMPLE 11.7 :  Computing shape numbers.\\nSuppose that \\nn\\n=\\n18\\n is speciﬁed for the boundary in Fig. 11.17(a). To obtain a shape number of this order \\nwe follow the steps just discussed.\\n First, we ﬁnd the basic rectangle, as shown in Fig. 11.17(b). Next we ﬁnd \\nthe closest rectangle of order 18. It is a \\n36\\n×\\n rectangle, requiring the subdivision of the basic rectangle \\nshown in F\\nig. 11.17(c). The chain-code directions are aligned with the resulting grid. The ﬁnal step is to \\nobtain the chain code and use its ﬁrst difference to compute the shape number, as shown in Fig. 11.17(d).\\nFOURIER DESCRIPTORS\\nFigure 11.18 shows a digital boundary in the \\nxy\\n-plane, consisting of \\nK\\n points. Starting \\nat an arbitrary point \\nxy\\n00\\n,,\\n()\\n coordinate pairs \\nxy xy xy x y\\nKK\\n00 11 22 1 1\\n,, ,,,, , ,\\n()\\n() ( ) ( )\\n−−\\n…\\n \\nare encountered in traversing the boundary\\n, say, in the counterclockwise direction. \\nThese coordinates can be expressed in the form \\nxk x\\nk\\n()\\n=\\n and \\nyk y\\nk\\n()\\n=\\n.\\n Using \\nthis notation,\\n the boundary itself can be represented as the sequence of coordinates \\nsk xk yk\\n()\\n=\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\n,\\n for \\nkK\\n=−\\n012\\n1\\n,,, , .\\n…\\n Moreover, each coordinate pair can be \\ntreated as a complex number so that\\n \\nsk xk j yk\\n()\\n=\\n()\\n+\\n()\\n \\n(11-7)\\nfor \\nkK\\n=−\\n012\\n1\\n,,, , .\\n…\\n That is, the \\nx\\n-axis is treated as the real axis and the \\ny\\n-axis as \\nthe imaginary axis of a sequence of complex numbers\\n. Although the interpretation \\nWe use the “conven-\\ntional” axis system here \\nfor consistency with the \\nliterature. However, the \\nsame result is obtained \\nif we use the book \\nimage coordinate system \\nwhose origin is at the \\ntop left because both are \\nright-handed coordinate \\nsystems (see Fig. 2.19). In \\nthe latter, the rows and \\ncolumns represent the \\nreal and imaginary parts \\nof the complex number. \\nChain code: 0  0  0  0  3  0  0  3  2  2  3  2  2  2  1  2  1  1\\nDifference: 3  0  0  0  3  1  0  3  3  0  1  3  0  0  3  1  3  0\\nShape no.:\\n0  0  0  3  1  0  3  3  0  1  3  0  0  3  1  3  0  3\\n1\\n3\\n0\\n2\\nb a\\nd c\\nFIGURE 11.17\\nSteps in the  \\ngeneration of a \\nshape number.\\nDIP4E_GLOBAL_Print_Ready.indb   835\\n6/16/2017   2:15:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 836}),\n",
       " Document(page_content='836\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nof the sequence was restated, the nature of the boundary itself was not changed. \\nOf course, this representation has one great advantage: It reduces a 2-D to a 1-D \\ndescription problem.\\nWe know from Eq. (4-44) that the discrete Fourier transform (DFT) of \\nsk\\n()\\n is\\n \\nau ske\\nk\\nK\\nju\\nkK\\n()\\n=\\n()\\n=\\n∑\\n0\\n1\\n2\\n–\\n–\\np\\n \\n(11-8)\\nfor \\nuK\\n=−\\n012\\n1\\n,,, , .\\n…\\n The complex coefficients \\nau\\n()\\n are called the \\nFourier descrip-\\ntors\\n of the boundary. The inverse Fourier transform of these coefficients restores \\nsk\\n()\\n. That is, from Eq. (4-45),\\n \\nsk\\nK\\naue\\nu\\nK\\nju\\nkK\\n()\\n=\\n()\\n=\\n∑\\n1\\n0\\n1\\n2\\n–\\np\\n \\n(11-9)\\nfor \\nkK\\n=−\\n012\\n1\\n,,, , .\\n…\\n We know from Chapter 4 that the inverse is identical to the \\noriginal input,\\n provided that all the Fourier coefficients are used in Eq. (11-9). How-\\never, suppose that, instead of all the Fourier coefficients, only the first \\nP\\n coefficients \\nare used. This is equivalent to setting \\nau\\n()\\n=\\n0\\n for \\nuP\\n>\\n–1\\n in Eq. (11-9). The result \\nis the following \\nappro\\nximation\\n to \\nsk\\n()\\n:\\n \\nˆ\\n–\\nsk\\nK\\naue\\nu\\nP\\nju\\nkK\\n()\\n=\\n()\\n=\\n∑\\n1\\n0\\n1\\n2\\np\\n \\n(11-10)\\nfor \\nkK\\n=−\\n012\\n1\\n,,, , .\\n…\\n Although only \\nP\\n terms are used to obtain each component \\nof \\nˆ\\n,\\nsk\\n()\\n parameter \\nk\\n still ranges from 0 to \\nK\\n–.\\n1\\n That is, the \\nsame\\n number of points \\nexists in the approximate boundary\\n, but not as many terms are used in the recon-\\nstruction of each point. \\nDeleting the high-frequency coefﬁcients is the same as ﬁltering the transform \\nwith an ideal lowpass ﬁlter. You learned in Chapter 4 that the periodicity of the \\nDFT requires that we center the transform prior to ﬁltering it by multiplying it by \\n() .\\n−\\n1\\nx\\n Thus, we use this procedure when implementing Eq. (11-8), and use it again \\njy\\nx\\nx\\n0\\ny\\n0\\ny\\n1\\nx\\n1\\nReal axis\\nImaginary axis\\nFIGURE 11.18\\nA digital  \\nboundary and its  \\nrepresentation \\nas sequence of \\ncomplex numbers. \\nThe points \\n(,)\\nxy\\n00\\n \\nand \\n(,)\\nxy\\n11\\n are \\n(arbitrarily) the \\nﬁrst two points in \\nthe sequence.\\nDIP4E_GLOBAL_Print_Ready.indb   836\\n6/16/2017   2:15:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 837}),\n",
       " Document(page_content='11.3\\n  \\nBoundary Feature Descriptors\\n    \\n837\\nto reverse the centering when computing the inverse in Eq. (11-10). Because of \\nsymmetry considerations in the DFT, the number of points in the boundary and its \\ninverse must be even. This implies that the number of coefﬁcients removed (set to 0) \\nbefore the inverse is computed must be even. Because the transform is centered, we \\nset to 0 half the number of coefﬁcients on each end of the transform to preserve \\nsymmetry. Of course, the DFT and its inverse are computed using an FFT algorithm.\\nRecall from discussions of the Fourier transform in Chapter 4 that high-frequency \\ncomponents account for ﬁne detail, and low-frequency components determine over-\\nall shape. Thus, the smaller we make \\nP\\n in Eq. (11-10), the more detail that will be lost \\non the boundary, as the following example illustrates. \\nEXAMPLE 11.8 :  Using Fourier descriptors.\\nFigure 11.19(a) shows the boundary of a human chromosome, consisting of 2868 points. The correspond-\\ning 2868 Fourier descriptors were obtained using Eq. (11-8). The objective of this example is to examine \\nthe effects of reconstructing the boundary using fewer Fourier descriptors. Figure 11.19(b) shows the \\nboundary reconstructed using one-half of the 2868 descriptors in Eq. (11-10). Observe that there is no \\nperceptible difference between this boundary and the original. Figures 11.19(c) through (h) show the \\nboundaries reconstructed with the number of Fourier descriptors being 10%, 5%, 2.5%, 1.25%, 0.63% \\nand 0.28% of 2868, respectively. When rounded to the nearest even integer, these percentages are equal \\nto 286, 144, 72, 36, 18, and 8 descriptors, respectively. The important point is that 18 descriptors, a mere \\nsix-tenths of one percent of the original 2868 descriptors, were sufﬁcient to retain the principal shape \\nfeatures of the original boundary: four long protrusions and two deep bays. Figure 11.19(h), obtained \\nwith 8 descriptors, is unacceptable because the principal features are lost. Further reductions to 4 and 2 \\ndescriptors would result in an ellipse and a circle, respectively (see Problem 11.18).\\nAs the preceding example demonstrates, a few Fourier descriptors can be used \\nto capture the essence of a boundary. This property is valuable, because these coef-\\nﬁcients carry shape information. Thus, forming a feature vector from these coef-\\nﬁcients can be used to differentiate between boundary shapes, as we will discuss in \\nChapter 12.\\nWe have stated several times that descriptors should be as insensitive as pos-\\nsible to translation, rotation, and scale changes. In cases where results depend on \\nthe order in which points are processed, an additional constraint is that descrip-\\ntors should be insensitive to the starting point. Fourier descriptors are not directly \\ninsensitive to these geometrical changes, but changes in these parameters can be \\nrelated to simple transformations on the descriptors. For example, consider rotation \\nand recall from basic mathematical analysis that rotation of a point by an angle \\nu\\n \\nabout the origin of the complex plane is accomplished by multiplying the point by \\ne\\nj\\nu\\n.\\n Doing so to every point of \\nsk\\n()\\n rotates the entire sequence about the origin. The \\nrotated sequence is \\nske\\nj\\n()\\nu\\n, whose Fourier descriptors are\\n \\nau s k ee\\naue\\nr\\nk\\nK\\njj u k K\\nj\\n()\\n=\\n()\\n=\\n()\\n=\\n∑\\n0\\n1\\n2\\n–\\n–\\nup\\nu\\n \\n(11-11)\\nDIP4E_GLOBAL_Print_Ready.indb   837\\n6/16/2017   2:15:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 838}),\n",
       " Document(page_content='838\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nfor \\nu\\n=−\\n012\\n1\\n,,, , .\\n…\\nK\\n Thus, rotation simply affects all coefficients equally by a \\nmultiplicative \\nconstant\\n term \\ne\\nj\\nu\\n.\\nTable 11.1 summarizes the Fourier descriptors for a boundary sequence \\nsk\\n()\\n that \\nundergoes rotation, translation, scaling, and changes in the starting point. The sym-\\nbol \\nΔ\\nxy\\n is deﬁned as \\nΔ= Δ + Δ\\nxy\\nxj y\\n,\\n so the notation \\nsk s k\\ntx\\ny\\n()\\n=\\n()\\n+Δ\\n indicates \\nredeﬁning (translating) the sequence as\\n \\nsk x k x jy k y\\nt\\n()\\n=\\n()\\n+Δ\\n⎡\\n⎣\\n⎤\\n⎦\\n+\\n()\\n+Δ\\n⎡\\n⎣\\n⎤\\n⎦\\n \\n(11-12)\\nNote that translation has no effect on the descriptors, except for \\nu\\n=\\n0,\\n which has the \\nvalue \\nd\\n()\\n.\\n0\\n Finally, the expression \\nsk s k k\\np\\n()\\n=\\n()\\n–\\n0\\n means redefining the sequence \\nas\\n \\nsk x k k j y k k\\np\\n()\\n=−\\n()\\n+−\\n()\\n00\\n \\n(11-13)\\nRecall from Chapter 4 \\nthat the Fourier transform \\nof a constant is an \\nimpulse located at the \\norigin. Recall also that \\nan impulse \\nδ\\n(\\nu\\n)\\n \\nis zero \\neverywhere, except when \\nu \\n= 0.\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 11.19\\n (a) Boundary of a human chromosome (2868 points). (b)–(h) Boundaries reconstructed using 1434, \\n286, 144, 72, 36, 18, and 8 Fourier descriptors, respectively. These numbers are approximately 50%, 10%, 5%, 2.5%, \\n1.25%, 0.63%, and 0.28% of 2868, respectively. Images (b)–(h) are shown as negatives to make the boundaries \\neasier to see.\\nDIP4E_GLOBAL_Print_Ready.indb   838\\n6/16/2017   2:15:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 839}),\n",
       " Document(page_content='11.3\\n  \\nBoundary Feature Descriptors\\n    \\n839\\nwhich changes the starting point of the sequence from \\nk\\n=\\n0\\n to \\nkk\\n=\\n0\\n.\\n The last entry \\nin \\nTable 11.1 shows that a change in starting point affects all descriptors in a differ-\\nent (but known) way, in the sense that the term multiplying \\nau\\n()\\n depends on \\nu\\n.\\nSTATISTICAL MOMENTS\\nStatistical moments of one variable are useful descriptors applicable to 1-D rendi-\\ntions of 2-D boundaries, such as signatures. To see how this can be accomplished, \\nconsider Fig. 11.20 which shows the signature from Fig. 11.10(b) sampled, and treated \\nas an ordinary discrete function \\ngr\\n()\\n of one variable, \\nr\\n.\\nSuppose that we treat the \\namplitude\\n of \\ng\\n as a discrete random variable \\nz\\n and \\nform an amplitude histogram \\npz\\ni\\n() ,\\n \\niA\\n=−\\n012\\n1\\n,,, , ,\\n…\\n where \\nA\\n is the number of \\ndiscrete amplitude increments in which we divide the amplitude scale\\n. If \\np\\n is normal-\\nized so that the sum of its elements equals 1, then \\npz\\ni\\n()\\n is an estimate of the prob-\\nability of intensity value \\nz\\ni\\n occurring. It then follows from Eq. (3-24) that the \\nn\\nth \\nmoment of \\nz\\n about its mean is\\n \\nm\\nni\\ni\\nA\\nn\\ni\\nzz m p z\\n()\\n=\\n() ( )\\n=\\n∑\\n–\\n–\\n0\\n1\\n \\n(11-14)\\nwhere\\n \\nmz p z\\ni\\ni\\nA\\ni\\n=\\n()\\n=\\n∑\\n0\\n1 –\\n \\n(11-15)\\nAs you know, \\nm\\n is the mean (average) value of \\nz\\n,\\n and \\nm\\n2\\n is its variance. Gener-\\nally, only the first few moments are required to differentiate between signatures of \\nclearly distinct shapes.\\nWe will discuss moments \\nof two variable in  \\nSection 11.4.\\nTransformation Boundary\\nFourier Descriptor\\nIdentity\\nsk\\n(\\n)\\nau\\n(\\n)\\nRotation\\nsk s k e\\nr\\nj\\n(\\n)\\n=\\n(\\n)\\nu\\nau a u e\\nr\\nj\\n(\\n)\\n=\\n(\\n)\\nu\\nTranslation \\nsk s k\\ntx\\ny\\n()\\n=\\n()\\n+Δ\\nau a u u\\ntx\\ny\\n()\\n=\\n()\\n+Δ\\n()\\nd\\nScaling\\nsk s k\\ns\\n()\\n=\\n()\\na\\nau a u\\ns\\n()\\n=\\n()\\na\\nStarting point\\nsk s k k\\np\\n(\\n)\\n=−\\n()\\n0\\nau a u e\\np\\njk\\nuK\\n(\\n)\\n=\\n(\\n)\\n–2\\n0\\np\\nTABLE \\n11.1\\nSome basic  \\nproperties of  \\nFourier  \\ndescriptors.\\nr\\ng\\n(\\nr\\n)\\nFIGURE 11.20\\nSampled  \\nsignature from \\nFig. 11.10(b) treat-\\ned as an ordinary, \\ndiscrete function \\nof one variable. \\nDIP4E_GLOBAL_Print_Ready.indb   839\\n6/16/2017   2:15:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 840}),\n",
       " Document(page_content='840\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nAn alternative approach is to normalize the area of \\ngr\\n()\\n in Fig. 11.20 to unity and \\ntreat it as a histogram.\\n In other words, \\ngr\\ni\\n()\\n is now treated as the probability of value \\nr\\ni\\n occurring. In this case, \\nr\\n is treated as the random variable and the moments are\\n \\nm\\nni\\ni\\nK\\nn\\ni\\nrr m g r\\n()\\n=\\n() ( )\\n=\\n∑\\n–\\n–\\n0\\n1\\n \\n(11-16)\\nwhere\\n \\nmr g r\\ni\\ni\\nK\\ni\\n=\\n()\\n=\\n∑\\n0\\n1 –\\n \\n(11-17)\\nIn these equations, \\nK\\n is the number of points on the boundary\\n, and \\nm\\nn\\nr\\n()\\n is related \\ndirectly to the shape of signature \\ngr\\n()\\n.\\n For example, the second moment \\nm\\n2\\n()\\nr\\n mea-\\nsures the spread of the curve about the mean value of \\nr\\n,\\n and the third moment \\nm\\n3\\n()\\nr\\n \\nmeasures its symmetry with respect to the mean.\\nAlthough moments are used frequently for characterizing signatures\\n, they are not \\nthe only descriptors used for this purpose. For instance, another approach is to com-\\npute the 1-D discrete Fourier transform of \\ngr\\n()\\n,\\n obtain its spectrum, and use the ﬁrst \\nfew components as descriptors\\n. The advantage of moments over other techniques is \\nthat their implementation is straightforward and they also carry a “physical” inter-\\npretation of signature (and by implication boundary) shape. The insensitivity of this \\napproach to rotation follows from the fact that signatures are independent of rota-\\ntion, provided that the starting point is always the same along the boundary. Size \\nnormalization can be achieved by scaling the values of \\ng \\nand\\n r\\n.\\n11.4  REGION FEATURE DESCRIPTORS  \\nAs we did with boundaries, we begin the discussion of regional features with some \\nbasic region descriptors.\\nSOME BASIC DESCRIPTORS\\nThe \\nmajor\\n and \\nminor\\n axes of a region, as well as the idea of a \\nbounding box\\n, are \\nas defined earlier for boundaries. The \\narea\\n of a region is defined as the number of \\npixels in the region. The \\nperimeter\\n of a region is the length of its boundary. When \\narea and perimeter are used as descriptors, they generally make sense only when \\nthey are normalized (Example 11.9 shows such a use). A more frequent use of these \\ntwo descriptors is in measuring \\ncompactness\\n of a region, defined as the perimeter \\nsquared over the area:\\n \\ncompactness\\n=\\np\\nA\\n2\\n \\n(11-18)\\nThis is a dimensionless measure that is \\n4\\np\\n for a circle (its minimum value) and 16 \\nfor a square\\n.\\nA similar dimensionless measure is \\ncircularity\\n (also called \\nroundness\\n), deﬁned as\\n \\ncircularity\\n=\\n4\\n2\\np\\nA\\np\\n \\n(11-19)\\n11.4\\nSometimes compactness \\nis deﬁned as the inverse of \\nthe circularity. Obviously, \\nthese two measures are \\nclosely related.\\nDIP4E_GLOBAL_Print_Ready.indb   840\\n6/16/2017   2:15:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 841}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n841\\nThe value of this descriptor is 1 for a circle (its maximum value) and \\np\\n4\\n for a square. \\nNote that these two measures are independent of size\\n, orientation, and translation. \\nAnother measure based on a circle is the \\neffective diameter\\n:\\n \\nd\\nA\\ne\\n=\\n2\\np\\n \\n(11-20)\\nThis is the diameter of a circle having the same area, \\nA\\n,\\n as the region being pro-\\ncessed. This measure is neither dimensionless nor independent of region size, but it \\nis independent of orientation and translation. It can be normalized for size and made \\ndimensionless by dividing it by the largest diameter expected in a given application. \\nIn a manner analogous to the way we defined compactness and circularity relative \\nto a circle, we define the \\neccentricity\\n of a region relative to an ellipse as the eccentric-\\nity of an ellipse that has the same second central \\nmoments as the region. \\nFor 1-D, the \\nsecond central moment is the variance. For 2-D discrete data, we have to consider \\nthe variance of each variable as well as the covariance between them. These are \\nthe components of the covariance matrix, which is estimated from samples using \\nEq. (11-21) below, with the samples in this case being 2-D vectors representing the \\ncoordinates of the data.\\nFigure 11.21(a) shows an ellipse in \\nstandard form\\n (i.e., an ellipse whose major and \\nminor axes are aligned with the coordinate axes). The eccentricity of such an ellipse \\nis defined as the ratio of the distance between foci (\\n2\\nc\\n in Fig. 11.21), and the length \\nof its major axis \\n() ,\\n2\\na\\n which gives the ratio \\n22\\nca c a\\n=\\n.\\n That is, \\n \\neccentricity\\n==\\n−\\n=−\\nc\\na\\nab\\na\\nba a b\\n22\\n2\\n1()\\n≥\\n \\nHowever\\n, we are interested in the eccentricity of an ellipse that has the same second \\ncentral moments as a given 2-D region, which means that our ellipses can have arbi-\\ntrary orientations. Intuitively, what we are trying to do is approximate our 2-D data \\nby an elliptical region whose axes are aligned with the principal axes of the data, as \\nFig. 11.21(b) illustrates. As you will learn in Section 11.5 (see Example 11.17), the \\nprincipal axes are the eigenvectors of the covariance matrix, \\nC\\n, of the data, which is \\ngiven by:\\n \\nCz z z z\\n=\\n−\\n−−\\n=\\n∑\\n1\\n1\\n1\\nK\\nkk\\nT\\nk\\nK\\n() ()\\n  \\n(11-21)\\nOften, you will the \\nconstant in Eq. (11-21) \\nwritten as 1/\\nK \\ninstead of \\n1/\\nK\\n−\\n1. The latter is used \\nto obtain a statistically- \\nunbiased estimate of \\nC\\n. \\nFor our purposes, either \\nformulation is acceptable.\\nb a\\nFIGURE 11.21\\n(a) An ellipse in \\nstandard form. \\n(b) An ellipse \\napproximating a \\nregion in arbitrary \\norientation.\\nc\\nb\\nFocus\\nFocus\\na\\n22 2\\nca b\\n=−\\nCentroid\\nof region\\n1\\ne\\n2\\ne\\n2\\nl\\n1\\nl\\nMajor axis\\nBinary\\nregion\\neigenvectors and\\ncorresponding eigenvalues\\nof the covariance matrix of \\nthe coordinates of the region\\n2\\ne\\n2\\nl\\n1\\ne\\n1\\nl\\nand are the\\nMinor axis\\nDIP4E_GLOBAL_Print_Ready.indb   841\\n6/16/2017   2:15:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 842}),\n",
       " Document(page_content='842\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nwhere \\nz\\nk\\n is a 2-D vector whose elements are the two spatial coordinates of a point in \\nthe region, \\nK\\n is the total number of points, and \\nz\\n is the mean vector:\\n \\nzz\\n=\\n=\\n∑\\n1\\n1\\nK\\nk\\nk\\nK\\n \\n(11-22)\\nThe main diagonal elements of \\nC\\n are the variances of the coordinate values of the \\npoints in the region,\\n and the off-diagonal elements are their covariances.\\nAn ellipse oriented in the same direction as the principal axes of the region can be \\ninterpreted as the intersection of a 2-D Gaussian function with the \\nxy\\n-plane. The ori-\\nentation of the axes of the ellipse are also in the direction of the eigenvectors of the \\ncovariance matrix, and the distances from the center of the ellipse to its intersection \\nwith its major and minor axes is equal to the largest and smallest eigenvalues of the \\ncovariance matrix, respectively, as Fig. 11.21(b) shows. With reference to Fig. 11.21, \\nand the equation of its eccentricity given above, we see by analogy that the eccen-\\ntricity of an ellipse with the same second moments as the region is given by\\n \\neccentricity\\n=\\n−\\n=−\\nll\\nl\\nll\\nll\\n2\\n2\\n1\\n2\\n2\\n12\\n2\\n21\\n1( )\\n≥\\n \\n(11-23)\\nFor circular regions, \\nll\\n12\\n=\\n and the eccentricity is 0. For a line, \\nl\\n1\\n0\\n=\\n and the eccen-\\ntricity is 1.\\n Thus, values of this descriptor are in the range \\n[,] .\\n01\\nEXAMPLE 11.9 :  Comparison of feature descriptors.\\nFigure 11.22 shows values of the preceding descriptors for several region shapes. None of the descriptors \\nfor the circle was exactly equal to its theoretical value because digitizing a circle introduces error into \\nthe computation, and because we approximated the length of a boundary as its number of elements. The \\neccentricity of the square did have an exact value of 0, because a square with no rotation aligns perfectly \\nwith the sampling grid. The other two descriptors for the square were close to their theoretical values also. \\nThe values listed in the ﬁrst two rows of Fig. 11.22 carry the same information. For example, we can \\ntell that the star is less compact and less circular than the other shapes. Similarly, it is easy to tell from the \\nnumbers listed that the teardrop region has by far the largest eccentricity, but it is harder to differentiate \\nfrom the other shapes using compactness or circularity.\\nAs we discussed in Section 11.1, feature descriptors typically are arranged in the form of feature \\nvectors for subsequent processing. Figure 11.23 shows the feature space for the descriptors in Fig. 11.22. \\n13.2308\\n42.2442\\n0.2975 0.9478\\n10.1701\\n1.2356\\n0.0411 0.0636 0.8117\\nCompactness\\nCircularity\\nEccentricity\\n15.9836\\n0.7862\\n0\\nDescriptor\\nb\\na\\nc\\nd\\nFIGURE 11.22\\nCompactness, \\ncircularity, and  \\neccentricity of \\nsome simple \\nbinary regions.\\nDIP4E_GLOBAL_Print_Ready.indb   842\\n6/16/2017   2:15:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 843}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n843\\nEach point in feature space “encapsulates” the three descriptor values for each object. Although we can \\ntell from looking at the values of the descriptors in the ﬁgure that the circle and square are much more \\nsimilar than the other two objects, note how much clearer this fact is in feature space. You can imagine \\nthat if we had multiple samples of those objects corrupted by noise, it could become difﬁcult to differ-\\nentiate between vectors (points) corresponding to squares or circles. In contrast, the star and teardrop \\nobjects are far from each other, and from the circle and square, so they are less likely to be misclassiﬁed \\nin the presence of noise. Feature space will play an important role in Chapter 12, when we discuss image \\npattern classiﬁcation. \\nEXAMPLE 11.10 :  Using area features.\\nEven a simple descriptor such as normalized area can be quite useful for extracting information from \\nimages. For instance, Fig. 11.24 shows a night-time satellite infrared image of the Americas. As we dis-\\ncussed in Section 1.3, such images provide a global inventory of human settlements. The imaging sensors \\nused to collect these images have the capability to detect visible and near infrared emissions, such as \\nlights, ﬁres, and ﬂares. The table alongside the images shows (by region from top to bottom) the ratio \\nof the area occupied by white (the lights) to the total light area in all four regions. A simple measure-\\nment like this can give, for example, a relative estimate by region of electrical energy consumption. The \\ndata can be reﬁned by normalizing it with respect to land mass per region, with respect to population \\nnumbers, and so on.\\nTOPOLOGICAL DESCRIPTORS\\nTopology\\n is the study of properties of a figure that are unaffected by any defor-\\nmation, provided that there is no tearing or joining of the figure (sometimes these \\nare called \\nrubber-sheet distortions\\n). For example, Fig. 11.25(a) shows a region with \\ntwo holes. Obviously, a topological descriptor defined as the number of holes in \\nthe region will not be affected by a stretching or rotation transformation. However, \\nthe number of holes can change if the region is torn or folded. Because stretching \\nFIGURE 11.23\\nThe descriptors \\nfrom Fig. 11.22 in \\n3-D feature space. \\nEach dot shown \\ncorresponds to \\na feature vector \\nwhose compo-\\nnents are the three \\ncorresponding \\ndescriptors in  \\nFig. 11.22.\\n10\\n30\\n40\\n50\\n60\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nx\\n1\\n \\n= compactness\\nx\\n2\\n = circularity\\nx\\n3\\n = eccentricit\\ny\\nStar\\nCircle\\nSquare\\nTeardrop\\n1\\n2\\n3\\nx\\nx\\nx\\n⎡⎤\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\n20\\nDIP4E_GLOBAL_Print_Ready.indb   843\\n6/16/2017   2:15:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 844}),\n",
       " Document(page_content='844\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\naffects distance, topological properties do not depend on the notion of distance or \\nany properties implicitly based on the concept of a distance measure.\\nAnother topological property useful for region description is the number of con-\\nnected components of an image or region. Figure 11.25(b) shows a region with three \\nconnected components. The number of holes \\nH\\n and connected components \\nC\\n in a \\nﬁgure can be used to deﬁne the \\nEuler number\\n, \\nE\\n: \\n \\nEC H\\n=−\\n(11-24)\\nSee Sections 2.5 and 9.5 \\nregarding connected \\ncomponents.\\nRegion no.\\n(from top)\\nRatio of lights per\\nregion to total lights\\n0.204\\n0.640\\n0.049\\n0.107\\n1\\n2\\n3\\n4\\nFIGURE 11.24\\nInfrared images \\nof the Americas at \\nnight. (Courtesy \\nof NOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   844\\n6/16/2017   2:15:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 845}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n845\\nThe Euler number is also a topological property. The regions shown in Fig. 11.26, for \\nexample, have Euler numbers equal to 0 and \\n−\\n1,\\n respectively, because the “A” has \\none connected component and one hole\\n, and the “B” has one connected component \\nbut two holes.\\nRegions represented by straight-line segments (referred to as \\npolygonal networks\\n) \\nhave a particularly simple interpretation in terms of the Euler number. Figure 11.27 \\nshows a polygonal network. Classifying interior regions of such a network into faces \\nand holes is often important. Denoting the number of vertices by \\nV\\n,\\n the number of \\nedges by \\nQ\\n,\\n and the number of faces by \\nF\\n gives the following relationship, called the \\nEuler formula\\n:\\n \\nVQF CH\\n−+\\n= −\\n \\n(11-25)\\nwhich, in view of Eq. (11-24), can be expressed as\\n \\nVQF E\\n−+\\n=\\n \\n(11-26)\\nThe network in Fig. 11.27 has seven vertices, eleven edges, two faces, one connected \\nregion,\\n and three holes; thus the Euler number is \\n−\\n2 \\n()\\n.\\ni.e., 7\\n11 2 1 3 2\\n−+ = − = −\\nEXAMPLE 11.11 :  Extracting and characterizing the largest feature in a segmented image.\\nFigure 11.28(a) shows a \\n512 512\\n×\\n,\\n 8-bit image of Washington, D.C. taken by a NASA LANDSAT satel-\\nlite\\n. This image is in the near infrared band (see Fig. 1.10 for details). Suppose that we want to segment \\nthe river using only this image (as opposed to using several multispectral images, which would simplify \\nthe task, as you will see later in this chapter). Because the river is a dark, uniform region relative to \\nthe rest of the image, thresholding is an obvious approach to try. The result of thresholding the image \\nwith the highest possible threshold value before the river became a disconnected region is shown in Fig. \\nb a\\nFIGURE 11.25\\n(a) A region with \\ntwo holes.  \\n(b) A region with \\nthree connected  \\ncomponents.\\nb a\\nFIGURE 11.26\\nRegions with \\nEuler numbers \\nequal to 0 and \\n−\\n1,\\n \\nrespectively\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   845\\n6/16/2017   2:15:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 846}),\n",
       " Document(page_content='846\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\n11.28(b). The threshold was selected manually to illustrate the point that it would be impossible in this \\ncase to segment the river by itself without other regions of the image also appearing in the thresholded \\nresult. \\nThe image in Fig. 11.28(b) has 1591 connected components (obtained using 8-connectivity) and its \\nEuler number is 1552, from which we deduce that the number of holes is 39. Figure 11.28(c) shows the \\nconnected component with the largest number of pixels (8479). This is the desired result, which we \\nalready know cannot be segmented by itself from the image using a threshold. Note how clean this result \\nis. The number of holes in the region deﬁned by the connected component just found would give us the \\nnumber of land masses within the river. If we wanted to perform measurements, like the length of each \\nbranch of the river, we could use the skeleton of the connected component [Fig. 11.28(d)] to do so. \\nTEXTURE\\nAn important approach to region description is to quantify its texture content. \\nWhile no formal definition of texture exists, intuitively this descriptor provides mea-\\nsures of properties such as smoothness, coarseness, and regularity (Fig. 11.29 shows \\nsome examples). In this section, we discuss statistical and spectral approaches for \\ndescribing the texture of a region. Statistical approaches yield characterizations of \\ntextures as smooth, coarse, grainy, and so on. Spectral techniques are based on prop-\\nerties of the Fourier spectrum and are used primarily to detect global periodicity in \\nan image by identifying high-energy, narrow peaks in its spectrum.\\nStatistical Approaches\\nOne of the simplest approaches for describing texture is to use statistical moments \\nof the intensity histogram of an image or region. Let \\nz\\n be a random variable denot-\\ning intensity, and let \\npz i L\\ni\\n()\\n=−\\n,, , , ,,\\n012 1\\n…\\n be the corresponding normalized his-\\ntogram,\\n where \\nL\\n is the number of distinct intensity levels. From Eq. (3-24), the \\nn\\nth \\nmoment of \\nz\\n about the mean is\\n \\nm\\nni\\ni\\nL\\nn\\ni\\nzz m p z\\n()\\n=−\\n() ( )\\n=\\n∑\\n0\\n1 –\\n \\n(11-27)\\nVertex\\nFace\\nHole\\nEdge\\nFIGURE 11.27\\nA region  \\ncontaining a  \\npolygonal  \\nnetwork.\\nDIP4E_GLOBAL_Print_Ready.indb   846\\n6/16/2017   2:15:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 847}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n847\\nb a\\nd c\\nFIGURE 11.28\\n(a) Infrared image \\nof the Washington, \\nD.C. area.  \\n(b) Thresholded \\nimage.  \\n(c) The largest  \\nconnected compo-\\nnent of (b).  \\n(d) Skeleton of (c). \\n(Original image \\ncourtesy of NASA.)\\nb a\\nc\\nFIGURE 11.29\\nThe white squares \\nmark, from left \\nto right, smooth, \\ncoarse, and regular \\ntextures. These are \\noptical microscope \\nimages of a  \\nsuperconductor, \\nhuman cholesterol, \\nand a microproces-\\nsor. (Courtesy of \\nDr. Michael W.  \\nDavidson, Florida \\nState University.)\\nDIP4E_GLOBAL_Print_Ready.indb   847\\n6/16/2017   2:15:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 848}),\n",
       " Document(page_content='848\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nwhere \\nm\\n is the mean value of \\nz\\n (i.e., the average intensity of the image or region):\\n \\nmz p z\\ni\\ni\\nL\\ni\\n=\\n()\\n=\\n−\\n∑\\n0\\n1\\n \\n(11-28)\\nNote from Eq. (11-27) that \\nm\\n0\\n1\\n=\\n and \\nm\\n1\\n0\\n=\\n.\\n The second moment [the variance \\nsm\\n2\\n2\\nzz\\n()\\n=\\n()\\n]\\n is particularly important in texture description. It is a measure of \\nintensity contrast that can be used to establish descriptors of relative intensity \\nsmoothness\\n. For example, the measure\\n \\nRz\\nz\\n()\\n=−\\n+\\n()\\n1\\n1\\n1\\n2\\ns\\n \\n(11-29)\\nis 0 for areas of constant intensity (the variance is zero there) and approaches 1 for \\nlarge values of \\ns\\n2\\nz\\n()\\n.\\n Because variance values tend to be large for grayscale images \\nwith values\\n, for example, in the range 0 to 255, it is a good idea to normalize the vari-\\nance to the interval [0, 1] for use in Eq. (11-29). This is done simply by dividing \\ns\\n2\\nz\\n()\\n \\nby \\nL\\n−\\n()\\n1\\n2\\n in Eq. (11-29). The standard deviation, \\ns\\n()\\n,\\nz\\n also is used frequently as a \\nmeasure of texture because its values are more intuitive\\n. \\nAs discussed in Section 2.6, the third moment, \\nm\\n3\\nz\\n()\\n,\\n is a measure of the skewness \\nof the histogram while the fourth moment,\\n \\nm\\n4\\nz\\n()\\n,\\n is a measure of its relative ﬂat-\\nness\\n. The ﬁfth and higher moments are not so easily related to histogram shape, but \\nthey do provide further quantitative discrimination of texture content. Some useful \\nadditional texture measures based on histograms include a measure of \\nuniformity\\n, \\ndeﬁned as\\n \\nUz p z\\ni\\ni\\nL\\n()\\n=\\n()\\n=\\n−\\n∑\\n2\\n0\\n1\\n \\n(11-30)\\nand a measure of \\naverage entrop\\ny\\n that, as you may recall from information theory, \\nis defined as\\n \\nez pz pz\\ni\\ni\\nL\\ni\\n()\\n=\\n() ()\\n=\\n−\\n∑\\n–l o g\\n0\\n1\\n2\\n \\n(11-31)\\nBecause values of \\np\\n are in the range [0,\\n 1] and their sum equals 1, the value of \\ndescriptor \\nU\\n is maximum for an image in which all intensity levels are equal (maxi-\\nmally uniform), and decreases from there. Entropy is a measure of variability, and is \\n0 for a constant image.\\nEXAMPLE 11.12 :  Texture descriptors based on histograms.\\nTable 11.2 lists the values of the preceding descriptors for the three types of textures highlighted in \\nFig. 11.29. The mean describes only the average intensity of each region and is useful only as a rough \\nidea of intensity, not texture. The standard deviation is more informative; the numbers clearly show \\nthat the ﬁrst texture has signiﬁcantly less variability in intensity (it is smoother) than the other two tex-\\ntures. The coarse texture shows up clearly in this measure. As expected, the same comments hold for \\nR\\n, \\nbecause it measures essentially the same thing as the standard deviation. The third moment is useful for \\nFor texture, typically we \\nare interested in signs \\nand relative magnitudes. \\nIf, in addition, normaliza-\\ntion proves to be useful, \\nwe normalize the third \\nand fourth moments. \\nDIP4E_GLOBAL_Print_Ready.indb   848\\n6/16/2017   2:15:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 849}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n849\\ndetermining the symmetry of histograms and whether they are skewed to the left (negative value) or the \\nright (positive value). This gives an indication of whether the intensity levels are biased toward the dark \\nor light side of the mean. In terms of texture, the information derived from the third moment is useful \\nonly when variations between measurements are large. Looking at the measure of uniformity, we again \\nconclude that the ﬁrst subimage is smoother (more uniform than the rest) and that the most random \\n(lowest uniformity) corresponds to the coarse texture. Finally, we see that the entropy values increase as \\nuniformity decreases, leading us to the same conclusions regarding the texture of the regions as the uni-\\nformity measure did. The ﬁrst subimage has the lowest variation in intensity levels, and the coarse image \\nthe most. The regular texture is in between the two extremes with respect to both of these measures.\\nMeasures of texture computed using only histograms carry no information regard-\\ning spatial relationships between pixels, which is important when describing texture. \\nOne way to incorporate this type of information into the texture-analysis process is \\nto consider not only the distribution of intensities, but also the \\nrelative positions\\n of \\npixels in an image. \\nLet \\nQ\\n be an operator that deﬁnes the position of two pixels relative to each other, \\nand consider an image\\n, \\nf\\n,\\n with \\nL\\n possible intensity levels. Let \\nG\\n be a matrix whose \\nelement \\ng\\nij\\n is the number of times that pixel pairs with intensities \\nz\\ni\\n and \\nz\\nj\\n occur in \\nimage \\nf\\n in the position speciﬁed by \\nQ\\n,\\n where \\n1\\n≤≤\\nij\\nL\\n,.\\n A matrix formed in this \\nmanner is referred to as a \\ngra\\nylevel\\n (or \\nintensity\\n) \\nco-occurrence matrix\\n. When the \\nmeaning is clear, \\nG\\n is referred to simply as a \\nco-occurrence matrix\\n.\\nFigure 11.30 shows an example of how to construct a co-occurrence matrix using  \\nL\\n=\\n8\\n and a position operator \\nQ\\n deﬁned as “one pixel immediately to the right” (i.e., \\nthe neighbor of a pixel is deﬁned as the pixel immediately to its right).\\n The array on \\nthe left is a small image and the array on the right is matrix \\nG\\n. We see that element \\n(,)\\n11\\n of \\nG\\n is 1,\\n because there is only one occurrence in \\nf\\n of a pixel valued 1 having \\na pixel valued 1 immediately to its right.\\n Similarly, element \\n(, )\\n62\\n of \\nG\\n is 3,\\n because \\nthere are three occurrences in \\nf\\n of a pixel with a value of 6 having a pixel valued 2 \\nimmediately to its right.\\n The other elements of \\nG\\n are similarly computed. If we had \\ndeﬁned \\nQ\\n as, say, “one pixel to the right and one pixel above,” then position \\n(,)\\n11\\n \\nin \\nG\\n would have been 0 because there are no instances in \\nf\\n of a 1 with another 1 in \\nthe position speciﬁed by \\nQ\\n.\\n On the other hand, positions \\n(, ) ,\\n13\\n \\n(, ) ,\\n15\\n and \\n(, )\\n17\\n in \\nG\\n would all be 1’\\ns, because intensity value 1 occurs in \\nf\\n with neighbors valued 3, 5, \\nand 7 in the position speciﬁed by \\nQ—\\none occurrence of each.\\n As an exercise, you \\nshould compute all the elements of \\nG\\n using this deﬁnition of \\nQ\\n.\\nNote that we are using \\nthe intensity range [1,  \\nL\\n] \\ninstead of the usual \\n[0,  \\nL\\n−\\n 1]. We do this so \\nthat intensity values will \\ncorrespond with “tradi-\\ntional” matrix indexing \\n(i.e., intensity value 1 \\ncorresponds to the ﬁrst \\nrow and column indices \\nof \\nG\\n).\\nTexture Mean\\nStandard  \\ndeviation\\nR\\n (normalized)\\n3rd moment Uniformity Entropy\\nSmooth 82.64 11.79\\n0.002\\n− \\n0.105\\n0.026\\n5.434\\nCoarse 143.56 74.63\\n0.079\\n−\\n 0.151\\n0.005\\n7.783\\nRegular 99.72 33.73\\n0.017 0.750 0.013\\n6.674\\nTABLE \\n11.2\\n  \\nStatistical texture measures for the subimages in Fig. 11.29.\\nDIP4E_GLOBAL_Print_Ready.indb   849\\n6/16/2017   2:15:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 850}),\n",
       " Document(page_content='850\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nThe number of possible intensity levels in the image determines the size \\nof matrix \\nG\\n. For an 8-bit image (256 possible intensity levels), \\nG\\n will be of size \\n256 256\\n×\\n.\\n This is not a problem when working with one matrix but, as you will see \\nin as Example 11.13,\\n co-occurrence matrices sometimes are used in sequences. One \\napproach for reducing computations is to quantize the intensities into a few bands \\nin order to keep the size of \\nG\\n manageable. For example, in the case of 256 intensities, \\nwe can do this by letting the ﬁrst 32 intensity levels equal to 1, the next 32 equal to 2, \\nand so on. This will result in a co-occurrence matrix of size \\n88\\n×\\n.\\nThe total number, \\nn\\n,\\n of pixel pairs that satisfy \\nQ\\n is equal to the sum of the ele-\\nments of \\nG\\n (\\nn\\n=\\n30 in the example of Fig. 11.30). Then, the quantity\\n \\np\\ng\\nn\\nij\\nij\\n=\\nis an estimate of the probability that a pair of points satisfying \\nQ\\n will have values \\nzz\\nij\\n,.\\n()\\n These probabilities are in the range \\n[, ]\\n01\\n and their sum is 1:\\n \\np\\nij\\nj\\nK\\ni\\nK\\n=\\n= =\\n∑ ∑\\n1\\n1 1\\nwhere \\nK\\n is the row and column dimension of square matrix \\nG\\n.\\nBecause \\nG\\n depends on \\nQ\\n,\\n the presence of intensity texture patterns can be detected \\nby choosing an appropriate position operator and analyzing the elements of \\nG\\n.\\n A set \\nof descriptors useful for characterizing the contents of \\nG\\n are listed in T\\nable 11.3. The \\nquantities used in the correlation descriptor (second row) are deﬁned as follows:\\n \\nmi p\\nri\\nj\\nj\\nK\\ni\\nK\\n=\\n= =\\n∑ ∑\\n1 1\\n \\nmj p\\nci\\nj\\ni\\nK\\nj\\nK\\n=\\n= =\\n∑ ∑\\n1 1\\nand\\n1\\n1\\n7\\n5\\n3\\n2\\n5\\n1\\n6\\n1\\n2\\n5\\n8\\n8\\n6\\n8\\n1\\n2\\n4\\n3\\n4\\n5\\n5\\n1\\n8\\n7\\n8\\n7\\n6\\n2\\n7\\n8\\n6\\n2\\n6\\n2\\n0\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n00110\\n20\\n45678\\n123\\n1\\n0\\n0\\n1\\n0\\n1\\n3\\n0\\n0\\n0\\n0\\n2\\n0\\n0\\n2\\n0\\n1\\n0\\n0\\n2\\n1\\n0\\n1\\n2\\n1\\n3\\n4\\n5\\n6\\n7\\n8\\n0\\n0\\n2\\n1\\nImage \\nf\\nCo-occurrence matrix \\nG\\nFIGURE 11.30\\nHow to construct \\na co-occurrence \\nmatrix.\\nDIP4E_GLOBAL_Print_Ready.indb   850\\n6/16/2017   2:15:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 851}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n851\\n \\ns\\ns\\nrr i\\nj\\nj\\nK\\ni\\nK\\ncc i\\nj\\ni\\nK\\nj\\nK\\nim p\\njm p\\n2\\n2\\n1\\n1\\n2\\n2\\n1\\n1\\n=\\n()\\n=\\n()\\n=\\n=\\n=\\n=\\n∑ ∑\\n∑ ∑\\n–\\n–\\nIf we let\\n \\nPi p\\nij\\nj\\nK\\n()\\n=\\n=\\n∑\\n1\\n \\nand\\n \\nPj p\\nij\\ni\\nK\\n()\\n=\\n=\\n∑\\n1\\nthen the preceding equations can be written as\\n \\nmi P i\\nr\\ni\\nK\\n=\\n()\\n=\\n∑\\n1\\nDescriptor\\nExplanation\\nFormula\\nMaximum \\nprobability\\nMeasures the strongest response of \\nG\\n. \\nThe range of values is [0, 1].\\nmax( )\\n,\\nij\\nij\\np\\nCorrelation A measure of how correlated a pixel is \\nto its neighbor over the entire image. The \\nrange of values is 1 to \\n−\\n1 corresponding \\nto perfect positive and perfect negative \\ncorrelations. This measure is not deﬁned \\nif either standard deviation is zero.\\nim jmp\\nrc i j\\nrc\\nj\\nK\\ni\\nK\\nrc\\n––\\n;\\n()\\n()\\n= =\\n∑ ∑\\nss\\nss\\n1 1\\n00\\n≠≠\\nContrast A measure of intensity contrast between a \\npixel and its neighbor over the entire image\\n. \\nThe range of values is 0 (when \\nG\\n is constant) \\nto \\n() .\\nK\\n−\\n1\\n2\\nijp\\nij\\nj\\nK\\ni\\nK\\n−\\n(\\n)\\n= =\\n∑ ∑\\n2\\n1 1\\nUniformity (also \\ncalled Energy)\\nA measure of uniformity in the range [0, 1]. \\nUniformity is 1 for a constant image.\\np\\nij\\nj\\nK\\ni\\nK\\n2\\n1 1\\n= =\\n∑ ∑\\nHomogeneity Measures the spatial closeness to the diagonal \\nof the distribution of elements in \\nG\\n. The range \\nof values is [0, 1], with the maximum being \\nachieved when \\nG\\n is a diagonal matrix.\\np\\nij\\nij\\nj\\nK\\ni\\nK\\n1\\n1 1\\n+−\\n= =\\n∑ ∑\\nEntropy\\nMeasures the randomness of the elements of \\nG\\n. The entropy is 0 when all \\np\\nij\\n’s are 0, and is \\nmaximum when the \\np\\nij\\n’s are uniformly distrib-\\nuted. The maximum value is thus \\n2\\n2\\nlog .\\nK\\n–l o g\\npp\\nij ij\\nj\\nK\\ni\\nK\\n2\\n1 1\\n= =\\n∑ ∑\\nTABLE \\n11.3\\nDescriptors used \\nfor characterizing \\nco-occurrence \\nmatrices of size \\nKK\\n×\\n.\\n The term \\np\\nij\\n is the \\nij\\n-th term \\nof \\nG\\n divided by \\nthe sum of the \\nelements of \\nG\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   851\\n6/16/2017   2:15:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 852}),\n",
       " Document(page_content='852\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\n \\nmj P j\\nc\\nj\\nK\\n=\\n()\\n=\\n∑\\n1\\n \\ns\\nrr\\ni\\nK\\nim P i\\n2\\n2\\n1\\n=\\n()\\n()\\n=\\n∑\\n–\\nand\\n \\ns\\ncc\\nj\\nK\\njm P j\\n2\\n2\\n1\\n=\\n()\\n()\\n=\\n∑\\n–\\nWith reference to Eqs. (11-27), (11-28), and to their explanation, we see that \\nm\\nr\\n is \\nin the form of a mean computed along rows of the normalized \\nG\\n, and \\nm\\nc\\n is a mean \\ncomputed along the columns. Similarly, \\ns\\nr\\n and \\ns\\nc\\n are in the form of standard devia-\\ntions (square roots of the variances) computed along rows and columns, respectively. \\nEach of these terms is a scalar, independently of the size of \\nG\\n.\\nKeep in mind when studying Table 11.3 that “neighbors” are with respect to the \\nway in which \\nQ\\n is deﬁned (i.e., neighbors do not necessarily have to be adjacent), \\nand also that the \\np\\nij\\n’s are nothing more than normalized counts of the number of \\ntimes that pixels having intensities \\nz\\ni\\n and \\nz\\nj\\n occur in \\nf\\n relative to the position speci-\\nﬁed in \\nQ\\n.\\n Thus, all we are doing here is trying to ﬁnd patterns (texture) in those \\ncounts\\n.\\nEXAMPLE 11.13 :  Using descriptors to characterize co-occurrence matrices.\\nFigures 11.31(a) through (c) show images consisting of random, horizontally periodic (sine), and mixed \\npixel patterns, respectively. This example has two objectives: (1) to show values of the descriptors in \\nTable 11.3 for the three co-occurrence matrices, \\nG\\n1\\n, \\nG\\n2\\n,and \\nG\\n3\\n,\\n corresponding (from top to bottom) \\nto these images;\\n and (2) to illustrate how sequences of co-occurrence matrices can be used to detect \\ntexture patterns in an image.\\nFigure 11.32 shows co-occurrence matrices \\nG\\n1\\n, \\nG\\n2\\n,\\n and \\nG\\n3\\n,\\n displayed as images. These matrices were \\nobtained using \\nL\\n=\\n256\\n and the position operator “one pixel immediately to the right.” The value at \\ncoordinates \\n(, )\\nij\\n in these images is the number of times that pixel pairs with intensities \\nz\\ni\\n and \\nz\\nj\\n occur \\nin \\nf\\n in the position speciﬁed by \\nQ\\n,\\n so it is not surprising that Fig. 11.32(a) is a random image, given the \\nnature of the image from which it was obtained.\\nF\\nigure 11.32(b) is more interesting. The ﬁrst obvious feature is the symmetry about the main diagonal. \\nBecause of the symmetry of the sine wave, the number of counts for a pair \\n(, )\\nzz\\nij\\n is the same as for the \\npair \\n(,) ,\\nzz\\nji\\n which produces a symmetric co-occurrence matrix. The nonzero elements of \\nG\\n2\\n are sparse \\nbecause value differences between horizontally adjacent pixels in a horizontal sine wave are relatively \\nsmall. It helps to remember in interpreting these concepts that a digitized sine wave is a staircase, with \\nthe height and width of each step depending on the frequency of the sine wave and the number of ampli-\\ntude levels used in representing the function.\\nThe structure of co-occurrence matrix \\nG\\n3\\n in Fig. 11.32(c) is more complex. High count values are \\ngrouped along the main diagonal also, but their distribution is more dense than for \\nG\\n2\\n,\\n a property \\nthat is indicative of an image with a rich variation in intensity values\\n, but few large jumps in intensity \\nbetween adjacent pixels. Examining Fig. 11.32(c), we see that there are large areas characterized by low \\nDIP4E_GLOBAL_Print_Ready.indb   852\\n6/16/2017   2:15:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 853}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n853\\nvariability in intensities. The high transitions in intensity occur at object boundaries, but these counts \\nare low with respect to the moderate intensity transitions over large areas, so they are obscured by the \\nability of an image display to show high and low values simultaneously, as we discussed in Chapter 3.\\nThe preceding observations are qualitative. To quantify the “content” of co-occurrence matrices, we \\nneed descriptors such as those in Table 11.3. Table 11.4 shows values of these descriptors computed \\nfor the three co-occurrence matrices in Fig. 11.32. To use these descriptors, the co-occurrence matrices \\nmust be normalized by dividing them by the sum of their elements, as discussed earlier. The entries in \\nTable 11.4 agree with what one would expect from the images in Fig. 11.31 and their corresponding co-\\noccurrence matrices in Fig. 11.32. For example, consider the Maximum Probability column in Table 11.4. \\nThe highest probability corresponds to the third co-occurrence matrix, which tells us that this matrix \\nhas the highest number of counts (largest number of pixel pairs occurring in the image relative to the \\npositions in \\nQ\\n)\\n than the other two matrices. This agrees with our analysis of \\nG\\n3\\n.\\n The second column indi-\\ncates that the highest correlation corresponds to \\nG\\n2\\n,\\n which in turn tells us that the intensities in the sec-\\nond image are highly correlated.\\n The repetitiveness of the sinusoidal pattern in Fig. 11.31(b) indicates \\nwhy this is so. Note that the correlation for \\nG\\n1\\n is essentially zero, indicating that there is virtually no \\ncorrelation between adjacent pixels, a characteristic of random images such as the image in Fig. 11.31(a).\\nb\\na\\nc\\nFIGURE 11.31\\nImages whose  \\npixels have  \\n(a) random,  \\n(b) periodic, and \\n(c) mixed texture \\npatterns. Each \\nimage is of size \\n263 800\\n×\\n pixels.\\nb a\\nc\\nFIGURE 11.32\\n256 256\\n×\\n  \\nco-occurrence  \\nmatrices \\nG\\n1\\n, \\nG\\n2\\n, \\nand \\nG\\n3\\n,  \\ncorresponding \\nfrom left to right \\nto the images in \\nF\\nig. 11.31.\\nDIP4E_GLOBAL_Print_Ready.indb   853\\n6/16/2017   2:15:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 854}),\n",
       " Document(page_content='854\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nThe contrast descriptor is highest for \\nG\\n1\\n and lowest for \\nG\\n2\\n.\\n Thus, we see that the less random an \\nimage is\\n, the lower its contrast tends to be. We can see the reason by studying the matrix displayed in \\nFig. 11.32. The \\n()\\nij\\n−\\n2\\n terms are differences of integers for \\n1\\n≤≤\\nij\\nL\\n,,\\n so they are the same for any \\nG\\n. \\nT\\nherefore, the probabilities of the elements of the normalized co-occurrence matrices are the factors \\nthat determine the value of contrast. Although \\nG\\n1\\n has the lowest maximum probability, the other two \\nmatrices have many more zero or near-zero probabilities (the dark areas in Fig. 11.32). Because the sum \\nof the values of \\nG\\nn\\n is 1, it is easy to see why the contrast descriptor tends to increase as a function of \\nrandomness\\n.\\nThe remaining three descriptors are explained in a similar manner. Uniformity increases as a func-\\ntion of the values of the probabilities squared. Thus, the less randomness there is in an image, the higher \\nthe uniformity descriptor will be, as the ﬁfth column in Table 11.4 shows. Homogeneity measures the \\nconcentration of values of \\nG\\n with respect to the main diagonal. The values of the denominator term \\n()\\n1\\n+−\\nij\\n are the same for all three co-occurrence matrices, and they decrease as \\ni\\n and \\nj\\n become closer \\nin value (i.e\\n., closer to the main diagonal). Thus, the matrix with the highest values of probabilities \\n(numerator terms) near the main diagonal will have the highest value of homogeneity. As we discussed \\nearlier, such a matrix will correspond to images with a “rich” gray-level content and areas of slowly vary-\\ning intensity values. The entries in the sixth column of Table 11.4 are consistent with this interpretation.\\nThe entries in the last column of the table are measures of randomness in co-occurrence matrices, \\nwhich in turn translate into measures of randomness in the corresponding images. As expected, \\nG\\n1\\n had \\nthe highest value because the image from which it was derived was totally random. The other two \\nentries are self-explanatory. Note that the entropy measure for \\nG\\n1\\n is near the theoretical maximum of \\n16 \\n(l o g ) .\\n2\\n256 16\\n2\\n=\\n The image in Fig. 11.31(a) is composed of uniform noise, so each intensity level has \\napproximately an equal probability of occurrence\\n, which is the condition stated in Table 11.3 for maxi-\\nmum entropy.\\nThus far, we have dealt with single images and their co-occurrence matrices. Suppose that we want \\nto “discover” (without looking at the images) if there are any sections in these images that contain \\nrepetitive components (i.e., periodic textures). One way to accomplish this goal is to examine the cor-\\nrelation descriptor for sequences of co-occurrence matrices, derived from these images by increasing \\nthe distance between neighbors. As mentioned earlier, it is customary when working with sequences of \\nco-occurrence matrices to quantize the number of intensities in order to reduce matrix size and corre-\\nsponding computational load. The following results were obtained using \\nL\\n=\\n8.\\nFigure 11.33 shows plots of the correlation descriptors as a function of horizontal “offset” (i.e., hori-\\nzontal distance between neighbors) from 1 (for adjacent pixels) to 50.\\n Figure 11.33(a) shows that all \\ncorrelation values are near 0, indicating that no such patterns were found in the random image. The \\nNormalized \\nCo-occurrence\\nMatrix\\nMaximum \\nProbability\\nCorrelation Contrast Uniformity Homogeneity Entropy\\nG\\n11\\nn\\n0.00006\\n−\\n0.0005\\n10838\\n0.00002\\n0.0366\\n15.75\\nG\\n22\\nn\\n0.01500 0.9650 00570\\n0.01230\\n0.0824\\n06.43\\nG\\n33\\nn\\n0.06860 0.8798 01356 0.00480 0.2048 13.58\\nTABLE \\n11.4\\nDescriptors evaluated using the co-occurrence matrices displayed as images in Fig. 11.32.\\nDIP4E_GLOBAL_Print_Ready.indb   854\\n6/16/2017   2:15:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 855}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n855\\nshape of the correlation in Fig. 11.33(b) is a clear indication that the input image is sinusoidal in the hori-\\nzontal direction. Note that the correlation function starts at a high value, then decreases as the distance \\nbetween neighbors increases, and then repeats itself.\\nFigure 11.33(c) shows that the correlation descriptor associated with the circuit board image \\ndecreases initially, but has a strong peak for an offset distance of 16 pixels. Analysis of the image in Fig. \\n11.31(c) shows that the upper solder joints form a repetitive pattern approximately 16 pixels apart (see \\nFig. 11.34). The next major peak is at 32, caused by the same pattern, but the amplitude of the peak is \\nlower because the number of repetitions at this distance is less than at 16 pixels. A similar observation \\nexplains the even smaller peak at an offset of 48 pixels.\\nSpectral Approaches\\nAs we discussed in Section 5.4, the Fourier spectrum is ideally suited for describing \\nthe directionality of periodic or semiperiodic 2-D patterns in an image. These global \\ntexture patterns are easily distinguishable as concentrations of high-energy bursts in \\nthe spectrum. Here, we consider three features of the Fourier spectrum that are use-\\nful for texture description: (1) prominent peaks in the spectrum give the principal \\ndirection of the texture patterns; (2) the location of the peaks in the frequency plane \\ngives the fundamental spatial period of the patterns; and (3) eliminating any peri-\\nodic components via filtering leaves nonperiodic image elements, which can then \\nbe described by statistical techniques. Recall that the spectrum is symmetric about \\nthe origin, so only half of the frequency plane needs to be considered. Thus, for the \\n1 10 20 30 40 50 1 10 20 30 40 50\\n1 1 02 03 04 0 5 0\\n/H11002\\n1\\n/H11002\\n0.5\\n0\\n1\\n0.5\\nCorrelation\\nHorizontal Offset\\nHorizontal Offset\\nHorizontal Offset\\nb a\\nc\\nFIGURE 11.33\\n Values of the correlation descriptor as a function of offset (distance between “adjacent” pixels) corre-\\nsponding to the (a) noisy, (b) sinusoidal, and (c) circuit board images in Fig. 11.31.\\n16 pixels\\nFIGURE 11.34\\nA zoomed section \\nof the circuit board  \\nimage showing  \\nperiodicity of  \\ncomponents.\\nDIP4E_GLOBAL_Print_Ready.indb   855\\n6/16/2017   2:15:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 856}),\n",
       " Document(page_content='856\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\npurpose of analysis, every periodic pattern is associated with only one peak in the \\nspectrum, rather than two.\\nDetection and interpretation of the spectrum features just mentioned often \\nare simpliﬁed by expressing the spectrum in polar coordinates to yield a function \\nSr\\n,,\\nu\\n()\\n where \\nS\\n is the spectrum function, and \\nr\\n and \\nu\\n are the variables in this coor-\\ndinate system.\\n For each direction \\nu\\n, \\nSr\\n,\\nu\\n()\\n may be considered a 1-D function \\nSr\\nu\\n()\\n. \\nSimilarly\\n, for each frequency \\nrS\\nr\\n,\\nu\\n()\\n is a 1-D function. Analyzing \\nSr\\nu\\n()\\n for a ﬁxed \\nvalue of \\nu\\n yields the behavior of the spectrum (e.g., the presence of peaks) along a \\nradial direction from the origin,\\n whereas analyzing \\nS\\nr\\nu\\n()\\n for a ﬁxed value of \\nr\\n yields \\nthe behavior along a circle centered on the origin.\\nA more global description is obtained by integrating (summing for discrete vari-\\nables) these functions:\\n \\nSr S r\\n()\\n=\\n()\\n=\\n∑\\nu\\nu\\np\\n0\\n \\n(11-32)\\nand\\n \\nSS\\nr\\nr\\nR\\nuu\\n()\\n=\\n()\\n=\\n∑\\n1\\n0\\n \\n(11-33)\\nwhere \\nR\\n0\\n is the radius of a circle centered at the origin.\\nThe results of Eqs. (11-32) and (11-33) constitute a pair of values \\nSr S\\n() ()\\n⎡\\n⎣\\n⎤\\n⎦\\n,\\nu\\n for \\neach pair of coordinates \\nr\\n,.\\nu\\n()\\n By varying these coordinates, we can generate two \\n1-D functions, \\nSr\\n()\\n and \\nS\\nu\\n()\\n,\\n that constitute a spectral-energy description of texture \\nfor an entire image or region under consideration.\\n Furthermore, descriptors of these \\nfunctions themselves can be computed in order to characterize their behavior quan-\\ntitatively. Descriptors useful for this purpose are the location of the highest value, \\nthe mean and variance of both the amplitude and axial variations, and the distance \\nbetween the mean and the highest value of the function.\\nEXAMPLE 11.14 :  Spectral texture.\\nFigure 11.35(a) shows an image containing randomly distributed objects, and Fig. 11.35(b) shows an \\nimage in which these objects are arranged periodically. Figures 11.35(c) and (d) show the corresponding \\nFourier spectra. The periodic bursts of energy extending quadrilaterally in two dimensions in both Fou-\\nrier spectra are due to the periodic texture of the coarse background material on which the objects rest. \\nThe other dominant components in the spectra in Fig. 11.35(c) are caused by the random orientation of \\nthe object edges in Fig. 11.35(a). On the other hand, the main energy in Fig. 11.35(d) not associated with \\nthe background is along the horizontal axis, corresponding to the strong vertical edges in Fig. 11.35(b).\\nFigures 11.36(a) and (b) are plots of \\nSr\\n()\\n and \\nS\\nu\\n()\\n for the random objects, and similarly in (c) and \\n(d) for the ordered objects. The plot of \\nSr\\n()\\n for the random objects shows no strong periodic compo-\\nnents (i.e., there are no dominant peaks in the spectrum besides the peak at the origin, which is the dc \\ncomponent). Conversely, the plot of \\nSr\\n()\\n for the ordered objects shows a strong peak near \\nr\\n=\\n15\\n and \\na smaller one near \\nr\\n=\\n25\\n,\\n corresponding to the periodic horizontal repetition of the light (objects) and \\ndark (background) regions in F\\nig. 11.35(b). Similarly, the random nature of the energy bursts in Fig. \\n11.35(c) is quite apparent in the plot of \\nS\\nu\\n()\\n in Fig. 11.36(b). By contrast, the plot in Fig. 11.36(d) shows \\nstrong energy components in the region near the origin and at 90° and 180°. This is consistent with the \\nenergy distribution of the spectrum in Fig. 11.35(d).\\nDIP4E_GLOBAL_Print_Ready.indb   856\\n6/16/2017   2:15:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 857}),\n",
       " Document(page_content='11.4\\n  \\nRegion Feature Descriptors\\n    \\n857\\nb a\\nd c\\nFIGURE 11.35\\n(a) and (b) Images \\nof random and \\nordered objects.  \\n(c) and (d) Cor-\\nresponding  \\nFourier spectra. All \\nimages are of size \\n600 600\\n×\\n pixels.\\n0\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\n7.0\\n8.0\\n9.0\\n0 50 100 150 200 250 300\\n2.7\\n1.8\\n1.9\\n2.0\\n2.1\\n2.2\\n2.3\\n2.4\\n2.5\\n2.6\\n0 20 40 60 80 100 120 140 160 180\\n3.6\\n1.6\\n1.8\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n0 20 40 60 80 100 120 140 160 180\\n0\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\n0 50 100 150 200 250 300\\nb a\\nd c\\nFIGURE 11.36\\n (a) and (b) Plots \\nof  \\nSr\\n()\\n and \\nS\\n()\\nu\\n \\nfor F\\nig. 11.35(a).  \\n(c) and (d) Plots \\nof \\nSr\\n()\\n and \\nS\\n()\\nu\\n \\nfor F\\nig. 11.35(b).  \\nAll vertical axes \\nare \\n×\\n10\\n5\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   857\\n6/16/2017   2:15:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 858}),\n",
       " Document(page_content='858\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nMOMENT INVARIANTS\\nThe 2-D \\nmoment\\n of order \\npq\\n+\\n()\\n of an \\nMN\\n×\\n digital image, \\nfx y\\n(,\\n) ,\\n is defined as\\n \\nmx y f x y\\npq\\npq\\ny\\nN\\nx\\nM\\n=\\n()\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1–\\n–\\n,  \\n(11-34)\\nwhere \\np\\n=\\n012\\n,,\\n,\\n…\\n and \\nq\\n=\\n012\\n,,\\n,\\n…\\n are integers. The corresponding \\ncentral moment\\n \\nof order \\npq\\n+\\n()\\n is defined as\\n \\nm\\npq\\ny\\nN\\nx\\nM\\np\\nq\\nxx yyf x y\\n=\\n() () ( )\\n= =\\n∑ ∑\\n–– ,\\n– –\\n0\\n1\\n0\\n1\\n \\n(11-35)\\nfor \\np\\n=\\n012\\n,,\\n,\\n…\\n and \\nq\\n=\\n012\\n,,\\n, ,\\n…\\n where\\n \\nx\\nm\\nm\\ny\\nm\\nm\\n==\\n10\\n00\\n01\\n00\\nand\\n \\n(11-36)\\nThe \\nnormaliz\\ned central moment\\n of order \\npq\\n+\\n()\\n, denoted \\nh\\npq\\n, is defined as\\n \\nh\\nm\\nm\\ng\\npq\\npq\\n=\\n00\\n \\n(11-37)\\nwhere\\n \\ng\\n=\\n+\\n+\\npq\\n2\\n1  \\n(11-38)\\nfor \\npq\\n+=\\n23\\n,,\\n.\\n…\\nA set of seven, 2-D \\nmoment invariants\\n can be derived from the \\nsecond and third normalized central moments:\\n†\\n \\nfh h\\n12 00 2\\n=+\\n \\n(11-39)\\n  \\nfhh h\\n22 0 0 2\\n2\\n11\\n2\\n4\\n=\\n()\\n+\\n–\\n \\n(11-40)\\n \\nfh h hh\\n33 0 1 2\\n2\\n21 03\\n2\\n33\\n=\\n()\\n+\\n()\\n––\\n \\n(11-41)\\n \\nfhh hh\\n43 0 1 2\\n2\\n21 03\\n2\\n=+\\n()\\n++\\n()\\n \\n(11-42)\\n†\\n Derivation of these results requires concepts that are beyond the scope of this discussion. The book by Bell \\n[1965] and the paper by Hu [1962] contain detailed discussions of these concepts. For generating moment invari-\\nants of an order higher than seven, see Flusser [2000]. Moment invariants can be generalized to \\nn\\n dimensions \\n(see Mamistvalov [1998]).\\nDIP4E_GLOBAL_Print_Ready.indb   858\\n6/16/2017   2:15:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 859}),\n",
       " Document(page_content='11.5\\n  \\nPrincipal Components as Feature Descriptors\\n    \\n859\\n \\nfh hhhhh hh\\n5 3 0 1 2 3 01 2 3 01 2\\n2\\n21 03\\n2\\n33\\n=\\n()\\n+\\n()\\n+\\n()\\n⎡\\n⎣\\n−+\\n()\\n⎤\\n⎦\\n–\\n \\n+\\n()\\n+\\n()\\n+\\n()\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n33\\n21 03 21 03 30 12\\n2\\n21 03\\n2\\nhhhh hh hh\\n––\\n \\n(11-43)\\n \\nfhh hh hh\\n6 2 00 2 3 01 2\\n2\\n21 03\\n2\\n=\\n()\\n+\\n()\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n––\\n \\n++\\n()\\n+\\n()\\n4\\n11 30 12 21 03\\nhh h h h\\n \\n(11-44)\\n \\nfh h h h h h h h\\n7 2 10 3 3 01 2 3 01 2\\n2\\n21 03\\n2\\n33\\n=\\n()\\n+\\n()\\n+\\n()\\n⎡\\n⎣\\n+\\n()\\n⎤\\n⎦\\n––\\n \\n+\\n()\\n+\\n()\\n+\\n()\\n+\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n33\\n12 30 21 03 30 12\\n2\\n21 03\\n2\\nhhhh hh hh\\n––\\n \\n(11-45)\\nThis set of moments is invariant to translation, scale change, mirroring (within a \\nminus sign),\\n and rotation. We can attach physical meaning to some of the low-order \\nmoment invariants. For example, \\nf\\n1\\n is the sum of two second moments with respect \\nto the principal axes of data spread, so this moment can be interpreted as a mea-\\nsure of data spread. Similarly, \\nf\\n3\\n is the difference of second moments, and may be \\ninterpreted as a measure of “slendernes\\ns.” \\nHowever, as the order of the moment \\ninvariants increases, the complexity of their formulation causes physical meaning to \\nbe lost. The importance of Eqs. (11-39) through (11-45) is their invariance, not their \\nphysical meaning.\\nEXAMPLE 11.15 :  Moment invariants.\\nThe objective of this example is to compute and compare the preceding moment invariants using the \\nimage in Fig. 11.37(a). The black (0) border was added to make all images in this example be of the \\nsame size; the zeros do not affect computation of the moment invariants. Figures 11.37(b) through (f) \\nshow the original image translated, scaled by 0.5 in both spatial dimensions, mirrored, rotated by 45°, \\nand rotated by 90°, respectively. Table 11.5 summarizes the values of the seven moment invariants for \\nthese six images. To reduce dynamic range and thus simplify interpretation, the values shown are scaled \\nusing the expression \\n−\\n()\\n()\\nsgn log .\\nff\\nii\\n10\\n The absolute value is needed to handle any numbers that may \\nbe negative. The term \\nsgn\\nf\\ni\\n()\\n preserves the sign of \\nf\\ni\\n,\\n and the minus sign in front is there to handle \\nfractions in the log computation.\\n The idea is to make the numbers easier to interpret. Interest in this \\nexample is on the invariance and relative signs of the moments, not on their actual values. The two key \\npoints in Table 11.5 are: (1) the closeness of the values of the moments, independent of translation, scale \\nchange, mirroring and rotation; and (2) the fact that the \\nsign\\n of \\nf\\n7\\n is different for the mirrored image.\\n11.5  PRINCIPAL COMPONENTS AS FEATURE DESCRIPTORS  \\nThe material in this section is applicable to boundaries and regions. It is different \\nfrom our discussion thus far, in the sense that features are based on more than one \\nimage. Suppose that we are given the three component images of a color image. The \\nthree images can be treated as a unit by expressing each group of three correspond-\\ning pixels as a vector, as discussed in Section 11.1. If we have a total of \\nn\\n registered \\n11.5\\nAs we show in Example \\n11.17, principal compo-\\nnents can be used also \\nto normalize regions or \\nboundaries for variations \\nin size, translation, and \\nrotation.\\nDIP4E_GLOBAL_Print_Ready.indb   859\\n6/16/2017   2:15:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 860}),\n",
       " Document(page_content='860\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.37\\n (a) Original image. (b)–(f) Images translated, scaled by one-half, mirrored, rotated by 45°, and rotated \\nby 90°, respectively.\\nMoment  \\nInvariant\\nOriginal  \\nImage\\nTranslated Half Size Mirrored Rotated 45° Rotated 90°\\nf\\n1\\n2.8662 2.8662 2.8664 2.8662 2.8661 2.8662\\nf\\n2\\n7.1265 7.1265 7.1257 7.1265 7.1266 7.1265\\nf\\n3\\n10.4109 10.4109 10.4047 10.4109 10.4115 10.4109\\nf\\n4\\n10.3742 10.3742 10.3719 10.3742 10.3742 10.3742\\nf\\n5\\n21.3674 21.3674 21.3924 21.3674 21.3663 21.3674\\nf\\n6\\n13.9417 13.9417 13.9383 13.9417 13.9417 13.9417\\nf\\n7\\n−\\n20.7809\\n−\\n20.7809\\n−\\n20.7724\\n20.7809\\n−\\n20.7813\\n−\\n20.7809\\nTABLE \\n11.5\\nMoment invariants for the images in Fig. 11.37.\\nDIP4E_GLOBAL_Print_Ready.indb   860\\n6/16/2017   2:15:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 861}),\n",
       " Document(page_content='11.5\\n  \\nPrincipal Components as Feature Descriptors\\n    \\n861\\nimages, then the corresponding pixels at the same spatial location in all images can \\nbe arranged as an \\nn\\n-dimensional vector:\\n \\nx\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nx\\nx\\nx\\nn\\n1\\n2\\n/vertellipsis\\n \\n(11-46)\\nThroughout this section, the assumption is that all vectors are column vectors (i.e., \\nmatrices of order \\nn\\n×\\n1)\\n.\\n We can write them on a line of text simply by expressing \\nthem as \\nx\\n=\\nxx\\nx\\nn\\nT\\n12\\n,,, ,\\n…\\n()\\n where \\nT\\n indicates the transpose.\\nWe can treat the vectors as random quantities, just like we did when constructing \\nan intensity histogram. The only difference is that, instead of talking about quanti-\\nties like the mean and variance of the random variables, we now talk about \\nmean \\nvectors\\n and \\ncovariance matrices.\\n The mean vector of the population is deﬁned as \\n \\nmx\\nx\\n=\\n{}\\nE\\n \\n(11-47)\\nwhere \\nE\\nx\\n{}\\n is the expected value of \\nx\\n, and the subscript denotes that \\nm\\n is associated \\nwith the population of \\nx\\n vectors. Recall that the expected value of a vector or matrix \\nis obtained by taking the expected value of each element.\\nThe covariance matrix of the vector population is deﬁned as\\n \\nCx m x m\\nxx x\\n=\\n() ()\\n{}\\nE\\nT\\n––\\n \\n(11-48)\\nBecause \\nx\\n is \\nn\\n dimensional,\\n \\nC\\nx\\n is an \\nnn\\n×\\n matrix. Element \\nc\\nii\\n of \\nC\\nx\\n is the variance \\nof \\nx\\ni\\n,\\n the \\ni\\nth component of the \\nx\\n vectors in the population,\\n and element \\nc\\nij\\n of \\nC\\nx\\n \\nis the covariance between elements \\nx\\ni\\n and \\nx\\nj\\n of these vectors. Matrix \\nC\\nx\\n is real \\nand symmetric. If elements \\nx\\ni\\n and \\nx\\nj\\n are uncorrelated, their covariance is zero and, \\ntherefore, \\nc\\nij\\n=\\n0,\\n resulting in a diagonal covariance matrix.\\nBecause \\nC\\nx\\n is real and symmetric, ﬁnding a set of \\nn\\n orthonormal eigenvectors \\nis always possible (Noble and Daniel [1988]). Let \\ne\\ni\\n and \\nl\\ni\\n, \\nin\\n=\\n12\\n,,\\n,,\\n…\\n be the \\neigenvectors and corresponding eigenvalues of \\nC\\nX\\n,\\n†\\n arranged (for convenience) in \\ndescending order so that \\n/H9261/H9261\\njj\\n≥\\n+\\n1\\n for \\njn\\n=−\\n12\\n1\\n,, , .\\n…\\n Let \\nA\\n be a matrix whose \\nro\\nws\\n are formed from the eigenvectors of \\nC\\nX\\n,\\n arranged in descending value of their \\neigenvalues\\n, so that the ﬁrst row of \\nA\\n is the eigenvector corresponding to the largest \\neigenvalue.\\nSuppose that we use \\nA\\n as a transformation matrix to map the \\nx\\n’s into vectors \\ndenoted by \\ny\\n’s, as follows:\\n \\nyA x m\\nx\\n=\\n()\\n–  \\n(11-49)\\nThis expression is called the \\nHotelling transform\\n,\\n which, as you will learn shortly, has \\nsome very interesting and useful properties.\\n†\\n  By deﬁnition, the eigenvector and eigenvalues of an \\nnn\\n×\\n matrix \\nC\\n satisfy the equation \\nC\\nee\\nii i\\n=\\nl\\n. \\nYou may ﬁnd it helpful \\nto review the tutorials on \\nprobability and matrix \\ntheory available on the \\nbook website.\\nThe Hotelling transform \\nis the same as the \\ndiscrete \\nKarhunen-Loève  \\ntransform\\n, so the \\ntwo names are used \\ninterchangeably in the \\nliterature.\\nDIP4E_GLOBAL_Print_Ready.indb   861\\n6/16/2017   2:15:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 862}),\n",
       " Document(page_content='862\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nIt is not difﬁcult to show (see Problem 11.25) that the mean of the \\ny\\n vectors result-\\ning from this transformation is zero; that is,\\n \\nmy 0\\ny\\n=\\n{}\\n=\\nE\\n \\n(11-50)\\nIt follows from basic matrix theory that the covariance matrix of the \\ny\\n’\\ns is given in \\nterms of \\nA\\n and \\nC\\nx\\n by the expression\\n \\nCA C A\\nyx\\n=\\nT\\n \\n(11-51)\\nFurthermore, because of the way \\nA\\n was formed,\\n \\nC\\ny\\n is a diagonal matrix whose ele-\\nments along the main diagonal are the eigenvalues of \\nC\\nx\\n; that is,\\n \\nC\\ny\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n/H9261\\n/H9261\\n/H9261\\n1\\n2\\n0\\n0\\n/downslopeellipsis\\nn\\n \\n(11-52)\\nThe off-diagonal elements of this covariance matrix are 0, so the elements of the \\ny\\n vectors are uncorrelated.\\n Keep in mind that the \\nl\\ni\\n are the eigenvalues of \\nC\\nx\\n and \\nthat the elements along the main diagonal of a diagonal matrix are its eigenvalues \\n(Noble and Daniel [1988]). Thus, \\nC\\nx\\n and \\nC\\ny\\n have the same eigenvalues.\\nAnother important property of the Hotelling transform deals with the reconstruc-\\ntion of \\nx\\n from \\ny\\n. Because the rows of \\nA\\n are orthonormal vectors, it follows that \\nAA\\n–\\n,\\n1\\n=\\nT\\n and any vector \\nx\\n can be recovered from its corresponding \\ny\\n by using the \\nexpression\\n \\nxA y m\\nx\\n=\\nT\\n+\\n \\n(11-53)\\nBut, suppose that, instead of using all the eigenvectors of \\nC\\nx\\n,\\n we form a matrix \\nA\\nk\\n \\nfrom the \\nk\\n eigenvectors corresponding to the \\nk\\n largest eigenvalues, yielding a trans-\\nformation matrix of order \\nkn\\n×\\n.\\n The \\ny\\n vectors would then be \\nk\\n dimensional,\\n and \\nthe reconstruction given in Eq. (11-53) would no longer be exact (this is somewhat \\nanalogous to the procedure we used in Section 11.3 to describe a boundary with a \\nfew Fourier coefficients).\\nThe vector reconstructed by using \\nA\\nk\\n is\\n \\nˆ\\nxA\\ny m\\nx\\n=\\nk\\nT\\n+\\n \\n(11-54)\\nIt can be shown that the mean squared error between \\nx\\n and \\nˆ\\nx\\n is given by the expres-\\nsion\\n \\ne\\nj\\nj\\nn\\nj\\nj\\nk\\nj\\njk\\nn\\nms\\n=−=\\n== = +\\n∑∑∑\\n/H9261/H9261 /H9261\\n11 1\\n \\n(11-55)\\nEquation (11-55) indicates that the error is zero if \\nkn\\n=\\n (that is, if all the eigen-\\nvectors are used in the transformation).\\n Because the \\n/H9261\\nj\\n’s\\n decrease monotonically, \\nDIP4E_GLOBAL_Print_Ready.indb   862\\n6/16/2017   2:15:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 863}),\n",
       " Document(page_content='11.5\\n  \\nPrincipal Components as Feature Descriptors\\n    \\n863\\nEq. (11-55) also shows that the error can be minimized by selecting the \\nk\\n eigenvec-\\ntors associated with the largest eigenvalues. Thus, the Hotelling transform is optimal \\nin the sense that it minimizes the mean squared error between the vectors \\nx\\n and \\ntheir approximations \\nˆ\\nx\\n.\\n Due to this idea of using the eigenvectors corresponding \\nto the largest eigenvalues\\n, the Hotelling transform also is known as the \\nprincipal \\ncomponents transform\\n.\\nEXAMPLE 11.16 :  Using principal components for image description.\\nFigure 11.38 shows six multispectral satellite images corresponding to six spectral bands: visible blue \\n(450–520 nm), visible green (520–600 nm), visible red (630–690 nm), near infrared (760–900 nm), middle \\ninfrared (1550–1,750 nm), and thermal infrared (10,400–12,500 nm). The objective of this example is to \\nillustrate how to use principal components as image features.\\nOrganizing the images as in Fig. 11.39 leads to the formation of a six-element vector \\nx\\n from each set \\nof corresponding pixels in the images, as discussed earlier in this section. The images in this example \\nare of size \\n564 564\\n×\\n pixels, so the population consisted of \\n564 318 096\\n2\\n()\\n=\\n,\\n vectors from which the \\nmean vector\\n, covariance matrix, and corresponding eigenvalues and eigenvectors were computed. The \\nb a\\nc\\ne\\nd\\nf\\n \\nFIGURE 11.38\\n Multispectral images in the (a) visible blue, (b) visible green, (c) visible red, (d) near infrared, (e) middle \\ninfrared, and (f) thermal infrared bands. (Images courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   863\\n6/16/2017   2:15:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 864}),\n",
       " Document(page_content='864\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\neigenvectors were then used as the rows of matrix \\nA\\n, and a set of \\ny\\n vectors were obtained using Eq. \\n(11-49). Similarly, we used Eq. (11-51) to obtain \\nC\\ny\\n.\\n Table 11.6 shows the eigenvalues of this matrix. \\nNote the dominance of the ﬁrst two eigenvalues\\n.\\nA set of principal component images was generated using the \\ny\\n vectors mentioned in the previous \\nparagraph (images are constructed from vectors by applying Fig. 11.39 in reverse). Figure 11.40 shows \\nthe results. Figure 11.40(a) was formed from the ﬁrst component of the 318,096 \\ny\\n vectors, Fig. 11.40(b) \\nfrom the second component of these vectors, and so on, so these images are of the same size as the origi-\\nnal images in Fig. 11.38. The most obvious feature in the principal component images is that a signiﬁcant \\nportion of the contrast detail is contained in the ﬁrst two images, and it decreases rapidly from there. The \\nreason can be explained by looking at the eigenvalues. As Table 11.6 shows, the ﬁrst two eigenvalues are \\nmuch larger than the others. Because the eigenvalues are the variances of the elements of the \\ny\\n vectors, \\nand variance is a measure of intensity contrast, it is not unexpected that the images formed from the \\nvector components corresponding to the largest eigenvalues would exhibit the highest contrast. In fact, \\nthe ﬁrst two images in Fig. 11.40 account for about 89% of the total variance. The other four images have \\nlow contrast detail because they account for only the remaining 11%.\\nAccording to Eqs. (11-54) and (11-55), if we used all the eigenvectors in matrix \\nA\\n we could recon-\\nstruct the original images from the principal component images with zero error between the original \\nand reconstructed images (i.e., the images would be identical). If the objective is to store and/or transmit \\nthe principal component images and the transformation matrix for later reconstruction of the original \\nimages, it would make no sense to store and/or transmit all the principal component images because \\nnothing would be gained. Suppose, however, that we keep and/or transmit only the two principal com-\\nponent images. Then there would be signiﬁcant savings in storage and/or transmission (matrix \\nA\\n would \\nbe of size \\n26\\n×\\n, so its impact would be negligible).\\nF\\nigure 11.41 shows the results of reconstructing the six multispectral images from the two principal \\ncomponent images corresponding to the largest eigenvalues. The ﬁrst ﬁve images are quite close in \\nSpectral band 1\\nSpectral band 2\\nSpectral band 3\\nSpectral band 4\\nSpectral band 5\\nSpectral band 6\\nx\\nx\\n1\\nx\\n2\\nx\\n3\\nx\\n4\\nx\\n5\\nx\\n6\\n/H11005\\nFIGURE 11.39\\nForming of a \\nfeature vector from \\ncorresponding  \\npixels in six images.\\nL\\n1\\nL\\n2\\nL\\n3\\nL\\n4\\nL\\n5\\nL\\n6\\n10344 2966 1401 203\\n94\\n31\\nTABLE \\n11.6\\nEigenvalues of \\nC\\nx\\n \\nobtained from the \\nimages in Fig. 11.38.\\nDIP4E_GLOBAL_Print_Ready.indb   864\\n6/16/2017   2:15:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 865}),\n",
       " Document(page_content='11.5\\n  \\nPrincipal Components as Feature Descriptors\\n    \\n865\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.40\\n The six principal component images obtained from vectors computed using Eq. (11-49). Vectors are \\nconverted to images by applying Fig. 11.39 in reverse.\\nappearance to the originals in Fig. 11.38, but this is not true for the sixth image. The reason is that the \\noriginal sixth image is actually blurry, but the two principal component images used in the reconstruc-\\ntion are sharp, therefore, the blurry “detail” is lost. Figure 11.42 shows the differences between the \\noriginal and reconstructed images. The images in Fig. 11.42 were enhanced to highlight the differences \\nbetween them. If they were shown without enhancement, the ﬁrst ﬁve images would appear almost all \\nblack, with the sixth (difference) image showing the most variability.\\nEXAMPLE 11.17 :  Using principal components for normalizing for variations in size, translation, and rotation.\\nAs we mentioned earlier in this chapter, feature descriptors should be as independent as possible of \\nvariations in size, translation, and rotation. Principal components provide a convenient way to normal-\\nize boundaries and/or regions for variations in these three variables. Consider the object in Fig. 11.43, \\nand assume that its size, location, and orientation (rotation) are arbitrary. The points in the region (or its \\nboundary) may be treated as 2-D vectors, \\nx\\n=\\n()\\nxx\\nT\\n12\\n,,\\n where \\nx\\n1\\n and \\nx\\n2\\n are the coordinates of any object \\npoint. All the points in the region or boundary constitute a 2-D vector population that can be used to \\ncompute the covariance matrix \\nC\\nx\\n and mean vector \\nm\\nx\\n.\\n One eigenvector of \\nC\\nx\\n points in the direction \\nDIP4E_GLOBAL_Print_Ready.indb   865\\n6/16/2017   2:15:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 866}),\n",
       " Document(page_content='866\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nof maximum variance (data spread) of the population, while the second eigenvector is perpendicular to \\nthe ﬁrst, as Fig. 11.43(b) shows. In terms of the present discussion, the principal components transform in \\nEq. (11-49) accomplishes two things: (1) it establishes the center of the transformed coordinates system \\nas the centroid (mean) of the population because \\nm\\nx\\n is subtracted from each \\nx\\n; and (2) the \\ny\\n coordinates \\n(vectors) it generates are rotated versions of the \\nx\\n’s, so that the data align with the eigenvectors. If we \\ndeﬁne a \\nyy\\n12\\n,\\n()\\n axis system so that \\ny\\n1\\n is along the ﬁrst eigenvector and \\ny\\n2\\n is along the second, then the \\ngeometry that results is as illustrated in Fig. 11.43(c). That is, the dominant data directions are aligned \\nwith the new axis system. The same result will be obtained regardless of the size, translation, or rotation \\nof the object, provided that all points in the region or boundary undergo the same transformation. If we \\nwished to size-normalize the transformed data, we would divide the coordinates by the corresponding \\neigenvalues.\\nObserve in Fig. 11.43(c) that the points in the \\ny\\n-axes system can have both positive and negative val-\\nues. To convert all coordinates to positive values, we simply subtract the vector \\nyy\\nT\\n12\\nmin min\\n,\\n()\\n from all \\nthe \\ny\\n vectors. To displace the resulting points so that they are all greater than 0, as in Fig. 11.43(d), we \\nadd to them a vector \\nab\\nT\\n,\\n()\\n where \\na\\n and \\nb\\n are greater than 0.\\nAlthough the preceding discussion is straightforward in principle, the mechanics are a frequent source \\nof confusion. Thus, we conclude this example with a simple manual illustration. Figure 11.44(a) shows \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.41\\n Multispectral images reconstructed using only the two principal component images corresponding to the \\ntwo principal component vectors with the largest eigenvalues. Compare these images with the originals in Fig. 11.38.\\nDIP4E_GLOBAL_Print_Ready.indb   866\\n6/16/2017   2:15:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 867}),\n",
       " Document(page_content='11.5\\n  \\nPrincipal Components as Feature Descriptors\\n    \\n867\\nfour points with coordinates (1, 1), (2, 4), (4, 2), and (5, 5). The mean vector, covariance matrix, and nor-\\nmalized (unit length) eigenvectors of this population are:\\n \\nmC\\nxx\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n3\\n3\\n3 333 2 00\\n2 00 3 333\\n,\\n..\\n..\\nand\\n \\nee\\n12\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n0 707\\n0 707\\n0 707\\n0 707\\n.\\n.\\n,\\n.\\n.\\nThe corresponding eigenvalues are \\n/H9261\\n1\\n5 333\\n=\\n.\\n and \\n/H9261\\n2\\n1 333\\n=\\n..\\n Figure 11.44(b) shows the eigenvec-\\ntors superimposed on the data.\\n From Eq. (11-49), the transformed points (the \\ny\\n’s) are \\n(. , ) ,\\n−\\n2\\n828 0\\nT\\n \\n(, . ),\\n0\\n1 414\\n−\\nT\\n \\n(, . ),\\n0\\n1 414\\nT\\n and \\n(. , ) .\\n2\\n828 0\\nT\\n These points are plotted in Fig. 11.44(c). Note that they are \\naligned with the \\ny\\n-axes and that they have fractional values. When working with images, coordinate \\nvalues are integers, making it necessary to round all values to their nearest integer value. Figure 11.44(d) \\nshows the points rounded to the nearest integer and their location shifted so that all coordinate values \\nare integers greater than 0, as in the original ﬁgure.\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.42\\n Differences between the original and reconstructed images. All images were enhanced by scaling them \\nto the full [0, 255] range to facilitate visual analysis.\\nDIP4E_GLOBAL_Print_Ready.indb   867\\n6/16/2017   2:15:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 868}),\n",
       " Document(page_content='868\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nWhen transforming image pixels, keep in mind that image coordinates are the same as matrix coor-\\ndinates; that is, \\n(,)\\nxy\\n represents \\n(, ) ,\\nrc\\n and the origin is the top left. Axes of the principal components \\njust illustrated are as shown in F\\nigs. 11.43(a) and (d). You need to keep this in mind in interpreting the \\nresults of applying a principal components transformation to objects in an image. \\n11.6  WHOLE-IMAGE FEATURES  \\nThe descriptors introduced in Sections 11.2 through 11.4 are well suited for appli-\\ncations (e.g., industrial inspection), in which \\nindividual\\n regions can be segmented \\nreliably using methods such as the ones discussed in Chapters 10 and 11. With the \\nexception of the application in Example 11.17, the principal components feature \\nvectors in Section 11.5 are different from the earlier material, in the sense that they \\nare based on multiple images. But even these descriptors are localized to sets of \\ncorresponding pixels. In some applications, such as searching image databases for \\nmatches (e.g., as in human face recognition), the variability between images is so \\nextensive that the methods in Chapters 10 and 11 are not applicable. \\n11.6\\nx\\n2\\nx\\n1\\nDirection perpendicular\\nto the direction of max\\nvariance\\nDirection of\\nmax variance\\ne\\n2\\ne\\n1\\ny\\n2\\ny\\n1\\nx\\n2\\nx\\n1\\nCentroid\\ny\\n2\\ny\\n1\\nb a\\nd c\\nFIGURE 11.43\\n(a) An object.  \\n(b) Object show-\\ning eigenvectors \\nof its covariance \\nmatrix.  \\n(c) Transformed \\nobject, obtained  \\nusing Eq. (11-49).  \\n(d) Object  \\ntranslated so that \\nall its coordinate \\nvalues are greater \\nthan 0.\\nDIP4E_GLOBAL_Print_Ready.indb   868\\n6/16/2017   2:15:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 869}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n869\\nThe state of the art in image processing is such that as the complexity of the task \\nincreases, the number of techniques suitable for addressing those tasks decreases. \\nThis is particularly true when dealing with feature descriptors applicable to entire \\nimages that are members of a large family of images. In this section, we discuss \\ntwo of the principal feature detection methods currently being used for this pur-\\npose. One is based on detecting corners, and the other works with entire regions \\nin an image. Then, in Section 11.7 we present a feature detection and description \\napproach designed speciﬁcally to work with these types of features.\\nTHE HARRIS-STEPHENS CORNER DETECTOR\\nIntuitively, we think of a corner as a rapid change of direction in a curve. Corners \\nare highly effective features because they are distinctive and reasonably invariant to \\nviewpoint. Because of these characteristics, corners are used routinely for matching \\nimage features in applications such as tracking for autonomous navigation, stereo \\nmachine vision algorithms, and image database queries.\\nIn this section, we discuss an algorithm for corner detection formulated by Har-\\nris and Stephens [1988]. The idea behind the \\nHarris-Stephens (HS) corner detec-\\ntor\\n is illustrated in Fig. 11.45. The basic approach is this: Corners are detected by \\nrunning a small window over an image, as we did in Chapter 3 for spatial ﬁltering. \\nThe detector window is designed to compute intensity changes. We are interested in \\nthree scenarios: (1) Areas of zero (or small) intensity changes in all directions, which \\nThe discussion in  \\nSections 12.5\\n \\nthrough \\n12.7 dealing with neural \\nnetworks is also impor-\\ntant in terms of process-\\ning large numbers of \\nentire images for the \\npurpose of characterizing \\ntheir content.\\nOur use the term \\n“corner” \\nis broader than just \\n90\\n°\\n corners; it refers to \\nfeatures that are “corner-\\nlike.” \\nx\\n2\\nx\\n1\\n0\\n01234567\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\ny\\n2\\ny\\n1\\n0\\n01234567\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\ny\\n2\\ny\\n1\\n/H11002\\n3\\n/H11002\\n2\\n/H11002\\n1 123\\n3\\n2\\n1\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n3\\nx\\n2\\ne\\n2\\ne\\n1\\nx\\n1\\n0\\n01234567\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nb a\\nd c\\nFIGURE 11.44\\nA manual  \\nexample.  \\n(a) Original points. \\n(b) Eigenvectors of \\nthe covariance  \\nmatrix of the points \\nin (a).  \\n(c) Transformed \\npoints obtained \\nusing Eq. (11-49). \\n(d) Points from (c), \\nrounded and trans-\\nlated so that all \\ncoordinate values \\nare integers greater \\nthan 0. The dashed \\nlines are included \\nto facilitate viewing. \\nThey are not part of \\nthe data.\\nDIP4E_GLOBAL_Print_Ready.indb   869\\n6/16/2017   2:15:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 870}),\n",
       " Document(page_content='870\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nhappens when the window is located in a constant (or nearly constant) region, as \\nin location A in Fig. 11.45; (2) areas of changes in one direction but no (or small) \\nchanges in the orthogonal direction, which this happens when the window spans a \\nboundary between two regions, as in location B; and (3) areas of signiﬁcant changes \\nin all directions, a condition that happens when the window contains a corner (or \\nisolated points), as in location C. The HS corner detector is a mathematical formula-\\ntion that attempts to differentiate between these three conditions.\\nLet \\nf\\n denote an image, and let \\nfs t\\n(,\\n)\\n denote a patch of the image deﬁned by the \\nvalues of \\n(,) .\\nst\\n A patch of the same size, but shifted by \\n(,) ,\\nxy\\n is given by \\nfs x t y\\n(,\\n) .\\n++\\n \\nT\\nhen, the weighted sum of squared differences between the two patches is given by\\n \\nCxy s t fs x t y fs t\\nt s\\n(, ) (,) ( , ) (,)\\n=+ + −\\n[]\\n∑ ∑\\nw\\n2\\n \\n(11-56)\\nwhere \\nw\\n(,\\n)\\nst\\n is a weighting function to be discussed shortly. The shifted patch can be \\napproximated by the linear terms of a \\nTaylor expansion\\n \\nf s xt y f st x f st y f st\\nxy\\n( , ) (,) (,) (,)\\n++ ≈ + +\\n \\n(11-57)\\nwhere \\nfs t f x\\nx\\n(, )\\n=∂ ∂\\n and \\nfs t f y\\ny\\n(, ) ,\\n=∂ ∂\\n both evaluated at \\n(,) .\\nst\\n We can then write \\nEq.\\n (11-56) as\\n \\nCxy s t x f s t y f s t\\nxy\\nt s\\n(, ) (,) (,) (,)\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n∑ ∑\\nw\\n2\\n \\n(11-58)\\nThis equation can written in matrix form as\\n \\nCxy xy\\nx\\ny\\n(,\\n)\\n=\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nM\\n \\n(11-59)\\nA \\npatch\\n is the image area \\nspanned by the detector \\nwindow at any given \\ntime.\\nFIGURE 11.45\\nIllustration of how \\nthe Harris-Stephens \\ncorner detector  \\noperates in the \\nthree types of sub-\\nregions indicated by \\nA (ﬂat), B (edge), \\nand C (corner). The \\nwiggly arrows  \\nindicate graphically \\na directional  \\nresponse in the \\ndetector as it moves \\nin the three areas \\nshown.\\nB\\nC\\nRegion 1\\nRegion 2\\nA\\nBoundary\\nDIP4E_GLOBAL_Print_Ready.indb   870\\n6/16/2017   2:15:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 871}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n871\\nwhere\\n \\nMA\\n=\\n∑ ∑\\nw\\n(,)\\nst\\nt s\\n \\n(11-60)\\nand\\n \\nA\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\nff\\nf\\nff f\\nxx y\\nxy y\\n2\\n2\\n \\n(11-61)\\nMatrix \\nM\\n sometimes is called the \\nHarris matrix\\n.\\n It is understood that its terms are \\nevaluated at \\n(,) .\\nst\\n If \\nw\\n(,\\n)\\nst\\n is isotropic, then \\nM\\n is symmetric because \\nA\\n is\\n. The \\nweighting function \\nw\\n(,\\n)\\nst\\n used in the HS detector generally has one of two forms: \\n(1) it is 1 inside the patch and 0 elsewhere (i.e\\n., it has the shape of a box lowpass filter \\nkernel), or (2) it is an exponential function of the form\\n \\nw\\n(,)\\n()\\nst e\\nst\\n=\\n−+\\n22 2\\n2\\ns\\n \\n(11-62)\\nThe box is used when computational speed is paramount and the noise level is low. \\nT\\nhe exponential form is used when data smoothing is important.\\nAs illustrated in Fig. 11.45, a corner is characterized by large values in region C, \\nin \\nboth\\n spatial directions. However, when the patch spans a boundary there will also \\nbe a response in one direction. The question is: How can we tell the difference? As \\nwe discussed in Section 11.5 (see Example 11.17), the eigenvectors of a real, sym-\\nmetric matrix (such as \\nM\\n above) point in the direction of maximum data spread, \\nand the corresponding eigenvalues are proportional to the amount of data spread in \\nthe direction of the eigenvectors. In fact, the eigenvectors are the major axes of an \\nellipse ﬁtting the data, and the magnitude of the eigenvalues are the distances from \\nthe center of the ellipse to the points where it intersects the major axes. Figure 11.46 \\nillustrates how we can use these properties to differentiate between the three cases \\nin which we are interested.\\nThe small image patches in Figs. 11.46(a) through (c) are representative of regions \\nA, B, and C in Fig. 11.45. In Fig. 11.46(d), we show values of \\n(,)\\nff\\nxy\\n computed using \\nthe derivative kernels \\nw\\ny\\n=−\\n[]\\n101\\n and \\nww\\nxy\\nT\\n=\\n (remember, we use the coordinate \\nsystem deﬁned in F\\nig. 2.19). Because we compute the derivatives at each point in the \\npatch, variations caused by noise result in scattered values, with the spread of the \\nscatter being directly related to the noise level and its properties. As expected, the \\nderivatives from the ﬂat region form a nearly circular cluster, whose eigenvalues are \\nalmost identical, yielding a nearly circular ﬁt to the points (we label these eigenvalues \\nas “small” in relation to the other two plots). Figure 11.46(e) shows the derivatives of \\nthe patch containing the edge. Here, the spread is greater along the \\nx\\n-axis, and about \\nnearly the same as Fig. 11.46 (a) in the \\ny\\n-axis. Thus, eigenvalue \\nl\\nx\\n is “large” while \\nl\\ny\\n is \\n“small.” Consequently, the ellipse ﬁtting the data is elongated in the \\nx\\n-direction. Final-\\nly, Fig. 11.46(f) shows the derivatives of the patch containing the corner. Here, the \\ndata is spread along both directions, resulting in two large eigenvalues and a much \\nlarger and nearly circular ﬁtting ellipse. From this we conclude that: (1) two small \\neigenvalues indicate nearly constant intensity; (2) one small and one large \\neigenvalue \\nAs noted in Chapter 3, we \\ndo not use bold notation \\nfor vectors and matrices \\nrepresenting spatial \\nkernels.\\nDIP4E_GLOBAL_Print_Ready.indb   871\\n6/16/2017   2:15:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 872}),\n",
       " Document(page_content='872\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nimply the presence of a vertical or horizontal boundary; and (3) two large eigenval-\\nues imply the presence of a corner or (unfortunately) isolated bright points.\\nThus, we see that the eigenvalues of the matrix formed from derivatives in the \\nimage patch can be used to differentiate between the three scenarios of interest. \\nHowever, instead of using the eigenvalues (which are expensive to compute), the HS \\ndetector utilizes a measure of corner response based on the fact that the trace of a \\nsquare matrix is equal to the sum of its eigenvalues, and its determinant is equal to \\nthe product of its eigenvalues. The measure is deﬁned as \\n \\nRk\\nk\\nxy x y\\n=− +\\n=−\\nll l l\\n()\\ndet( ) ( )\\n2\\n2\\nMM\\ntrace\\n \\n(11-63)\\nwhere \\nk\\n is a constant to be explained shortly\\n. Measure \\nR\\n has large positive values \\nwhen both eigenvalues are large, indicating the presence of a corner; it has large \\nnegative values when one eigenvalue is large and the other small, indicating an edge; \\nThe eigenvalues of the \\n2 \\n× \\n2 matrix \\nM\\n can be \\nexpressed in a closed \\nform (see Problem 11.31). \\nHowever, their computa-\\ntion requires squares and \\nsquare roots, which are \\nexpensive to process.\\nThe advantage of this for-\\nmulation is that the trace \\nis the sum of the main \\ndiagonal terms of \\nM\\n (just \\ntwo numbers). The deter-\\nminant of a 2 \\n× \\n2 matrix \\nis the product of the main \\ndiagonal elements minus \\nthe product of the cross \\nelements. These are trivial \\ncomputations.\\n/H11002\\n1\\n1\\ny\\nf\\nFlat\\n1\\n1\\n/H11002\\n1\\ny\\nf\\nx\\nf\\n: small\\nx\\nl\\n: small\\ny\\nl\\n/H11002\\n11\\n1\\n/H11002\\n1\\ny\\nf\\nx\\nf\\n: small\\ny\\nl\\n: large\\nx\\nl\\nStraight\\nEdge\\n/H11002\\n1\\n1\\n/H11002\\n1\\nx\\nf\\n: large\\nx\\nl\\n: large\\ny\\nl\\nCorner\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.46\\n (a)–(c) Noisy images and image patches (small squares) encompassing image regions similar in content \\nto those in Fig. 11.45. (d)–(f) Plots of value pairs \\n(,)\\nff\\nxy\\n showing the characteristics of the eigenvalues of \\nM\\n that are \\nuseful for detecting the presence of a corner in an image patch.\\nDIP4E_GLOBAL_Print_Ready.indb   872\\n6/16/2017   2:15:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 873}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n873\\nand its absolute value is small when both eigenvalues are small, indicating that the \\nimage patch under consideration is flat. \\nConstant \\nk\\n is determined empirically, and its range of values depends on the imple-\\nmentation. For example, the MATLAB Image Processing Toolbox uses \\n00 2 5\\n<<\\nk\\n..\\n \\nY\\nou can interpret \\nk\\n as a “sensitivity factor;” the smaller it is, the more likely the detec-\\ntor is to ﬁnd corners. Typically, \\nR\\n is used with a threshold, \\nT\\n.  We say that a corner at \\nan image location has been detected only if \\nRT\\n>\\n for a patch at that location.\\nEXAMPLE 11.18 :  Applying the HS corner detector.\\nFigure 11.47(a) shows a noisy image, and Fig. 11.47(b) is the result of using the HS corner detector \\nwith \\nk\\n=\\n00\\n4\\n.\\n and \\nT\\n=\\n00\\n1\\n.\\n (the default values in our implementation). All corners of the squares were \\ndetected correctly\\n, but the number of false detections is too high (note that all errors occurred on the \\nright side of the image, where the difference in intensity between squares is less). Figure 11.47(c) shows \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.47\\n (a) A \\n600 600\\n×\\n image with values in the range \\n[,] ,\\n01\\n corrupted by additive Gaussian noise with 0 mean \\nand variance of 0.006.\\n (b) Result of applying the HS corner detector with \\nk\\n=\\n00\\n4\\n.\\n and \\nT\\n=\\n00\\n1\\n.\\n (the defaults). Sev-\\neral errors are visible\\n. (c) Result using \\nk\\n=\\n01\\n.\\n and \\nT\\n=\\n00\\n1\\n..\\n (d) Result using \\nk\\n=\\n01\\n.\\n and \\nT\\n=\\n01\\n..\\n (e) Result using \\nk\\n=\\n00\\n4\\n.  and \\nT\\n=\\n01\\n..\\n (f) Result using \\nk\\n=\\n00\\n4\\n.  and \\nT\\n=\\n03\\n.  (only the strongest corners on the left were detected).\\nDIP4E_GLOBAL_Print_Ready.indb   873\\n6/16/2017   2:16:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 874}),\n",
       " Document(page_content='874\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nthe result obtained by increasing \\nk\\n to 0.1 and leaving \\nT\\n at 0.01. This time, all corners were detected cor-\\nrectly. As Fig. 11.47(d) shows, increasing the threshold to \\nT\\n=\\n01\\n.\\n yielded the same result. In fact, using \\nthe default value of \\nk\\n and leaving \\nT\\n at 0.1 also produced the same result,\\n as Fig. 11.47(e) shows. The \\npoint of all this is that there is considerable ﬂexibility in the interplay between the values of \\nk\\n and \\nT\\n. \\nFigure 11.47(f) shows the result obtained using the default value for \\nk\\n and using \\nT\\n=\\n03\\n..\\n As expected, \\nincreasing the value of the threshold eliminated some corners\\n, yielding in this case only the corner of \\nthe squares with larger intensity differences. Increasing the value of \\nk\\n to 0.1 and setting \\nT\\n to its default \\nvalue yielded the same result, as did using \\nk\\n=\\n01\\n.\\n and \\nT\\n=\\n03\\n.,\\n demonstrating again the ﬂexibility in the \\nvalues chosen for these two parameters\\n. However, as the level of noise increases, the range of possible \\nvalues becomes narrower, as the results in the next paragraph illustrate.\\nFigure 11.48(a) shows the checkerboard corrupted by a much higher level of additive Gaussian noise \\n(see the ﬁgure caption). Although this image does not appear much different than Fig. 11.47(a), the \\nresults using the default values of \\nk\\n and \\nT\\n are much worse than before. False corners were detected even \\non the left side of the image, where the intensity differences are much stronger. Figure 11.48(c) is the \\nresult of increasing \\nk\\n near the maximum value in our implementation (2.5) while keeping \\nT\\n at its default \\nvalue. This time, \\nk\\n alone could not overcome the higher noise level. On the other hand, decreasing \\nk\\n to \\nits default value and increasing \\nT\\n to 0.15 produced a perfect result, as Fig. 11.48(d) shows.\\nFigure 11.49(a) shows a more complex image with a signiﬁcant number of corners embedded in \\nvarious ranges of intensities. Figure 11.49(b) is the result obtained using the default values for \\nk\\n and \\nT\\n. \\nb a\\nd c\\nFIGURE 11.48\\n(a) Same as Fig. \\n11.47(a), but  \\ncorrupted with \\nGaussian noise of \\nmean 0 and  \\nvariance 0.01.  \\n(b) Result of using \\nthe HS detector \\nwith \\nk\\n=\\n00\\n4\\n.  and \\nT\\n=\\n00\\n1\\n.  [compare \\nwith F\\nig. 11.47(b)]. \\n(c) Result with \\nk\\n=\\n0\\n249\\n.,\\n (near \\nthe highest value \\nin our implementa-\\ntion),\\n and \\nT\\n=\\n00\\n1\\n..\\n  \\n(d) Result of using \\nk\\n=\\n00\\n4\\n.  and \\nT\\n=\\n01\\n5\\n..\\nDIP4E_GLOBAL_Print_Ready.indb   874\\n6/16/2017   2:16:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 875}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n875\\nAs you can see, numerous detection errors occurred (see, for example, the large number of wrong corner \\ndetections in the right edge of the building). Increasing \\nk\\n alone had little effect on the over-detection \\nof corners until \\nk\\n was near its maximum value. Using the same values as in Fig. 11.48(c) resulted in the \\nimage in 11.49(c), which shows a reduced number of erroneous corners, at the expense of missing numer-\\nous important ones in the front of the building. Reducing \\nk\\n to 0.17 and increasing \\nT\\n to 0.05 did a much \\nbetter job\\n, as Fig. 11.49(d) show. Parameter \\nk\\n did not play a major role in corner detection for the building \\nimage. In fact, Figs. 11.49(e) and (f) show essentially the same level of performance obtained by reducing \\nk\\n to its default value of 0.04, and using \\nT\\n=\\n00\\n5\\n.\\n and \\nT\\n=\\n00\\n7\\n.,\\n respectively.\\nF\\ninally, Fig. 11.50 shows corner detection on a rotated image. The result in Fig. 11.50(b) was obtained \\nusing the same parameters we used in Fig. 11.49(f), showing the relative insensitivity of the method to \\nrotation. Figures 11.49(f) and 11.50(b) show detection of at least one corner in every major structural \\nfeature of the image, such as the front door, all the windows, and the corners that deﬁne the apex of the \\nfacade. For matching purposes, these are excellent results.\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 11.49\\n 600 600\\n×\\n image of a building. (b) Result of applying the HS corner detector with \\nk\\n=\\n00\\n4\\n.\\n and \\nT\\n=\\n00\\n1\\n.  \\n(the default values in our implementation).\\n Numerous irrelevant corners were detected. (c) Result using \\nk\\n=\\n0\\n249 .\\n \\nand the default value for \\nT\\n.\\n (d) Result using \\nk\\n=\\n01\\n7\\n.\\n and \\nT\\n=\\n00\\n5\\n..\\n (e) Result using the default value for \\nk\\n and \\nT\\n=\\n00\\n5\\n..\\n (f) Result using the default value of \\nk\\n and \\nT\\n=\\n00\\n7\\n..\\n \\nDIP4E_GLOBAL_Print_Ready.indb   875\\n6/16/2017   2:16:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 876}),\n",
       " Document(page_content='876\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nb a\\nFIGURE 11.50\\n(a) Image  \\nrotated 5°.  \\n(b) Corners \\ndetected using the \\nparameters used \\nto obtain  \\nFig. 11.49(f). \\nMAXIMALLY STABLE EXTREMAL REGIONS (MSERs)\\nThe Harris-Stephens corner detector discussed in the previous section is useful in \\napplications characterized by sharp transitions of intensities, such as the intersec-\\ntion of straight edges, that result in corner-like features in an image. Conversely, the \\nmaximally stable extremal regions (MSERs) introduced by Matas et al. [2002] are \\nmore “blob” oriented. As with the HS corner detector, MSERs are intended to yield \\nwhole image features for the purpose of establishing correspondence between two \\nor more images.\\nWe know from Fig. 2.18 that a grayscale image can be viewed as a topographic \\nmap, with the \\nxy\\n-axes representing spatial coordinates, and the \\nz\\n-axis representing \\nintensities. Imagine that we start thresholding an 8-bit grayscale image one intensity \\nlevel at a time. The result of each thresholding is a binary image in which we show \\nthe pixels at or above the threshold in white, and the pixels below the threshold as \\nblack. When the threshold, \\nT\\n, is 0, the result is a white image (all pixel values are \\nat or above 0). As we start increasing \\nT\\n in increments of one intensity level, we will \\nbegin to see black components in the resulting binary images. These correspond to \\nlocal minima in the topographic map view of the image. These black regions may \\nbegin to grow and merge, but they never get smaller from image to image. Finally, \\nwhen we reach \\nT\\n=\\n255\\n,\\n the resulting image will be black (there are no pixel values \\nabove this level).\\n Because each stage of thresholding results in a binary image, there \\nwill be one or more connected components of white pixels in each image. The set of \\nall such components resulting from all thresholdings is the set of \\nextremal regions\\n. \\nExtremal regions that do not change size (number of pixels) appreciably over a \\nrange of threshold values are called \\nmaximally stable extremal regions\\n.\\nAs you will see shortly, the procedure just discussed can be cast in the form of a \\nrooted, connected tree called a \\ncomponent tree\\n, where each level of the tree corre-\\nsponds to a value of the threshold discussed in the previous paragraph. Each node \\nof this tree represents an extremal region, \\nR\\n, deﬁned as\\n \\n∀∈ ∀∈\\npR\\nq RI p I q\\n and boundary( ) : ( ) ( )\\n>\\n \\n(11-64)\\nRemember, \\n∀  \\nmeans “for any,” \\n∈ \\nmeans “belonging to,” \\nand a colon, :,  \\nis used to  \\nmean “it is true that.”\\nDIP4E_GLOBAL_Print_Ready.indb   876\\n6/16/2017   2:16:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 877}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n877\\nwhere \\nI\\n is the image under consideration, and \\np\\n and \\nq\\n are image points. This equa-\\ntion indicates that an extremal region \\nR\\n is a region of \\nI\\n, with the property that the \\nintensity of any point in the region is higher than the intensity at any point in the \\nboundary of the region. As usual, we assume that image intensities are integers, \\nordered from 0 (black) to the maximum intensity (e.g., 255 for 8-bit images), which \\nare represented by white.\\nMSERs are found by analyzing the nodes of the component tree. For each con-\\nnected region in the tree, we compute a stability measure, \\nc\\n, deﬁned as\\n \\nc\\n()\\n()\\n()\\nR\\nRR\\nR\\nj\\nTn T\\ni\\nTn T\\nk\\nTn T\\nj\\nTn T\\n+\\n+−\\n++\\n+\\n=\\n−\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n1\\n1\\n \\n(11-65)\\nwhere \\nR\\n is the size of the area (number of pixels) of connected region \\nR\\n, \\nT\\n is a \\nthreshold value in the range \\nTI I\\n∈\\n[min\\n( ), max( )],\\n and \\n/H9004\\nT\\n is a specified thresh-\\nold increment.\\n Regions \\nR\\ni\\nTn T\\n+−\\n()\\n,\\n1\\n/H9004\\n \\nR\\nj\\nTn T\\n+\\n/H9004\\n,\\n and \\nR\\nk\\nTn T\\n++\\n()\\n1\\n/H9004\\n are connected regions \\nobtained at threshold levels \\nTn T\\n+−\\n()\\n,\\n1\\n/H9004\\n \\nTn T\\n+\\n/H9004\\n,\\n and \\nTn T\\n++\\n()\\n,\\n1\\n/H9004\\n respectively. \\nIn terms of the component tree\\n, regions \\nR\\ni\\n and \\nR\\nk\\n are respectively the \\nparent\\n and \\nchild\\n of region \\nR\\nj\\n.\\n Because \\nTn TTn T\\n+−\\n<++\\n() () ,\\n11\\n/H9004/H9004\\n we are guaranteed that \\n|| || .\\n()\\n()\\nRR\\ni\\nTn T\\nk\\nTn T\\n+−\\n++\\n1\\n1\\n/H9004\\n/H9004\\n≥\\n It then follows from Eq. (11-65) that \\nc\\n≥\\n0.\\n MSREs \\nare the regions corresponding to the nodes in the tree that have a stability value \\nthat is a \\nlocal minimum\\n along the path of the tree containing that region.\\n What this \\nmeans in practice is that maximally stable regions are regions whose sizes do not \\nchange appreciably across two, \\n2\\n/H9004\\nT\\n neighboring thresholded images.\\nF\\nigure 11.51 illustrates the concepts just introduced. The grayscale image at the \\ntop consists of some simple regions of constant intensity, with values in the range \\n[, ] .\\n0\\n255\\n Based on the explanation of Eqs. (11-64) and (11-65), we used the threshold \\nT\\n=\\n10\\n,\\n which is in the range \\nmin( ) , max( ) .\\nII\\n==\\n[]\\n5 225\\n Choosing \\n/H9004\\nT\\n=\\n50\\n segmen-\\nted all the different regions of the image\\n. The column of binary images on the left con-\\ntains the results \\nof thresholding the grayscale image with the threshold values shown. \\nThe resulting component tree is on the right. Note that the tree is shown “root up,” \\nwhich is the way you would normally program it.\\nAll the squares in the grayscale image are of the same size (area); therefore, \\nregardless of the image size, we can normalize the size of each square to 1. For exam-\\nple, if the image is of size \\n400 400\\n×\\n pixels, the size of each square is \\n100 100 10\\n4\\n×=\\n \\npixels\\n. Normalizing the size to 1 means that size 1 corresponds to \\n10\\n4\\n pixels (one \\nsquare), size 2 corresponds to \\n21 0\\n4\\n×\\n pixels (two squares), and so forth. You can \\narrive at the same conclusion by noticing that the ratio in Eq.\\n (11-65) eliminates the \\ncommon 10\\n4\\n factor.\\nThe component tree in Fig. 11.51 is a good summary of how the MSER algorithm \\nworks. The ﬁrst level is the result of thresholding \\nI\\n with \\nTT\\n+=\\n/H9004\\n60.\\n There is only \\none connected component (white pixels) in the thresholded image on the left.\\n The \\nsize of the connected component is 11 normalized units. As mentioned above, each \\nnode of a component tree, denoted by a subscripted \\nR\\n, contains \\none\\n connected \\ncomponent consisting of white pixels. The next level in the tree is formed from the \\nDIP4E_GLOBAL_Print_Ready.indb   877\\n6/16/2017   2:16:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 878}),\n",
       " Document(page_content='878\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nregions in the binary image obtained by thresholding \\nI\\n using \\nTT\\n+=\\n2\\n110\\n/H9004\\n.\\n As you \\ncan see on the left,\\n this image has three connected components, so we create three \\nnodes in the component tree at the level of the thresholded image. Similarly, the \\nbinary image obtained by thresholding \\nI\\n with \\nTT\\n+=\\n3\\n160\\n/H9004\\n has two connected \\n225\\n225\\n225\\n225 225\\n175\\n125\\n125\\n55\\n5\\n5\\n5\\n90\\n90\\n90\\nRegion \\nR\\n1\\nRegion \\nR\\n8\\nRegion \\nR\\n7\\nRegion \\nR\\n6\\nRegion \\nR\\n5\\nRegion \\nR\\n4\\nRegion \\nR\\n3\\nRegion \\nR\\n2\\nArea = 11 \\nArea = 2 \\nArea = 3 \\nArea = 1 \\nArea = 3 \\nArea = 3 \\nArea = 3 \\nArea = 1\\n3\\n=\\nc\\n \\n83\\n=\\nc\\n0\\n=\\nc\\n1\\n=\\nc\\n60\\nTT\\n+=\\n/H9004\\n2 110\\nTT\\n+=\\n/H9004\\n3 160\\nTT\\n+=\\n/H9004\\n4 210\\nTT\\n+=\\n/H9004\\nFIGURE 11.51\\n Detecting MSERs. Top: Grayscale image. Left: Thresholded images using \\nT\\n=\\n10\\n and \\n/H9004\\nT\\n=\\n50.\\n Right: \\nComponent tree\\n, showing the individual regions. Only one MSER was detected (see dashed tree node on the \\nrightmost branch of the tree). Each level of the tree is formed from the thresholded image on the left, at that same \\nlevel. Each node of the tree contains one extremal region (connected component) shown in \\nwhite,\\n and denoted by \\na subscripted \\nR\\n. \\nDIP4E_GLOBAL_Print_Ready.indb   878\\n6/16/2017   2:16:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 879}),\n",
       " Document(page_content='11.6\\n  \\nWhole-Image Features\\n    \\n879\\ncomponents, so we create two nodes in the tree at this level. These two connected \\ncomponents are children of the connected components in the previous level, so we \\nplace the new nodes in the same path as their respective parents. The next level of \\nthe tree is explained in the same manner.  Note that the center node in the previous \\nlevel had no children, so that path of the tree ends in the second level. \\nBecause we need to check size variations between parent and child regions to deter-\\nmine stability, only the two middle regions (corresponding to threshold values of 110 \\nand 160) are relevant in this example. As you can see in our component tree, only \\nR\\n6\\n \\nhas a parent and child of similar size (the sizes are identical in this case). Therefore, \\nregion \\nR\\n6\\n is the only MSER detected in this case. Observe that if we had used a single \\nglobal threshold to detect the brightest regions, region \\nR\\n7\\n would have been detected \\nalso (an undesirable result in this context). Thus, we see that although MSERs are \\nbased on intensity, they also depend on the nature of the background surrounding a \\nregion. In this case, \\nR\\n6\\n was surrounded by a darker background than \\nR\\n7\\n,\\n and the darker \\nbackground was thresholded earlier in the tree\\n, allowing the size of \\nR\\n6\\n to remain con-\\nstant over the two, \\n2\\n/H9004\\nT\\n neighboring range required for detection as an MSER.\\nIn our example\\n, it was easy to detect an MSER as the only region that did not \\nchange size, which gave a stability factor 0. A value of zero automatically implies \\nthat an MSER has been found because the parent and child regions are of the \\nsame size. When working with more complex images, the values of stability fac-\\ntors seldom are zero because of variations in intensity caused by variables such \\nas illumination, viewpoint, and noise. The concept of a local minimum mentioned \\nearlier is simply a way of saying that MSERs are extremal regions that do change \\nsize signiﬁcantly over a \\n2\\n/H9004\\nT\\n thresholding range. What is considered a “signiﬁcant” \\nchange depends on the application.\\nIt is not unusual for numerous MSERs to be detected,\\n many of which may not be \\nmeaningful because of their size. One way to control the number of regions detected \\nis by the choice of \\n/H9004\\nT\\n.\\n Another is to label as insigniﬁcant any region whose size is \\nnot in a speciﬁed size range\\n. We illustrate this in Example 11.19.\\nMatas et al. [2002] indicate that MSERs are afﬁne-covariant (see Section 11.1). \\nThis follows directly from the fact that area ratios are preserved under afﬁne trans-\\nformations, which in turn implies that for an afﬁne transformation the original and \\ntransformed regions are related by that transformation. We illustrate this property \\nin Figs. 11.54 and 11.55.\\nFinally, keep in mind that the preceding MSER formulation is designed to detect \\nbright regions with darker surroundings. The same formulation applied to the nega-\\ntive (in the sense deﬁned in Section 3.2) of an image will detect dark regions with \\nlighter surroundings. If interest lies in detecting both types of regions simultaneously, \\nwe form the union of both sets of MSERs.\\nEXAMPLE 11.19 :  Extracting MSERs from grayscale images.\\nFigure 11.52(a) shows a slice image from a CT scan of a human head, and Fig. 11.52(b) shows the result \\nof smoothing Fig. 11.52(a) with a box kernel of size \\n15 15\\n×\\n elements. Smoothing is used routinely as a \\nDIP4E_GLOBAL_Print_Ready.indb   879\\n6/16/2017   2:16:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 880}),\n",
       " Document(page_content='880\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\npreprocessing step when \\n/H9004\\nT\\n is relatively small. In this case, we used \\nT\\n=\\n0\\n and \\n/H9004\\nT\\n=\\n10.\\n This increment \\nwas small enough to require smoothing for proper MSER detection.\\n In addition, we used a “size ﬁlter,” \\nin the sense that the size (area) of an MSER had to be between 10,262 and 34,200 pixels; these size limits \\nare 3% and 10% of the size of the image, respectively. \\nFigure 11.53 illustrates MSER detection on a more complex image. We used less blurring (a \\n55\\n×\\n box \\nkernel) in this image because is has more ﬁne detail.\\n We used the same \\nT\\n and \\n/H9004\\nT\\n as in Fig. 11.52, and \\na valid MSER size in the range 10,000 to 30,000 pixels\\n, corresponding approximately to 3% and 8% of \\nimage size, respectively. Two MSERs were detected using these parameters, as Figs. 11.53(c) and (d) \\nshow. The composite MSER, shown in Fig. 11.53(e), is a good representation of the front of the building.\\nFigure 11.54 shows the behavior under rotation of the MSERs detected in Fig. 11.53. Figure 11.54(a) \\nis the building image rotated 5° in the conterclockwise direction. The image was cropped after rota-\\ntion to eliminate the resulting black areas (see Fig. 2.41), which would change the nature of the image \\ndata and thus inﬂuence the results. Figure 11.54(b) is the result of performing the same smoothing as \\nin Fig. 11.53, and Fig. 11.54(c) is the composite MSER detected using the same parameters as in Fig. \\n11.53(e). As you can see, the composite MSER of the rotated image corresponds quite closely to the \\nMSER in Fig. 11.53(e).\\nFinally, Fig. 11.55 shows the behavior of the MSER detector under scale changes. Figure 11.55(a) is the \\nbuilding image scale to 0.5 of its original dimensions, and Fig. 11.55(b) shows the image smoothed with \\na correspondingly smaller box kernel of size \\n33\\n×\\n.\\n Because the image area is now one-fourth the size \\nb a\\nd c\\nFIGURE 11.52\\n(a) 600 570\\n×\\n CT \\nslice of a human \\nhead.\\n (b) Image \\nsmoothed with a \\nbox kernel of size \\n15 15\\n×\\n elements. (c) \\nA extremal region \\nalong the path of the \\ntree containing one \\nMSER.\\n  \\n(d) The MSER.  \\n(All MSER regions \\nwere limited to the \\nrange 10,260 – 34,200  \\npixels, correspond-\\ning to a range \\nbetween 3% \\nand 10% of image \\nsize.)  \\n(Original image \\ncourtesy of Dr. \\nDavid R.  \\nPickens, Vanderbilt \\nUniversity.)\\nDIP4E_GLOBAL_Print_Ready.indb   880\\n6/16/2017   2:16:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 881}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n881\\nof the original area, we reduced the valid MSER range by one-fourth to 2500 –7500 pixels. Other than \\nthese changes, we used the same parameters as in Fig. 11.53. Figure 11.55(c) shows the resulting MSER. \\nAs you can see, this ﬁgure is quite close to the full-size result in Fig. 11.53(e).\\n11.7  SCALE-INVARIANT FEATURE TRANSFORM (SIFT) \\nSIFT is an algorithm developed by Lowe [2004] for extracting invariant features from \\nan image. It is called a \\ntransform\\n because it transforms image data into scale-invariant \\ncoordinates relative to local image features. SIFT is by far the most complex feature \\ndetection and description approach we discuss in this chapter. \\nAs you progress though this section, you will notice the use of a signiﬁcant num-\\nber of experimentally determined parameters. Thus, unlike most of the formulations \\nof individual approaches we have discussed thus far, SIFT is strongly heuristic. This \\nis a consequence of the fact that our current knowledge is insufﬁcient to tell us how \\n11.7\\nb a\\nc\\ne\\nd\\nFIGURE 11.53\\n (a) Building image of size \\n600 600\\n×\\n pixels. (b) Image smoothed using a \\n55\\n×\\n box kernel. (c) and \\n(d) MSERs detected using \\nT\\n=\\n0,\\n \\n/H9004\\nT\\n=\\n10\\n,\\n and MSER size range between 10,000 and 30,000 pixels, corresponding \\napproximately to 3% and 8% of the area of the image\\n. (e) Composite image.\\nDIP4E_GLOBAL_Print_Ready.indb   881\\n6/16/2017   2:16:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 882}),\n",
       " Document(page_content='882\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nto assemble a set of reasonably well-understood individual methods into a “system” \\ncapable of addressing problems that cannot be solved by any single known method \\nacting alone. Thus, we are forced to determine experimentally the interplay between \\nthe various parameters controlling the performance of more complex systems. \\nWhen images are similar in nature (same scale, similar orientation, etc), cor-\\nner detection and MSERs are suitable as whole image features. However, in the \\npresence of variables such as scale changes, rotation, changes in illumination, and \\nchanges in viewpoint, we are forced to use methods like SIFT.\\nSIFT features (called \\nkeypoints\\n) are invariant to image scale and rotation, and \\nare robust across a range of afﬁne distortions, changes in 3-D viewpoint, noise, and \\nchanges of illumination. The input to SIFT is an image. Its output is an \\nn\\n-dimensional \\nfeature vector whose elements are the invariant feature descriptors. We begin our \\ndiscussion by analyzing how scale invariance is achieved by SIFT.\\nb a\\nc\\nFIGURE 11.54\\n (a) Building image rotated 5° counterclockwise. (b) Smoothed image using the same kernel as in \\nFig. 11.53(b). (c) Composite MSER detected using the same parameters we used to obtain Fig. 11.53(e). The MSERs \\nof the original and rotated images are almost identical.\\nb a\\nc\\nFIGURE 11.55\\n (a) Building image reduced to half-size. (b) Image smoothed with a \\n33\\n×\\n box \\nkernel.\\n (c) Composite MSER obtained with the same parameters as Fig. 11.53(e), but using a \\nvalid MSER region size range of 2,500 -–7,500 pixels.\\nDIP4E_GLOBAL_Print_Ready.indb   882\\n6/16/2017   2:16:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 883}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n883\\nSCALE SPACE\\nThe first stage of the SIFT algorithm is to find image locations that are invariant \\nto scale change. This is achieved by searching for stable features across all possible \\nscales, using a function of scale known as \\nscale space\\n, which is a multi-scale rep-\\nresentation suitable for handling image structures at different scales in a consis-\\ntent manner. The idea is to have a formalism for handling the fact that objects in \\nunconstrained scenes will appear in different ways, depending on the scale at which \\nimages are captured. Because these scales may not be known beforehand, a reason-\\nable approach is to work with all relevant scales simultaneously. Scale space repre-\\nsents an image as a one-parameter \\nfamily\\n of smoothed images, with the objective of \\nsimulating the loss of detail that would occur as the scale of an image decreases. The \\nparameter controlling the smoothing is referred to as the \\nscale parameter\\n.\\nIn SIFT, Gaussian kernels are used to implement smoothing, so the scale param-\\neter is the standard deviation. The reason for using Gaussian kernels in based on \\nwork performed by Lindberg [1994], who showed that the only smoothing kernel \\nthat meets a set of important constraints, such as linearity and shift-invariance, is \\nthe Gaussian lowpass kernel. Based on this, the scale space, \\nLxy\\n(,\\n, ) ,\\ns\\n of a grayscale \\nimage\\n, \\nfx y\\n(,\\n) ,\\n†\\n is produced by convolving \\nf\\n with a variable-scale Gaussian kernel, \\nGxy\\n(,\\n, ) :\\ns\\n \\nLxy Gxy\\nfx y\\n(,, ) (,, )\\n(, )\\nss\\n=\\n/H22841\\n \\n(11-66)\\nwhere the scale is controlled by parameter \\ns\\n, and \\nG\\n is of the form\\n \\nGxy e\\nxy\\n(,, )\\n()\\ns\\nps\\ns\\n=\\n−+\\n1\\n2\\n2\\n2\\n22 2\\n \\n(11-67)\\nThe input image \\nfx y\\n(,\\n)\\n is successively convolved with Gaussian kernels having \\nstandard deviations \\nss s s\\n,\\n, , ,...\\nkk k\\n23\\n to generate a “stack” of Gaussian-filtered \\n(smoothed) images that are separated by a constant factor \\nk\\n, as shown in the lower \\nleft of Fig. 11.56.\\nSIFT subdivides scale space into \\noctaves\\n, with each octave corresponding to a \\ndoubling of \\ns\\n,\\n just as an octave in music theory corresponds to doubling the fre-\\nquenc\\ny of a sound signal. SIFT further subdivides each octave into an integer num-\\nber, \\ns\\n,\\n of intervals, so that an interval of 1 consists of two images, an interval of 2 \\nconsists of three images\\n, and so forth. It then follows that the value used in the Gauss-\\nian kernel that generates the image corresponding to an octave is \\nk\\ns\\nss\\n=\\n2\\n which \\nmeans that \\nk\\ns\\n=\\n2\\n1\\n.\\n For example, for \\ns\\n=\\n2,\\n \\nk\\n=\\n2,\\n and the input image is succes-\\nsively smoothed using standard deviations of \\nss s\\n,( ) , ,\\n22\\n2\\nand ( )\\n so that the third \\nimage (i.e\\n., the octave image for \\ns\\n=\\n2)\\n in the sequence is ﬁltered using a Gaussian \\nkernel with standard deviation \\n()\\n22\\n2\\nss\\n=\\n. \\n†\\n  Experimental results reported by Lowe [2004] suggest that smoothing the original image using a Gaussian \\nkernel with \\ns\\n=\\n05\\n.\\n and then doubling its size by linear (nearest-neighbor) interpolation improves the number \\nof stable features detected by SIFT\\n. This preprocessing step is an integral part of the algorithm. Images are \\nassumed to have values in the range \\n[,] .\\n01\\nAs in Chapter 3, “\\n/H22841\\n” \\nindicates spatial convolu-\\ntion.\\nDIP4E_GLOBAL_Print_Ready.indb   883\\n6/16/2017   2:16:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 884}),\n",
       " Document(page_content='884\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nThe preceding discussion indicates that the number of smoothed images gener-\\nated in an octave is \\ns\\n+\\n1.\\n However, as you will see in the next section, the smoothed \\nimages in scale space are used to compute differences of Gaussians [see Eq.\\n (10-32)] \\nwhich, in order to cover a full octave, implies that an additional two images past the \\noctave image are required, giving a total of \\ns\\n+\\n3\\n images. Because the octave image is \\nalways the \\n()\\ns\\n+\\n1t\\nh\\n image in the stack (counting from the bottom), it follows that this \\nimage is the third image from the top in the expanded sequence of \\ns\\n+\\n3\\n images. Each \\noctave in F\\nig. 11.56 contains ﬁve images, indicating that \\ns\\n=\\n2\\n was used in this case.\\nT\\nhe \\nﬁrst\\n image in the \\nsecond\\n octave is formed by downsampling the original \\nimage (by skipping every other row and column), and then smoothing it using a \\nkernel with twice the standard deviation used in the ﬁrst octave (i.e., \\nss\\n21\\n2\\n=\\n)\\n. \\nSubsequent images in that octave are smoothed using \\ns\\n2\\n,\\n with the same sequence \\nof values of \\nk\\n as in the ﬁrst octave (this is denoted by dots in F\\nig. 11.56). The same \\nbasic procedure is then repeated for subsequent octaves. That is, the \\nﬁrst \\nimage of \\nthe \\nnew\\n octave is formed by: (1) downsampling the original image enough times \\nto achieve half the size of the image in the previous octave, and (2) smoothing the \\ndownsampled image with a new standard deviation that is twice the standard devia-\\ntion of the previous octave. The rest of the images in the new octave are obtained by \\nsmoothing the downsampled image with the new standard deviation multiplied by \\nthe same sequence of values of \\nk\\n as before. \\nWhen \\nk\\n=\\n2,\\n we can obtain the ﬁrst image of a new octave without having to \\nsmooth the downsampled image\\n. This is because, for this value of \\nk\\n, the kernel used \\nto smooth the ﬁrst image of every octave is the same as the kernel used to smooth \\nInstead of repeatedly \\ndownsampling the \\noriginal image, we can \\ncarry the previously \\ndownsampled image, \\nand downsample it \\nby 2 to obtain the image \\nrequired for the next \\noctave.\\nImages smoothed using\\nGaussian lowpass kernels\\nOctave 1\\nScale\\nScale\\nScale\\nOctave 2\\nOctave 3\\n.\\n.\\n.\\nMore octaves\\n6\\n6\\nStandard deviations used \\nin the Gaussian lowpass\\nkernels of each octave (the\\nsame number of images \\nwith the same powers of \\nk\\n is\\ngenerated in each octave)\\n.\\n.\\n.\\n1\\ns\\n1\\nk\\ns\\n2\\n1\\nk\\ns\\n3\\n1\\nk\\ns\\n4\\n1\\nk\\ns\\n21\\n2\\n=\\nss\\n2\\nk\\ns\\n4\\n2\\nk\\ns\\n.\\n.\\n.\\n2\\n=\\n32\\nss\\n4\\n3\\nk\\ns\\n.\\n.\\n.\\n3\\nk\\ns\\n6\\nFIGURE 11.56\\nScale space,  \\nshowing three \\noctaves. Because \\ns\\n=\\n2  in this case, \\neach octave has ﬁve \\nsmoothed  \\nimages\\n. A  \\nGaussian ker-\\nnel was used for \\nsmoothing, so the \\nspace parameter \\nis \\ns\\n. \\nDIP4E_GLOBAL_Print_Ready.indb   884\\n6/16/2017   2:16:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 885}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n885\\nthe \\nthird\\n image from the top of the previous octave. Thus, the ﬁrst image of a new \\noctave can be obtained directly by downsampling that third image of the previous \\noctave by 2. The result will be the same (see Problem 11.36). The third image from \\nthe top of any octave is called the \\noctave image\\n because the standard deviation used \\nto smooth it is twice (i.e., \\nk\\n2\\n2\\n=\\n)\\n the value of the standard deviation used to smooth \\nthe ﬁrst image in the octave\\n.\\nFigure 11.57 uses grayscale images to further illustrate how scale space is con-\\nstructed in SIFT. Because each octave is composed of ﬁve images, it follows that \\nwe are again using \\ns\\n=\\n2.\\n We chose \\ns\\n1\\n2 2 0 707\\n==\\n.\\n and \\nk\\n==\\n2 1 414 .\\n for this \\nexample so that the numbers would result in familiar multiples\\n. As in Fig. 11.56, the \\nimages going up scale space are blurred by using Gaussian kernels with progressively \\nlarger standard deviations, and the ﬁrst image of the second and subsequent octaves \\nis obtained by downsampling the octave image from the previous octave by 2. As \\nyou can see, the images become signiﬁcantly more blurred (and consequently lose \\nmore ﬁne detail) as they go up both in scale as well as in octave. The images in the \\nthird octave show signiﬁcantly fewer details, but their gross appearance is unmistak-\\nably that of the same structure.\\nDETECTING LOCAL EXTREMA\\nSIFT initially finds the locations of keypoints using the Gaussian filtered images, \\nthen refines the locations and validity of those keypoints using two processing steps.\\nFinding the Initial Keypoints\\nKeypoint locations in scale space are found initially by SIFT by detecting extrema \\nin the difference of Gaussians of two adjacent scale-space images in an octave, con-\\nvolved with the input image that corresponds to that octave. For example, to find \\nkeypoint locations related to the first two levels of octave 1 in scale space, we look \\nfor extrema in the function\\n \\nDxy Gxyk Gxy\\nfx y\\n(,, ) (,, ) (,, )\\n(, )\\nss s\\n=−\\n[]\\n/H22841\\n \\n(11-68)\\nIt follows from Eq. (11-66) that\\n \\nDxy\\nLxyk Lxy\\n(,, ) ) )\\n(,, (,,\\nsss\\n=−\\n  \\n(11-69)\\nIn other words, all we have to do to form function \\nD\\nxy\\n(,, )\\ns\\n is subtract the first two \\nimages of octave 1.\\n Recall from the discussion of the Marr-Hildreth edge detector \\n(Section 10.2) that the difference of Gaussians is an approximation to the Laplacian \\nof a Gaussian (LoG). Therefore, Eq. (11-69) is nothing more than an approximation \\nto Eq. (10-30). The key difference is that SIFT looks for extrema in \\nD\\nxy\\n(,, ) ,\\ns\\n where-\\nas the Marr\\n-Hildreth detector would look for the zero crossings of this function.\\nLindberg [1994] showed that true scale invariance in scale space requires that the \\nLoG be normalized by \\ns\\n2\\n (i.e., that \\ns\\n22\\n/H11612\\nG\\n be used). It can be shown (see Problem \\n11.34) that\\nDIP4E_GLOBAL_Print_Ready.indb   885\\n6/16/2017   2:16:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 886}),\n",
       " Document(page_content='886\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nGxyk Gxy k G\\n(,, ) (,, ) ( )\\nsss\\n−≈ −\\n1\\n22\\n/H11612\\n \\n(11-70)\\nTherefore, DoGs already have the necessary scaling “built in.” The factor \\n()\\nk\\n−\\n1\\n is \\nconstant over all scales\\n, so it does not influence the process of locating extrema in \\nscale space. Although Eqs. (11-68) and (11-69) are applicable to the first two images \\n1\\n2\\n3\\nScale\\n1 2345\\n0.707 1.000 1.414 2.000 2.828\\n1.414 2.000 2.828 4.000 5.657\\n2.828 4.000 5.657 8.000 11.314\\nOctave\\nOctave 1\\nBook Page Writable Area (45p6 by 37p0)\\n2 1.414\\nk\\n==\\nScale\\n1\\n2 2 0.707\\n==\\ns\\n1\\ns\\n1\\nk\\ns\\n2\\n1\\nk\\ns\\n3\\n1\\nk\\ns\\n4\\n1\\nk\\ns\\nOctave 2\\nScale\\n2\\nk\\ns\\n2\\n2\\nk\\ns\\n3\\n2\\nk\\ns\\n4\\n2\\nk\\ns\\nOctave 3\\nScale\\n321\\n24\\n==\\nsss\\n3\\nk\\ns\\n2\\n3\\nk\\ns\\n3\\n3\\nk\\ns\\n4\\n3\\nk\\ns\\n21\\n2\\n=\\nss\\nFIGURE 11.57\\nIllustration using \\nimages of the ﬁrst \\nthree octaves of \\nscale space in \\nSIFT. The entries \\nin the table are \\nvalues of standard \\ndeviation used \\nat each scale of \\neach octave. For \\nexample the  \\nstandard  \\ndeviation used in \\nscale 2 of octave 1 \\nis \\nk\\ns\\n1\\n, which is \\nequal to 1.0.\\n  \\n(The images \\nof octave 1 are \\nshown slightly \\noverlapped to \\nﬁt in the ﬁgure \\nspace.)\\nDIP4E_GLOBAL_Print_Ready.indb   886\\n6/16/2017   2:16:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 887}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n887\\nof octave 1, the same form of these equations is applicable to any two images from \\nany octave, provided that the appropriate downsampled image is used, and the DoG \\nis computed from two adjacent images in the octave.\\nFigure 11.58 illustrates the concepts just discussed, using the building image from \\nFig. 11.57. A total of \\ns\\n+\\n2\\n difference functions, \\nD\\nxy\\n(,, ) ,\\ns\\n are formed in each octave \\nfrom all adjacent pairs of Gaussian-ﬁltered images in that octave\\n. These difference \\nfunctions can be viewed as images, and one sample of such an image is shown for each \\nof the three octaves in Fig. 11.58. As you might expect from the results in Fig. 11.57, \\nthe level of detail in these images decreases the further up we go in scale space.\\nFigure 11.59 shows the procedure used by SIFT to ﬁnd extrema in a \\nD\\nxy\\n(,, )\\ns\\nimage. At each location (shown in black) in a \\nD\\nxy\\n(,, )\\ns\\n image, the value of the pixel \\nat that location is compared to the values of its eight neighbors in the current image \\nand its nine neighbors in the images above and below\\n. The point is selected as an \\nextremum (maximum or minimum) point if its value is larger than the values of all \\nits neighbors, or smaller than all of them. No extrema can be detected in the ﬁrst \\n(last) scale of an octave because it has no lower (upper) scale image of the same size.\\nImproving the Accuracy of Keypoint Locations\\nWhen a continuous function is sampled, its true maximum or minimum may actually \\nbe located between sample points. The usual approach used to get closer to the true \\nOctave 2\\nOctave 3\\nOctave 1\\nScale\\n(,, )\\nDxy\\ns\\n(,, )\\nDxy\\ns\\nSample\\nGaussian-filtered images,\\n(,, )\\nLxy\\ns\\nFIGURE 11.58\\n How Eq. (11-69) is implemented in scale space. There are \\ns\\n+\\n3 \\nLxy\\n(,\\n, )\\ns\\n images and \\ns\\n+\\n2 corre-\\nsponding \\nDxy\\n(,\\n, )\\ns\\n images in each octave.\\nDIP4E_GLOBAL_Print_Ready.indb   887\\n6/16/2017   2:16:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 888}),\n",
       " Document(page_content='888\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nextremum (to achieve subpixel accuracy) is to fit an interpolating function at each \\nextremum point found in the digital function, then look for an improved extremum \\nlocation in the interpolated function. SIFT uses the linear and quadratic terms of \\na Taylor series expansion of \\nD\\nxy\\n(,, ) ,\\ns\\n shifted so that the origin is located at the \\nsample point being examined.\\n In vector form, the expression is\\n \\nDD\\nDD\\nDD\\nT\\nT\\nT\\nT\\n()\\nx\\nx\\nxx\\nxx\\nx\\nxx H x\\n=+\\n∂\\n∂\\n+\\n∂\\n∂\\n∂\\n∂\\n=+\\n()\\n+\\nab ab\\n1\\n2\\n1\\n2\\n/H11612\\n \\n(11-71)\\nwhere \\nD\\n and its derivatives are evaluated at the sample point,\\n \\nx\\n=\\n(,\\n, )\\nxy\\nT\\ns\\n is the \\noffset from that point,\\n \\n/H11612\\n is the familiar gradient operator,\\n \\n/H11612\\nD\\nD\\nDx\\nDy\\nD\\n=\\n∂\\n∂\\n=\\n∂∂\\n∂∂\\n∂∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\ns\\n \\n(11-72)\\nand \\nH\\n is the \\nHessian matrix\\n \\nH\\n=\\n∂∂ ∂∂ ∂ ∂∂ ∂\\n∂∂ ∂ ∂∂ ∂∂ ∂\\n∂∂ ∂ ∂∂ ∂∂\\n22 2 2\\n22 2 2\\n22 2\\nDx Dx y Dx\\nDy x Dy Dy\\nDx Dy\\ns\\ns\\nss\\nD D\\n∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\ns\\n2\\n \\n(11-73)\\nThe location of the extremum, \\nˆ\\n,\\nx\\n is found by taking the derivative of Eq. (11-71) \\nwith respect to \\nx\\n and setting it to zero\\n, which gives us (see Problem 11.37):\\n \\nˆ\\nxH\\n=−\\n()\\n−\\n1\\n/H11612\\nD\\n \\n(11-74)\\nBecause \\nD\\n and its \\nderivatives are evalu-\\nated at the sample point, \\nthey are constants with \\nrespect to \\nx\\n.\\nScale\\n(,, )\\nDxy\\ns\\nCorresponding sections of three \\ncontiguous\\nimages\\nFIGURE 11.59\\nExtrema (maxima \\nor minima) of the \\nDxy\\n(,\\n, )\\ns\\n images \\nin an octave are  \\ndetected by \\ncomparing a pixel \\n(shown in black) \\nto its 26 neighbors \\n(shown shaded) in \\n33\\n×\\n regions at the \\ncurrent and  \\nadjacent scale  \\nimages\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   888\\n6/16/2017   2:16:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 889}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n889\\nThe Hessian and gradient of \\nD\\n are approximated using differences of neighbor-\\ning points, as we did in Section 10.2. The resulting \\n33\\n×\\n system of linear equations \\nis easily solved computationally\\n. If the offset \\nˆ\\nx\\n is greater than 0.5 in any of its three \\ndimensions\\n, we conclude that the extremum lies closer to another sample point, in \\nwhich case the sample point is changed and the interpolation is performed about \\nthat point instead. The ﬁnal offset \\nˆ\\nx\\n is added to the location of its sample point to \\nobtain the interpolated estimate of the location of the extremum.\\nT\\nhe function value at the extremum, \\nD\\n()\\n,\\nx\\n⁄\\n is used by SIFT for rejecting unstable \\nextrema with low contrast, where \\nD\\n()\\nx\\n⁄\\n is obtained by substituting Eq. (11-74) into \\nEq. (11-71), giving (see\\n \\nProblem 11.37):\\n \\nDD D\\nT\\n()\\nxx\\n⁄⁄\\n=+\\n()\\n1\\n2\\n/H11612\\n \\n(11-75)\\nIn the experimental results reported by Lowe [2004], any extrema for which \\nD\\n()\\nx\\n⁄\\n \\nwas less than 0.03 was rejected, based on all image values being in the range \\n[, ] .\\n01\\n \\nT\\nhis eliminates keypoints that have low contrast and/or are poorly localized.\\nEliminating Edge Responses\\nRecall from Section 10.2 that using a difference of Gaussians yields edges in an \\nimage. But keypoints of interest in SIFT are “corner-like” features, which are signifi-\\ncantly more localized. Thus, intensity transitions caused by edges are eliminated. To \\nquantify the difference between edges and corners, we can look at local curvature. \\nAn edge is characterized by high curvature in one direction, and low curvature in the \\northogonal direction. Curvature at a point in an image can be estimated from the \\n22\\n×\\n Hessian matrix evaluated at that point. Thus, to estimate local curvature of the \\nDoG at any level in scalar space\\n, we compute the Hessian matrix of \\nD\\n at that level:\\n \\nH\\n=\\n∂∂ ∂∂ ∂\\n∂∂ ∂∂∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n22 2\\n22 2\\nDx Dx y\\nDy x Dy\\nDD\\nDD\\nxx xy\\nyx yy\\n \\n(11-76)\\nwhere the form on the right uses the same notation as the \\nA\\n term [Eq.\\n (11-61)] of \\nthe Harris matrix (but note that the main diagonals are different). The eigenvalues \\nof \\nH\\n are proportional to the curvatures of \\nD\\n. As we explained in connection with the \\nHarris-Stephens corner detector, we can avoid direct computation of the eigenvalues \\nby formulating tests based on the trace and determinant of \\nH\\n, which are equal to \\nthe sum and product of the eigenvalues, respectively. To use notation different from \\nthe HS discussion, let \\na\\n and \\nb\\n be the eigenvalues of \\nH\\n with the largest and smallest \\nmagnitude\\n, respectively. Using the relationship between the eigenvalues of \\nH\\n and \\nits trace and determinant we have (remember, \\nH\\n is is symmetric and of size \\n22\\n×\\n):\\n \\nTr\\nD\\n()\\net(\\n)\\n( )\\nH\\nH\\n=+= +\\n=− =\\nDD\\nDD D\\nxx yy\\nxx yy xy\\nab\\nab\\n2\\n \\n(11-77)\\nIf you display an image \\nas a topographic map \\n(see Fig. 2.18), edges \\nwill appear as ridges \\nthat have low curvature \\nalong the ridge and high \\ncurvature perpendicular \\nto it.\\nDIP4E_GLOBAL_Print_Ready.indb   889\\n6/16/2017   2:16:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 890}),\n",
       " Document(page_content='890\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nIf the determinant is negative, the curvatures have different signs and the keypoint \\nin question cannot be an extremum, so it is discarded.\\nLet \\nr\\n denote the ratio of the largest to the smallest eigenvalue. Then \\nab\\n=\\nr\\n and\\n \\nTr\\nDet\\n()\\n()\\nH\\nH\\n[]\\n=\\n+\\n()\\n=\\n+\\n()\\n=\\n+\\n()\\n2\\n22\\n2\\n2\\n1\\nab\\nab\\nbb\\nb\\nr\\nr\\nr\\nr\\n \\n(11-78)\\nwhich depends on the ratio of the eigenvalues, rather than their individual values. \\nT\\nhe minimum of \\n()\\nrr\\n+\\n1\\n2\\n occurs when the eigenvalues are equal, and it increases \\nwith \\nr\\n.\\n Therefore, to check that the ratio of principal curvatures is below some \\nthreshold,\\n \\nr\\n, we only need to check\\n \\nTr\\nDet\\n()\\n()\\nH\\nH\\n[]\\n<\\n+\\n()\\n2\\n2\\n1\\nr\\nr\\n \\n(11-79)\\nwhich is a simple computation. In the experimental results reported by Lowe [2004], \\na value of \\nr\\n=\\n10\\n was used, meaning that keypoints with ratios of curvature greater \\nthan 10 were eliminated.\\nF\\nigure 11.60 shows the SIFT keypoints detected in the building image using the \\napproach discussed in this section. Keypoints for which \\nD\\n()\\nx\\n⁄\\n in Eq. (11-75) was less \\nthan 0.03 were rejected, as were keypoints that failed to satisfy Eq. (11-79) with \\nr\\n=\\n10. \\nKEYPOINT ORIENTATION\\nAt this point in the process, we have computed keypoints that SIFT considers stable. \\nBecause we know the location of each keypoint in scale space, we have achieved \\nscale independence\\n. The next step is to assign a consistent orientation to each key-\\npoint based on local image properties. This allows us to represent a keypoint rela-\\ntive to its orientation and thus achieve \\ninvariance to image rotation\\n. SIFT uses a \\nAs with the HS corner \\ndetector, the advantage \\nof this formulation is \\nthat the trace and deter-\\nminants of 2 \\n× \\n2 matrix \\nH\\n are easy to compute. \\nSee the margin note in \\nEq. (11-63).\\nFIGURE 11.60\\nSIFT keypoints \\ndetected in the \\nbuilding image. \\nThe points were \\nenlarged slightly \\nto make them \\neasier to see.\\nDIP4E_GLOBAL_Print_Ready.indb   890\\n6/16/2017   2:16:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 891}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n891\\nstraightforward approach for this. The scale of the keypoint is used to select the \\nGaussian smoothed image, \\nL\\n, that is closest to that scale. In this way, all orienta-\\ntion computations are performed in a scale-invariant manner. Then, for each image \\nsample, \\nLxy\\n(,\\n) ,\\n at this scale, we compute the gradient magnitude, \\nMxy\\n(,\\n) ,\\n and ori-\\nentation angle\\n, \\nu\\n(,\\n) ,\\nxy\\n using pixel differences:\\n          \\nMxy Lx y Lx y Lxy Lxy\\n(,) ( ,) ( ,) (, ) (, )\\n=+ − −\\n()\\n++ − −\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n11\\n11\\n22\\n1\\n2\\n \\n(11-80)\\nand\\n            \\nu\\n(,) t a n (, ) (, ) ( ,) ( ,)\\nxy Lxy Lxy Lx y Lx y\\n=+ − −\\n()\\n+−−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n1\\n11 11\\n \\n(11-81)\\nA histogram of orientations is formed from the gradient orientations of sample \\npoints in a neighborhood of each keypoint.\\n The histogram has 36 bins covering the \\n360\\n°\\n range of orientations on the image plane. Each sample added to the histogram \\nis weighed by its gradient magnitude\\n, and by a circular Gaussian function with a stan-\\ndard deviation 1.5 times the scale of the keypoint.\\nPeaks in the histogram correspond to dominant local directions of local gradients. \\nThe highest peak in the histogram is detected and any other local peak that is within \\n80% of the highest peak is used also to create another keypoint with that orienta-\\ntion. Thus, for the locations with multiple peaks of similar magnitude, there will be \\nmultiple keypoints created at the same location and scale, but with \\ndifferent\\n orienta-\\ntions. SIFT assigns multiple orientations to only about 15% of points with multiple \\norientations, but these contribute signiﬁcant to image matching (to be discussed \\nlater and in Chapter 12). Finally, a parabola is ﬁt to the three histogram values clos-\\nest to each peak to interpolate the peak position for better accuracy.\\nFigure 11.61 shows the same keypoints as Fig. 11.60 superimposed on the image \\nand showing keypoint orientations as arrows. Note the consistency of orientation \\nSee Section 10.2\\n \\nregard-\\ning computation of the \\ngradient magnitude and \\nangle.\\nFIGURE 11.61\\nThe keypoints \\nfrom Fig. 11.60 \\nsuperimposed \\non the original \\nimage. The arrows \\nindicate keypoint \\norientations.\\nDIP4E_GLOBAL_Print_Ready.indb   891\\n6/16/2017   2:16:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 892}),\n",
       " Document(page_content='892\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nof similar sets of keypoints in the image. For example, observe the keypoints on the \\nright, vertical corner of the building. The lengths of the arrows vary, depending on \\nillumination and image content, but their direction is unmistakably consistent. Plots \\nof keypoint orientations generally are quite cluttered and are not intended for gen-\\neral human interpretation. The value of keypoint orientation is in image matching, \\nas we will illustrate later in our discussion.\\nKEYPOINT DESCRIPTORS\\nThe procedures discussed up to this point are used for assigning an image location, \\nscale, and orientation to each keypoint, thus providing invariance to these three \\nvariables. The next step is to compute a descriptor for a local region around each \\nkeypoint that is highly distinctive, but is at the same time as invariant as possible to \\nchanges in scale, orientation, illumination, and image viewpoint. The idea is to be \\nable to use these descriptors to identify matches (similarities) between local regions \\nin two or more images.\\nThe approach used by SIFT to compute descriptors is based on experimental \\nresults suggesting that local image gradients appear to perform a function similar \\nto what human vision does for matching and recognizing 3-D objects from different \\nviewpoints (Lowe [2004]). Figure 11.62 summarizes the procedure used by SIFT \\nto generate the descriptors associated with each keypoint. A region of size \\n16 16\\n×\\n8-directional histogram (the \\nbins are multiples of 45°)\\nKeypoint descriptor = 128-dimensional vector\\n= Keypoint\\n}\\n}\\nGaussian weighting function\\nGradients\\nin 16\\n*16\\nregion\\nFIGURE 11.62\\nApproach used to \\ncompute a  \\nkeypoint  \\ndescriptor.\\nDIP4E_GLOBAL_Print_Ready.indb   892\\n6/16/2017   2:16:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 893}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n893\\npixels is centered on a keypoint, and the gradient magnitude and direction are com-\\nputed at each point in the region using pixel differences. These are shown as ran-\\ndomly oriented arrows in the upper-left of the ﬁgure. A Gaussian weighting function \\nwith standard deviation equal to one-half the size of the region is then used to assign \\na weight that multiplies the magnitude of the gradient at each point. The Gaussian \\nweighting function is shown as a circle in the ﬁgure, but it is understood that it is a \\nbell-shaped surface whose values (weights) decrease as a function of distance from \\nthe center. The purpose of this function is to reduce sudden changes in the descriptor \\nwith small changes in the position of the function.\\nBecause there is one gradient computation for each point in the region surround-\\ning a keypoint, there are \\n()\\n16\\n2\\n gradient directions to process for each keypoint. \\nThere are 16 directions in each \\n44\\n×\\n subregion. The top-rightmost subregion is \\nshown zoomed in the ﬁgure to simplify the explanation of the next step\\n, which \\nconsists of quantizing all gradient orientations in the \\n44\\n×\\n subregion into eight pos-\\nsible directions differing by \\n45\\n°\\n.\\n Rather than assigning a directional value as a full \\ncount to the bin to which it is closest,\\n SIFT performs interpolation that \\ndistributes\\n a \\nhistogram entry among \\nall\\n bins proportionally, depending on the distance from that \\nvalue to the center of each bin. This is done by multiplying each entry into a bin by \\na weight of \\n1\\n−\\nd\\n,\\n where \\nd\\n is the shortest distance from the value to the center of a \\nbin,\\n measured in the units of the histogram spacing, so that the maximum possible \\ndistance is 1. For example, the center of the ﬁrst bin is at \\n45 2 22 5\\n°°\\n=\\n.,\\n the next cen-\\nter is at \\n22 5 45 67 5\\n..\\n,\\n°° °\\n+=\\n and so on. Suppose that a particular directional value is \\n22 5\\n..\\n°\\n The distance from that value to the center of the ﬁrst histogram bin is 0, so we \\nwould assign a full entry (i.e\\n., a count of 1) to that bin in the histogram. The distance \\nto the next center would be greater than 0, so we would assign a fraction of a full \\nentry, that is \\n11\\n*\\n()\\n,\\n−\\nd\\n to that bin, and so forth for all bins. In this way, every bin \\ngets a proportional fraction of a count,\\n thus avoiding “boundary” effects in which a \\ndescriptor changes abruptly as a small change in orientation causes it to be assigned \\nfrom one bin to another.\\nFigure 11.62 shows the eight directions of a histogram as a small cluster of vec-\\ntors, with the length of each vector being equal to the value of its correspond ing bin. \\nSixteen histograms are computed, one for each \\n44\\n×\\n subregion of the \\n16 16\\n×\\n region \\nsurrounding a keypoint.\\n A descriptor, shown on the lower left of the ﬁgure, then con-\\nsists of a \\n44\\n×\\n array, each containing eight directional values. In SIFT, this descriptor \\ndata is organized as a 128-dimensional vector\\n.\\nIn order to achieve orientation invariance, the coordinates of the descriptor and \\nthe gradient orientations are rotated relative to the keypoint orientation. In order to \\nreduce the effects of illumination, a feature vector is normalized in two stages. First, \\nthe vector is normalized to unit length by dividing each component by the vector \\nnorm. A change in image contrast resulting from each pixel value being multiplied \\nby a constant will multiply the gradients by the same constant, so the change in \\ncontrast will be cancelled by the ﬁrst normalization. A brightness change caused \\nby a constant being added to each pixel will not affect the gradient values because \\nthey are computed from pixel differences. Therefore, the descriptor is invariant to \\nafﬁne changes in illumination. However, nonlinear illumination changes resulting, \\nfor example, from camera saturation, can also occur. These types of changes can \\ncause large variations in the relative magnitudes of some of the gradients, but they \\nDIP4E_GLOBAL_Print_Ready.indb   893\\n6/16/2017   2:16:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 894}),\n",
       " Document(page_content='894\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nare less likely to affect gradient orientation. SIFT reduces the inﬂuence of large \\ngradient magnitudes by thresholding the values of the normalized feature vector \\nso that all components are below the experimentally determined value of 0.2. After \\nthresholding, the feature vector is renormalized to unit length.\\nSUMMARY OF THE SIFT ALGORITHM\\nAs the material in the preceding sections shows, SIFT is a complex procedure con-\\nsisting of many parts and empirically determined constants. The following is a step-\\nby-step summary of the method.\\n1. \\nConstruct the scale space.\\n \\nThis is done using the procedure outlined in Figs. 11.56 \\nand 11.57. The parameters that need to be speciﬁed are \\ns\\n, \\ns\\n,\\n (\\nk\\n is computed \\nfrom \\ns\\n),\\n and the number of octaves. Suggested values are \\ns\\n=\\n16\\n.,\\n \\ns\\n=\\n2,\\n and \\nthree octaves\\n.\\n2. \\nObtain the initial keypoints.\\n Compute the difference of Gaussians\\n, \\nD\\nxy\\n(,, ) ,\\ns\\n \\nfrom the smoothed images in scale space\\n, as explained in Fig. 11.58 and Eq. (11-69). \\nFind the extrema in each \\nD\\nxy\\n(,, )\\ns\\n image using the method explained in Fig. \\n11.59.\\n These are the initial keypoints.\\n3. \\nImprove the accuracy of the location of the keypoints.\\n Interpolate the values \\nof \\nD\\nxy\\n(,, )\\ns\\n via a Taylor expansion. The improved key point locations are given \\nby Eq.\\n (11-74). \\n4. \\nDelete unsuitable keypoints.\\n \\nEliminate keypoints that have low contrast and/or \\nare poorly localized.\\n This is done by evaluating \\nD\\n from Step 3 at the improved \\nlocations, using Eq. (11-75). All keypoints whose values of \\nD\\n are lower than a \\nthreshold are deleted. A suggested threshold value is 0.03. Keypoints associated \\nwith edges are deleted also, using Eq. (11-79). A value of 10 is suggested for \\nr\\n. \\n5. \\nCompute keypoint orientations.\\n Use Eqs\\n. (11-80) and (11-81) to compute the \\nmagnitude and orientation of each keypoint using the histogram-based proce-\\ndure discussed in connection with these equations.\\n6. \\nCompute keypoint descriptors.\\n Use the method summarized in F\\nig. 11.62 to \\ncompute a feature (descriptor) vector for each keypoint. If a region of size \\n16 16\\n×\\n around each keypoint is used, the result will be a \\n128-dimensional feature\\n \\nvector for each keypoint. \\nThe following example illustrates the power of this algorithm.\\nEXAMPLE 11.20 :  Using SIFT for image matching.\\nWe illustrate the performance of the SIFT algorithm by using it to ﬁnd the number of matches between \\nan image of a building and a subimage formed by extracting part of the right corner edge of the building. \\nWe also show results for rotated and scaled-down versions of the image and subimage. This type of pro-\\ncess can be used in applications such as ﬁnding correspondences between two images for the purpose of \\nimage registration, and for ﬁnding instances of an image in a database of images. \\nFigure 11.63(a) shows the keypoints for the building image (this is the same as Fig. 11.61), and the \\nkeypoints for the subimage, which is a separate, much smaller image. The keypoints were computed \\nAs indicated at the \\nbeginning of this section, \\nsmoothing and doubling \\nthe size of the input \\nimage is assumed. Input \\nimages are assumed to \\nhave values in the range \\n[0, 1].\\nDIP4E_GLOBAL_Print_Ready.indb   894\\n6/16/2017   2:16:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 895}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n895\\nusing SIFT \\nindependently\\n for each image. The building shows 643 keypoints and the subimage 54 key-\\npoints. Figure 11.63(b) shows the matches found by SIFT between the image and subimage; 36 keypoint \\nmatches were found and, as the ﬁgure shows, only three were incorrect. Considering the large number \\nof initial keypoints, you can see that keypoint descriptors offer a high degree of accuracy for establishing \\ncorrespondences between images.\\nFigure 11.64(a) shows keypoints for the building image after it was rotated by 5\\n°\\n counterclockwise, \\nand for a subimage extracted from its right corner edge. The rotated image is smaller than the original \\nbecause it was cropped to eliminate the constant areas created by rotation (see Fig. 2.41). Here, SIFT \\nfound 547 keypoints for the building and 49 for the subimage. A total of 26 matches were found and, as \\nFig. 11.64(b) shows, only two were incorrect.\\nFigure 11.65 shows the results obtained using SIFT on an image of the building reduced to half the \\nsize in both spatial directions. When SIFT was applied to the downsampled image and a correspond-\\ning subimage, no matches were found. This was remedied by brightening the reduced image slightly \\nby manipulating the intensity gamma. The subimage was extracted from this image. Despite the fact \\nthat SIFT has the capability to handle some degree of changes in intensity, this example indicates that \\nperformance can be improved by enhancing the contrast of an image prior to processing. When work-\\ning with a database of images, histogram speciﬁcation (see Chapter 3) is an excellent tool for normal-\\nizing the intensity of all images using the characteristics of the image being queried. SIFT found 195 \\nkeypoints for the half-size image and 24 keypoints for the corresponding subimage. A total of seven \\nmatches were found between the two images, of which only one was incorrect.\\nThe preceding two ﬁgures illustrate the insensitivity of SIFT to rotation and scale changes, but they \\nare not ideal tests because the reason for seeking insensitivity to these variables in the ﬁrst place is \\nb a\\nFIGURE 11.63\\n (a) Keypoints and their directions (shown as gray arrows) for the building image and for a section of \\nthe right corner of the building. The subimage is a separate image and was processed as such. (b) Corresponding \\nkey points between the building and the subimage (the straight lines shown connect pairs of matching points). Only \\nthree of the 36 matches found are incorrect.\\nDIP4E_GLOBAL_Print_Ready.indb   895\\n6/16/2017   2:16:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 896}),\n",
       " Document(page_content='896\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nthat we do not always know a priori when images have been acquired under different conditions and \\ngeometrical arrangements. A more practical test is to compute features for a prototype image and test \\nthem against unknown samples. Figure 11.66 shows the results of such tests. Figure 11.66(a) is the origi-\\nnal building image, for which SIFT features vectors were already computed (see Fig. 11.63). SIFT was \\nused to compare the rotated subimage from Fig. 11.64(a) against the original, unrotated image. As Fig. \\n11.66(a) shows, 10 matches were found, of which two were incorrect. These are excellent results, con-\\nsidering the relatively small size of the subimage, and the fact that it was rotated. Figure 11.66(b) shows \\nthe results of matching the half-sized subimage against the original image. Eleven matches were found, \\nb a\\nFIGURE 11.64\\n (a) Keypoints for the rotated (by 5\\n°\\n) building image and for a section of the right corner of the building. \\nThe subimage is a separate image and was processed as such. (b) Corresponding keypoints between the corner and \\nthe building. Of the 26 matches found, only two are in error.\\nb a\\nFIGURE 11.65\\n (a) Keypoints for the half-sized building and a section of the right corner. (b) Corresponding keypoints \\nbetween the corner and the building. Of the seven matches found, only one is in error.\\nDIP4E_GLOBAL_Print_Ready.indb   896\\n6/16/2017   2:16:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 897}),\n",
       " Document(page_content='11.7\\n  \\nScale-Invariant Feature Transform (SIFT)\\n    \\n897\\nb a\\nFIGURE 11.66\\n (a) Matches between the original building image and a rotated version of a segment of its right corner. \\nTen matches were found, of which two are incorrect. (b) Matches between the original image and a half-scaled ver-\\nsion of a segment of its right corner. Here, 11 matches were found, of which four were incorrect.\\nof which four were incorrect. Again, these are good results, considering the fact that signiﬁcant detail \\nwas lost in the subimage when it was rotated or reduced in size. If asked in both cases: Based solely on \\nthe matches found by SIFT, from which part of the building did the two subimages come? The obvious \\nanswer in both is that the subimages are from the right corner of the building. The preceding two tests \\nillustrate the adaptability of SIFT to variations in rotation and scale. \\nSummary, References, and Further Reading\\n \\nFeature extraction is a fundamental process in the operation of most automated image processing applications. \\nAs indicated by the range of feature detection and description techniques covered in this chapter, the choice of \\none method over another is determined by the problem under consideration. The objective is to choose feature \\ndescriptors that “capture” essential differences between objects, or classes of objects, while maintaining as much \\nindependence as possible to changes in variables such as location, scale, orientation, illumination, and viewing angle.\\nThe Freeman chain code discussed in Section 11.2 was ﬁrst proposed by Freeman [1961, 1974], while the slope \\nchain code is due to Bribiesca [2013]. See Klette and Rosenfeld [2004] regarding the minimum-perimeter polygon \\nalgorithm. For additional reading on signatures see Ballard and Brown [1982]. The medial axis transform is gener-\\nally credited to Blum [1967]. For efﬁcient computation of the Euclidean distance transform used for skeletonizing \\nsee Maurer et al. [2003].\\nFor additional reading on the basic boundary feature descriptors in Section 11.3, see Rosenfeld and Kak [1982]. \\nThe discussion on shape numbers is based on the work of Bribiesca and Guzman [1980]. For additional reading on \\nFourier descriptors, see the early paper by Zahn and Roskies [1972]. For an example of current uses of this tech-\\nnique, see Sikic and Konjicila [2016]. The discussion on statistical moments as boundary descriptors is from basic \\nprobability (for example, see Montgomery and Runger [2011]).\\nFor additional reading on the basic region descriptors discussed in Section 11.4, see Rosenfeld and Kak [1982]. \\nFor further introductory reading on texture, see Haralick and Shapiro [1992] and Shapiro and Stockman [2001]. \\nDIP4E_GLOBAL_Print_Ready.indb   897\\n6/16/2017   2:16:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 898}),\n",
       " Document(page_content='898\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n11.1 \\nDo the following:\\n(a) * \\nProvide all the missing steps in Fig. 11.1. \\nShow your results using the same format as \\nin that ﬁgure\\n.\\n(b) \\nWhen applied to binary regions, the bound-\\nary-following algorithm in Section 11.2 typi-\\ncally yields boundaries that are one pixel \\nthick,\\n but this is not always the case. Give a \\nsmall image example in which the boundary \\nis thicker than one pixel in at least one place.\\n11.2 \\nWith reference to the Moore boundary-following \\nalgorithm explained in Section 11.2,\\n answer the \\nfollowing, using the same grid as in Fig. 11.2 to \\nidentify boundary points in your explanation \\n[remember, the origin is at \\n(,) ,\\n11\\n instead of our \\nusual \\n(,) ]\\n00\\n. Include the position of points \\nb \\nand \\nc\\n at each point you mention.\\n \\n(a) * \\nGive the coordinates in Fig. 11.2(a) at which \\nthe algorithm starts and ends\\n. What would \\nit do when it arrived at the end point of the \\nboundary?\\n(b) \\nHow would the algorithm behave when \\nit arrived at the intersection point in F\\nig. \\n11.2(b) for the ﬁrst time, and then for the \\nsecond time?\\n11.3 \\nAnswer the following:\\n(a) * \\nDoes normalizing the Freeman chain code \\nof a closed curve so that the starting point \\nis the smallest integer always give a unique \\nstarting point?\\n(b) \\nDoes a chain-coded closed curve always \\nhave an even number of segments? If your \\nanswer is yes\\n, prove it. If it is no, give an ex- \\nample.\\n(c) \\nFind the normalized starting point of the \\ncode 11076765543322.\\n11.4 \\nDo the following:\\n(a) * \\nShow that the ﬁrst difference of a chain code \\nnormalizes it to rotation,\\n as explained in \\nSection 11.2.\\n(b) \\nCompute the ﬁrst difference of the code \\n0101030303323232212111.\\n11.5 \\nAnswer the following:\\n(a) * \\nGiven a one-pixel-thick, open or closed, \\n4-connected simple (does not intersect \\nitself) digital curve\\n, can a slope chain code \\nbe formulated so that it behaves exactly as \\na Freeman chain code? If your answer is no, \\nexplain why. If your answer is yes, explain \\nhow you would do it, detailing any assump-\\ntions you need to make for your answer to \\nhold.\\n(b) \\nRepeat (a) for an 8-connected curve.\\n(c) \\nHow would you normalize a slope chain code \\nfor scale changes?\\n11.6 * \\nExplain why a slope chain code with an angle \\naccurac\\ny of \\n10\\n1\\n−\\n produces 19 symbols.\\n11.7 \\nLet \\nL\\n be the length of the straight-line segments \\nused in a slope chain code\\n. Assume that \\nL\\n is such \\nthat an integral number of line segments ﬁt the \\nOur discussion of moment-invariants is based on Hu [1962]. For generating moments of arbitrary order, see Flusser \\n[2000]. \\nHotelling [1933] was the ﬁrst to derive and publish the approach that transforms discrete variables into uncor-\\nrelated coefﬁcients (Section 11.5). He referred to this technique as the method of \\nprincipal components\\n. His paper \\ngives considerable insight into the method and is worth reading. Principal components are still used widely in \\nnumerous ﬁelds, including image processing, as evidenced by Xiang et al. [2016]. The corner detector in Section 11.6 \\nis from Harris and Stephens [1988], and our discussion of MSERs is based on Matas et al. [2002]. The SIFT material \\nin Section 11.7 is from Lowe [2004]. For details on the software aspects of many of the examples in this chapter, see \\nGonzalez, Woods, and Eddins [2009].\\nDIP4E_GLOBAL_Print_Ready.indb   898\\n6/16/2017   2:16:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 899}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n899\\ncurve under consideration. Assume also that the \\nangle accuracy is high enough so that it may be \\nconsidered inﬁnite for your purposes, answer the \\nfollowing:\\n(a) * \\nWhat is the tortuosity of a square boundary \\nof size \\ndd\\n×\\n?\\n(b) * \\nWhat is the tortuosity of a circle of radius \\nr\\n?\\n(c) \\nWhat is the tortuosity of a closed convex \\ncurve?\\n11.8 * \\nAdvance an argument that explains why the \\nuppermost-leftmost point of a digital closed \\ncurve has the property that a polygonal approxi-\\nmation to the curve has a convex vertex at that \\npoint.\\n11.9 \\nWith reference to Example 11.2, start with vertex \\nV\\n7\\n and apply the MPP algorithm through, and \\nincluding, \\nV\\n11\\n.\\n11.10 \\nDo the following:\\n(a) * \\nExplain why the rubber-band polygonal \\napproximation approach discussed in Sec-\\ntion 11.2 yields a polygon with minimum \\nperimeter for a convex curve\\n.\\n(b) \\nShow that if each cell corresponds to a pixel \\non the boundary\\n, the maximum possible \\nerror in that cell is \\n2\\nd\\n,\\n where \\nd\\n is the mini-\\nmum possible horizontal or vertical distance \\nbetween adjacent pixels (i.e\\n., the distance \\nbetween lines in the sampling grid used to \\nproduce the digital image).\\n11.11 \\nExplain how the MPP algorithm in Section 11.2 \\nbehaves under the following conditions:\\n(a) * \\nOne-pixel wide, one-pixel deep indentations.\\n(b) * \\nOne-pixel wide, two-or-more pixel deep \\nindentations\\n.\\n(c) \\nOne-pixel wide, \\nn\\n-pixel long protrusions\\n.\\n11.12 \\nDo the following.\\n(a) * \\nPlot the signature of a square boundary using \\nthe tangent-angle method discussed in Sec-\\ntion 11.2.\\n(b) \\nRepeat (a) for the slope density function. \\nAssume that the square is aligned with the \\nx\\n- \\nand \\ny\\n-axes\\n, and let the \\nx\\n-axis be the reference \\nline. Start at the corner closest to the origin.\\n11.13 \\nFind an expression for the signature of each of \\nthe following boundaries\\n, and plot the signatures.\\n(a) * \\nAn equilateral triangle.\\n(b) \\nA rectangle.\\n(c) \\nAn ellipse\\n11.14 \\nDo the following:\\n(a) * \\nWith reference to Figs. 11.11(c) and (f), give \\na word description of an algorithm for count-\\ning the peaks in the two waveforms\\n. Such an \\nalgorithm would allow us to differentiate \\nbetween triangles and rectangles.\\n(b) \\nHow can you make your solution indepen-\\ndent of scale changes? \\nYou may assume that \\nthe scale changes are the same in both direc-\\ntions.\\n11.15 \\nDraw the medial axis of:\\n(a) * \\nA circle.\\n(b) * \\nA square.\\n(c) \\nAn equilateral triangle.\\n11.16 \\nFor the ﬁgure shown,\\n(a) * \\nWhat is the order of the shape number?\\n(b) \\nObtain the shape number.\\n11.17 * \\nThe procedure discussed in Section 11.3 for using \\nF\\nourier descriptors consists of expressing the \\ncoordinates of a contour as complex numbers, \\ntaking the DFT of these numbers, and keeping \\nonly a few components of the DFT as descriptors \\nof the boundary shape. The inverse DFT is then \\nan approximation to the original contour. What \\nclass of contour shapes would have a DFT con-\\nsisting of real numbers, and how would the axis \\nsystem in Fig. 11.18 have to be set up to obtain \\nthose real numbers?\\n11.18 \\nShow that if you use only two Fourier descrip-\\ntors \\n()\\nuu\\n==\\n01\\n and \\n to reconstruct a bound-\\nary with Eq.\\n (11-10), the result will always be a \\ncircle. (\\nHint:\\n Use the parametric representation \\nof a circle in the complex plane, and express the \\nequation of a circle in polar coordinates.)\\nDIP4E_GLOBAL_Print_Ready.indb   899\\n6/16/2017   2:16:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 900}),\n",
       " Document(page_content='900\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\n11.19 * \\nGive the smallest number of statistical moment \\ndescriptors needed to differentiate between the \\nsignatures of the ﬁgures in F\\nig. 11.10.\\n11.20 \\nGive two boundary shapes that have the same \\nmean and third statistical moment descriptors\\n, \\nbut different second moments.\\n11.21 * \\nPropose a set of descriptors capable of differen-\\ntiating between the shapes of the characters 0,\\n 1, \\n8, 9, and \\nX\\n. (\\nHint:\\n Use topological descriptors in \\nconjunction with the convex hull.)\\n11.22 \\nConsider a binary image of size \\n200 200\\n×\\n pix-\\nels\\n, with a vertical black band extending from \\ncolumns 1 to 99 and a vertical white band extend-\\ning from columns 100 to 200.\\n(a) \\nObtain the co-occurrence matrix of this \\nimage using the position operator \\n“one pixel \\nto the right.”\\n(b) * \\nNormalize this matrix so that its elements \\nbecome probability estimates\\n, as explained \\nin Section 11.4.\\n(c) \\nUse your matrix from (b) to compute the six \\ndescriptors in \\nTable 11.3.\\n11.23 \\nConsider a checkerboard image composed of \\nalternating black and white squares\\n, each of size \\nmm\\n×\\n pixels. Give a position operator that will \\nyield a diagonal co-occurrence matrix.\\n11.24 \\nObtain the gray-level co-occurrence matrix of \\nan  array pattern of alternating single 0’\\ns and 1’s \\n(starting with 0) if:\\n(a) * \\nThe position operator \\nQ\\n is deﬁned as \\n“one \\npixel to the right.”\\n(b) \\nThe position operator \\nQ\\n is deﬁned as \\n“two \\npixels to the right.”\\n11.25 \\nDo the following.\\n(a) * \\nProve the validity of Eqs. (11-50) and (11-51).\\n(b) \\nProve the validity of Eq. (11-52).\\n11.26 * \\nWe mentioned in Example 11.16 that a credible \\njob could be done of reconstructing approxima-\\ntions to the six original images by using only the \\ntwo principal-component images associated with \\nthe largest eigenvalues\\n. What would be the mean \\nsquared error incurred in doing so? Express your \\nanswer as a percentage of the maximum possible \\nerror.\\n11.27 \\nFor a set of images of size \\n64 64\\n×\\n,\\n assume that \\nthe covariance matrix given in Eq.\\n (11-52) is \\nthe identity matrix. What would be the mean \\nsquared error between the original images and \\nimages reconstructed using Eq. (11-54) with only \\nhalf of the original eigenvectors?\\n11.28 \\nUnder what conditions would you expect the \\nmajor axes of a boundary\\n, deﬁned in the discus-\\nsion of Eq. (11-4), to be equal to the eigen axes of \\nthat boundary?\\n11.29 * \\nYou are contracted to design an image process-\\ning system for detecting imperfections on the \\ninside of certain solid plastic wafers\\n. The wafers \\nare examined using an X-ray imaging system, \\nwhich yields 8-bit images of size \\n512 512\\n×\\n.\\n In \\nthe absence of imperfections\\n, the images appear \\nuniform, having a mean intensity of 100 and vari-\\nance of 400. The imperfections appear as blob-\\nlike regions in which about 70% of the pixels \\nhave excursions in intensity of 50 intensity levels \\nor less about a mean of 100. A wafer is consid-\\nered defective if such a region occupies an area \\nexceeding \\n20 20\\n×\\n pixels in size. Propose a system \\nbased on texture analysis for solving this prob-\\nlem.\\n11.30 \\nWith reference to Fig. 11.46, answer the following:\\n(a) * \\nWhat is the cause of nearly identical clusters \\nnear the origin in F\\nigs. 11.46(d)-(f).\\n(b) \\nLook carefully, and you will see a single point \\nnear \\ncoordinates \\n(.,.)\\n08\\n08\\n in Fig. 11.46(f). \\nW\\nhat caused this point?\\n(c) \\nThe results in Fig. 11.46(d)–(e) are for \\nthe small image patches shown in F\\nigs. \\n11.46(a)–(b). What would the results look \\nlike if we performed the computations over \\nthe entire \\nimage, instead of limiting the com-\\nputation to the patches?\\n11.31 \\nWhen we discussed the Harris-Stephens corner \\ndetector\\n, we \\nmentioned that there is a closed-form \\nformula for computing the eigenvalues of a \\n22\\n×\\nmatrix.\\n(a) * \\nGiven matrix \\nM\\n=\\n[;\\n] ,\\nabcd\\n give the gen-\\neral \\nformula for ﬁnding its eigenvalues\\n. \\nExpress your formula in terms of the trace \\nand determinant of \\nM\\n.\\n(b) \\nGive the formula for \\nsymmetric\\n matrices of \\nDIP4E_GLOBAL_Print_Ready.indb   900\\n6/16/2017   2:16:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 901}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n901\\nsize \\n22\\n×\\n in terms of its four elements, with-\\nout using the trace nor the determinant.\\n11.32 * \\nWith reference to the component tree in Fig. \\n11.51,\\n assume that any pixels extending past the \\nborder of the small image are 0. Is region \\nR\\n1\\n an \\nextremal region? Explain.\\n11.33 \\nWith reference to the discussion of maximally \\nstable extremal regions in Section 11.6,\\n can the \\nroot of a component tree contain an MSER? \\nExplain.\\n11.34 * \\nThe well known heat-diffusion equation of a \\ntemperature function \\ngxy z t\\n(,\\n,,)\\n of three spatial \\nvariables\\n, \\n(,,) ,\\nxy\\nz\\n is given by \\n∂∂ − =\\ngt g\\na\\n/H11612\\n2\\n0,\\n \\nwhere \\na\\n is the thermal diffusivity and \\n/H11612\\n2\\n is the \\nLaplacian operator. In terms of our discussion of \\nSIFT, the form of this equation is used to estab-\\nlish a relationship between the difference of \\nGaussians and the scaled Laplacian, \\ns\\n22\\n/H11612\\n.\\n Show \\nhow this can be done to derive Eq.\\n (11-70).\\n11.35 \\nWith reference to the SIFT algorithm discussed \\nin Section 11.7,\\n assume that the input image is \\nsquare, of size \\nMM\\n×\\n (with \\nM\\nn\\n=\\n2) ,\\n and let the \\nnumber of intervals per octave be \\ns\\n=\\n2.\\n(a) \\nHow many smoothed images will there be in \\neach octave?\\n(b) * \\nHow many octaves could be generated before \\nit is no longer possible to down-sample the \\nimage by 2?\\n(c) \\nIf the standard deviation used to smooth \\nthe ﬁrst image in the ﬁrst octave is \\ns\\n,\\n what \\nare the values of standard deviation used to \\nsmooth the ﬁrst image in each of the remain-\\ning octaves in (b)?\\n11.36 \\nAdvance an argument showing that smoothing \\nan image and then downsampling it by 2 gives \\nthe same result as ﬁrst downsampling the image \\nby 2 and then smoothing it with the same kernel.\\n \\nBy downsampling we mean skipping every other \\nrow and column. (\\nHint:\\n Consider the fact that \\nconvolution is a linear process.)\\n11.37 \\nDo the following:\\n(a) * \\nShow how to obtain Eq. (11-74) from Eq. \\n(11-71).\\n(b) \\nShow how Eq. (11-75) follows from Eqs. \\n(11-74) and (11-71).\\n11.38 \\nA company that bottles a variety of industrial \\nchemicals employs you to design an approach for \\ndetecting when bottles of their product are not \\nfull.\\n As they move along a conveyor line past an \\nautomatic ﬁlling and capping station, the bottles \\nappear as shown in the following image. A bottle \\nis considered imperfectly ﬁlled when the level \\nof the liquid is below the midway point between \\nthe bottom of the neck and the shoulder of the \\nbottle. The shoulder is deﬁned as the intersection \\nof the sides and slanted portions of the bottle. \\nThe bottles move at a high rate of speed, but the \\ncompany has an imaging system equipped with \\nan illumination ﬂash front end that effectively \\nstops motion, so you will be given images that \\nlook very close to the sample shown here. Based \\non the material you have learned up to this point, \\npropose a solution for detecting bottles that are \\nnot ﬁlled properly. State clearly all assumptions \\nthat you make and that are likely to impact the \\nsolution you propose.\\n11.39 \\nHaving heard about your success with the \\nbottle \\ninspection problem, you are contacted by a \\nﬂuids company that wishes to automate bubble-\\ncounting in certain processes for quality control. \\nThe company has solved the imaging problem \\nand can obtain 8-bit images of size \\n700 700\\n×\\n pix-\\nels\\n, such as the one shown in the ﬁgure below. \\nEach image represents an area of \\n7\\n2\\ncm .\\n The \\ncompany wishes to do two things with each \\nDIP4E_GLOBAL_Print_Ready.indb   901\\n6/16/2017   2:16:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 902}),\n",
       " Document(page_content='902\\n    \\nChapter\\n \\n11\\n  \\nFeature Extraction \\nimage: (1) Determine the ratio of the area occu-\\npied by bubbles to the total area of the image;\\n \\nand (2) count the number of distinct bubbles. \\nBased on the material you have learned up to \\nthis point, propose a solution to this problem. In \\nyour report, state the physical dimensions of the \\nsmallest bubble your solution can detect. State \\nclearly all assumptions that you make and that \\nare likely to impact the solution you propose.\\nDIP4E_GLOBAL_Print_Ready.indb   902\\n6/16/2017   2:16:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 903}),\n",
       " Document(page_content='90312\\nImage Pattern Classiﬁcation\\nPreview\\nWe conclude our coverage of digital image processing with an introduction to techniques for image \\npattern classiﬁcation. The approaches developed in this chapter are divided into three principal catego-\\nries: classiﬁcation by \\nprototype\\n \\nmatching\\n, classiﬁcation based on an \\noptimal statistical\\n formulation, and \\nclassiﬁcation based on \\nneural networks\\n. The ﬁrst two approaches are used extensively in applications in \\nwhich the nature of the data is well understood, leading to an effective pairing of features and classiﬁer \\ndesign. These approaches often rely on a great deal of engineering to deﬁne features and elements of a \\nclassiﬁer. Approaches based on neural networks rely less on such knowledge, and lend themselves well \\nto applications in which pattern class characteristics (e.g., features) are learned by the system, rather \\nthan being speciﬁed a priori by a human designer. The focus of the material in this chapter is on prin-\\nciples, and on how they apply speciﬁcally in image pattern classiﬁcation.\\nUpon completion of this chapter, readers should:\\n Understand the meaning of patterns and pat-\\ntern classes, and how they relate to digital \\nimage processing.\\n Be familiar with the basics of minimum-dis-\\ntance classiﬁcation. \\n Know how to apply image correlation tech-\\nniques for template matching.\\n Understand the concept of string matching.\\n Be familiar with Bayes classiﬁers.\\n Understand perceptrons and their history.\\n Be familiar with the concept of learning from \\ntraining samples.\\n Understand neural network architectures. \\n Be familiar with the concept of deep learning \\nin fully connected and deep convolutional neu-\\nral networks. In particular, be familiar with the \\nimportance of the latter in digital image pro-\\ncessing.\\nOne of the most interesting aspects of the world is that it can be  \\nconsidered to be made up of patterns.\\nA pattern is essentially an arrangement. It is characterized by  \\nthe order of the elements of which it is made, rather than by the  \\nintrinsic nature of these elements.\\nNorbert Wiener\\nDIP4E_GLOBAL_Print_Ready.indb   903\\n6/16/2017   2:16:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 904}),\n",
       " Document(page_content='904\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n12.1  BACKGROUND  \\nHumans possess the most sophisticated pattern recognition capabilities in the known \\nbiological world. By contrast, the capabilities of current recognition machines pale \\nin comparison with tasks humans perform routinely, from being able to interpret the \\nmeaning of complex images, to our ability for generalizing knowledge stored in our \\nbrains. But recognition machines play an important, sometimes even crucial role in \\neveryday life. Imagine what modern life would be like without machines that read \\nbarcodes, process bank checks, inspect the quality of manufactured products, read \\nfingerprints, sort mail, and recognize speech. \\nIn image pattern recognition, we think of a \\npattern\\n as a spatial arrangement of \\nfeatures. A \\npattern class\\n is a set of patterns that share some common properties. Pat-\\ntern recognition by machine encompasses techniques for automatically assigning \\npatterns to their respective classes. That is, given a pattern or sets of patterns whose \\nclass is unknown, the job of a pattern recognition system is to assign a class label to \\neach of its input patterns.\\nThere are four main stages involved in recognition: (1) sensing, (2) preprocessing, \\n(3) feature extraction, and (4) classiﬁcation. In terms of image processing, sensing is \\nconcerned with generating signals in a spatial (2-D) or higher-dimensional format. \\nWe covered numerous aspects of image sensing in Chapter 1. Preprocessing deals \\nwith techniques for tasks such as noise reduction, enhancement, restoration, and \\nsegmentation, as discussed in earlier chapters. You learned about feature extraction \\nin Chapter 11. Classiﬁcation, the focus of this chapter, deals with using a set of fea-\\ntures as the basis for assigning class labels to unknown input image patterns.\\nIn the following section, we will discuss three basic approaches used for image \\npattern classiﬁcation: (1) classiﬁcation based on matching unknown patterns against \\nspeciﬁed prototypes, (2) optimum statistical classiﬁers, and (3) neural networks. \\nOne way to characterize the differences between these approaches is in the level \\nof “engineering” required to transform raw data into formats suitable for computer \\nprocessing. Ultimately, recognition performance is determined by the discriminative \\npower of the features used. \\nIn classiﬁcation based on prototypes, the objective is to make the features so \\nunique and easily detectable that classiﬁcation itself becomes a simple task. A good \\nexample of this are bank-check processors, which use stylized font styles to simplify \\nmachine processing (we will discuss this application in\\n \\nSection 12.3). \\nIn the second category\\n, classiﬁcation is cast in decision-theoretic, statistical terms, \\nand the classiﬁcation approach is based on selecting parameters that can be shown \\nto yield optimum classiﬁcation performance in a statistical sense. Here, emphasis is \\nplaced on both the features used, and the design of the classiﬁer. We will illustrate \\nthis approach in Section 12.4 by deriving the Bayes pattern classiﬁer, starting from \\nbasic principles. \\nIn the third category, classiﬁcation is performed using neural networks. As you \\nwill learn in Sections 12.5 and 12.6, neural networks can operate using engineered \\nfeatures too, but they have the unique ability of being able to generate, on their own, \\nrepresentations (features) suitable for recognition. These systems can accomplish \\nthis using raw data, without the need for engineered features. \\n12.1\\nDIP4E_GLOBAL_Print_Ready.indb   904\\n6/16/2017   2:16:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 905}),\n",
       " Document(page_content='12.1\\n  \\nBackground\\n    \\n905\\nOne characteristic shared by the preceding three approaches is that they are \\nbased on parameters that must be either speciﬁed or learned from patterns that rep-\\nresent the recognition problem we want to solve. The patterns can be \\nlabeled\\n, mean-\\ning that we know the class of each pattern, or \\nunlabeled\\n, meaning that the data are \\nknown to be patterns, but the class of each pattern is unknown. A classic example \\nof labeled data is the character recognition problem, in which a set of character \\nsamples is collected and the identity of each character is recorded as a label from \\nthe group 0 through 9 and \\na\\n through \\nz\\n. An example of unlabeled data is when we are \\nseeking clusters in a data set, with the aim of utilizing the resulting cluster centers as \\nbeing prototypes of the pattern classes contained in the data.\\nWhen working with a labeled data, a given data set generally is subdivided into \\nthree subsets: a \\ntraining set\\n, a v\\nalidation set\\n, and a \\ntest set\\n (a typical subdivision might \\nbe 50% training, and 25% each for the validation and test sets). The process by \\nwhich a training set is used to generate classifier parameters is called \\ntraining.\\n In \\nthis mode, a classifier is given the class label of each pattern, the objective being to \\nmake adjustments in the parameters if the classifier makes a mistake in identify-\\ning the class of the given pattern. At this point, we might be working with several \\ncandidate designs. At the end of training, we use the validation set to compare the \\nvarious designs against a performance objective. Typically, several iterations of train-\\ning/validation are required to establish the design that comes closest to meeting the \\ndesired objective. Once a design has been selected, the final step is to determine how \\nit will perform “in the field.” For this, we use the test set, which consists of patterns \\nthat the system has never “seen” before. If the training and validation sets are truly \\nrepresentative of the data the system will encounter in practice, the results of train-\\ning/validation should be close to the performance using the test set. If training/vali-\\ndation results are acceptable, but test results are not, we say that training/validation \\n“over fit” the system parameters to the available data, in which case further work on \\nthe system architecture is required. Of course all this assumes that the given data are \\ntruly representative of the problem we want to solve, and that the problem in fact \\ncan be solved by available technology.\\nA system that is designed using training data is said to undergo \\nsupervised\\n \\nlearn-\\ning.\\n If we are working with unlabeled data, the system learns the pattern classes \\nthemselves while in an \\nunsupervised\\n learning mode. In this chapter, we deal only \\nwith supervised learning. As you will see in this and the next chapter, supervised \\nlearning covers a broad range of approaches, from applications in which a system \\nlearns parameters of features whose form is fixed by a designer, to systems that uti-\\nlize \\ndeep learning\\n and large sets of raw data sets to learn, \\non their own\\n, the features \\nrequired for classification. These systems accomplish this task without a human \\ndesigner having to specify the features, a priori. \\nAfter a brief discussion in the next section of how patterns are formed, and on \\nthe nature of patterns classes, we will discuss in Section 12.3 various approaches for \\nprototype-based classification. In Section 12.4, we will start from basic principles \\nand derive the equations of the Bayes classifier, an approach characterized by opti-\\nmum classification performance on an average basis. We will also discuss supervised \\ntraining of a Bayes classifier based on the assumption of multivariate Gaussian \\nBecause the examples in \\nthis chapter are intended \\nto demonstrate basic \\nprinciples and are not \\nlarge scale, we dispense \\nwith validation and \\nsubdivide the pattern \\ndata into training and \\ntest sets.\\nGenerally, we associate \\nthe concept of deep \\nlearning with large sets \\nof data. These ideas are \\ndiscussed in more detail \\nlater in this section and \\nnext.\\nDIP4E_GLOBAL_Print_Ready.indb   905\\n6/16/2017   2:16:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 906}),\n",
       " Document(page_content='906\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\ndistributions. Starting with Section 12.5, we will spend the rest of the chapter discuss-\\ning neural networks. We will begin Section 12.5 with a brief introduction to percep-\\ntrons  and some historical facts about machine learning. Then, we will introduce the \\nconcept of deep neural networks and derive the equations of backpropagation, the \\nmethod of choice for training deep neural nets. These networks are well-suited for \\napplications in which input patterns are vectors. In Section 12.6, we will introduce \\ndeep convolutional neural networks, which currently are the preferred approach \\nwhen the system inputs are digital images. After deriving the backpropagation equa-\\ntions used for training convolutional nets, we will give several examples of appli-\\ncations involving classes of images of various complexities. In addition to working \\ndirectly with image inputs, deep convolutional nets are capable of learning, on their \\nown, image features suitable for classification. This is accomplished starting with raw \\nimage data, as opposed to the other classification methods discussed in Sections 12.3 \\nand 12.4, which rely on “engineered” features whose form, as noted earlier, is speci-\\nfied a priori by a human designer.\\n12.2  PATTERNS AND PATTERN CLASSES  \\nIn image pattern classification, the two principal pattern arrangements are quantita-\\ntive and structural. \\nQuantitative patterns\\n are arranged in the form of \\npattern\\n \\nvectors\\n. \\nStructural\\n \\npatterns\\n typically are composed of symbols, arranged in the form of \\nstrings\\n, \\ntrees\\n, or, less frequently, as \\ngraphs\\n. Most of the work in this chapter is based on pat-\\ntern vectors, but we will discuss structural patterns briefly at the end of this section, \\nand give an example at the end of Section 12.3. \\nPATTERN VECTORS\\nPattern vectors are represented by lowercase letters, such as \\nx\\n, \\ny\\n, and \\nz\\n, and have \\nthe form\\n \\nx\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nx\\nx\\nx\\nn\\n1\\n2\\n/vertellipsis\\n \\n(12-1)\\nwhere each component, \\nx\\ni\\n,\\n represents the \\ni\\nth feature descriptor\\n, and \\nn\\n is the total \\nnumber of such descriptors\\n. We can express a vector in the form of a column, as \\nin Eq. (12-1), or in the equivalent row form \\nx\\n=\\n()\\nxx x\\nn\\nT\\n12\\n,,, ,\\n…\\n where \\nT\\n indicates \\ntransposition.\\n A pattern vector may be “viewed” as a point in \\nn\\n-dimensional Euclid-\\nean space, and a pattern class may be interpreted as a “hypercloud” of points in this \\npattern space\\n. For the purpose of recognition, we like for our pattern classes to be \\ngrouped tightly, and as far away from each other as possible. \\nPattern vectors can be formed directly from image pixel intensities by vector-\\nizing the image using, for example, linear indexing, as in Fig. 12.1. A more common \\napproach is for pattern elements to be features. An early example is the work of \\nFisher [1936] who, close to a century ago, reported the use of what then was a new \\n12.2\\nWe discussed linear  \\nindexing in Section 2.4\\n \\n(see Fig. 2.22).\\nDIP4E_GLOBAL_Print_Ready.indb   906\\n6/16/2017   2:16:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 907}),\n",
       " Document(page_content='12.2\\n  \\nPatterns and Pattern Classes\\n    \\n907\\ntechnique called \\ndiscriminant analysis\\n to recognize three types of iris ﬂowers (\\nIris \\nsetosa\\n, \\nvirginica\\n, and \\nversicolor\\n). Fisher described each ﬂower using four features: \\nthe length and width of the petals, and similarly for the sepals (see Fig. 12.2). This \\nleads to the 4-D vectors shown in the ﬁgure. A set of these vectors, obtained for ﬁfty \\nsamples of each ﬂower gender, constitutes the three famous \\nFisher iris pattern class-\\nes\\n. Had Fisher been working today, he probably would have added spectral colors \\nand shape features to his measurements, yielding vectors of higher dimensionality. \\nWe will be working with the original iris data set later in this chapter. \\nA higher-level representation of patterns is based on feature descriptors of the \\ntypes you learned in Chapter 11. For instance, pattern vectors formed from descrip-\\ntors of boundary shape are well-suited for applications in controlled environments, \\nsuch as industrial inspection.\\n \\nFigure 12.3 illustrates the concept. Here, we are inter-\\nested in classifying different types of noisy shapes\\n, a sample of which is shown in \\nthe ﬁgure. If we represent an object by its signature, we would obtain 1-D signals \\nof the form shown in Fig. 12.3(b). We can express a signature as a vector by sam-\\npling its amplitude at increments of \\nu\\n,\\n then formimg a vector by letting \\nxr\\nii\\n=\\n() ,\\nu\\n \\nfor \\nin\\n=\\n012\\n,,\\n, ,.\\n…\\n Instead of using “raw” sampled signatures, a more common \\napproach is to compute some function,\\n \\nxg r\\nii\\n=\\n()\\n() ,\\nu\\n of the signature samples and \\nuse them to form vectors\\n. You learned in Section 11.3 several approaches to do this, \\nsuch as statistical moments. \\nSepals are the undergrowth \\nbeneath the petals. \\n225\\n225\\n225\\n225 225\\n175\\n125\\n125\\n55\\n5\\n5\\n5\\n90\\n90\\n90\\n175\\n225\\n125\\n5\\n225\\n175\\n225\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\n/vertellipsis\\nb a\\nFIGURE 12.1\\nUsing linear  \\nindexing to  \\nvectorize a  \\ngrayscale image. \\nFIGURE 12.2\\nPetal and sepal \\nwidth and length \\nmeasurements \\n(see arrows) \\nperformed on iris \\nﬂowers for the \\npurpose of data \\nclassiﬁcation. The \\nimage shown is of \\nthe \\nIris virginica\\n \\ngender. (Image \\ncourtesy of \\nUSDA.)\\n1\\n2\\n3\\n4\\nx\\nx\\nx\\nx\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\nx\\n1 \\n= Petal width\\nx\\n2 \\n= Petal length\\nx\\n3 \\n= Sepal width\\nx\\n4\\n = Sepal length\\nSepal\\nPetal\\nDIP4E_GLOBAL_Print_Ready.indb   907\\n6/16/2017   2:16:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 908}),\n",
       " Document(page_content='908\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nVectors can be formed also from features of both boundary and regions. For \\nexample, the objects in Fig. 12.4 can be represented by 3-D vectors whose compo-\\nnents capture shape information related to both boundary and region properties \\nof single binary objects. Pattern vectors can be used also to represent properties of \\nimage regions. For example, the elements of the 6-D vector in Fig. 12.5 are texture \\nmeasures based on the feature descriptors in Table 11.3. Figure 12.6 shows an exam-\\nple in which pattern vector elements are features that are invariant to transforma-\\ntions, such as image rotation and scaling (see Section 11.4).\\nWhen working with sequences of registered images, we have the option of using \\npattern vectors formed from corresponding pixels in those images (see Fig. 12.7). \\nForming pattern vectors in this way implies that recognition will be based on infor-\\nmation extracted from the same spatial location across the images. Although this \\nmay seem like a very limiting approach, it is ideally suited for applications such as \\nrecognizing regions in multispectral images, as you will see in Section 12.4.\\nWhen working with entire images as units, we need the detail afforded by vectors \\nof much-higher dimensionality, such as those we discussed in Section 11.7 in connec-\\ntion with the SIFT algorithm. However, a more powerful approach when working \\nwith entire images is to use deep convolutional neural networks. We will discuss \\nneural nets in detail in Sections 12.5 and 12.6.\\nSTRUCTURAL PATTERNS\\nPattern vectors are not suitable for applications in which objects are represented \\nby structural features, such as strings of symbols. Although they are used much less \\nthan vectors in image processing applications, patterns containing structural descrip-\\ntions of objects are important in applications where shape is of interest. Figure 12.8 \\nshows an example. The boundaries of the bottles were approximated by a polygon \\nx\\n1 \\n=\\n \\ncompactness\\nx\\n2 \\n=\\n \\ncircularity\\nx\\n3 \\n=\\n \\neccentricity\\n1\\n2\\n3\\nx\\nx\\nx\\n⎡⎤\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\nb\\na\\nc\\nd\\n \\nFIGURE 12.4\\nPattern vectors \\nwhose components \\ncapture both bound-\\nary and regional \\ncharacteristics.\\nb a\\n \\nFIGURE 12.3\\n(a) A noisy object \\nboundary, and (b)\\nits corresponding \\nsignature.\\n0\\nu\\nr\\nu\\np\\n4\\np\\n2\\n3\\np\\n4\\n5\\np\\n4\\n3\\np\\n2\\n7\\np\\n4\\np\\n2\\np\\nr\\n(\\nu\\n)\\n()\\n()\\n()\\n1\\n2\\n()\\n()\\n()\\nn\\ngr\\ngr\\ngr\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\n/vertellipsis\\nu\\nu\\nu\\nDIP4E_GLOBAL_Print_Ready.indb   908\\n6/16/2017   2:16:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 909}),\n",
       " Document(page_content=\"12.2\\n  \\nPatterns and Pattern Classes\\n    \\n909\\nSpectral band 1\\nSpectral band 2\\nSpectral band 3\\nSpectral band 4\\nSpectral band 5\\nSpectral band 6\\nx\\nx\\n1\\nx\\n2\\nx\\n3\\nx\\n4\\nx\\n5\\nx\\n6\\n/H11005\\nImages in spectral bands 1 3\\n–\\nImages in spectral bands 4\\n6\\n–\\nFIGURE 12.5\\nAn example of  \\npattern vectors \\nbased on  \\nproperties of \\nsubimages. See \\nTable 11.3 for an \\nexplanation of the \\ncomponents of \\nx\\n.\\n1\\n2\\n3\\n4\\n5\\n6\\nx\\nx\\nx\\nx\\nx\\nx\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\nx\\n1\\n = max probability\\nx\\n2\\n = correlation\\nx\\n3\\n = contrast\\nx\\n4\\n = uniformity\\nx\\n5\\n = homogeneity\\nx\\n6\\n \\n= entropy\\nFIGURE 12.6\\n Feature \\nvectors with \\ncomponents that \\nare invariant to \\ntransformations \\nsuch as rotation, \\nscaling, and  \\ntranslation. The \\nvector compo-\\nnents are moment  \\ninvariants. \\n11\\n22\\n33\\n44\\n55\\n66\\n77\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\n⎡⎤ ⎡⎤\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n==\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎢⎥ ⎢⎥\\n⎣⎦ ⎣⎦\\nx\\nf\\nf\\nf\\nf\\nf\\nf\\nf\\nThe 's are moment invariants\\nf\\nFIGURE 12.7\\n Pattern (feature) vectors formed by concatenating corresponding pixels from a set of registered images. \\n(Original images courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   909\\n6/16/2017   2:16:27 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 910}),\n",
       " Document(page_content='910\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nusing the approach explained in Section 11.2. The boundary is subdivided into line \\nsegments (denoted by \\nb\\n in the figure), and the interior angle, \\nu\\n,\\n is computed at each \\nintersection of two line segments\\n. A string of sequential symbols is generated as the \\nboundary is traversed in the counterclockwise direction, as the figure shows. \\nStrings\\n \\nof this form are structural patterns, and the objective, as you will see in Section 12.3, \\nis to match a given string against stored string prototypes.\\nA \\ntree\\n is another structural representation, suitable for higher-level descriptions \\nof an entire image in terms of its component regions. Basically, most hierarchical \\nordering schemes lead to tree structures. For example, Fig. 12.9 shows a satellite \\nimage of a heavily built downtown area and surrounding residential areas. Let the \\nsymbol $ represent the root of a tree. The (upside down) tree shown in the ﬁgure \\nwas obtained using the structural relationship “composed of.” Thus, the root of the \\ntree represents the entire image. The next level indicates that the image is composed \\nof a downtown and residential areas. In turn, the residential areas are composed \\nof housing, highways, and shopping malls. The next level down in the tree further \\ndescribes the housing and highways. We can continue this type of subdivision until \\nwe reach the limit of our ability to resolve different regions in the image.\\n12.3  PATTERN CLASSIFICATION BY PROTOTYPE MATCHING  \\nPrototype matching involves comparing an unknown pattern against a set of pro-\\ntotypes, and assigning to the unknown pattern the class of the prototype that is the \\nmost “similar” to the unknown. Each prototype represents a unique pattern class, \\nbut there may be more than one prototype for each class. What distinguishes one \\nmatching method from another is the measure used to determine similarity. \\nMINIMUM-DISTANCE CLASSIFIER\\nOne of the simplest and most widely used prototype matching methods is the \\nminimum-distance classifier\\n which, as its name implies, computes a distance-based  \\nmeasure between an unknown pattern vector and each of the class prototypes. It \\nthen assigns the unknown pattern to the class of its closest prototype. The prototype \\n12.3\\nThe minimum-distance \\nclassiﬁer is also referred \\nto as the \\nnearest-neighbor \\nclassiﬁer\\n. \\n=\\n/midhorizellipsis/midhorizellipsis\\nab u b b\\nb\\nb\\nb\\n/vertellipsis\\n/vertellipsis\\nu\\nDirection of travel\\nSymbol string\\n interior angle\\n=\\nu\\n line segment of specified length\\n=\\nb\\nFIGURE 12.8\\nSymbol string  \\ngenerated from \\na polygonal \\napproximation of \\nthe boundaries of \\nmedicine bottles.\\nDIP4E_GLOBAL_Print_Ready.indb   910\\n6/16/2017   2:16:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 911}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n911\\nvectors of the minimum-distance classifier usually are the mean vectors of the vari-\\nous pattern classes:\\n \\nmx\\nx\\nj\\nj\\nc\\nc\\nn\\njN\\nj\\n==\\n∈\\n∑\\n1\\n12\\n,, ,\\n…\\n \\n(12-2)\\nwhere \\nn\\nj\\n is the number of pattern vectors used to compute the \\nj\\nth mean vector, \\nc\\nj\\n is the \\nj\\nth pattern class, and \\nN\\nc\\n is the number of classes. If we use the Euclidean \\ndistance to determine similarity, the minimum-distance classifier computes the dis-\\ntances\\n \\nDj N\\njj c\\nxx m\\n()\\n=− =\\n/H20648/H20648\\n12\\n,, ,\\n…\\n \\n(12-3)\\nwhere \\n/H20648/H20648\\naa a\\n=\\n()\\nT\\n12\\n is the Euclidean norm. The classifier then assigns an unknown \\npattern \\nx\\n to class \\nc\\ni\\n if \\nDD\\nij\\n() ()\\nxx\\n<\\n for \\njN j i\\nc\\n=\\n12\\n,, , , .\\n…\\n≠\\n Ties [i.e., \\nDD\\nij\\n() () ]\\nxx\\n=\\nare resolved arbitrarily.\\nIt is not difﬁcult to show (see Problem 12.2) that selecting the smallest distance is \\nequivalent to evaluating the functions\\n \\ndj\\nN\\njj\\nT\\nj\\nT\\njc\\nxm x m m\\n()\\n=− =\\n1\\n2\\n12\\n,, ,\\n…\\n \\n(12-4)\\nBuildings\\nDowntown\\nResidential\\nImage\\n$\\nHighways\\nHighways\\nSingle\\nSmall\\nstructures\\nLow\\ndensity\\nMultiple\\nHigh\\ndensitity\\nLarge\\nstructures\\nLoops\\nNumerous\\nintersections\\nWooded\\nareas\\nFew\\nintersections\\nHousing\\nShopping\\nmalls\\nFIGURE 12.9\\n Tree representation of a satellite image showing a heavily built downtown area (Washington, D.C.) and \\nsurrounding residential areas. (Original image courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   911\\n6/16/2017   2:16:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 912}),\n",
       " Document(page_content='912\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nand assigning an unknown pattern \\nx\\n to the class whose prototype yielded the \\nlargest\\n \\nvalue of \\nd\\n.\\n That is, \\nx\\n is assigned to class \\nc\\ni\\n, if\\n \\ndd j N j i\\nij\\nc\\n() () , , , ;\\nxx\\n>=\\n12\\n…\\n≠\\n \\n(12-5)\\nWhen used for recognition, functions of this form are referred to as \\ndecision\\n or \\ndis-\\ncriminant functions\\n. \\nT\\nhe \\ndecision boundary\\n separating class \\nc\\ni\\n from \\nc\\nj\\n is given by the values of \\nx \\nfor \\nwhich \\n \\ndd\\nij\\n() ()\\nxx\\n=\\n  \\n(12-6)\\nor, equivalently, by values of \\nx\\n for which\\n \\ndd\\nij\\n() ()\\nxx\\n−=\\n0  \\n(12-7)\\nThe decision boundaries for a minimum-distance classifier follow directly from this \\nequation and Eq.\\n (12-4):\\n \\nddd\\nij i\\nj\\nij\\nT\\nij\\nT\\nij\\n() () ()\\n()()\\n()\\nxx x\\n=−\\n=− − − +=\\nmmx mm mm\\n1\\n2\\n0\\n \\n(12-8)\\nThe boundary given by Eq. (12-8) is the perpendicular bisector of the line segment \\njoining \\nm\\ni\\n and \\nm\\nj\\n (see Problem 12.3). In 2-D (i.e., \\nn\\n=\\n2)\\n,\\n the perpendicular bisector \\nis a line\\n, for \\nn\\n=\\n3 it is a plane, and for \\nn\\n>\\n3 it is called a \\nhyperplane\\n.\\nEXAMPLE 12.1 :  Illustration of the minimum-distance classiﬁer for two classes in 2-D.\\nFigure 12.10 shows scatter plots of petal width and length values for the classes Iris versicolor and Iris \\nsetosa. As mentioned in the previous section, pattern vectors in the iris database consists of four mea-\\nsurements for each ﬂower. We show only two here so that you can visualize the pattern classes and the \\ndecision boundary between them. We will work with the complete database later in this chapter. \\nWe denote the Iris versicolor and setosa data as classes \\nc\\n1\\n and \\nc\\n2\\n,\\n respectively. The means of the two \\nclasses are \\nm\\n1\\n43 13\\n=\\n()\\n., .\\nT\\n and \\nm\\n2\\n15 03\\n=\\n()\\n., . .\\nT\\n It then follows from Eq. (12-4) that\\n \\nd\\nxx\\nTT\\n11 1 1\\n12\\n1\\n2\\n43 13 1 01\\nxm x m m\\n()\\n=−\\n=+ −\\n.. .\\nand\\n \\nd\\nxx\\nTT\\n22 2 2\\n12\\n1\\n2\\n15 03 11 7\\nxm x m m\\n()\\n=−\\n=+ −\\n...\\nFrom Eq. (12-8), the equation of the boundary is\\n \\nddd\\nxx\\n12 1\\n2\\n12\\n28 10 89 0\\n() () ()\\n...\\nx\\n=−\\n=+− =\\nxx\\n \\nDIP4E_GLOBAL_Print_Ready.indb   912\\n6/16/2017   2:16:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 913}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n913\\nFigure 12.10 shows a plot of this boundary. Substituting any pattern vector from class \\nc\\n1\\n into this equa-\\ntion would yield \\nd\\n12\\n0\\n() .\\nx\\n>\\n Conversely, any pattern from class \\nc\\n2\\n would give \\nd\\n12\\n0\\n() .\\nx\\n<\\n Thus, given an \\nunknown pattern \\nx\\n belonging to one of these two classes, the sign of \\nd\\n12\\n()\\nx\\n would be sufﬁcient to deter-\\nmine the class to which that pattern belongs\\n.\\nThe minimum-distance classiﬁer works well when the distance between means is \\nlarge compared to the spread or randomness of each class with respect to its mean. \\nIn Section 12.4 we will show that the minimum-distance classiﬁer yields optimum \\nperformance (in terms of minimizing the average loss of misclassiﬁcation) when the \\ndistribution of each class about its mean is in the form of a spherical “hypercloud” in \\nn\\n-dimensional pattern space.\\nAs noted earlier, one of the keys to accurate recognition performance is to specify \\nfeatures that are effective discriminators between classes. As a rule, the better the \\nfeatures are at meeting this objective, the better the recognition performance will be. \\nIn the case of the minimum-distance classiﬁer this implies wide separation between \\nmeans and tight grouping of the classes. \\nSystems based on the Banker’s Association E-13B font character are a classic \\nexample of how highly engineered features can be used in conjunction with a simple \\nclassiﬁer to achieve superior results. In the mid-1940s, bank checks were processed \\nmanually, which was a laborious, costly process prone to mistakes. As the volume \\nof check writing increased in the early 1950s, banks became keenly interested in \\nautomating this task. In the middle 1950s, the E-13B font and the system that reads \\nit became the standard solution to the problem. As Fig. 12.11 shows, this font set con-\\nsists of 14 characters laid out on a \\n97\\n×\\n grid. The characters are stylized to maximize \\nthe difference between them.\\n The font was designed to be compact and readable by \\nhumans, but the overriding purpose was that the characters should be readable by \\nmachine, quickly, and with very high accuracy. \\nx\\n2\\nx\\n1\\nPetal width (cm)\\n2.0\\n1.5\\n1.0\\n0.5\\n0\\n01234567\\nPetal length (cm)\\nIris\\nv\\nersicolor\\nIris setosa\\n2.8\\nx\\n1\\n/H11001\\n 1.0\\nx\\n2\\n/H11002\\n 8.9 \\n/H11005\\n 0\\n/H11001 /H11002\\nFIGURE 12.10\\nDecision  \\nboundary of a \\nminimum distance \\nclassiﬁer (based \\non two measure-\\nments) for the \\nclasses of Iris \\nversicolor and Iris \\nsetosa. The dark \\ndot and square \\nare the means of \\nthe two classes.\\nDIP4E_GLOBAL_Print_Ready.indb   913\\n6/16/2017   2:16:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 914}),\n",
       " Document(page_content='914\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nIn addition to a stylized font design, the operation of the reading system is further \\nenhanced by printing each character using an ink that contains ﬁnely ground mag-\\nnetic material. To improve character detectability in a check being read, the ink is \\nsubjected to a magnetic ﬁeld that accentuates each character against the background. \\nThe stylized design further enhances character detectability. The characters are \\nscanned in a horizontal direction with a single-slit reading head that is narrower but \\ntaller than the characters. As a check passes through the head, the sensor produces a \\n1-D electrical signal (a signature) that is conditioned to be proportional to the rate \\nof increase or decrease of the character area under the head. For example, consider \\nthe waveform of the number 0 in Fig. 12.11. As a check moves to the right past the \\nhead, the character area seen by the sensor begins to increase, producing a positive \\nderivative (a positive rate of change). As the right leg of the character begins to pass \\nunder the head, the character area seen by the sensor begins to decrease, produc-\\ning a negative derivative. When the head is in the middle zone of the character, the \\narea remains nearly constant, producing a zero derivative. This waveform repeats \\nitself as the other leg of the character enters the head. The design of the font ensures \\nthat the waveform of each character is distinct from all others. It also ensures that \\nthe peaks and zeros of each waveform occur approximately on the vertical lines of \\nthe background grid on which these waveforms are displayed, as the ﬁgure shows. \\nThe E-13B font has the property that sampling the waveforms only at these (nine) \\nAppropriately, recogni-\\ntion of magnetized char-\\nacters is referred to as \\nMagnetic Ink Character \\nRecognition \\n(MICR)\\n.\\nT\\nr\\na\\nn\\ns\\ni\\nt\\nA\\nm\\no\\nu\\nn\\nt\\nO\\nn\\nU\\ns\\nD\\na\\ns\\nh\\nFIGURE 12.11\\nThe American  \\nBankers  \\nAssociation \\nE-13B font \\ncharacter set and \\ncorresponding \\nwaveforms.\\nDIP4E_GLOBAL_Print_Ready.indb   914\\n6/16/2017   2:16:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 915}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n915\\npoints yields enough information for their accurate classiﬁcation. The effectiveness \\nof these highly engineered features is further reﬁned by the magnetized ink, which \\nresults in clean waveforms with almost no scatter.\\nDesigning a minimum-distance classiﬁer for this application is straightforward. \\nWe simply store the sample values of each waveform at the vertical lines of the grid,  \\nand let each set of the resulting samples be represented as a 9-D prototype vector, \\nm\\nj\\nj\\n,, , , .\\n=\\n12 1 4\\n…\\n When an unknown character is to be classiﬁed, the approach is \\nto scan it in the manner just described,\\n express the grid samples of the waveform as \\na 9-D vector, \\nx\\n, and identify its class by selecting the class of the prototype vector \\nthat yields the highest value in Eq. (12-4). We do not even need a computer to do \\nthis. Very high classiﬁcation speeds can be achieved with analog circuits composed \\nof resistor banks (see Problem 12.4).\\nThe most important lesson in this example is that a recognition problem often can \\nbe made trivial if we can control the environment in which the patterns are gener-\\nated. The development and implementation of the E13-B font reading system is a \\nstriking example of this fact. On the other hand, this system would be inadequate if \\nwe added the requirement that it has to recognize the textual content and signature \\nwritten on each check. For this, we need systems that are signiﬁcantly more complex, \\nsuch as the convolutional neural networks we will discuss in Section 12.6.\\nUSING CORRELATION FOR 2-D PROTOTYPE MATCHING\\nWe introduced the basic idea of spatial correlation and convolution in Section 3.4, \\nand used these concepts extensively in Chapter 3 for spatial filtering. From Eq. (3-34), \\nwe know that correlation of a kernel \\nw\\n with an image \\nfx y\\n(,\\n)\\n is given by\\n \\n(w\\nw(\\n/H22845\\nfx y s t f xs yt\\nt s\\n)( , )\\n, ) ( , )\\n=+\\n+\\n∑ ∑\\n \\n(12-9)\\nwhere the limits of summation are taken over the region shared by \\nw\\n and \\nf\\n.\\n This \\nequation is evaluated for all values of the displacement variables \\nx\\n and \\ny\\n so all ele-\\nments of \\nw\\n visit every pixel of \\nf\\n.\\n As you know, correlation has its highest value(s) \\nin the region(s) where \\nf\\n and \\nw\\n are equal or nearly equal. In other words, Eq. (12-9) \\nfinds locations where \\nw\\n \\nmatc\\nhes\\n a region of \\nf\\n.\\n But this equation has the drawback \\nthat the result is sensitive to changes in the amplitude of either function.\\n In order \\nto normalize correlation to amplitude changes in one or both functions, we perform \\nmatching using the \\ncorrelation coefficient\\n instead:\\n \\ng\\n(, )\\n(,) ( , )\\n(,) ( ,\\nxy\\nst f x sy t f\\nst\\nf x sy\\nxy\\nt s\\n=\\n[]\\n++ −\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n[]\\n++\\n∑ ∑\\nww\\nww\\n-\\n2\\nt\\ntf\\nxy\\nt s\\nt s\\n)\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n∑ ∑\\n∑ ∑\\n2\\n1\\n2\\n \\n(12-10)\\nwhere the limits of summation are taken over the region shared by \\nw\\n and \\nf\\n, \\nw\\n is the \\naverage value of the kernel (computed only once),\\n and \\nf\\nxy\\n is the average value of \\nf\\n in \\nthe region coincident with \\nw\\n.\\n In image correlation work, \\nw\\n is often referred to as a \\ntemplate\\n (i.e\\n., a prototype subimage) and correlation is referred to as \\ntemplate matching\\n. \\nTo be formal, we should \\nrefer to correlation (and \\nthe correlation  \\ncoefﬁcient) as \\ncross-\\ncorrelation\\n when the \\nfunctions are different, \\nand as \\nautocorrelation\\n \\nwhen they are the same. \\nHowever, it is customary \\nto use the generic term \\ncorrelation\\n and  \\ncorrelation coefﬁcient,\\n \\nexcept when the distinc-\\ntion is important (as in \\nderiving equations, in \\nwhich it makes a dif-\\nference which is being \\napplied).\\nDIP4E_GLOBAL_Print_Ready.indb   915\\n6/16/2017   2:16:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 916}),\n",
       " Document(page_content='916\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nIt can be shown (see Problem 12.5) that \\ng\\n(,\\n)\\nxy\\n has values in the range \\n[, ]\\n−\\n11\\n and is thus \\nnormalized to changes in the amplitudes of \\nw\\n and \\nf\\n.\\n The maximum value of \\ng\\n occurs \\nwhen the normalized \\nw\\n and the corresponding normalized region in \\nf\\n are identical. \\nT\\nhis indicates \\nmaximum correlation\\n (the best possible match). The \\nminimum\\n occurs \\nwhen the two normalized functions exhibit the least similarity in the sense of Eq. (12-10). \\nFigure 12.12 illustrates the mechanics of the procedure just described. The border \\naround image \\nf\\n is padding, as explained in Section 3.4. In template matching, values \\nof correlation when the center of the template is past the border of the image gener\\n-\\nally are of no interest, so the padding is limited to half the kernel width.\\nThe template in Fig. 12.12 is of size \\nmn\\n×\\n,\\n and it is shown with its center at an \\narbitrary location \\n(,) .\\nxy\\n The value of the correlation coefﬁcient at that point is com-\\nputed using Eq.\\n (12-10). Then, the center of the template is incremented to an adja-\\ncent location and the procedure is repeated. Values of the correlation coefﬁcient \\ng\\n(,\\n)\\nxy\\n are obtained by moving the center of the template (i.e., by incrementing \\nx\\n \\nand \\ny\\n) so the center of \\nw\\n visits every pixel in \\nf\\n.\\n At the end of the procedure, we \\nlook for the maximum in \\ng\\n(,\\n)\\nxy\\n to ﬁnd where the best match occurred. It is possible \\nto have multiple locations in \\ng\\n(,\\n)\\nxy\\n with the same maximum value, indicating sev-\\neral matches between \\nw\\n and \\nf\\n.\\nEXAMPLE 12.2 :  Matching by correlation.\\nFigure 12.13(a) shows a \\n913 913\\n×\\n satellite image of 1992 Hurricane Andrew, in which the eye of the \\nstorm is clearly visible\\n. We want to use correlation to ﬁnd the location of the best match in Fig. 12.13(a) \\nof the template in Fig. 12.13(b), which is a \\n31 1\\n×3\\n subimage of the eye of the storm. Figure 12.13(c) \\nshows the result of computing the correlation coefﬁcient in Eq.\\n (12-10) for all values of \\nx\\n and \\ny\\n in \\nthe original image. The size of this image was \\n943 943\\n×\\n pixels due to padding (see Fig. 12.12), but we \\ncropped it to the size of the original image for display\\n. The intensity in this image is proportional to the \\ncorrelation values, and all negative correlations were clipped at 0 (black) to simplify the visual analysis \\nof the image. The area of highest correlation values appears as a small white region in this image. The \\nbrightest point in this region matches with the center of the eye of the storm. Figure 12.13(d) shows as a \\n(\\nn\\n/H11002\\n 1)\\n/\\n2\\n(\\nm\\n/H11002\\n 1)\\n/\\n2\\nOrigin\\nPadding\\nImage,\\nf\\nTemplate\\nw\\ncentered at an arbitrary\\nlocation (\\nx\\n,\\ny\\n)\\n(\\nx\\n,\\ny\\n)\\nn\\nm\\nFIGURE 12.12\\nThe mechanics of \\ntemplate  \\nmatching.\\nDIP4E_GLOBAL_Print_Ready.indb   916\\n6/16/2017   2:16:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 917}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n917\\nwhite dot the location of this maximum correlation value (in this case there was a unique match whose \\nmaximum value was 1), which we see corresponds closely with the location of the eye in Fig. 12.13(a). \\nMATCHING SIFT FEATURES\\nWe discussed the scale-invariant feature transform (SIFT) in Section 11.7. SIFT \\ncomputes a set of invariant features that can be used for matching between known \\n(prototype) and unknown images. The SIFT implementation in Section 11.7 yields \\n128-dimensional feature vectors for each local region in an image. SIFT performs \\nmatching by looking for correspondences between sets of stored feature vector pro-\\ntotypes and feature vectors computed for an unknown image. Because of the large \\nnumber of features involved, searching for exact matches is computationally inten-\\nsive. Instead, the approach is to use a best-bin-first method that can identify the near-\\nest neighbors with high probability using only a limited amount of computation (see \\nLowe [1999], [2004]). The search is further simplified by looking for clusters of poten-\\ntial solutions using the generalized Hough transform proposed by Ballard [1981]. We \\nb a\\nd c\\n \\nFIGURE 12.13\\n(a) 913 913\\n×\\n \\nsatellite image \\nof Hurricane \\nAndrew\\n.  \\n(b) \\n31 31\\n×\\n  \\ntemplate of the \\neye of the storm.\\n  \\n(c) Correlation \\ncoefﬁcient shown \\nas an image (note \\nthe brightest \\npoint, indicated \\nby an arrow). \\n(d) Location of \\nthe best match \\n(identiﬁed by the \\narrow). This point \\nis a single pixel, \\nbut its size was \\nenlarged to make \\nit easier to see. \\n(Original image \\ncourtesy of \\nNOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   917\\n6/16/2017   2:16:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 918}),\n",
       " Document(page_content='918\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nknow from the discussion in Section 10.2 that the Hough transform simplifies looking \\nfor data patterns by utilizing bins that reduce the level of detail with which we look at \\na data set. We already discussed the SIFT algorithm in Section 11.7. The focus in this \\nsection is to further illustrate the capabilities of SIFT for prototype matching. \\nFigure 12.14 shows the circuit board image we have used several times before. \\nThe small rectangle enclosing the rightmost connector on the top of the large image \\nidentiﬁes an area from which an image of the connector was extracted. The small \\nimage is shown zoomed for clarity.  The sizes of the large and small images are shown \\nin the ﬁgure caption. Figure 12.15 shows the keypoints found by SIFT, as explained \\nin Section 11.7. They are visible as faint lines on both images. The zoomed view of \\nthe subimage shows them a little clearer. It is important to note that the keypoints \\nfor the image and subimage were found independently by SIFT.  The large image \\nhad 2714 keypoints, and the small image had 35. \\nFigure 12.16 shows the matches between keypoints found by SIFT. A total of 41 \\nmatches were found between the two images. Because there are only 35 keypoints \\nFIGURE 12.15\\nKeypoints found \\nby SIFT. The \\nlarge image has \\n2714 keypoints \\n(visible as faint \\ngray lines). The \\nsubimage has 35 \\nkeypoints. This is \\na separate image, \\nand SIFT found \\nits keypoints inde-\\npendently of the \\nlarge image. The \\nzoomed section is \\nshown for clarity.\\nFIGURE 12.14\\nCircuit board \\nimage of size \\n948 915\\n×\\n pixels, \\nand a subimage \\nof one of the \\nconnectors\\n. The \\nsubimage is of size \\n212 128\\n×\\n pixels, \\nshown zoomed \\non the right for \\nclarity\\n. (Original \\nimage courtesy of \\nMr. Joseph E.  \\nPascente, Lixi, \\nInc.)\\nDIP4E_GLOBAL_Print_Ready.indb   918\\n6/16/2017   2:16:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 919}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n919\\nin the small image, obviously at least six matches are either incorrect, or there are \\nmultiple matches. Three of the errors are clearly visible as matches with connectors \\nin the middle of the large image. However, if you compare the shape of the connec-\\ntors in the middle of the large image, you can see that they are virtually identical to \\nparts\\n of the connectors on the right. Therefore, these errors can be explained on that \\nbasis. The other three extra matches are easier to explain. All connectors on the top \\nright of the circuit board are identical, and we are comparing one of them against \\nthe rest. There is no way for a system to tell the difference between them. In fact, by \\nlooking at the connecting lines, we can see that the matches are between the subim-\\nage and all ﬁve connectors. These in fact are correct matches between the subimage \\nand other connectors that are identical to it.\\nMATCHING STRUCTURAL PROTOTYPES\\nThe techniques discussed up to this point deal with patterns quantitatively, and \\nlargely ignore any structural relationships inherent in pattern shapes. The methods \\ndiscussed in this section seek to achieve pattern recognition by capitalizing precisely \\non these types of relationships. In this section, we introduce two basic approaches \\nfor the recognition of boundary shapes based on string representations, which are \\nthe most practical approach in structural pattern recognition.\\nMatching Shape Numbers\\nA procedure similar in concept to the minimum-distance classifier introduced ear-\\nlier for pattern vectors can be formulated for comparing region boundaries that are \\nErrors\\nFIGURE 12.16\\nMatches found by \\nSIFT between the \\nlarge and small \\nimages. A total of \\n41 matching pairs \\nwere found. They \\nare shown  \\nconnected by \\nstraight lines. \\nOnly three of the \\nmatches were \\n“real” errors \\n(labeled “Errors” \\nin the ﬁgure).\\nDIP4E_GLOBAL_Print_Ready.indb   919\\n6/16/2017   2:16:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 920}),\n",
       " Document(page_content='920\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\ndescribed by shape numbers. With reference to the discussion in Section 11.3, the \\ndegree of similarity\\n, \\nk\\n,\\n between two region boundaries, is defined as the largest order \\nfor which their shape numbers still coincide\\n. For example, let \\na\\n and \\nb\\n denote shape \\nnumbers of closed boundaries represented by 4-directional chain codes\\n. These two \\nshapes have a degree of similarity \\nk\\n if\\n \\nsa sb\\nj k\\nsa\\nsb\\nj k k\\njj\\njj\\n()\\n=\\n()\\n=\\n()\\n≠\\n()\\n=+ +\\nfo\\nra\\nn\\nd\\nfor\\n468\\n24\\n,,, , ;\\n,,\\n…\\n…\\n \\n(12-11)\\nwhere \\ns\\n indicates shape number, and the subscript indicates shape order. The \\ndis-\\ntance\\n between two shapes \\na\\n and \\nb\\n is defined as the inverse of their degree of simi-\\nlarity:\\n \\nDa b\\nk\\n,\\n()\\n=\\n1\\n \\n(12-12)\\nThis expression satisfies the following properties:\\n \\nDa b\\nDa\\nba\\nb\\nDa c Da b Db c\\n,\\n,\\n, max , , ,\\n()\\n≥\\n()\\n==\\n()\\n≤\\n()()\\n⎡\\n⎣\\n0\\n0 if and only if\\n⎤ ⎤\\n⎦\\n \\n(12-13)\\nEither \\nk\\n or \\nD\\n may be used to compare two shapes. If the degree of similarity is used, \\nthe larger \\nk\\n is, the more similar the shapes are (note that \\nk\\n is infinite for identical \\nshapes).\\n The reverse is true when Eq. (12-12) is used.\\nEXAMPLE 12.3 :  Matching shape numbers.\\nSuppose we have a shape, \\nf\\n,\\n and want to ﬁnd its closest match in a set of ﬁve shape prototypes, denoted \\nby \\na\\n, \\nb\\n, \\nc\\n, \\nd,\\n \\nand \\ne\\n, as shown in Fig. 12.17(a). The search may be visualized with the aid of the \\nsimilarity \\ntree\\n in Fig. 12.17(b). The root of the tree corresponds to the lowest possible degree of similarity, which \\nis 4. Suppose shapes are identical up to degree 8, with the exception of shape \\na\\n, whose degree of simi-\\nlarity with respect to all other shapes is 6. Proceeding down the tree, we ﬁnd that shape \\nd\\n has degree of \\nsimilarity 8 with respect to all others, and so on. Shapes \\nf\\n and \\nc\\n match uniquely, having a higher degree \\nof similarity than any other two shapes. Conversely, if \\na\\n had been an unknown shape, all we could have \\nsaid using this method is that \\na\\n was similar to the other ﬁve shapes with degree of similarity 6. The same \\ninformation can be summarized in the form of the \\nsimilarity matrix\\n in Fig. 12.17(c).\\nString Matching\\nSuppose two region boundaries, \\na\\n and \\nb,\\n are coded into strings of symbols, denot-\\ned as \\naa a\\nn\\n12\\n…\\n and \\nbb b\\nm\\n12\\n…\\n,\\n respectively. Let \\na\\n represent the number of matches \\nbetween the two strings\\n, where a match occurs in the \\nk\\nth position if \\nab\\nkk\\n=\\n.\\n The \\nnumber of symbols that do not match is\\n \\nba\\n=\\n()\\n−\\nmax ,\\nab\\n \\n(12-14)\\nParameter \\nj\\n starts at \\n4 and is always even \\nbecause we are working \\nwith 4-connectivity, and \\nwe require that  \\nboundaries be closed.\\nDIP4E_GLOBAL_Print_Ready.indb   920\\n6/16/2017   2:16:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 921}),\n",
       " Document(page_content='12.3\\n  \\nPattern Classiﬁcation by Prototype Matching\\n    \\n921\\nwhere \\narg\\n is the length (number of symbols) of string in the argument. It can be \\nshown that \\nb\\n=\\n0  if and only if \\na\\n and \\nb\\n are identical (see Problem 12.7).\\nAn effective measure of similarity is the ratio\\n \\nR\\nab\\n==\\n()\\n−\\na\\nb\\na\\na\\nmax ,\\n \\n(12-15)\\nWe see that \\nR\\n is infinite for a perfect match and 0 when none of the corresponding \\nsymbols in \\na\\n and \\nb\\n match \\n(\\na\\n=\\n0\\n in this case). Because matching is done symbol by \\nsymbol,\\n the starting point on each boundary is important in terms of reducing the \\namount of computation required to perform a match. Any method that normalizes \\nto, or near, the same starting point is helpful if it provides a computational advan-\\ntage over brute-force matching, which consists of starting at arbitrary points on each \\nstring, then shifting one of the strings (with wraparound) and computing Eq. (12-15) \\nfor each shift. The largest value of \\nR\\n gives the best match.\\nEXAMPLE 12.4 : String matching.\\nFigures 12.18(a) and (b) show sample boundaries from each of two object classes, which were approxi-\\nmated by a polygonal ﬁt (see Section 11.2). Figures 12.18(c) and (d) show the polygonal approximations \\nRefer to Section 11.2 \\nfor examples of how the \\nstarting point of a curve \\ncan be normalized.\\n4\\nDegree\\nabcdef\\na\\na\\nb\\nc\\nd\\ne\\nf\\nbcdef\\n66666\\n8 8 10 8\\n88 1 2\\n88\\n8\\nabcdef\\nbcdef\\na\\na\\na\\ncf\\ncf\\nbe\\nbe\\nd\\nd\\nad c f b e\\na\\nb\\nc\\nd\\ne\\nf\\n6\\n8\\n10\\n12\\n14\\nb\\na\\nc\\nFIGURE 12.17\\n(a) Shapes.  \\n(b) Similarity \\ntree. (c) Similarity \\nmatrix. \\n(Bribiesca and \\nGuzman.)\\nDIP4E_GLOBAL_Print_Ready.indb   921\\n6/16/2017   2:16:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 922}),\n",
       " Document(page_content='922\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\ncorresponding to the boundaries in Figs. 12.18(a) and (b), respectively. Strings were formed from the \\npolygons by computing the interior angle, \\nu\\n,\\n between segments as each polygon was traversed clock-\\nwise\\n. Angles were coded into one of eight possible symbols, corresponding to multiples of \\n45\\n°\\n;\\n that is, \\naua u a u\\n12 8\\n0 45 45 90 315 360\\n:; : ; ; : .\\n°< ≤ ° °< ≤ ° °< ≤ °\\n…\\nFigure 12.18(e) shows the results of computing the measure \\nR\\n for six samples of object 1 against \\nthemselves\\n. The entries are values of \\nR\\n and, for example, the notation 1.c refers to the third string from \\nobject class 1. Figure 12.18(f) shows the results of comparing the strings of the second object class \\nagainst themselves. Finally, Fig. 12.18(g) shows the \\nR\\n values obtained by comparing strings of one class \\nagainst the other. These values of \\nR\\n are signiﬁcantly smaller than any entry in the two preceding tabu-\\nlations. This indicates that the \\nR\\n measure achieved a high degree of discrimination between the two \\nclasses of objects. For example, if the class of string 1.a had been unknown, the \\nsmallest\\n value of \\nR\\n result-\\ning from comparing this string against sample (prototype) strings of class 1 would have been 4.7 [see \\nFig. 12.18(e)]. By contrast, the \\nlargest\\n value in comparing it against strings of class 2 would have been \\n1.24 [see Fig. 12.18(g)]. This result would have led to the conclusion that string 1.a is a member of object \\nclass 1. This approach to classiﬁcation is analogous to the minimum-distance classiﬁer introduced earlier.\\nR\\n1.a 1.b 1.c 1.d 1.e 1.f\\n1.a\\n1.b 16.0\\n1.c 9.6 26.3\\n1.d 5.1 8.1 10.3\\n1.e 4.7 7.2 10.3 14.2\\n1.f 4.7 7.2 10.3 8.4 23.7\\nR\\n2.a 2.b 2.c 2.d 2.e 2.f\\n2.a\\n2.b 33.5\\n2.c 4.8 5.8\\n2.d 3.6 4.2 19.3\\n2.e 2.8 3.3 9.2 18.3\\n2.f 2.6 3.0 7.7 13.5 27.0\\nR\\n1.a 1.b 1.c 1.d 1.e 1.f\\n2.a 1.24 1.50 1.32 1.47 1.55 1.48\\n2.b 1.18 1.43 1.32 1.47 1.55 1.48\\n2.c 1.02 1.18 1.19 1.32 1.39 1.48\\n2.d 1.02 1.18 1.19 1.32 1.29 1.40\\n2.e 0.93 1.07 1.08 1.19 1.24 1.25\\n2.f 0.89 1.02 1.02 1.24 1.22 1.18\\nb a\\nd c\\nf e\\ng\\nFIGURE 12.18\\n(a) and (b) sample  \\nboundaries of two \\ndifferent object \\nclasses; (c) and (d) \\ntheir corresponding \\npolygonal  \\napproximations; \\n(e)–(g) tabulations \\nof \\nR\\n. \\n(Sze and Yang.)\\nDIP4E_GLOBAL_Print_Ready.indb   922\\n6/16/2017   2:16:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 923}),\n",
       " Document(page_content='12.4\\n  \\nOptimum (Bayes) Statistical Classiﬁers\\n    \\n923\\n12.4  OPTIMUM (BAYES) STATISTICAL CLASSIFIERS  \\nIn this section, we develop a probabilistic approach to pattern classification. As is \\ntrue in most fields that deal with measuring and interpreting physical events, prob-\\nability considerations become important in pattern recognition because of the ran-\\ndomness under which pattern classes normally are generated. As shown in the fol-\\nlowing discussion, it is possible to derive a classification approach that is optimal in \\nthe sense that, on average, it yields the lowest probability of committing classifica-\\ntion errors (see Problem 12.12).\\nDERIVATION OF THE BAYES CLASSIFIER\\nThe probability that a pattern vector \\nx\\n comes from class \\nc\\ni\\n is denoted by \\npc\\ni\\nx\\n()\\n.\\n If \\nthe pattern classifier decides that \\nx\\n came from class \\nc\\nj\\n when it actually came from \\nc\\ni\\n \\nit incurs a \\nloss\\n (to be defined shortly), denoted by \\nL\\nij\\n.\\n Because pattern \\nx\\n may belong \\nto any one of \\nN\\nc\\n possible classes, the average loss incurred in assigning \\nx\\n to class \\nc\\nj\\n is\\n \\nrL p c\\njk\\nj\\nk\\nN\\nk\\nc\\nxx\\n()\\n=\\n()\\n=\\n∑\\n1\\n \\n(12-16)\\nQuantity \\nr\\nj\\n()\\nx\\n is called the \\nconditional average risk\\n or \\nloss\\n in decision-theory termi-\\nnology\\n.\\nWe know from Bayes’ rule that \\npab papba pb\\n() ( ) ()( ) ,\\n=\\n[]\\n so we can write Eq. \\n(12-16) as\\n \\nr\\np\\nLp c P c\\njk\\nj\\nk\\nN\\nkk\\nc\\nx\\nx\\nx\\n()\\n=\\n()\\n() ( )\\n=\\n∑\\n1\\n1\\n \\n(12-17)\\nwhere \\npc\\nk\\nx\\n()\\n is the probability density function (PDF) of the patterns from class \\nc\\nk\\n,\\n and \\nPc\\nk\\n()\\n is the probability of occurrence of class \\nc\\nk\\n (sometimes \\nPc\\nk\\n()\\n is referred \\nto as the \\na priori\\n,\\n or simply the \\nprior\\n, \\nprobability\\n). Because \\n1\\np\\n()\\nx\\n is positive and \\ncommon to all the \\nrj N\\njc\\nx\\n()\\n=\\n,, , , ,\\n12\\n…\\n it can be dropped from Eq. (12-17) without \\naffecting the relative order of these functions from the smallest to the largest value\\n. \\nThe expression for the average loss then reduces to\\n \\nrL p c P c\\njk\\nj\\nk\\nN\\nkk\\nc\\nxx\\n()\\n=\\n() ( )\\n=\\n∑\\n1\\n \\n(12-18)\\nGiven an unknown pattern, the classifier has \\nN\\nc\\n possible classes from which to \\nchoose. If the classifier computes \\nrr r\\nN\\nc\\n12\\n() , () , , ()\\nxx x\\n…\\n for each pattern \\nx\\n and\\n \\nassigns the pattern to the class with the smallest loss, the total average loss with \\nrespect to all decisions will be minimum. The classifier that minimizes the total \\naverage loss is called the \\nBayes classifier\\n. This classifier assigns an unknown pat-\\ntern \\nx\\n to class \\nc\\ni\\n if \\nrr\\nij\\n() ()\\nxx\\n<\\n for \\njN j i\\nc\\n=≠\\n12\\n,, , ; .\\n…\\n In other words, \\nx\\n is assigned\\n \\nto class \\nc\\ni\\n if\\n \\nLp c Pc Lp c Pc\\nki k\\nk\\nN\\nkq\\nj q q\\nq\\nN\\ncc\\nxx\\n() ( )\\n<\\n(\\n)\\n(\\n)\\n==\\n∑∑\\n11\\n \\n(12-19)\\n12.4\\nDIP4E_GLOBAL_Print_Ready.indb   923\\n6/16/2017   2:16:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 924}),\n",
       " Document(page_content='924\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nfor all \\njj i\\n;.\\n≠\\n The loss for a correct decision generally is assigned a value of 0, and \\nthe loss for any incorrect decision usually is assigned a value of 1.\\n Then, the loss \\nfunction becomes\\n \\nL\\nij ij\\n=−\\n1\\nd\\n \\n(12-20)\\nwhere \\nd\\nij\\n=\\n1\\n if \\nij\\n=\\n,\\n and \\nd\\nij\\n=\\n0\\n if \\nij\\n≠\\n.\\n Equation (12-20) indicates a loss of unity for \\nincorrect decisions and a loss of zero for correct decisions\\n. Substituting Eq. (12-20)\\ninto Eq. (12-18) yields\\n \\nrp\\nc\\nP\\nc\\npp c P c\\njk\\nj k\\nk\\nk\\nN\\njj\\nc\\nxx\\nxx\\n()\\n=−\\n()\\n() ( )\\n=\\n()\\n−\\n() ( )\\n=\\n∑\\n1\\n1\\nd\\n \\n(12-21)\\nThe Bayes classifier then assigns a pattern \\nx\\n to class \\nc\\ni\\n if, for all \\nji\\n≠\\n,\\n \\npp c P c pp c P c\\nii\\nj j\\nxx xx\\n()\\n−\\n() ( )\\n<\\n()\\n−\\n() ( )\\n \\n(12-22)\\nor, equivalently, if\\n \\npc P c pc P c j Nj i\\nii j j\\nc\\nxx\\n() ( )\\n>\\n() ( )\\n=≠\\n12\\n,, , ;\\n…\\n \\n(12-23)\\nThus, the Bayes classifier for a 0-1 loss function computes \\ndecision functions\\n of the \\nform\\n \\ndp c P c j N\\njj j c\\nxx\\n()\\n=\\n() ( )\\n=\\n12\\n,, ,\\n…\\n \\n(12-24)\\nand assigns a pattern to class \\nc\\ni\\n if \\ndx dx\\nij\\n() ()\\n>\\n for all \\nji\\n≠\\n.\\n This is exactly the same \\nprocess described in Eq.\\n (12-5), but we are now dealing with decision functions that \\nhave been shown to be optimal in the sense that they minimize the average loss in \\nmisclassiﬁcation.\\nFor the optimality of Bayes decision functions to hold, the probability density \\nfunctions of the patterns in each class, as well as the probability of occurrence of \\neach class, must be known. The latter requirement usually is not a problem. For \\ninstance, if all classes are equally likely to occur, then \\nPc N\\njc\\n() .\\n=\\n1\\n Even if this con-\\ndition is not true\\n, these probabilities generally can be inferred from knowledge of \\nthe problem. Estimating the probability density functions \\npc\\nj\\n()\\nx\\n is more difﬁcult. If \\nthe pattern vectors are \\nn\\n-dimensional,\\n then \\npc\\nj\\n()\\nx\\n is a function of \\nn\\n variables\\n. If the \\nform of \\npc\\nj\\n()\\nx\\n is not known, estimating it requires using multivariate estimation \\nmethods\\n. These methods are difﬁcult to apply in practice, especially if the number \\nof representative patterns from each class is not large, or if the probability density \\nfunctions are not well behaved. For these reasons, uses of the Bayes classiﬁer often \\nare based on assuming an analytic expression for the density functions. This in turn \\nreduces the problem to one of estimating the necessary parameters from sample \\npatterns from each class using training patterns. By far, the most prevalent form \\nassumed for \\npc\\nj\\n()\\nx\\n is the Gaussian probability density function. The closer this \\nassumption is to reality\\n, the closer the Bayes classiﬁer approaches the minimum \\naverage loss in classiﬁcation.\\nDIP4E_GLOBAL_Print_Ready.indb   924\\n6/16/2017   2:16:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 925}),\n",
       " Document(page_content='12.4\\n  \\nOptimum (Bayes) Statistical Classiﬁers\\n    \\n925\\nBAYES CLASSIFIER FOR GAUSSIAN PATTERN CLASSES\\nTo begin, let us consider a 1-D problem \\n()\\nn\\n=\\n1\\n involving two pattern classes \\n()\\nN\\nc\\n=\\n2  \\ngoverned by Gaussian densities\\n, with means \\nm\\n1\\n and \\nm\\n2\\n,\\n and standard deviations \\ns\\n1\\n \\nand \\ns\\n2\\n, respectively. From Eq. (12-24) the Bayes decision functions have the form\\n \\ndx p x cP c\\neP c j\\njj j\\nj\\nxm\\nj\\nj\\nj\\n()\\n=\\n() ( )\\n=\\n()\\n=\\n−\\n−\\n(\\n)\\n1\\n2\\n12\\n2\\n2\\n2\\nps\\ns\\n,\\n \\n(12-25)\\nwhere the patterns are now scalars, denoted by \\nx\\n.\\n Figure 12.19 shows a plot of the \\nprobability density functions for the two classes\\n. The boundary between the two \\nclasses is a single point, \\nx\\n0\\n,\\n such that \\ndx dx\\n10 20\\n() () .\\n=\\n If the two classes are equally \\nlikely to occur\\n, then \\nPc Pc\\n() () ,\\n12\\n12\\n==\\n and the decision boundary is the value of \\nx\\n0\\n for which \\npx c px c\\n() () .\\n01 02\\n=\\n This point is the intersection of the two probabil-\\nity density functions\\n, as shown in Fig. 12.19. Any pattern (point) to the right of \\nx\\n0\\n is \\nclassified as belonging to class \\nc\\n1\\n.\\n Similarly, any pattern to the left of \\nx\\n0\\n is classified \\nas belonging to class \\nc\\n2\\n.\\n When the classes are not equally likely to occur, \\nx\\n0\\n moves to \\nthe left if class \\nc\\n1\\n is more likely to occur or, conversely, it moves to the right if class \\nc\\n2\\n is more likely to occur. This result is to be expected, because the classifier is trying \\nto minimize the loss of misclassification. For instance, in the extreme case, if class \\nc\\n2\\n \\nnever occurs, the classifier would never make a mistake by always assigning all pat-\\nterns to class \\nc\\n1\\n (that is, \\nx\\n0\\n would move to negative infinity).\\nIn the \\nn\\n-dimensional case, the Gaussian density of the vectors in the \\nj\\nth pattern \\nclass has the form\\n \\npc\\ne\\nj\\nn\\nj\\nj\\nT\\njj\\nx\\nC\\nxm C xm\\n()\\n=\\n()\\n−−\\n(\\n)\\n−\\n(\\n)\\n−\\n1\\n2\\n2\\n12\\n1\\n2\\n1\\np\\n \\n(12-26)\\nwhere each density is specified completely by its mean vector \\nm\\nj\\n and covariance \\nmatrix \\nC\\nj\\n, which are defined as\\nYou may ﬁnd it helpful \\nto review the tutorial on \\nprobability available in \\nthe book website.\\nProbability density\\nm\\n2\\nm\\n1\\nx\\nx\\n0\\n1\\n()\\npxc\\n2\\n()\\npxc\\nFIGURE 12.19\\nProbability  \\ndensity functions \\nfor two 1-D  \\npattern classes. \\nPoint \\nx\\n0\\n (at the \\nintersection of the \\ntwo curves) is the \\nBayes decision \\nboundary if the \\ntwo classes are \\nequally likely to \\noccur.\\nDIP4E_GLOBAL_Print_Ready.indb   925\\n6/16/2017   2:16:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 926}),\n",
       " Document(page_content='926\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n \\nmx\\njj\\nE\\n=\\n{}\\n \\n(12-27)\\nand\\n \\nCx m x m\\njj j j\\nT\\nE\\n=−\\n()\\n−\\n()\\n{}\\n \\n(12-28)\\nwhere \\nE\\nj\\n{}\\n⋅\\n is the expected value of the argument over the patterns of class \\nc\\nj\\n.\\n In \\nEq.\\n (12-26), \\nn\\n is the dimensionality of the pattern vectors, and \\nC\\nj\\n is the determinant \\nof matrix \\nC\\nj\\n.\\n Approximating the expected value \\nE\\nj\\n by the sample average yields an \\nestimate of the mean vector and covariance matrix:\\n \\nmx\\nx\\nj\\nj\\nc\\nn\\nj\\n=\\n∈\\n∑\\n1\\n \\n(12-29)\\nand\\n \\nCx x m m\\nx\\nj\\nj\\nT\\njj\\nT\\nc\\nn\\nj\\n=−\\n∈\\n∑\\n1\\n \\n(12-30)\\nwhere \\nn\\nj\\n is the number of sample pattern vectors from class \\nc\\nj\\n and the summation \\nis taken over these vectors. We will give an example later in this section of how to \\nuse these two expressions.\\nThe covariance matrix is symmetric and positive semideﬁnite. Its \\nk\\nth diagonal ele-\\nment is the variance of the \\nk\\nth element of the pattern vectors. The \\nkj\\nth off-diagonal \\nmatrix element is the covariance of elements \\nx\\nk\\n and \\nx\\nj\\n in these vectors. The multi-\\nvariate Gaussian density function reduces to the product of the univariate Gauss-\\nian density of each element of \\nx\\n when the off-diagonal elements of the covariance \\nmatrix are zero, which happens when the vector elements \\nx\\nk\\n and \\nx\\nj\\n are uncorrelated.\\nFrom Eq. (12-24), the Bayes decision function for class \\nc\\nj\\n is \\ndp c P c\\njj j\\n() ( )( ) .\\nxx\\n=\\n \\nHowever\\n, the exponential form of the Gaussian density allows us to work with the \\nnatural logarithm of this decision function, which is more convenient. In other words, \\nwe can use the form\\n \\ndp c P c\\npc P c\\njj\\nj\\njj\\nxx\\nx\\n()\\n=\\n() ( )\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n()\\n+\\n()\\nln\\nln ln\\n \\n(12-31)\\nThis expression is equivalent to Eq. (12-24) in terms of classification performance \\nbecause the logarithm is a monotonically increasing function.\\n That is, the numerical \\norder of the decision functions in Eqs. (12-24) and (12-31) is the same. Substituting \\nEq. (12-26) into Eq. (12-31) yields\\n \\ndP c\\nn\\njj\\nj j\\nT\\njj\\nxC\\nx\\nm\\nC\\nx\\nm\\n()\\n=\\n()\\n−− −\\n() ()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\nln ln ln\\n2\\n2\\n1\\n2\\n1\\n2\\n1\\np\\n−−\\n \\n(12-32)\\nThe term \\nn\\n22\\n()\\nln\\np\\n is the same for all classes, so it can be eliminated from Eq. \\n(12-32),\\n which then becomes\\n \\ndP c\\njj j j\\nT\\njj\\nxC x\\nm\\nC\\nx\\nm\\n()\\n=\\n()\\n−−\\n() ()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\nln ln\\n1\\n2\\n1\\n2\\n1\\n−−\\n \\n(12-33)\\nAs noted in Section 6.7 \\n[see Eq. (6-49)], the \\nsquare root of the \\nrightmost term in this \\nequation is called the \\nMahalanobis distance\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   926\\n6/16/2017   2:16:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 927}),\n",
       " Document(page_content='12.4\\n  \\nOptimum (Bayes) Statistical Classiﬁers\\n    \\n927\\nfor \\njN\\nc\\n=\\n12\\n,, , .\\n…\\n This equation gives the Bayes decision functions for Gaussian \\npattern classes under the condition of a 0-1 loss function.\\nT\\nhe decision functions in Eq. (12-33) are hyperquadrics (quadratic functions in \\nn\\n-dimensional space), because no terms higher than the second degree in the com-\\nponents of \\nx\\n appear in the equation. Clearly, then, the best that a Bayes classiﬁer \\nfor Gaussian patterns can do is to place a second-order decision boundary between \\neach pair of pattern classes. If the pattern populations are truly Gaussian, no other \\nboundary would yield a lesser average loss in classiﬁcation.\\nIf all covariance matrices are equal, then \\nCC\\nj\\n=\\n for \\njN\\nc\\n=\\n12\\n,, , .\\n…\\n By expanding \\nEq.\\n (12-33), and dropping all terms that do not depend on \\nj\\n, we obtain\\n \\ndP c\\njj\\nT\\njj\\nT\\nj\\nxx\\nC\\nm m\\nC\\nm\\n()\\n=\\n()\\n+−\\n−−\\nln\\n11\\n1\\n2\\n \\n(12-34)\\nwhich are linear decision functions (hyperplanes) for \\njN\\nc\\n=\\n12\\n,, , .\\n…\\nIf, in addition, \\nCI\\n=\\n,\\n where \\nI\\n is the identity matrix,\\n and also if the classes are \\nequally likely (i.e., \\nP\\ncN\\njc\\n()\\n=\\n1\\n for all \\nj\\n),\\n then we can drop the term \\nln ( )\\nPc\\nj\\n because \\nit would be the same for all values of \\nj\\n. Equation (12-34) then becomes\\n \\ndj\\nN\\njj\\nT\\nj\\nT\\njc\\nxm x m m\\n()\\n=− =\\n1\\n2\\n12\\n,, ,\\n…\\n \\n(12-35)\\nwhich we recognize as the decision functions for a minimum-distance classifier [see \\nEq.\\n (12-4)]. Thus, as mentioned earlier, the minimum-distance classifier is optimum \\nin the Bayes sense if (1) the pattern classes follow a Gaussian distribution, (2) all \\ncovariance matrices are equal to the identity matrix, and (3) all classes are equally \\nlikely to occur. Gaussian pattern classes satisfying these conditions are spherical \\nclouds of identical shape in \\nn\\n dimensions (called \\nhyperspheres\\n).\\n The minimum-\\ndistance classifier establishes a hyperplane between every pair of classes, with the \\nproperty that the hyperplane is the perpendicular bisector of the line segment join-\\ning the center of the pair of hyperspheres. In 2-D, the patterns are distributed in cir-\\ncular regions, and the boundaries become lines that bisect the line segment joining \\nthe center of every pair of such circles.\\nEXAMPLE 12.5 :  A Bayes classiﬁer for 3-D patterns.\\nWe illustrate the mechanics of the preceding development using the simple patterns in Fig. 12.20. We \\nassume that the patterns are samples from two Gaussian populations, and that the classes are equally \\nlikely to occur. Applying Eq. (12-29) to the patterns in the ﬁgure results in\\n \\nmm\\n12\\n1\\n3\\n3\\n1\\n1\\n1\\n3\\n1\\n3\\n3\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nand\\nAnd, from Eq. (12-30),\\nDIP4E_GLOBAL_Print_Ready.indb   927\\n6/16/2017   2:16:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 928}),\n",
       " Document(page_content='928\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nCC\\n12\\n1\\n16\\n31 1\\n13 1\\n11 3\\n== −\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nThe inverse of this matrix is\\n \\nCC\\n1\\n1\\n2\\n1\\n84 4\\n48 4\\n44 8\\n−−\\n==\\n−−\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nNext, we obtain the decision functions. Equation (12-34) applies because the covariance matrices are \\nequal,\\n and we are assuming that the classes are equally likely:\\n \\nd\\nj\\nT\\njj\\nT\\nj\\nxx C m m C m\\n()\\n=−\\n−−\\n11\\n1\\n2\\nCarrying out the vector-matrix expansion, we obtain the two decision functions:\\n \\ndx d x x x\\n11 2 1 2 3\\n41 5\\n4885 5\\nxx\\n()\\n=−\\n()\\n=− + + −\\n..\\nand\\nThe decision boundary separating the two classes is then\\n \\ndd x x x\\n12 1 2 3\\n8884 0\\nxx\\n()\\n−\\n()\\n=−−+ =\\nFigure 12.20 shows a section of this planar surface. Note that the classes were separated effectively.\\nEXAMPLE 12.6 :  Classiﬁcation of multispectral data using a Bayes classiﬁer.\\nAs discussed in Sections 1.3 and 11.5, a multispectral scanner responds to selected bands of the electro-\\nmagnetic energy spectrum, such as the bands: 0.45– 0.52, 0.53– 0.61, 0.63– 0.69, and 0.78– 0.90 microns. \\nThese ranges are in the visible blue, visible green, visible red, and near infrared bands, respectively. A \\nregion on the ground scanned using these multispectral bands produces four digital images of the region, \\nx\\n3\\nx\\n1\\n(1, 0, 0)\\n(1, 0, 1)\\n(0, 0, 0)\\n(1, 1, 1)\\n(0, 0, 1)\\n(0, 1, 1)\\n(0, 1, 0)\\n(1, 1, 0)\\nx\\n2\\n1\\nc\\n∈\\n2\\nc\\n∈\\nFIGURE 12.20\\nTwo simple \\npattern classes \\nand the portion \\nof their Bayes \\ndecision bound-\\nary (shaded) that \\nintersects the \\ncube.\\nDIP4E_GLOBAL_Print_Ready.indb   928\\n6/16/2017   2:16:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 929}),\n",
       " Document(page_content='12.4\\n  \\nOptimum (Bayes) Statistical Classiﬁers\\n    \\n929\\none for each band. If the images are registered spatially, they can be visualized as being stacked one \\nbehind the other, as illustrated in Fig. 12.7. As we explained in that ﬁgure, every point on the ground \\nin this example can be represented by a 4-D pattern vector of the form \\nx=\\nxx\\nxx\\nT\\n1234\\n,,, ,\\n()\\n where \\nx\\n1\\n \\nis a shade of blue, \\nx\\n2\\n a shade of green, and so on. If the images are of size \\n512 512\\n×\\n pixels, each stack \\nof four multispectral images can be represented by 266,144 four\\n-dimensional pattern vectors. As noted \\npreviously, the Bayes classiﬁer for Gaussian patterns requires estimates of the mean vector and covari-\\nance matrix for each class. In remote sensing applications, these estimates are obtained using training \\nmultispectral data whose classes are known from each region of interest (this knowledge sometimes is \\nreferred to as \\nground truth\\n). The resulting vectors are then used to estimate the required mean vectors \\nand covariance matrices, as in Example 12.5.\\nFigures 12.21(a) through (d) show four \\n512 512\\n×\\n multispectral images of the Washington, D.C. area, \\ntaken in the bands mentioned in the previous paragraph.\\n We are interested in classifying the pixels in \\nthese images into one of three pattern classes: \\nwater\\n, \\nurban\\n \\ndevelopment\\n, or \\nvegetation\\n. The masks in \\nFig. 12.21(e) were superimposed on the images to extract samples representative of these three classes. \\nHalf of the samples were used for training (i.e., for estimating the mean vectors and covariance matri-\\nces), and the other half were used for independent testing to assess classiﬁer performance. We assume \\nthat the a priori probabilities are equal, \\nP\\ncj\\nj\\n() ; ,,.\\n==\\n13 123\\n \\nT\\nable 12.1 summarizes the classiﬁcation results we obtained with the training and test data sets. The \\npercentage of training and test pattern vectors recognized correctly was about the same with both data \\nsets, indicating that the learned parameters did not over-ﬁt the parameters to the training data. The larg-\\nest error in both cases was with patterns from the urban area. This is not unexpected, as vegetation is \\npresent there also (note that no patterns in the vegetation or urban areas were misclassiﬁed as water). \\nFigure 12.21(f) shows as black dots the training and test patterns that were misclassiﬁed, and as white \\ndots the patterns that were classiﬁed correctly. No black dots are visible in region 1, because the seven \\nmisclassiﬁed points are very close to the boundary of the white region. You can compute from the num-\\nbers in the table that the correct recognition rate was 96.4% for the training patterns, and 96.1% for the \\ntest patterns. \\nFigures 12.21(g) through (i) are more interesting. Here, we let the system classify \\nall\\n image pixels into \\none of the three categories. Figure 12.21(g) shows in white all pixels that were classiﬁed as water. Pixels \\nnot classiﬁed as water are shown in black. We see that the Bayes classiﬁer did an excellent job of deter-\\nmining which parts of the image were water. Figure 12.21(h) shows in white all pixels classiﬁed as urban \\ndevelopment; observe how well the system performed in recognizing urban features, such as the bridges \\nand highways. Figure 12.21(i) shows the pixels classiﬁed as vegetation. The center area in Fig. 12.21(h) \\nshows a high concentration of white pixels in the downtown area, with the density decreasing as a func-\\ntion of distance from the center of the image. Figure 12.21(i) shows the opposite effect, indicating the \\nleast vegetation toward the center of the image, where urban development is the densest.\\nWe mentioned in Section 10.3 when discussing Otsu’s method that thresholding \\nmay be viewed as a Bayes classiﬁcation problem, which optimally assigns patterns \\nto two or more classes. In fact, as the previous example shows, pixel-by-pixel classi-\\nﬁcation may be viewed as a segmentation that partitions an image into two or more \\npossible types of regions. If only one single variable (e.g., intensity) is used, then \\nEq. (12-24) becomes an optimum function that similarly partitions an image based \\non the intensity of its pixels, as we did in Section 10.3. Keep in mind that optimal-\\nity requires that the PDF and a priori probability of each class be known. As we \\nDIP4E_GLOBAL_Print_Ready.indb   929\\n6/16/2017   2:16:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 930}),\n",
       " Document(page_content='930\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 12.21\\n Bayes classiﬁcation of multispectral data. (a)–(d) Images in the visible blue, visible green, visible red, \\nand near infrared wavelength bands. (e) Masks for regions of water (labeled 1), urban development (labeled 2), \\nand vegetation (labeled 3). (f) Results of classiﬁcation; the black dots denote points classiﬁed incorrectly. The other \\n(white) points were classiﬁed correctly. (g) All image pixels classiﬁed as water (in white). (h) All image pixels clas-\\nsiﬁed as urban development (in white). (i) All image pixels classiﬁed as vegetation (in white).\\nDIP4E_GLOBAL_Print_Ready.indb   930\\n6/16/2017   2:16:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 931}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n931\\nTraining Patterns\\nTest Patterns\\nClass\\nNo. of \\nSamples\\nClassiﬁed into Class\\n% \\nCorrect Class\\nNo. of \\nSamples\\nClassiﬁed into Class\\n% \\nCorrect\\n12 3\\n12 3\\n1 484 482 2 0 99.6 1 483 478 3 2 98.9\\n2 933 0 885 48 94.9 2 932 0 880 52 94.4\\n3 483 0 19 464 96.1 3 482 0 16 466 96.7\\nTABLE \\n12.1\\nBayes classiﬁcation of multispectral image data. Classes 1, 2, and 3 are water, urban, and vegetation, respectively.\\nhave mentioned previously, estimating these densities is not a trivial task. If assump-\\ntions have to be made (e.g., as in assuming Gaussian densities), then the degree of \\noptimality achieved in classiﬁcation depends on how close the assumptions are to \\nreality.\\n12.5  NEURAL NETWORKS AND DEEP LEARNING  \\nThe principal objectives of the material in this section and in Section 12.6 are to \\npresent an introduction to deep neural networks, and to derive the equations that \\nare the foundation of deep learning. We will discuss two types of networks. In this \\nsection, we focus attention on multilayer, fully connected neural networks, whose \\ninputs are pattern vectors of the form introduced in Section 12.2. In Section 12.6, we \\nwill discuss convolutional neural networks, which are capable of accepting images \\nas inputs. We follow the same basic approach in presenting the material in these two \\nsections. That is, we begin by developing the equations that describe how an input is \\nmapped through the networks to generate the outputs that are used to classify that \\ninput. Then, we derive the equations of backpropagation, which are the tools used \\nto train both types of networks. We give examples in both sections that illustrate the \\npower of deep neural networks and deep learning for solving complex pattern clas-\\nsification problems.\\nBACKGROUND\\nThe essence of the material that follows is the use of a multitude of elemental non-\\nlinear computing elements (called \\nartificial neurons\\n), organized as networks whose \\ninterconnections are similar in some respects to the way in which neurons are inter-\\nconnected in the visual cortex of mammals. The resulting models are referred to \\nby various names, including \\nneural networks\\n, \\nneurocomputers\\n, \\nparallel distributed \\nprocessing\\n \\nmodels\\n, \\nneuromorphic systems\\n, \\nlayered self-adaptive networks\\n, and \\ncon-\\nnectionist models\\n. Here, we use the name \\nneural networks\\n, or \\nneural nets\\n for short. \\nWe use these networks as vehicles for adaptively learning the parameters of decision \\nfunctions via successive presentations of training patterns.\\nInterest in neural networks dates back to the early 1940s, as exempliﬁed by the \\nwork of McCulloch and Pitts [1943], who proposed neuron models in the form of \\n12.5\\nDIP4E_GLOBAL_Print_Ready.indb   931\\n6/16/2017   2:16:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 932}),\n",
       " Document(page_content='932\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nbinary thresholding devices, and stochastic algorithms involving sudden 0–1 and 1–0 \\nchanges of states, as the basis for modeling neural systems. Subsequent work by \\nHebb [1949] was based on mathematical models that attempted to capture the con-\\ncept of learning by reinforcement or association.\\nDuring the mid-1950s and early 1960s, a class of so-called \\nlearning machines\\n origi-\\nnated by Rosenblatt [1959, 1962] caused a great deal of excitement among research-\\ners and practitioners of pattern recognition. The reason for the interest in these \\nmachines, called \\nperceptrons\\n, was the development of mathematical proofs showing \\nthat perceptrons, when trained with linearly separable training sets (i.e., training sets \\nseparable by a hyperplane), would converge to a solution in a ﬁnite number of itera-\\ntive steps. The solution took the form of parameters (coefﬁcients) of hyperplanes \\nthat were capable of correctly separating the classes represented by patterns of the \\ntraining set.\\nUnfortunately, the expectations following discovery of what appeared to be a \\nwell-founded theoretical model of learning soon met with disappointment. The \\nbasic perceptron, and some of its generalizations, were inadequate for most pattern \\nrecognition tasks of practical signiﬁcance. Subsequent attempts to extend the power \\nof perceptron-like machines by considering multiple layers of these devices lacked \\neffective training algorithms, such as those that had created interest in the percep-\\ntron itself. The state of the ﬁeld of learning machines in the mid-1960s was sum-\\nmarized by Nilsson [1965]. A few years later, Minsky and Papert [1969] presented \\na discouraging analysis of the limitation of perceptron-like machines. This view was \\nheld as late as the mid-1980s, as evidenced by comments made by Simon [1986]. In \\nthis work, originally published in French in 1984, Simon dismisses the perceptron \\nunder the heading “Birth and Death of a Myth.”\\nMore recent results by Rumelhart, Hinton, and Williams [1986] dealing with the \\ndevelopment of new training algorithms for multilayers of perceptron-like units \\nhave changed matters considerably. Their basic method, called \\nbackpropagation\\n \\n(\\nbackprop\\n for short), provides an effective training method for multilayer networks. \\nAlthough this training algorithm cannot be shown to converge to a solution in the \\nsense of the proof for the single-layer perceptron, backpropagation is capable of \\ngenerating results that have revolutionized the ﬁeld of pattern recognition. \\nThe approaches to pattern recognition we have studied up to this point rely on \\nhuman-engineered techniques to transform raw data into formats suitable for com-\\nputer processing. The methods of feature extraction we studied in Chapter 11 are \\nexamples of this. Unlike these approaches, neural networks can use backpropaga-\\ntion to automatically learn representations suitable for recognition, starting with \\nraw data. Each layer in the network “reﬁnes” the representation into more abstract \\nlevels. This type of multilayered learning is commonly referred to as \\ndeep learning\\n, \\nand this capability is one of the underlying reasons why applications of neural net-\\nworks have been so successful. As we noted at the beginning of this section, practical \\nimplementations of deep learning generally are associated with large data sets.\\nOf course, these are not “magical” systems that assemble themselves. Human \\nintervention is still required for specifying parameters such as the number of layers, \\nthe number of artiﬁcial neurons per layer, and various coefﬁcients that are problem \\nDIP4E_GLOBAL_Print_Ready.indb   932\\n6/16/2017   2:16:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 933}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n933\\ndependent. Teaching proper recognition to a complex multilayer neural network is \\nnot a science; rather, it is an art that requires considerable knowledge and experi-\\nmentation on the part of the designer. Countless applications of pattern recogni-\\ntion, especially in constrained environments, are best handled by more “traditional” \\nmethods. A good example of this is stylized font recognition. It would be senseless \\nto develop a neural network to recognize the E-13B font we studied in Fig. 12.11. A \\nminimum-distance classiﬁer implemented on a hard-wired architecture is the ideal \\nsolution to this problem, provided that interest is limited to reading only the E-13B \\nfont printed on bank checks. On the other hand, neural networks have proved to be \\nthe ideal solution if the scope of application is expanded to require that all relevant \\ntext written on checks, including cursive script, be read with high accuracy. \\nDeep learning has shined in applications that defy other methods of solution. In \\nthe two decades following the introduction of backpropagation, neural networks \\nhave been used successfully in a broad range of applications. Some of them, such as \\nspeech recognition, have become an integral part of everyday life. When you speak \\ninto a smart phone, the nearly ﬂawless recognition is performed by a neural network. \\nThis type of performance was unachievable just a few years ago. Other applications \\nfrom which you beneﬁt, perhaps without realizing it, are smart ﬁlters that learn user \\npreferences for rerouting spam and other junk mail from email accounts, and the \\nsystems that read zip codes on postal mail. Often, you see television clips of vehicles \\nnavigating autonomously, and robots that are capable of interacting with their envi-\\nronment. Most are solutions based on neural networks. Less familiar applications \\ninclude the automated discovery of new medicines, the prediction of gene mutations \\nin DNA research, and advances in natural language understanding.\\nAlthough the list of practical uses of neural nets is long, applications of this tech-\\nnology in image pattern classiﬁcation has been slower in gaining popularity. As \\nyou will learn shortly, using neural nets in image processing is based principally on \\nneural network architectures called \\nconvolutional neural nets\\n (denoted by \\nCNNs\\n \\nor \\nConvNets\\n). One of the earliest well-known applications of CNNs is the work of \\nLeCun et al. [1989] for reading handwritten U.S. postal zip codes. A number of other \\napplications followed shortly thereafter, but it was not until the results of the 2012 \\nImageNet Challenge were published (e.g., see Krizhevsky, Sutskever, and Hinton \\n[2012]) that CNNs became widely used in image pattern recognition. Today, this is \\nthe approach of choice for addressing complex image recognition tasks.\\nThe neural network literature is vast and rapidly evolving, so as usual, our \\napproach is to focus on fundamentals. In this and the following sections, we will \\nestablish the foundation of how neural nets are trained, and how they operate after \\ntraining. We will begin by brieﬂy discussing perceptrons. Although these computing \\nelements are not used per se in current neural network architectures, the opera-\\ntions they perform are almost identical to artiﬁcial neurons, which are the basic \\ncomputing units of neural nets. In fact, an introduction to neural networks would \\nbe incomplete without a discussion of perceptrons. We will follow this discussion by \\ndeveloping in detail the theoretical foundation of backpropagation. After develop-\\ning the basic backpropagation equations, we will recast them in matrix form, which \\nDIP4E_GLOBAL_Print_Ready.indb   933\\n6/16/2017   2:16:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 934}),\n",
       " Document(page_content='934\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nreduces the training and operation of neural nets to a simple, straightforward cas-\\ncade of matrix multiplications.\\nAfter studying several examples of fully connected neural nets, we will follow a \\nsimilar approach in developing the foundation of CNNs, including how they differ \\nfrom fully connected neural nets, and how their training is different. This is followed \\nby several examples of how CNNs are used for image pattern classiﬁcation.\\nTHE PERCEPTRON\\nA single perceptron unit learns a linear boundary between two linearly separable \\npattern classes. Figure 12.22(a) shows the simplest possible example in two dimen-\\nsions: two pattern classes, consisting of a single pattern each. A linear boundary in \\n2-D is a straight line with equation \\nya x b\\n=+\\n,\\n where coefficient \\na \\nis the \\nslope\\n and \\nb\\n \\nis the \\ny\\n-\\nintercept\\n.\\n Note that if \\nb\\n=\\n0,\\n the line goes through the origin. Therefore, the \\nfunction of parameter \\nb\\n is to displace the line from the origin without affecting its \\nslope\\n. For this reason, this “floating” coefficient that is not multiplied by a coordi-\\nnate is often referred to as the \\nbias\\n, the \\nbias coefficient\\n, or the \\nbias weight\\n.\\nWe are interested in a line that separates the two classes in Fig. 12.22. This is a line \\npositioned in such a way that pattern \\n(,)\\nxy\\n11\\n from class \\nc\\n1\\n lies on one side of the line, \\nand pattern \\n(,)\\nxy\\n22\\n from class \\nc\\n2\\n lies on the other. The locus of points \\n(,)\\nxy\\n that are \\non\\n the line\\n, satisfy the equation \\nya xb\\n−−\\n=\\n0.\\n It then follows that any point on one \\nside of the line would yield a positive value when its coordinates are plugged into \\nthis equation,\\n and conversely for a point on the other side. \\nGenerally, we work with patterns in much higher dimensions than two, so we need \\nmore general notation. Points in \\nn\\n dimensions are vectors. The components of a vec-\\ntor, \\nxx x\\nn\\n12\\n,,,,\\n…\\n are the coordinates of the point. For the coefﬁcients of the boundary \\nseparating the two classes\\n, we use the notation \\nww ww\\n12\\n1\\n,, ,, ,\\n…\\nnn\\n+\\n where \\nw\\nn\\n+\\n1\\n is the \\nbias. The general equation of our line using this notation is \\nwww\\n11 22 3\\n0\\nxx\\n++ =\\n (we \\ncan express this equation in slope-intercept form as \\nxx\\n21 2 1\\n0\\n++ =\\n(w w w w\\n32\\n))\\n.\\n \\nF\\nigure 12.22(b) is the same as (a), but using this notation. Comparing the two ﬁg-\\nures, we see that \\nyx\\n=\\n2\\n, \\nxx\\n=\\n1\\n, \\na\\n=\\nww\\n1\\n2\\n,\\n and \\nb\\n=\\nww\\n3\\n2\\n.\\n Equipped with our more \\nFIGURE 12.22\\n(a) The simplest \\ntwo-class example \\nin 2-D, showing one \\npossible decision \\nboundary out of an \\ninﬁnite number of \\nsuch boundaries.  \\n(b) Same as (a), but \\nwith the  \\ndecision boundary \\nexpressed using \\nmore general  \\nnotation.\\n,o r\\nya x b\\n=+\\n0\\nya xb\\n−− =\\nx\\ny\\n1\\nx\\n2\\nx\\n2\\nc\\n∈\\n+\\n−\\n1\\nc\\n∈\\n2\\nc\\n∈\\n+\\n−\\n1\\nc\\n∈\\n11 22 3\\n0\\nxx\\n++ =\\nwww\\nDIP4E_GLOBAL_Print_Ready.indb   934\\n6/16/2017   2:16:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 935}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n935\\ngeneral notation, we say that an arbitrary point \\n(, )\\nxx\\n12\\n is on the positive side of a \\nline if \\nwww\\n11 22 3\\n0\\nxx\\n++ >\\n,\\n and conversely for any point on the negative side. For \\npoints in 3-D\\n, we work with the equation of a plane, \\nwwww\\n11 22 33 4\\n0\\nxxx\\n+++ =\\n, \\nbut would perform exactly the same test to see if a point lies on the positive or \\nnegative side of the plane\\n. For a point in \\nn\\n dimensions, the test would be against a \\nhyperplane\\n, whose equation is\\n \\nww ww\\n11 22\\n1\\n0\\nxx x\\nnn n\\n++ ++ =\\n+\\n/midhorizellipsis\\n \\n(12-36)\\nThis equation is expressed in summation form as\\n \\nww\\nii n\\ni\\nn\\nx\\n+=\\n+\\n=\\n∑\\n1\\n1\\n0  \\n(12-37)\\nor in vector form as\\n \\nw\\nT\\nn\\nx\\n+=\\n+\\nw\\n1\\n0  \\n(12-38)\\nwhere \\nw\\n and \\nx\\n are \\nn\\n-dimensional \\ncolumn\\n vectors and \\nw\\nT\\nx\\n is the dot (inner) prod-\\nuct of the two vectors\\n. Because the inner product is commutative, we can express \\nEq. (12-38) in the equivalent form \\nx\\nT\\nn\\nw\\n+=\\n+\\nw\\n1\\n0.\\n We refer to \\nw\\n as a \\nweight vector\\n \\nand,\\n as above, to \\nw\\nn\\n+\\n1\\n as a \\nbias\\n. Because the bias is a weight that is always multiplied \\nby 1, sometimes we avoid repetition by using the term \\nweights\\n, \\ncoefficients, or param-\\neters\\n when referring to the bias and the elements of a weight vector collectively.\\nStating the class separation problem in general form we say that, given any pat-\\ntern vector \\nx\\n from a vector population, we want to ﬁnd a set of weights with the \\nproperty\\n \\nw\\nT\\nn\\nc\\nc\\nx\\nx\\nx\\n+=\\n>∈\\n<∈\\n⎧\\n⎨\\n⎩\\n+\\nw\\n1\\n1\\n2\\n0\\n0\\nif \\nif \\n \\n(12-39)\\nFinding a line that separates two \\nlinearly separable\\n pattern classes in 2-D can be \\ndone by inspection.\\n Finding a separating plane by visual inspection of 3-D data is \\nmore difficult, but it is doable. For \\nn\\n>\\n3,\\n finding a separating hyperplane by inspec-\\ntion becomes impossible in general.\\n We have to resort instead to an algorithm to find \\na solution. The perceptron is an implementation of such an algorithm. It attempts \\nto find a solution by iteratively stepping through the patterns of each of two classes. \\nIt starts with an arbitrary weight vector and bias, and is guaranteed to converge in a \\nfinite number of iterations if the classes are linearly separable. \\nThe perceptron algorithm is simple. Let \\na\\n>\\n0\\n denote a \\ncorrection increment\\n (also \\ncalled the \\nlearning increment\\n or the \\nlearning rate\\n),\\n let \\nw\\n()\\n1\\n be a vector with arbi-\\ntrary values\\n, and let \\nw\\nn\\n+\\n1\\n1\\n()\\n be an arbitrary constant. Then, do the following for \\nk\\n=\\n23\\n,,\\n:\\n…\\n For a pattern vector, \\nx\\n()\\n,\\nk\\n at step \\nk\\n, \\n1)\\n If \\nx\\n()\\nkc\\n∈\\n1\\n and \\nw\\nT\\nn\\nkk k\\n() () () ,\\nx\\n+\\n+\\nw\\n1\\n0\\n≤\\n let\\n \\nww\\n(\\n) () ()\\n() ( )\\nkk k\\nkk\\nnn\\n+= +\\n+= +\\n++\\n1\\n1\\n11\\na\\nvv a\\nx\\n \\n(12-40)\\nIt is customary to  \\nassociate > with class \\nc\\n1\\n \\nand < with class \\nc\\n2\\n, but \\nthe sense of the  \\ninequality is arbitrary, \\nprovided that you are \\nconsistent. Note that this \\nequation implements a \\nlinear\\n \\ndecision function\\n.\\nLinearly separable class-\\nes satisfy Eq. (12-39). \\nThat is, they are  \\nseparable by single \\nhyperplanes.\\nDIP4E_GLOBAL_Print_Ready.indb   935\\n6/16/2017   2:17:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 936}),\n",
       " Document(page_content='936\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n2)\\n If \\nx\\n()\\nkc\\n∈\\n2\\n and \\nw\\nT\\nn\\nkk k\\n() () () ,\\nx\\n+\\n+\\nw\\n1\\n0\\n≥\\n let\\n \\nww\\n(\\n) () ()\\n() ( )\\nkk k\\nkk\\nnn\\n+= −\\n+= −\\n++\\n1\\n1\\n11\\na\\nvv a\\nx\\n \\n(12-41)\\n3)\\n Otherwise, let\\n \\nww\\n()\\n( )\\n() ( )\\nkk\\nkk\\nnn\\n+=\\n+=\\n++\\n1\\n1\\n11\\nvv\\n \\n(12-42)\\nThe correction in Eq. (12-40) is applied when the pattern is from class \\nc\\n1\\n and \\nEq. (12-39) does not give a positive response. Similarly, the correction in Eq. (12-41) \\nis applied when the pattern is from class \\nc\\n2\\n and Eq. (12-39) does not give a negative \\nresponse. As Eq. (12-42) shows, no change is made when Eq. (12-39) gives the cor-\\nrect response. \\nThe notation in Eqs. (12-40) through (12-42) can be simpliﬁed if we add a 1 at \\nthe end of every pattern vector and include the bias in the weight vector. That is, \\nwe deﬁne\\nx\\n≜…\\n[,\\n, , , ]\\nxx x\\nn\\nT\\n12\\n1\\n and \\nw\\n≜…\\n[,\\n,, , ] .\\nww ww\\n12\\n1\\nnn\\nT\\n+\\n Then, Eq. (12-39) \\nbecomes\\n \\nw\\nT\\nc\\nc\\nx\\nx\\nx\\n=\\n>∈\\n<∈\\n⎧\\n⎨\\n⎩\\n0\\n0\\n1\\n2\\nif \\nif \\n \\n(12-43)\\nwhere both vectors are now \\n()\\nn\\n+\\n1\\n-dimensional.\\n In this formulation, \\nx\\n and \\nw\\n are \\nreferred to as \\naugmented\\n pattern and weight vectors\\n, respectively. The algorithm in \\nEqs. (12-40) through (12-42) then becomes: For any pattern vector, \\nx\\n()\\n,\\nk\\n at step \\nk\\n1\\n/H11032\\n) \\nIf \\nx\\n()\\nkc\\n∈\\n1\\n and \\nw\\nT\\nkk\\n()() ,\\nx\\n≤\\n0  let\\n \\nww\\n(\\n) () ()\\nkk k\\n+= +\\n1\\na\\nx\\n \\n(12-44)\\n2\\n/H11032\\n)\\n If \\nx\\n()\\nkc\\n∈\\n2\\n and \\nw\\nT\\nkk\\n() () ,\\nx\\n≥\\n0  let \\n \\nww\\n(\\n) () ()\\nkk k\\n+= −\\n1\\na\\nx\\n \\n(12-45)\\n3\\n/H11032\\n)\\n Otherwise, let\\n \\nww\\n()\\n( )\\nkk\\n+=\\n1  \\n(12-46)\\nwhere the starting weight vector, \\nw\\n()\\n,\\n1\\n is arbitrary and, as above, \\na\\n is a positive \\nconstant.\\n The procedure implemented by Eqs. (12-40)–(12-42) or (12-44)–(12-46) is \\ncalled the \\nperceptron training algorithm\\n. The \\nperceptron convergence theorem \\nstates \\nthat the algorithm is guaranteed to converge to a solution (i.e., a separating hyper-\\nplane) in a finite number of steps if the two pattern classes are linearly separable \\n(see Problem 12.15). Normally, Eqs. (12-44)–(12-46) are the basis for implementing \\nthe perceptron training algorithm, and we will use it in the following paragraphs \\nof this section. However, the notation in Eqs. (12-40)–(12-42), in which the bias is \\nDIP4E_GLOBAL_Print_Ready.indb   936\\n6/16/2017   2:17:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 937}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n937\\nshown separately, is more prevalent in neural networks, so you need to be familiar \\nwith it as well.\\nFigure 12.23 shows a schematic diagram of the perceptron. As you can see, all \\nthis simple “machine” does is form a\\n sum of products \\nof an input pattern using the \\nweights and bias found during training. The output of this operation is a scalar value \\nthat is then passed through an \\nactivation function\\n to produce the unit’s output. For \\nthe perceptron, the activation function is a thresholding function (we will consider \\nother forms of activation when we discuss neural networks). If the thresholded out-\\nput is a \\n+\\n1,\\n we say that the pattern belongs to class \\nc\\n1\\n.\\n Otherwise, a \\n−\\n1\\n indicates that \\nthe pattern belongs to class \\nc\\n2\\n.\\n Values 1 and 0 sometimes are used to denote the two \\npossible states of the output.\\nEXAMPLE 12.7 :  Using the perceptron algorithm to learn a decision boundary.\\nWe illustrate the steps taken by a perceptron in learning the coefﬁcients of a linear boundary by solving \\nthe mini problem in Fig. 12.22. To simplify manual computations, let the pattern vector furthest from the \\norigin be \\nx\\n=\\n[]\\n,\\n331\\nT\\n and the other be \\nx\\n=\\n[]\\n,\\n111\\nT\\n where we augmented the vectors by appending a \\n1 at the end, as discussed earlier. To match the ﬁgure, let these two patterns belong to classes \\nc\\n1\\n and \\nc\\n2\\n, \\nrespectively\\n. Also, assume the patterns are “cycled” through the perceptron in that order during training \\n(one complete iteration through all patterns of the training is called an \\nepoch\\n). To start, we let \\na\\n=\\n1\\n and \\nw\\n()\\n[ ];\\n1 000\\n==\\n0\\nT\\n then, \\nFor \\nk\\n=\\n1,\\n \\nx\\n()\\n[ ] ,\\n13 3 1\\n1\\n=∈\\nT\\nc\\n and \\nw\\n()\\n[ ].\\n1 000\\n=\\nT\\n Their inner product is zero,\\n \\nw\\nT\\n()()\\n1 1 000\\n3\\n3\\n1\\n0\\nx\\n=\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\nso Step 1\\n/H11032\\n of the second version of the training algorithm applies:\\n \\nww\\n() () () ()\\n21 1\\n0\\n0\\n0\\n1\\n3\\n3\\n1\\n3\\n3\\n1\\n=+ =\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n+\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\na\\nx\\n⎥ ⎥\\n⎥\\nFor \\nk\\n=\\n2,\\n \\nx\\n()\\n[ ]\\n2 111\\n2\\n=∈\\nT\\nc\\n and \\nw\\n()\\n[ ].\\n23 3 1\\n=\\nT\\n Their inner product is\\nNote that the perceptron \\nmodel implements Eq. \\n(12-39), which is in \\nthe form of a decision \\nfunction. \\nFIGURE 12.23\\nSchematic of a \\nperceptron,  \\nshowing the  \\noperations it  \\nperforms. \\n1\\nw\\n2\\nw\\nn\\nw\\n+ 1\\nn\\nw\\n1\\n1\\nx\\n2\\nx\\nn\\nx\\n1\\n−\\n+1\\n1\\n1\\nn\\nkk n\\nk\\nx\\n+\\n=\\n+\\n∑\\nww\\n.\\n.\\n.\\n.\\n.\\n1 or 1\\n+−\\nDIP4E_GLOBAL_Print_Ready.indb   937\\n6/16/2017   2:17:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 938}),\n",
       " Document(page_content='938\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n \\nw\\nT\\n()()\\n22 3 3 1\\n1\\n1\\n1\\n7\\nx\\n=\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\nThe result is positive when it should have been negative, so Step 2\\n/H11032\\n applies:\\n \\nww\\n() () () ()\\n32 2\\n3\\n3\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n0\\n=− =\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\na\\nx\\n⎥ ⎥\\n⎥\\nWe have gone through a complete training epoch with at least one correction, so we cycle through the \\ntraining set again.\\nFor \\nk\\n=\\n3,\\n \\nx\\n()\\n[ ] ,\\n33 3 1\\n1\\n=∈\\nT\\nc\\n and \\nw\\n()\\n[ ].\\n32 2 0\\n=\\nT\\n Their inner product is positive (i.e., 6) as it should \\nbe because \\nx\\n()\\n.\\n3\\n1\\n∈\\nc\\n Therefore, Step 3\\n/H11032\\n applies and the weight vector is not changed:\\n \\nww\\n()\\n()\\n43\\n2\\n2\\n0\\n==\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nFor \\nk\\n=\\n4,\\n \\nx\\n()\\n[ ] ,\\n4 111\\n2\\n=∈\\nT\\nc\\n and \\nw\\n()\\n[ ].\\n42 2 0\\n=\\nT\\n Their inner product is positive (i.e., 4) and it should \\nhave been negative, so Step 2\\n/H11032\\n applies:\\n \\nww\\n() () () ()\\n54 4\\n2\\n2\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n=− =\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\na\\nx\\n⎥ ⎥\\n⎥\\n⎥\\nAt least one correction was made, so we cycle through the training patterns again. For \\nk\\n=\\n5,\\n we have\\nx\\n()\\n[ ] ,\\n53 3 1\\n1\\n=∈\\nT\\nc\\n and, using \\nw\\n()\\n,\\n5\\n we compute their inner product to be 5. This is positive as it should \\nbe\\n, so Step 3\\n/H11032\\n applies and we let \\nww\\n()\\n() [ ].\\n65 1 1 1\\n== −\\nT\\n Following this procedure just discussed, you \\ncan show (see Problem 12.13) that the algorithm converges to the solution weight vector\\n \\nww\\n==\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n()\\n12\\n1\\n1\\n3\\nwhich gives the decision boundary\\n \\nxx\\n12\\n30\\n+− =\\nFigure 12.24(a) shows the boundary deﬁned by this equation. As you can see, it clearly separates the \\npatterns of the two classes. In terms of the terminology we used in the previous section, the \\ndecision \\nsurface\\n learned by the perceptron is \\ndd x x x x\\n()\\n( , )\\n,\\nx\\n== + −\\n12 1 2\\n3\\n which is a plane. As before, the \\ndecision boundary\\n is the locus of points such that \\ndd x x\\n()\\n( , ) ,\\nx\\n==\\n12\\n0\\n which is a line. Another way to \\nvisualize this boundary is that it is the intersection of the decision surface (a plane) with the \\nxx\\n12\\n-plane,\\n \\nas F\\nig. 12.24(b) shows. All points \\n(, )\\nxx\\n12\\n such that \\ndx x\\n(,\\n)\\n12\\n0\\n>\\n are on the positive side of the boundary, \\nand vice versa for \\ndx x\\n(,\\n) .\\n12\\n0\\n<\\nDIP4E_GLOBAL_Print_Ready.indb   938\\n6/16/2017   2:17:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 939}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n939\\nEXAMPLE 12.8 :  Using the perceptron to classify two sets of iris data measurements.\\nIn Fig. 12.10 we showed a reduced set of the iris database in two dimensions, and mentioned that the \\nonly class that was separable from the others is the class of Iris setosa. As another illustration of the \\nperceptron, we now ﬁnd the full decision boundary between the Iris setosa and the Iris versicolor classes. \\nAs we mentioned when discussing Fig. 12.10, these are 4-D data sets. Letting \\na\\n=\\n05\\n.,\\n and starting with \\nall parameters equal to zero\\n, the perceptron converged in only four epochs to the solution weight vector \\nw\\n=−\\n−\\n[. , . , . , . , . ],\\n06 5 20 5 26 0 11 0 05 0\\nT\\n where the last element is \\nw\\nn\\n+\\n1\\n.\\nIn practice, linearly separable pattern classes are rare, and a signiﬁcant amount \\nof research effort during the 1960s and 1970s went into developing techniques for \\ndealing with nonseparable pattern classes\\n. With recent advances in neural networks, \\nmany of those methods have become items of mere historical interest, and we will \\nnot dwell on them here. However, we mention brieﬂy one approach because it is rel-\\nevant to the discussion of neural networks in the next section. The method is based \\non minimizing the error between the actual and desired response at any training step.\\nLet \\nr \\ndenote the response we want the perceptron to have for any pattern during \\ntraining. The output of our perceptron is either \\n+\\n1\\n or \\n−\\n1,\\n so these are the two pos-\\nsible values that \\nr\\n can have\\n. We want to ﬁnd the augmented weight vector, \\nw\\n,\\n that \\nminimizes the mean squared error (MSE) between the desired and actual responses \\nof the perceptron.\\n The function should be differentiable and have a unique mini-\\nmum. The function of choice for this purpose is a quadratic of the form\\n \\nEr\\nT\\n()\\nww\\n=−\\n(\\n)\\n1\\n2\\n2\\nx\\n \\n(12-47)\\nwhere \\nE\\n is our error measure\\n, \\nw\\n is the weight vector we are seeking, \\nx\\n is any pattern \\nfrom the training set,\\n and \\nr\\n is the response we desire for that pattern. Both \\nw\\n and \\nx\\nare augmented vectors. \\nThe 1 ⁄ 2 is used to cancel \\nout the 2 that will result \\nfrom taking the deriva-\\ntive of this expression. \\nAlso, remember that \\nw\\nT\\nx\\n \\nis a scalar. \\n01 23\\n1\\n2\\n3\\nx\\n1\\n0\\n1\\n2\\n3\\n1\\n2\\n3\\nx\\n1\\nx\\n2\\n12 1 2\\n() ( , )\\n3\\ndd x x x x\\n== + −\\nx\\n12\\n3\\nxx\\n+−\\n12\\n30\\nxx\\n+− =\\n+\\n12\\n30\\nxx\\n+− =\\nx\\n2\\nb a\\nFIGURE 12.24\\n(a) Segment \\nof the decision \\nboundary learned \\nby the perceptron \\nalgorithm.  \\n(b) Section of the \\ndecision surface. \\nThe decision \\nboundary is the \\nintersection of the \\ndecision surface \\nwith the \\nx\\nx\\n12\\n-\\nplane.\\nDIP4E_GLOBAL_Print_Ready.indb   939\\n6/16/2017   2:17:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 940}),\n",
       " Document(page_content='940\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nWe ﬁnd the minimum of \\nE\\n()\\nw\\n using an iterative gradient descent algorithm, whose \\nform is\\n \\nww\\nw\\nw\\nww\\nkk\\nE\\nk\\n+\\n()\\n=−\\n∂\\n()\\n∂\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n()\\n1( )\\na\\n \\n(12-48)\\nwhere the starting weight vector is arbitrary, and \\na\\n>\\n0. \\nF\\nigure 12.25(a) shows a plot of \\nE\\n for scalar values, \\nw\\n and \\nx\\n,\\n of \\nw\\n and \\nx\\n.\\n We want \\nto move \\nw\\n incrementally so \\nE\\n()\\nw\\n approaches a minimum, which implies that \\nE\\n \\nshould stop changing or\\n, equivalently, that \\n∂∂ =\\nE\\n() .\\nww\\n0\\n Equation (12-48) does \\nprecisely this\\n. If \\n∂∂ >\\nE\\n() ,\\nww\\n0\\n a portion of this quantity (determined by the value \\nof the learning increment \\na\\n)\\n is subtracted from \\nw\\n()\\nk\\n to create a new, updated value \\nw\\n()\\n,\\nk\\n+\\n1\\n of the weight. The opposite happens if \\n∂∂ <\\nE\\n() .\\nww\\n0\\n If \\n∂∂ =\\nE\\n() ,\\nww\\n0  \\nthe weight is unchanged,\\n meaning that we have arrived at a minimum, which is the \\nsolution we are seeking. The value of \\na\\n determines the relative magnitude of the \\ncorrection in weight value\\n. If \\na\\n is too small, the step changes will be correspond-\\ningly small and the weight would move slowly toward convergence\\n, as Fig. 12.25(a) \\nillustrates. On the other hand, choosing \\na\\n too large could cause large oscillations \\non either side of the minimum,\\n or even become unstable, as Fig. 12.25(b) illustrates. \\nThere is no general rule for choosing \\na\\n.\\n However, a logical approach is to start small \\nand experiment by increasing \\na\\n to determine its inﬂuence on a particular set of \\ntraining patterns\\n. Figure 12.25(c) shows the shape of the error function for two vari-\\nables.\\nBecause the error function is given analytically and it is differentiable, we can \\nexpress Eq. (12-48) in a form that does not require computing the gradient explicitly \\nat every step. The partial of \\nE\\n()\\nw\\n with respect to \\nw\\n is\\n \\n∂\\n()\\n∂\\n=− −\\n(\\n)\\nE\\nr\\nT\\nw\\nw\\nw\\nx\\nx\\n \\n(12-49)\\nNote that the right side \\nof this equation is the \\ngradient of \\nE\\n(\\nw\\n).\\nE\\nx\\nw\\n01 2\\n0.25\\n0.50\\n0\\nE\\nx\\nw\\n01 2\\n0.25\\n0.50\\n0\\n0\\n1\\n2\\n1\\n2\\n0\\n0.5\\n1\\nb a\\nc\\nFIGURE 12.25\\n Plots of \\nE\\n as a function of \\nw\\nx\\n for \\nr\\n=\\n1.\\n (a) A value of \\na\\n that is too small can slow down convergence. \\n(b) If \\na\\n is too large, large oscillations or divergence may occur. (c) Shape of the error function in 2-D.\\nDIP4E_GLOBAL_Print_Ready.indb   940\\n6/16/2017   2:17:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 941}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n941\\nSubstituting this result into Eq. (12-48) yields\\n \\nww w\\nkk\\nr k k k k\\nT\\n+\\n()\\n=\\n()\\n+\\n()\\n−\\n()()\\n⎡\\n⎣\\n⎤\\n⎦\\n()\\n1\\na\\nx\\nx\\n \\n(12-50)\\nwhich is in terms of known or easily computable terms. As before, \\nw\\n()\\n1  is arbitrary.\\nW\\nidrow and Stearns [1985] have shown that it is necessary (but not sufﬁcient) \\nfor \\na\\n to be in the range \\n02\\n<<\\na\\n for the algorithm in Eq. (12-50) to converge. A \\ntypical range for \\na\\n is \\n01 10\\n..\\n.\\n<<\\na\\n Although the proof is not shown here, the algo-\\nrithm converges to a solution that minimizes the mean squared error over the pat-\\nterns of the training set.\\n For this reason, the algorithm is often referred to as the \\nleast-mean-squared-error\\n (LMSE) algorithm. In practice, we say that the algorithm \\nhas converged when the error decreases below a speciﬁed threshold. The solution \\nat convergence may not be a hyperplane that fully partitions two linearly separable \\nclasses. That is, a mean-square-error solution does not imply a solution in the sense of \\nthe perceptron training theorem. This uncertainty is the price of using an algorithm \\nwhose convergence is independent of the linear separability of the pattern classes.\\nEXAMPLE 12.9 :  Using the LMSE algorithm.\\nIt will be interesting to compare the performance of the LMSE algorithm using the same set of separa-\\nble iris data as in Example 12.8. Figure 12.26(a) is a plot of the error [Eq. (12-47)] as a function of epoch \\nfor 50 epochs, using Eq. (12-50) (with \\na\\n=\\n0\\n001\\n.)\\n to obtain the weights (we started with \\nw\\n()\\n) .\\n1\\n=\\n0\\n Each \\nepoch of training consisted of sequentially updating the weights\\n, one pattern at a time, and computing \\nEq. (12-47) for each weight and the corresponding pattern. At the end of the epoch, the errors were \\nadded and divided by 100 (the total number of patterns) to obtain the mean squared error (MSE). This \\nyielded one point of the curve of Fig. 12.26(a). After increasing and then decreasing rapidly, no appre-\\nciable difference in error occurred after about 20 epochs. For example, the error at the end of the 50th \\nepoch was 0.02 and, at the end of 1,000 epochs, it was 0.0192. Getting smaller error values is possible by \\nfurther decreasing \\na\\n,\\n but at the expense of slower decay in the error, as noted in Fig. 12.25. Keep in mind \\nalso that MSE is not directly proportional to correct recognition rate\\n.\\n0.1\\n0.2\\n0.3\\n0\\n10 20 30 40 50\\n1\\n\\x11\\n180 720 900\\n360 540\\nMean squared error (MSE)\\nTraining epochs\\nTraining epochs\\nb a\\nFIGURE 12.26\\nMSE as a function \\nof epoch for:  \\n(a) the linearly \\nseparable Iris \\nclasses (setosa \\nand versicolor); \\nand (b) the \\nlinearly nonsepa-\\nrable Iris classes \\n(versicolor and \\nvirginica).\\nDIP4E_GLOBAL_Print_Ready.indb   941\\n6/16/2017   2:17:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 942}),\n",
       " Document(page_content='942\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nThe weight vector at the end of 50 epochs of training was \\nw\\n=\\n[]\\n.\\n0.098 0.357 0.548 0.255 0.075\\n−−\\nT\\n \\nAll patterns were classiﬁed correctly into their two respective classes using this vector. That is, although \\nthe MSE did not become zero, the resulting weight vector was able to classify all the patterns correctly. \\nBut keep in mind that the LMSE algorithm does not always achieve 100% correct recognition of lin-\\nearly separable classes. \\nAs noted earlier, only the Iris setosa samples are linearly separable from the others. But the Iris ver-\\nsicolor and virginica samples are not. The perceptron algorithm would not converge when presented \\nwith these data, whereas the LMSE algorithm does. Figure 12.26(b) is the MSE as a function of training \\nepoch for these two data sets, obtained using the same values for \\nw\\n()\\n1\\n and \\na\\n as in (a). This time, it took \\n900 epochs for the MSE to stabilize at 0.09,\\n which is much higher than before. The resulting weight vec-\\ntor was \\nw\\n=\\n[\\n0.534 0.584 0.878 1.028  0.651] .\\n−−\\nT\\n Using this vector resulted in seven misclassiﬁcation \\nerrors out of 100 patterns, giving a recognition rate of 93%.\\nA classic example used to show the limitations of single linear decision boundar-\\nies (and hence single perceptron units) is the XOR classiﬁcation problem. The table \\nin Fig. 12.27(a) shows the deﬁnition of the XOR operator for two variables. As you \\ncan see, the XOR operation produces a logical true (1) value when either of the \\nvariables (but not both) is true; otherwise, the result is false (0). The XOR two-class \\npattern classiﬁcation problem is set up by letting each pair of values \\nA\\n and \\nB\\n be a \\npoint in 2-D space, and letting the true (1) XOR values deﬁne one class, and the false \\n(0) values deﬁne the other. In this case, we assigned the class \\nc\\n1\\n label to patterns \\n(, ) , (, ),\\n00\\n11\\n{}\\n and the \\nc\\n2\\n label to patterns \\n(, ) , (, ).\\n10\\n01\\n{}\\n A classiﬁer capable of solv-\\ning the XOR problem must respond with a value, say, 1, when a pattern from class \\nc\\n1\\n \\nis presented, and a different value, say, \\n0 or \\n−\\n1,\\n when the input pattern is from class \\nc\\n2\\n.\\n You can tell by inspection of Fig. 12.27(b) that a single linear decision boundary \\n(a straight line) cannot separate the two classes correctly\\n. This means that we cannot \\nsolve the problem with a single perceptron. The simplest linear boundary consists \\nof two straight lines, as Fig. 12.27(b) shows. A more complex, nonlinear, boundary \\ncapable of solving the problem is a quadratic function, as in Fig. 12.27(c).\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n0\\n1\\n0\\n1\\nAB\\nAB\\nXOR\\n1\\nc\\n∈\\n2\\nc\\n∈\\n1\\n1\\nx\\n1\\nx\\n2\\n+\\n+\\n–\\n–\\n0\\n1\\n1\\nx\\n1\\nx\\n2\\n+\\n–\\n0\\nb a\\nc\\nFIGURE 12.27\\n The XOR classiﬁcation problem in 2-D. (a) Truth table deﬁnition of the XOR \\noperator. (b) 2-D pattern classes formed by assigning the XOR truth values (1) to one pattern \\nclass, and false values (0) to another. The simplest decision boundary between the two classes \\nconsists of two straight lines. (c) Nonlinear (quadratic) boundary separating the two classes.\\nDIP4E_GLOBAL_Print_Ready.indb   942\\n6/16/2017   2:17:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 943}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n943\\nNatural questions at this point are: Can more than one perceptron solve the XOR \\nproblem? If so, what is the minimum number of units required? We know that a \\nsingle perceptron can implement one straight line, and we need to implement two \\nlines, so the obvious answers are: yes to the ﬁrst question, and two units to the sec-\\nond. Figure 12.28(a) shows the solution for two variables, which requires a total of \\nsix coefﬁcients because we need two lines. The solution coefﬁcients are such that, \\nfor either of the two patterns from class \\nc\\n1\\n,\\n one output is true (1) and the other is \\nfalse (0).\\n The opposite condition must hold for either pattern from class \\nc\\n2\\n.\\n This \\nsolution requires that we analyze two outputs\\n. If we want to implement the truth \\ntable, meaning that a single output should give the same response as the XOR func-\\ntion [the third column in Fig. 12.27(a)], then we need one additional perceptron. \\nFigure 12.28(b) shows the architecture for this solution. Here, one perceptron in the \\nﬁrst layer maps any input from one class into a 1, and the other perceptron maps a \\npattern from the other class into a 0. This reduces the four possible inputs into two \\noutputs, which is a two-point problem. As you know from Fig. 12.24, a single percep-\\ntron can solve this problem. Therefore, we need three perceptrons to implement the \\nXOR table, as in Fig. 12.28(b).\\nWith a little work, we could determine by inspection the coefﬁcients needed to \\nimplement either solution in Fig. 12.28. However, rather than dwell on that, we focus \\nattention in the following section on a more general, layered architecture, of which \\nthe XOR solution is a trivial, special case.\\nMULTILAYER FEEDFORWARD NEURAL NETWORKS\\nIn this section, we discuss the architecture and operation of multilayer neural net-\\nworks, and derive the equations of backpropagation used to train them. We then \\ngive several examples illustrating the capabilities of neural nets\\nModel of an Artiﬁcial Neuron \\nNeural networks are interconnected perceptron-like computing elements called \\nartificial neurons\\n. These neurons perform the same computations as the perceptron, \\nbut they differ from the latter in how they process the result of the computations. \\nAs illustrated in Fig. 12.23, the perceptron uses a “hard” thresholding function that \\noutputs two values, such as \\n+\\n1\\n and \\n−\\n1,\\n to perform classification. Suppose that in a \\nnetwork of perceptrons\\n, the output before thresholding of one of the perceptrons \\nis infinitesimally greater than zero. When thresholded, this very small signal will be \\nturned into a \\n+\\n1.\\n But a similarly small signal with the opposite sign would cause \\n1\\nx\\n2\\nx\\n1\\nw\\n2\\nw\\n1\\n3\\nw\\n1\\n4\\nw\\n5\\nw\\n6\\nw\\n1\\nx\\n2\\nx\\n1\\nw\\n2\\nw\\n1\\n3\\nw\\n1\\n4\\nw\\n5\\nw\\n6\\nw\\n7\\nw\\n8\\nw\\n9\\nw\\n1\\nb a\\nFIGURE 12.28\\n(a) Minimum  \\nperceptron solution \\nto the XOR problem \\nin 2-D. (b) A solution \\nthat implements the \\nXOR truth table in \\nFig. 12.27(a).\\nDIP4E_GLOBAL_Print_Ready.indb   943\\n6/16/2017   2:17:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 944}),\n",
       " Document(page_content='944\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\na large swing in value from \\n+\\n1\\n to \\n−\\n1.\\n Neural networks are formed from layers of \\ncomputing units\\n, in which the output of one unit affects the behavior of all units fol-\\nlowing it. The perceptron’s sensitivity to the sign of small signals can cause serious \\nstability problems in an interconnected system of such units, making perceptrons \\nunsuitable for layered architectures.\\nThe solution is to change the characteristic of the activation function from a hard-\\nlimiter to a smooth function. Figure 12.29 shows an example based on using the \\nactivation function\\n \\nhz\\ne\\nz\\n()\\n=\\n+\\n−\\n1\\n1\\n \\n(12-51)\\nwhere \\nz\\n is the result of the computation performed by the neuron,\\n as shown in Fig. \\n12.29. Except for more complicated notation, and the use of a smooth function rath-\\ner than a hard threshold, this model performs the same \\nsum-of-products\\n operations \\nas in Eq. (12-36) for the perceptron. Note that the \\nbias\\n term is denoted by \\nb\\n instead \\n0.0\\n0.5\\n1.0\\n1.0\\n−\\n0.5\\n−\\n0.0\\n0.5\\n1.0\\n0\\n2\\n4\\n6\\n2\\n−\\n024\\n6\\n6\\n−\\n4\\n−\\n2\\n−\\n024\\n6\\n6\\n−\\n4\\n−\\n2\\n−\\n024\\n6\\n6\\n−\\n4\\n−\\n1\\n()\\n1\\nz\\nhz\\ne\\n−\\n=\\n+\\n[]\\n() ()1 ()\\nh z hz hz\\n=−\\n/H11032\\n() t a n h ()\\nhz z\\n=\\n[]\\n2\\n() 1 ()\\nhz h z\\n=−\\n/H11032\\n() m a x ( 0 , )\\nhz z\\n=\\n1i f 0\\n()\\n0i f 0\\nz\\nhz\\nz\\n>\\n⎧\\n=\\n⎨\\n≤\\n⎩\\n/H11032\\nSigmoid\\ntanh\\nReLu\\nb a\\nc\\nFIGURE 12.30\\n Various activation functions. (a) Sigmoid. (b) Hyperbolic tangent (also has a sigmoid shape, but it is \\ncentered about 0 in both dimensions). (c) Rectiﬁer linear unit (ReLU).\\n1\\n.\\n.\\n.\\n.\\n.\\n()\\ni\\nb\\n+\\nℓ\\n1\\n1\\n() () ( 1 )\\nn\\nii\\nj j\\nj\\nza\\n−\\n=\\n=−\\n∑\\nℓ\\nℓℓ ℓ\\nw\\n2\\n(1 )\\na\\n−\\nℓ\\n1\\n(1 )\\na\\n−\\nℓ\\n1\\n(1 )\\nn\\na\\n−\\n−\\nℓ\\nℓ\\nh\\n()\\n() ()\\nii\\nah z\\n=\\nℓℓ\\n2\\n()\\ni\\nℓ\\nw\\n1\\n()\\ni\\nℓ\\nw\\n1\\n()\\nin\\n−\\nℓ\\nℓ\\nw\\n()\\ni\\nb\\nℓ\\nNeuron \\ni\\n in layer \\nℓ\\nFIGURE 12.29\\n  \\nModel of an \\nartiﬁcial neuron, \\nshowing all the \\noperations it \\nperforms. The  \\n“\\nℓ\\n” is used to  \\ndenote a  \\nparticular layer in \\na layered  \\nnetwork.\\nDIP4E_GLOBAL_Print_Ready.indb   944\\n6/16/2017   2:17:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 945}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n945\\nof \\nw\\nn\\n+\\n1\\n,\\n as we do the perceptron. It is customary to use different notation, typically \\nb\\n,\\n in neural networks to denote the bias term, so we are following convention. The \\nmore complicated notation used in F\\nig. 12.29, which we will explain shortly, is need-\\ned because we will be dealing with multilayer arrangements with several neurons \\nper layer. We use the symbol \\n“”\\nℓ\\n to denote layers.\\nAs you can see by comparing F\\nigs. 12.29 and 12.23, we use variable \\nz\\n to denote \\nthe sum-of-products computed by the neuron.\\n The output of the unit, denoted by \\na\\n, \\nis obtained by passing \\nz\\n through \\nh\\n.\\n We call \\nh\\n the \\nactivation function\\n,\\n and refer to its \\noutput, \\nah z\\n=\\n()\\n,\\n as the \\nactivation value\\n of the unit.\\n Note in Fig. 12.29 that the inputs \\nto a neuron are activation values from neurons in the previous layer. Figure 12.30(a) \\nshows a plot of \\nhz\\n()\\n from Eq. (12-51). Because this function has the shape of a sig-\\nmoid function,\\n the unit in Fig. 12.29 is sometimes called an\\n artiﬁcial sigmoid neuron\\n, \\nor simply a \\nsigmoid neuron\\n. Its derivative has a very nice form, expressible in terms \\nof \\nhz\\n()\\n [see Problem 12.16(a)]:\\n \\nhz\\nhz\\nz\\nhz hz\\n/H11032\\n()\\n()\\n() ()\\n=\\n∂\\n∂\\n=−\\n[]\\n1  \\n(12-52)\\nFigures 12.30(b) and (c) show two other forms of \\nhz\\n()\\n used frequently. The hyper-\\nbolic tangent also has the shape of a sigmoid function,\\n but it is symmetric about both \\naxes. This property can help improve the convergence of the backpropagation algo-\\nrithm to be discussed later. The function in Fig. 12.30(c) is called the \\nrectifier func-\\ntion\\n, and a unit using it is referred to a \\nrectifier linear unit \\n(ReLU). Often, you see \\nthe function itself referred to as the ReLU \\nactivation function\\n. Experimental results \\nsuggest that this function tends to outperform the other two in deep neural networks. \\nInterconnecting Neurons to Form a Fully Connected Neural Network\\nFigure 12.31 shows a generic diagram of a multilayer neural network. A \\nlayer\\n in the \\nnetwork is the set of nodes (neurons) in a column of the network. As indicated by \\nthe zoomed node in Fig. 12.31, all the nodes in the network are artificial neurons of \\nthe form shown in Fig. 12.29, except for the input layer, whose nodes are the com-\\nponents of an input pattern vector \\nx\\n. Therefore, the outputs (activation values) of \\nthe first layer are the values of the elements of \\nx\\n. The outputs of all other nodes are \\nthe activation values of neurons in a particular layer. Each layer in the network can \\nhave a different number of nodes, but each node has a \\nsingle\\n output. The multiple \\nlines shown at the outputs of the neurons in Fig. 12.31 indicate that the output of \\nevery node is connected to the input of all nodes in the next layer, to form a \\nfully \\nconnected\\n network. We also require that there be no loops in the network. Such \\nnetworks are called \\nfeedforward\\n \\nnetworks\\n. Fully connected, feedforward neural nets \\nare the only types of networks considered in this section. \\nWe obviously know the values of the nodes in the ﬁrst layer, and we can observe \\nthe values of the output neurons. All others are \\nhidden neurons, \\nand the layers that \\ncontain them are called\\n hidden layers\\n. Generally, we call a neural net with a single \\nhidden layer a \\nshallow neural network\\n, and refer to network with two or more hid-\\nden layers as a \\ndeep neural network\\n. However, this terminology is not universal, and \\nDIP4E_GLOBAL_Print_Ready.indb   945\\n6/16/2017   2:17:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 946}),\n",
       " Document(page_content='946\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nsometimes you will see the words “shallow” and “deep” used subjectively to denote \\nnetworks with a “few” and with “many” layers, respectively.\\nWe used the notation in Eq. (12-37) to label all the inputs and weights of a per-\\nceptron. In a neural network, the notation is more complicated because we have to \\naccount for neuron weights, inputs, and outputs within a layer, and also from layer \\nto layer. Ignoring layer notation for a moment, we denote by \\nw\\nij\\n the weight that \\nassociates the link connecting the \\noutput\\n of neuron \\nj\\n to the input of neuron \\ni.\\n That is, \\nx\\n1\\nx\\n2\\nx\\n3\\nx\\nn\\nLayer 1\\n(Input)\\nLayer\\n \\nL\\n(Output)\\nHidden Layers\\n(The number of nodes in \\nthe hidden layers can be \\ndifferent from layer to layer )\\nNeuron  in hidden layer \\ni\\nℓ\\nlayer \\nℓ\\n()\\ni\\na\\nℓ\\nOutput ( ) goes to all neurons in layer 1\\ni\\na\\n+\\nℓℓ\\n1\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n()\\ni\\nb\\n+\\nℓ\\n1\\n1\\n() () ( 1 )\\nn\\nii\\nj j\\nj\\nza\\n−\\n=\\n=−\\n∑\\nℓ\\nℓℓ ℓ\\nw\\n2\\n(1 )\\na\\n−\\nℓ\\n1\\n(1 )\\na\\n−\\nℓ\\n(1 )\\nj\\na\\n−\\nℓ\\n1\\n(1 )\\nn\\na\\n−\\n−\\nℓ\\nℓ\\nh\\n()\\n() ()\\nii\\nah z\\n=\\nℓℓ\\nFIGURE 12.31\\nGeneral model \\nof a feedforward, \\nfully connected \\nneural net. The \\nneuron is the \\nsame as in  \\nFig. 12.29. Note \\nhow the output of \\neach neuron goes \\nto the input of all \\nneurons in the \\nfollowing layer, \\nhence the name \\nfully connected\\n \\nfor this type of \\narchitecture.\\nDIP4E_GLOBAL_Print_Ready.indb   946\\n6/16/2017   2:17:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 947}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n947\\nthe ﬁrst subscript denotes the neuron that \\nreceives\\n the signal, and the second refers \\nto the neuron that \\nsends\\n the signal. Because \\ni\\n precedes \\nj\\n alphabetically, it would \\nseem to make more sense for \\ni\\n to send and for \\nj\\n to receive. The reason we use the \\nnotation as stated is to avoid a matrix transposition in the equation that describes \\npropagation of signals through the network. This notation is convention, but there is \\nno doubt that it is confusing, so special care is necessary to keep the notation straight. \\nBecause the biases depend only on the neuron containing it, a single subscript \\nthat associates a bias with a neuron is sufﬁcient. For example, we use \\nb\\ni\\n to denote the \\nbias value associated with the\\n i\\nth neuron in a given layer of the network. Our use of \\nb\\n instead of \\nw\\nn\\n+\\n1\\n (as we did for perceptrons) follows notational convention used in \\nneural networks. The weights, biases, and activation function(s) completely deﬁne a \\nneural network. Although the activation function of any neuron in a neural network \\ncould be different from the others, there is no convincing evidence to suggest that \\nthere is anything to be gained by doing so. We assume in all subsequent discussions \\nthat the same form of activation function is used in all neurons. \\nLet \\nℓ\\n denote a layer in the network, for \\nℓ…\\n=\\n12\\n,,\\n, .\\nL\\n With reference to Fig. 12.31, \\nℓ\\n=\\n1\\n denotes the input layer, \\nℓ\\n=\\nL\\n is the output layer, and all other values of \\nℓ\\n \\ndenote hidden layers\\n. The number of neurons in layer \\nℓ\\n is denoted \\nn\\nℓ\\n.\\n We have two \\noptions to include layer indexing in the parameters of a neural network.\\n We can do \\nit as a superscript, for example, \\nw\\nij\\nℓ\\n and \\nb\\ni\\nℓ\\n;\\n or we can use the notation \\nw\\nij\\n()\\nℓ\\n and \\nb\\ni\\n() .\\nℓ\\n The ﬁrst option is more prevalent in the literature on neural network. We use \\nthe second option because it is more consistent with the way we describe iterative \\nexpressions in the book,\\n and also because you may ﬁnd it easier to follow. Using this \\nnotation, the output (activation value) of neuron \\nk\\n in layer \\nℓ\\n is denoted \\na\\nk\\n() .\\nℓ\\nKeep in mind that our objective in using neural networks is the same as for per-\\nceptrons:\\n to determine the class membership of unknown input patterns. The most \\ncommon way to perform pattern classiﬁcation using a neural network is to assign a \\nclass label to each output neuron. Thus, a neural network with \\nn\\nL\\n outputs can clas-\\nsify an unknown pattern into one of \\nn\\nL\\n classes. The network assigns an unknown \\npattern vector \\nx\\n to class \\nc\\nk\\n if output neuron \\nk\\n has the largest activation value; that is, \\nif \\naL aL\\nkj\\n() () ,\\n>\\n \\njn j k\\nL\\n=\\n12\\n,, , ; .\\n…\\n≠\\n†\\n \\nIn this and the following section, the number of outputs of our neural networks \\nwill always equal the number of classes. But this is not a requirement. For instance, a \\nnetwork for classifying two pattern classes could be structured with a single output \\n(Problem 12.17 illustrates such a case) because all we need for this task is two states, \\nand a single neuron is capable of that. For three and four classes, we need three and \\nfour states, respectively, which can be achieved with two output neurons. Of course, \\nthe problem with this approach is that we would need additional logic to decipher \\nthe output combinations. It is simply more practical to have one neuron per output, \\nand let the neuron with the highest output value determine the class of the input.\\n†\\n  Instead of a sigmoid or similar function in the ﬁnal output layer, you will sometimes see a \\nsoftmax function\\n used \\ninstead. The concept is the same as we explained earlier, but the activation values in a softmax implementation \\nare given by \\naL zL zL\\nii\\nk\\ni\\n( ) exp[ ( )] exp[ ( )],\\n=\\n∑\\n where the summation is over all outputs. In this formulation, the \\nsum of all activations is 1, thus giving the outputs a probabilistic interpretation. \\nRemember, a bias is a \\nweight that is always \\nmultiplied by 1.\\nDIP4E_GLOBAL_Print_Ready.indb   947\\n6/16/2017   2:17:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 948}),\n",
       " Document(page_content='948\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nFORWARD PASS THROUGH A FEEDFORWARD NEURAL NETWORK\\nA \\nforward pass\\n through a neural network maps the input layer (i.e., values of \\nx\\n)\\n to \\nthe output layer\\n. The values in the output layer are used for determining the class of \\nan input vector. The equations developed in this section explain how a feedforward \\nneural network carries out the computations that result in its output. Implicit in the \\ndiscussion in this section is that the network parameters (weights and biases) are \\nknown. The important results in this section will be summarized in Table 12.2 at the \\nend of our discussion, but understanding the material that gets us there is important \\nwhen we discuss training of neural nets in the next section.\\nThe Equations of a Forward Pass\\nThe outputs of the layer 1 are the components of input vector \\nx\\n:\\n \\naxj n\\njj\\n()\\n, , ,\\n11\\n2\\n1\\n==\\n…\\n \\n(12-53)\\nwhere \\nnn\\n1\\n=\\n is the dimensionality of \\nx\\n.\\n As illustrated in Figs. 12.29 and 12.31, the \\ncomputation performed by neuron \\ni\\n in layer \\nℓ\\n is given by\\n \\nza b\\nii\\nj j i\\nj\\nn\\n() () ( ) ()\\nℓℓ ℓℓ\\nℓ\\n=− +\\n=\\n−\\n∑\\nw\\n1\\n1\\n1\\n \\n(12-54)\\nfor \\nin\\n=\\n12\\n,,\\n,\\n…\\nℓ\\n and \\nℓ…\\n=\\n2,\\n, .\\nL\\n Quantity \\nz\\ni\\n()\\nℓ\\n is called the \\nnet \\n(or\\n total\\n)\\n input\\n to \\nneuron \\ni\\n in layer \\nℓ\\n,\\n and is sometimes denoted by \\nnet\\ni\\n.\\n The reason for this terminol-\\nogy is that \\nz\\ni\\n()\\nℓ\\n is formed using \\nall\\n outputs from layer \\nℓ\\n−\\n1.\\n The output (activation \\nvalue) of neuron \\ni\\n in layer \\nℓ\\n is given by\\n \\nah z i n\\nii\\n() () , , ,\\nℓℓ …\\nℓ\\n=\\n()\\n=\\n12\\n \\n(12-55)\\nwhere \\nh\\n is an activation function.\\n The value of network \\noutput\\n node \\ni\\n is\\n \\naL h zL i n\\nii\\nL\\n() () ,, ,\\n=\\n()\\n=\\n12\\n…\\n \\n(12-56)\\nEquations (12-53) through (12-56) describe all the operations required to map the \\ninput of a fully connected feedforward network to its output.\\nEXAMPLE 12.10 :  Illustration of a forward pass through a fully connected neural network.\\nIt will be helpful to consider a simple numerical example. Figure 12.32 shows a three-layer neural network \\nconsisting of the input layer, one hidden layer, and the output layer. The network accepts three inputs, and \\nhas two outputs. Thus, this network is capable of classifying 3-D patterns into one of two classes.\\nThe numbers shown above the arrow heads on each input to a node are the weights of that node \\nassociated with the outputs from the nodes in the preceding layer. Similarly, the number shown in the \\noutput of each node is the activation value, \\na\\n,\\n of that node. As noted earlier, there is only one output \\nvalue for each node\\n, but it is routed to the input of every node in the next layer. The inputs associated \\nwith the 1’s are bias values. \\nLet us look at the computations performed at each node, starting with the ﬁrst (top) node in layer 2. \\nWe use Eq. (12-54) to compute the net input, \\nz\\n1\\n2\\n() ,\\n for that node:\\nDIP4E_GLOBAL_Print_Ready.indb   948\\n6/16/2017   2:17:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 949}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n949\\nx\\n1\\nx\\n2\\nx\\n3\\n0.1\\n3\\n0\\n1\\n0.2\\n1\\n1\\n1\\n0.4\\n1\\n0.2\\n0.6\\n0.3\\n0.6\\n0.4\\n0.3\\n0.1\\n0.2\\n0.1\\n0.1\\n0.4\\n0.7858\\n0.8176\\n0.6982\\n0.6694\\n1\\n2\\n3\\n3\\n0\\n1\\nx\\nx\\nx\\n⎡⎤\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n==\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\n⎣⎦\\nx\\nFIGURE 12.32\\nA small,  \\nfully connected,  \\nfeedforward \\nnet with labeled \\nweights, biases, \\nand outputs. The \\nactivation  \\nfunction is a \\nsigmoid.\\n \\nza b\\nj\\nj\\n1\\n1\\n3\\n1\\n2 2 1 2 01 3 02 0 06 1 0\\n() () () () (.) () (.) () (.) () .\\n=+ = + + +\\n=\\n∑\\nw\\n1j\\n4\\n41 3\\n=\\n.\\nWe obtain the output of this node using Eqs. (12-51) and (12-55):\\n \\nah z\\ne\\n11\\n13\\n22\\n1\\n1\\n0 7858\\n() ()\\n.\\n.\\n=\\n()\\n=\\n+\\n=\\n−\\nA similar computation gives the value for the output of the second node in the second layer,\\n \\nza b\\nj\\nj\\n2\\n1\\n3\\n2\\n2 2 1 2 04 3 03 0 01 1 0\\n() () () () (.) () (.) () (.) () .\\n=+ = + + +\\n=\\n∑\\nw\\n2j\\n2\\n21 5\\n=\\n.\\nand\\n \\nah z\\ne\\n22\\n15\\n22\\n1\\n1\\n0 8176\\n() ()\\n.\\n.\\n=\\n()\\n=\\n+\\n=\\n−\\nWe use the outputs of the nodes in layer 2 to obtain the net values of the neurons in layer 3:\\n \\nza b\\nj\\nj\\n1\\n1\\n2\\n1\\n3 3 2 3 0 2 0 7858 0 1 0 8176 0\\n() () () () (.) (. ) (.) (. )\\n=+ = + +\\n=\\n∑\\nw\\n1j\\n.. .\\n6 0 8389\\n=\\nThe output of this neuron is\\n \\nah z\\ne\\n11\\n0 8389\\n33\\n1\\n1\\n0 6982\\n() ()\\n.\\n.\\n=\\n()\\n=\\n+\\n=\\n−\\nSimilarly,\\n \\nza b\\nj\\nj\\n2\\n1\\n2\\n2\\n3 3 2 3 0 1 0 7858 0 4 0 8176 0\\n() () () () (.) (. ) (.) (. )\\n=+ = + +\\n=\\n∑\\nw\\n2j\\n.. .\\n3 0 7056\\n=\\nand\\n \\nah z\\ne\\n22\\n0 7056\\n32\\n1\\n1\\n0 6694\\n() ()\\n.\\n.\\n=\\n()\\n=\\n+\\n=\\n−\\nIf we were using this network to classify the input, we would say that pattern \\nx\\n belongs to class \\nc\\n1\\n \\nbecause \\naL aL\\n12\\n() () ,\\n>\\n where \\nL\\n=\\n3 and \\nn\\nL\\n=\\n2 in this case.\\nDIP4E_GLOBAL_Print_Ready.indb   949\\n6/16/2017   2:17:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 950}),\n",
       " Document(page_content='950\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nMatrix Formulation\\nThe details of the preceding example reveal that there are numerous individual \\ncomputations involved in a pass through a neural network. If you wrote a computer \\nprogram to automate the steps we just discussed, you would find the code to be very \\ninefficient because of all the required loop computations, the numerous node and \\nlayer indexing you would need, and so forth. We can develop a more elegant (and \\ncomputationally faster) implementation by using matrix operations. This means \\nwriting Eqs. (12-53) through (12-55) as follows. \\nFirst, note that the number of outputs in layer 1 is always of the same dimension \\nas an input pattern, \\nx\\n, so its matrix (vector) form is simple:\\n \\nax\\n()\\n1\\n=\\n \\n(12-57)\\nNext, we look at Eq. (12-54). We know that the summation term is just the inner \\nproduct of two vectors [see Eqs\\n. (12-37) and (12-38)]. However, this equation has \\nto be evaluated for all nodes in every layer past the first. This implies that a loop is \\nrequired if we do the computations node by node. The solution is to form a matrix, \\nW\\n()\\n,\\nℓ\\n that contains \\nall\\n the weights in layer \\nℓ\\n.\\n The structure of this matrix is simple—\\neach of its \\nro\\nws\\n contains the weights for one of the nodes in layer \\nℓ\\n:\\n \\nW\\n()\\n()\\n() ()\\n() () ()\\n(\\nℓ\\nℓℓ\\n/midhorizellipsisℓ\\nℓℓ /midhorizellipsisℓ\\n/vertellipsis/vertellipsis /midhorizellipsis\\nℓ\\nℓ\\nℓ\\nℓ\\n=\\n−\\n−\\nww w\\nww w\\nw\\n11\\n12\\n21\\nn1\\n1\\n22\\n2\\n1\\n1\\nn\\nn\\n)\\n)( ) ( )\\nww\\nnn\\nℓℓ\\nℓ\\nℓ/midhorizellipsis ℓ\\n2\\n1\\nn\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(12-58)\\nThen, we can obtain all the sum-of-products computations, \\nz\\ni\\n() ,\\nℓ\\n for layer \\nℓ\\n simulta-\\nneously:\\n \\nzW a b\\n()\\n()( ) () , , ,\\nℓℓ ℓ ℓ ℓ…\\n=− + =\\n12\\n3\\nL\\n \\n(12-59)\\nwhere \\na\\n()\\nℓ\\n−\\n1\\n is a column vector of dimension \\nn\\nℓ\\n−\\n1\\n1\\n×\\n containing the outputs of \\nlayer \\nℓ\\n−\\n1,\\n \\nb\\n()\\nℓ\\n is a column vector of dimension \\nn\\nℓ\\n×\\n1\\n containing the bias values \\nof all the neurons in layer \\nℓ\\n,\\n and \\nz\\n()\\nℓ\\n is an \\nn\\nℓ\\n×\\n1\\n column vector containing the net \\ninput values\\n, \\nzi n\\ni\\n() , , , , ,\\nℓ…\\nℓ\\n=\\n12\\n to all the nodes in layer \\nℓ\\n.\\n You can easily verify \\nthat Eq.\\n (12-59) is dimensionally correct.\\nBecause the activation function is applied to each net input independently of the \\nothers, the outputs of the network at any layer can be expressed in vector form as:\\n \\naz\\n()\\n()\\n()\\n()\\n()\\nℓℓ\\nℓ\\nℓ\\n/vertellipsis\\nℓ\\nℓ\\n=\\n[]\\n=\\n()\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\nh\\nhz\\nhz\\nhz\\nn\\n1\\n2\\n \\n(12-60)\\nImplementing Eqs. (12-57) through (12-60) requires just a series of matrix opera-\\ntions\\n, with no loops.\\nWith reference to our \\nearlier discussion on the \\norder of the subscripts \\ni\\n and \\nj\\n, if we had let \\ni\\n \\nbe the sending node \\nand \\nj\\n the receiver, this \\nmatrix would have to be \\ntransposed.\\nDIP4E_GLOBAL_Print_Ready.indb   950\\n6/16/2017   2:17:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 951}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n951\\nEXAMPLE 12.11 : Redoing Example 12.10 using matrix operations.\\nFigure 12.33 shows the same neural network as in Fig. 12.32, but with all its parameters shown in matrix \\nform. As you can see, the representation in Fig. 12.33 is more compact. Starting with\\n \\na\\n()\\n1\\n3\\n0\\n1\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nit follows that\\n \\nzW a b\\n()\\n()() ()\\n...\\n...\\n22 1 2\\n01 02 06\\n04 03 01\\n3\\n0\\n1\\n0\\n=+ =\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n+\\n..\\n.\\n.\\n.\\n4\\n02\\n13\\n15\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nThen,\\n \\naz\\n()\\n()\\n()\\n()\\n(.)\\n(.)\\n.\\n22\\n2\\n2\\n13\\n15\\n07\\n1\\n2\\n=\\n[]\\n=\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\nh\\nhz\\nhz\\nh\\nh\\n8 858\\n0 8176 .\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nWith \\na\\n()\\n2  as input to the next layer, we obtain\\n \\nzW a b\\n()\\n()() ()\\n..\\n..\\n.\\n.\\n.\\n33 23\\n02 01\\n01 04\\n0 7858\\n0 8176\\n0\\n=+ =\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n+\\n6 6\\n03\\n0 8389\\n0 7056\\n.\\n.\\n.\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nand, as before,\\n \\naz\\n()\\n()\\n()\\n()\\n(. )\\n(. )\\n33\\n3\\n3\\n0 8389\\n0 7056\\n1\\n2\\n=\\n[]\\n=\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\nh\\nhz\\nhz\\nh\\nh\\n⎦ ⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n0 6982\\n0 6694\\n.\\n.\\nThe clarity of the matrix formulation over the indexed notation used in Example 12.10 is evident.\\nEquations (12-57) through (12-60) are a signiﬁcant improvement over node-by-\\nnode computations\\n, but they apply only to one pattern. To classify multiple pat-\\ntern vectors, we would have to loop through each pattern using the same set of \\nmatrix equations per loop iteration. What we are after is one set of matrix equations \\nx\\n1\\nx\\n2\\nx\\n3\\n0.1 0.2 0.6\\n(2)\\n0.4 0.3 0.1\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\nW\\n0.4\\n(2)\\n0.2\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\nb\\n0.2 0.1\\n(3)\\n0.1 0.4\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\nW\\n0.6\\n(3)\\n0.3\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\nb\\n3\\n(1) 0\\n1\\n⎡⎤\\n⎢⎥\\n==\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nax\\n0.7858\\n(2)\\n0.8176\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\na\\n0.6982\\n(3)\\n0.6694\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\na\\nFIGURE 12.33\\nSame as Fig. 12.32, \\nbut using matrix \\nlabeling.\\nDIP4E_GLOBAL_Print_Ready.indb   951\\n6/16/2017   2:17:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 952}),\n",
       " Document(page_content='952\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\ncapable of processing \\nall\\n patterns in a single forward pass. Extending Eqs. (12-57) \\nthrough (12-60) to this more general formulation is straightforward. We begin by \\narranging all our input pattern vectors as \\ncolumns\\n of a single matrix, \\nX\\n, of dimension \\nnn\\np\\n×\\n where, as before, \\nn\\n is the dimensionality of the vectors and \\nn\\np\\n is the number \\nof pattern vectors. It follows from Eq. (12-57) that\\n \\nAX\\n()\\n1\\n=\\n \\n(12-61)\\nwhere each column of matrix \\nA\\n()\\n1\\n contains the initial activation values (i.e., the vec-\\ntor values) for one pattern.\\n This is a straightforward extension of Eq. (12-57), except \\nthat we are now dealing with an \\nnn\\np\\n×\\n matrix instead of an \\nn\\n×\\n1 vector.\\nT\\nhe parameters of a network do not change because we are processing more \\npattern vectors, so the weight matrix is as given in Eq. (12-58). This matrix is of size \\nnn\\nℓℓ\\n×\\n−\\n1\\n.\\n When \\nℓ\\n=\\n2,\\n we have that \\nW\\n()\\n2\\n is of size \\nnn\\n2\\n×\\n,\\n because \\nn\\n1\\n is always equal \\nto \\nn\\n. Then, extending the product term of Eq. (12-59) to use \\nA\\n()\\n2\\n instead of \\na\\n()\\n,\\n2  \\nresults in the matrix product \\nWA\\n()\\n() ,\\n22\\n which is of size \\n() () .\\nnn\\nn n nn\\npp\\n22\\n×× ×\\n=\\n \\nT\\no this, we have to add the bias vector for the second layer, which is of size \\nn\\n2\\n1\\n×\\n. \\nObviously\\n, we cannot add a matrix of size \\nnn\\np\\n2\\n×\\n and a vector of size \\nn\\n2\\n1\\n×\\n.\\n How-\\never\\n, as is true of the weight matrices, the bias vectors do not change because we \\nare processing more pattern vectors. We just have to account for one identical bias \\nvector, \\nb\\n()\\n,\\n2\\n per input vector. We do this by creating a matrix \\nB\\n()\\n2\\n of size \\nnn\\np\\n2\\n×\\n, \\nformed by concatenating column vector \\nb\\n()\\n2  \\nn\\np\\n times, horizontally. Then, Eq. (12-59) \\nwritten in matrix becomes \\nZ\\nWA B\\n() () () () .\\n22 1 2\\n=+\\n Matrix \\nZ\\n()\\n2\\n is of size \\nnn\\np\\n2\\n×\\n;\\n it \\ncontains the computation performed by Eq.\\n (12-59), but for \\nall\\n input patterns. That \\nis, each column of \\nZ\\n()\\n2\\n is exactly the computation performed by Eq. (12-59) for one \\ninput pattern.\\nT\\nhe concept just discussed applies to the transition from any layer to the next \\nin the neural network, provided that we use the weights and bias appropriate for a \\nparticular location in the network. Therefore, the full matrix version of Eq. (12-59) is\\n \\nZW A B\\n()\\n() ( ) ()\\nℓℓ ℓ ℓ\\n=− +\\n1  \\n(12-62)\\nwhere \\nW\\n()\\nℓ\\n is given by Eq. (12-58) and \\nB\\n()\\nℓ\\n is an \\nnn\\np\\nℓ\\n×\\n matrix whose columns are \\nduplicates of \\nb\\n()\\n,\\nℓ\\n the bias vector containing the biases of the neurons in layer \\nℓ\\n. \\nAll that remains is the matrix formulation of the output of layer \\nℓ\\n.\\n As Eq. (12-60) \\nshows\\n, the activation function is applied independently to each element of the vec-\\ntor \\nz\\n()\\n.\\nℓ\\n Because each column of \\nZ\\n()\\nℓ\\n is simply the application of Eq. (12-60) cor-\\nresponding to a particular input vector\\n, it follows that\\n \\nAZ\\n()\\n()\\nℓℓ\\n=\\n[]\\nh\\n \\n(12-63)\\nwhere activation function \\nh\\n is applied to each element of matrix \\nZ\\n()\\n.\\nℓ\\n \\nSummarizing the dimensions in our matrix formulation,\\n we have: \\nX\\n and \\nA\\n()\\n1  \\nare of size \\nnn\\np\\n×\\n, \\nZ\\n()\\nℓ\\n is of size \\nnn\\np\\nℓ\\n×\\n, \\nW\\n()\\nℓ\\n is of size \\nnn\\nℓℓ\\n×\\n−\\n1\\n, \\nA\\n()\\nℓ\\n−\\n1\\n is of \\nDIP4E_GLOBAL_Print_Ready.indb   952\\n6/16/2017   2:17:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 953}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n953\\nsize \\nnn\\np\\nℓ\\n−\\n1\\n×\\n, \\nB\\n()\\nℓ\\n is of size \\nnn\\np\\nℓ\\n×\\n,\\n and \\nA\\n()\\nℓ\\n is of size \\nnn\\np\\nℓ\\n×\\n.\\n Table 12.2 summa-\\nrizes the matrix formulation for the forward pass through a fully connected,\\n feed-\\nforward neural network for all pattern vectors. Implementing these operations in a \\nmatrix-oriented language like MATLAB is a trivial undertaking. Performance can \\nbe improved signiﬁcantly by using dedicated hardware, such as one or more graphics \\nprocessing units (GPUs). \\nThe equations in Table 12.2 are used to classify each of a set of patterns into one \\nof \\nn\\nL\\n pattern classes. Each column of output matrix \\nA\\n()\\nL\\n contains the activation \\nvalues of the \\nn\\nL\\n output neurons for a speciﬁc pattern vector. The class membership \\nof that pattern is given by the location of the output neuron with the highest activa-\\ntion value. Of course, this assumes we know the weights and biases of the network. \\nThese are obtained during training using backpropagation, as we explain next.\\nUSING BACKPROPAGATION TO TRAIN DEEP NEURAL NETWORKS\\nA neural network is defined completely by its weights, biases, and activation func-\\ntion. Training a neural network refers to using one or more sets of training patterns \\nto estimate these parameters. During training, we know the desired response of \\nevery output neuron of a multilayer neural net. However, we have no way of know-\\ning what the values of the outputs of hidden neurons should be. In this section, we \\ndevelop the equations of \\nbackpropagation\\n, the tool of choice for finding the value \\nof the weights and biases in a multilayer network. This \\ntraining by backpropaga-\\ntion\\n involves four basic steps: (1) inputting the pattern vectors; (2) a forward pass \\nthrough the network to classify all the patterns of the training set and determine the \\nclassification error; (3) a backward (backpropagation) pass that feeds the output \\nerror back through the network to compute the changes required to update the \\nparameters; and (4) updating the weights and biases in the network. These steps are \\nrepeated until the error reaches an acceptable level. We will provide a summary of \\nall principal results derived in this section at the end of the discussion (see Table \\n12.3). As you will see shortly, the principal mathematical tool needed to derive the \\nequations of backpropagation is the chain rule from basic calculus.\\nThe Equations of Backpropagation\\nGiven a set of training patterns and a multilayer feedforward neural network archi-\\ntecture, the approach in the following discussion is to find the network parameters \\nStep Description\\nEquations\\nStep 1 Input patterns\\nAX\\n()\\n1\\n=\\nStep 2 Feedforward\\nF\\nor \\nℓ…\\n=\\n2,\\n, ,\\nL\\n compute \\nZW A B\\n()\\n() ( ) ()\\nℓℓ ℓ ℓ\\n=− +\\n1  and \\nAZ\\n()\\n()\\nℓℓ\\n=\\n(\\n)\\nh\\nStep 3 Output\\nAZ\\n()\\n()\\nLhL\\n=\\n(\\n)\\nTABLE \\n12.2\\nSteps in the matrix computation of a forward pass through a fully connected, feedforward multilayer neural net.\\nDIP4E_GLOBAL_Print_Ready.indb   953\\n6/16/2017   2:17:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 954}),\n",
       " Document(page_content='954\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nthat minimize an \\nerror\\n (also called \\ncost\\n or \\nobjective\\n) \\nfunction\\n. Our interest is in \\nclassification performance, so we define the error function for a neural network as \\nthe average of the differences between desired and actual responses. Let \\nr\\n denote \\nthe desired response for a given pattern vector, \\nx\\n, and let \\na\\n()\\nL\\n denote the actu-\\nal response of the network to that input.\\n For example, in a ten-class recognition \\napplication, \\nr\\n and \\na\\n()\\nL\\n would be 10-D column vectors. The ten components of \\na\\n()\\nL\\n \\nwould be the ten outputs of the neural network,\\n and the components of \\nr\\n would be \\nzero, except for the element corresponding to the class of \\nx\\n, which would be 1. For \\nexample, if the input training pattern belongs to class 6, the 6th element of \\nr\\n would \\nbe 1 and the rest would be 0’s.\\nThe activation values of neuron \\nj\\n in the output layer is \\naL\\nj\\n() .\\n We deﬁne the error \\nof that neuron as\\n \\nEr a L\\njj j\\n=−\\n()\\n1\\n2\\n2\\n()\\n \\n(12-64)\\nfor \\njn\\nL\\n=\\n12\\n,, , ,\\n…\\n where \\nr\\nj\\n is the desired response of output neuron \\naL\\nj\\n()\\n for a \\ngiven pattern \\nx\\n.\\n The output error with respect to a single \\nx\\n is the sum of the errors of \\nall output neurons with respect to that vector:\\n \\nEE r a L\\nL\\nj\\nj\\nn\\njj\\nj\\nn\\nLL\\n== −\\n()\\n=−\\n==\\n∑∑\\n1\\n2\\n1\\n2\\n1\\n2\\n1\\n2\\n()\\n()\\n/H20648/H20648\\nra\\n \\n(12-65)\\nwhere the second line follows from the definition of the Euclidean vector norm. The \\ntotal network output error\\n over all training patterns is defined as the sum of the errors \\nof the individual patterns\\n. We want to find the weights that minimize this total error. \\nAs we did for the LMSE perceptron, we find the solution using gradient descent. \\nHowever, unlike the perceptron, we have no way for computing the gradients of the \\nweights in the hidden nodes. The beauty of backpropagation is that we can achieve an \\nequivalent result by propagating the output error back into the network.\\nThe key objective is to ﬁnd a scheme to adjust all weights in a network using train-\\ning patterns. In order to do this, we need to know how \\nE\\n changes with respect to the \\nweights in the network. The weights are contained in the expression for the net input \\nto each node [see Eq. (12-54)], so the quantity we are after is \\n∂∂\\nEz\\nj\\n()\\nℓ\\n where, as \\ndeﬁned in Eq.\\n (12-54), \\nz\\nj\\n()\\nℓ\\n is the net input to node \\nj\\n in layer \\nℓ\\n.\\n In order to simplify \\nthe notation later\\n, we use the symbol \\nd\\nj\\n()\\nℓ\\n to denote \\n∂∂\\nEz\\nj\\n() .\\nℓ\\n Because backpropa-\\ngation starts with the output and works backward from there\\n, we look ﬁrst at\\n \\nd\\nj\\nj\\nL\\nE\\nzL\\n()\\n()\\n=\\n∂\\n∂\\n \\n(12-66)\\nWe can express this equation in terms of the output \\naL\\nj\\n()\\n using the chain rule:\\nSee Eqs. (2-50) and \\n(2-51) regarding the \\nEuclidean vector norm.\\nWhen the meaning is \\nclear, we sometimes \\ninclude the bias term in \\nthe word “weights.” \\nWe use “\\nj\\n” generically \\nto mean any node in the \\nnetwork. We are not \\nconcerned at the moment \\nwith inputs to, or outputs \\nfrom, a node. \\nDIP4E_GLOBAL_Print_Ready.indb   954\\n6/16/2017   2:17:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 955}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n955\\n \\nd\\nj\\njj\\nj\\njj\\nj\\nj\\nL\\nE\\nzL\\nE\\naL\\naL\\nzL\\nE\\naL\\nhz L\\nzL\\n()\\n() ()\\n()\\n() ()\\n()\\n(\\n=\\n∂\\n∂\\n=\\n∂\\n∂\\n∂\\n∂\\n=\\n∂\\n∂\\n∂\\n()\\n∂\\n) )\\n()\\n()\\n=\\n∂\\n∂\\n()\\nE\\naL\\nhzL\\nj\\nj\\n/H11032\\n \\n(12-67)\\nwhere we used Eq. (12-56) to obtain the last expression in the first line. This equa-\\ntion gives us the value of \\nd\\nj\\nL\\n()\\n in terms of quantities that can be observed or com-\\nputed.\\n For example, if we use Eq. (12-64) as our error measure, and Eq. (12-52) for \\nhzx\\nj\\n/H11032\\n()\\n,\\n()\\n then\\n \\nd\\njj j j j\\nLh z L h z L a L r\\n() () () ()\\n=\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n1\\n \\n(12-68)\\nwhere we interchanged the order of the terms. The \\nhz L\\nj\\n()\\n()\\n are computed in the \\nforward pass, \\naL\\nj\\n()\\n can be observed in the output of the network, and \\nr\\nj\\n is given \\nalong with \\nx\\n during training. Therefore, we can compute \\nd\\nj\\nL\\n() .\\nBecause the relationship between the net input and the output of any neuron in \\nany layer (except the ﬁrst) is the same\\n, the form of Eq. (12-66) is valid for any node \\nj\\n in any hidden layer:\\n \\nd\\nj\\nj\\nE\\nz\\n()\\n()\\nℓ\\nℓ\\n=\\n∂\\n∂\\n \\n(12-69)\\nThis equation tells us how \\nE\\n changes with respect to a change in the net input to any \\nneuron in the network.\\n What we want to do next is express \\nd\\nj\\n()\\nℓ\\n in terms of \\nd\\nj\\n() .\\nℓ\\n+\\n1  \\nBecause we will be proceeding backward in the network,\\n this means that if we have \\nthis relationship, then we can start with \\nd\\nj\\nL\\n()\\n and find \\nd\\nj\\nL\\n() .\\n−\\n1\\n We then use this \\nresult to find \\nd\\nj\\nL\\n() ,\\n−\\n2\\n and so on until we arrive at layer 2. We obtain the desired \\nexpression using the chain rule (see Problem 12.25):\\n \\nd\\nd\\nj\\nji\\ni\\nj\\nj\\nj\\ni\\ni\\nE\\nz\\nE\\nz\\nz\\na\\na\\nz\\n()\\n() ( )\\n()\\n()\\n()\\n()\\n(\\nℓ\\nℓℓ\\nℓ\\nℓ\\nℓ\\nℓ\\nℓ\\n=\\n∂\\n∂\\n=\\n∂\\n∂+\\n∂+\\n∂\\n∂\\n∂\\n=\\n∑\\n1\\n1\\n+ +\\n∂+\\n∂\\n()\\n=\\n()\\n++\\n∑\\n∑\\n1\\n1\\n11\\n)\\n()\\n()\\n()\\n( ) () ()\\nz\\na\\nhz\\nhz\\ni\\nj\\ni\\nj\\nji j i\\ni\\nℓ\\nℓ\\nℓ\\nℓℓ ℓ\\n/H11032\\n/H11032\\nw\\nd\\n \\n(12-70)\\nfor \\nℓ…\\n=−\\n−\\nLL\\n12 2\\n,, ,\\n where we used Eqs. (12-55) and (12-69) to obtain the mid-\\ndle line\\n, and Eq. (12-54), plus some rearranging to obtain the last line.\\nThe preceding development tells us how we can start with the error in the output \\n(which we can compute) and obtain how that error changes as function of the net \\ninputs to every node in the network. This is an intermediate step toward our ﬁnal \\nobjective, which is to obtain expressions for \\n∂∂\\nE\\nw(\\nij\\nℓ\\n)\\n and \\n∂∂\\nE\\ni\\nb(\\nℓ\\n)\\n in terms of \\nd\\njj\\nEz\\n() () .\\nℓℓ\\n=∂\\n For this, we use the chain rule again:\\nDIP4E_GLOBAL_Print_Ready.indb   955\\n6/16/2017   2:17:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 956}),\n",
       " Document(page_content='956\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n \\n∂\\n∂\\n=\\n∂\\n∂\\n∂\\n∂\\n=\\n∂\\n∂\\n=−\\nEE\\nz\\nz\\nz\\na\\ni\\ni\\ni\\ni\\nj\\nw( w(\\nw(\\nij\\nij\\nij\\nℓℓ\\nℓ\\nℓ\\nℓ\\nℓ\\nℓ\\nℓ\\n)( )\\n()\\n)\\n()\\n()\\n)\\n()\\nd\\n1\\nd d\\ni\\n()\\nℓ\\n \\n(12-71)\\nwhere we used Eq. (12-54), Eq. (12-69), and interchanged the order of the results \\nto clarify matrix formulations later in our discussion.\\n Similarly (see Problem 12.26),\\n \\n∂\\n∂\\n=\\nE\\nb\\ni\\ni\\n()\\n()\\nℓ\\nℓ\\nd\\n \\n(12-72)\\nNow we have the rate of change of \\nE\\n with respect to the network weights and biases \\nin terms of quantities we can compute\\n. The last step is to use these results to update \\nthe network parameters using gradient descent:\\n \\nww\\nw\\nw\\nij ij\\nij\\nij\\ni j\\nE\\na\\n() ()\\n()\\n()\\n() ()\\n()\\nℓℓ\\nℓ\\nℓ\\nℓℓ ℓ\\n=−\\n∂\\n∂\\n=− −\\na\\nad\\n1\\n \\n(12-73)\\nand\\n \\nbb\\nE\\nb\\nb\\nii\\ni\\nii\\n() ()\\n()\\n() ()\\nℓℓ\\nℓ\\nℓℓ\\n=−\\n∂\\n∂\\n=−\\na\\nad\\n \\n(12-74)\\nfor \\nℓ…\\n=−\\n−\\nLL\\n12 2\\n,, ,\\n where the \\na\\n’s\\n are computed in the forward pass, and the \\nd\\n’s\\n \\nare computed during backpropagation.\\n As with the perceptron, \\na\\n is the learning \\nrate constant used in gradient descent.\\n There are numerous approaches that attempt \\nto find optimal learning rates, but ultimately this is a problem-dependent parameter \\nthat involves experimenting. A reasonable approach is to start with a small value of \\na\\n (e.g., 0.01), then experiment with vectors from the training set to determine a suit-\\nable value in a given application.\\n Remember, \\na\\n is used only during training, so it has \\nno effect on post-training operating performance\\n.\\nMatrix Formulation\\nAs with the equations that describe the forward pass through a neural network, the \\nequations of backpropagation developed in the previous discussion are excellent for \\ndescribing how the method works at a fundamental level, but they are clumsy when \\nit comes to implementation. In this section, we follow a procedure similar to the one \\nwe used for the forward pass to develop the matrix equations for backpropagation.\\nDIP4E_GLOBAL_Print_Ready.indb   956\\n6/16/2017   2:17:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 957}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n957\\nAs before, we arrange all the pattern vectors as columns of matrix \\nX\\n,\\n and package \\nthe weights of layer \\nℓ\\n as matrix \\nW\\n()\\n.\\nℓ\\n We use \\nD\\n()\\nℓ\\n to denote the matrix equiva-\\nlent of \\nÎ\\n()\\n,\\nℓ\\n the vector containing the errors in layer \\nℓ\\n.\\n Our ﬁrst step is to ﬁnd an \\nexpression for \\nD\\n()\\n.\\nL\\n We begin at the output and proceed backward, as before. From \\nEq.\\n (12-67),\\nD\\nd\\nd\\nd\\n()\\n()\\n()\\n()\\n()\\n()\\nL\\nL\\nL\\nL\\nE\\naL\\nhzL\\nE\\na\\nn\\nL\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n∂\\n∂\\n()\\n∂\\n∂\\n1\\n2\\n1\\n1\\n/vertellipsis\\n/H11032\\n2 2\\n2\\n()\\n()\\n()\\n()\\nL\\nhzL\\nE\\naL\\nhz L\\nn\\nn\\nL\\nL\\n/H11032\\n/H11032\\n()\\n∂\\n∂\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n=\\n∂\\n/vertellipsis\\nE E\\naL\\nE\\naL\\nE\\naL\\nhzL\\nn\\nL\\n∂\\n∂\\n∂\\n∂\\n∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n()\\n1\\n2\\n1\\n()\\n()\\n()\\n()\\n/vertellipsis\\n}\\n/H11032\\nh\\nhzL\\nhz L\\nn\\nL\\n/H11032\\n/H11032\\n2\\n()\\n()\\n()\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n/vertellipsis\\n \\n(12-75)\\nwhere, as defined in Section 2.6, “\\n}\\n” denotes elementwise multiplication (of two \\nvectors in this case).\\n We can write the vector on the left of this symbol as \\n∂∂\\nEL\\na\\n() ,\\n \\nand the vector on the right as \\nhL\\n/H11032\\nz\\n()\\n.\\n()\\n Then, we can write Eq. (12-75) as\\n \\nÎ\\n()\\n()\\n()\\nL\\nE\\nL\\nhL\\n=\\n∂\\n∂\\n()\\na\\nz\\n}\\n/H11032\\n \\n(12-76)\\nThis \\nn\\nL\\n×\\n1\\n column vector contains the activation values of all the output neurons \\nfor \\none\\n pattern vector\\n. The only error function we use in this chapter is a quadratic \\nfunction, which is given in vector form in Eq. (12-65). The partial of that quadratic \\nfunction with respect to \\na\\n()\\nL\\n is \\nar\\n()\\nL\\n−\\n()\\n which, when substituted into Eq. (12-76), \\ngives us\\n \\nÎ\\n()\\n() ()\\nLL h L\\n=−\\n() ( )\\nar z\\n}\\n/H11032\\n \\n(12-77)\\nColumn vector \\nÎ\\n()\\nL\\n accounts for one pattern vector. To account for \\nall\\n \\nn\\np\\n patterns \\nsimultaneously we form a matrix \\nD\\n()\\n,\\nℓ\\n whose columns are the \\nÎ\\n()\\nL\\n from Eq. (12-77), \\nevaluated for a specific pattern vector\\n. This is equivalent to writing Eq. (12-77) \\ndirectly in matrix form as\\n \\nDAR Z\\n()\\n() ()\\nLL h L\\n=−\\n() ( )\\n}\\n/H11032\\n \\n(12-78)\\nEach column of \\nA\\n()\\nL\\n is the network output for one pattern. Similarly, each col-\\numn of \\nR\\n is a binary vector with a 1 in the location corresponding to the class of a \\nparticular pattern vector\\n, and 0’s elsewhere, as explained earlier. Each column of \\nthe difference \\nAR\\n()\\nL\\n−\\n()\\n contains the components of \\n/H20648/H20648\\nar\\n−\\n.\\n Therefore, squaring \\nthe elements of a column,\\n adding them, and dividing by 2 is the same as computing \\nthe error measure defined in Eq. (12-65), for one pattern. Adding all the column \\ncomputations gives an average measure of error for all the patterns. Similarly, the \\ncolumns of matrix \\nhL\\n/H11032\\nZ\\n()\\n()\\n are values of the net inputs to all output neurons, with \\nDIP4E_GLOBAL_Print_Ready.indb   957\\n6/16/2017   2:17:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 958}),\n",
       " Document(page_content=\"958\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\neach column corresponding to one pattern vector. All matrices in Eq. (12-78) are of \\nsize \\nnn\\nLp\\n×\\n.\\nFollowing a similar line of reasoning, we can express Eq. (12-70) in matrix form as\\n \\nDW D Z\\n(\\n) () () ( )\\nℓℓ ℓ ℓ\\n=+ +\\n(\\n)\\n()\\nT\\nh\\n11\\n}\\n'\\n \\n(12-79)\\nIt is easily confirmed by dimensional analysis that the matrix \\nD\\n()\\nℓ\\n is of size \\nnn\\np\\nℓ\\n×\\n \\n(see Problem 12.27).\\n Note that Eq. (12-79) uses the weight matrix transposed. This \\nreflects the fact that the inputs to layer \\nℓ\\n are coming from layer \\nℓ\\n+\\n1,\\n because in \\nbackpropagation we move in the direction opposite of a forward pass\\n.\\nWe complete the matrix formulation by expressing the weight and bias update \\nequations in matrix form. Considering the weight matrix ﬁrst, we can tell from Eqs. \\n(12-70) and (12-73) that we are going to need matrices \\nW\\n()\\n,\\nℓ\\n \\nD\\n()\\n,\\nℓ\\n and \\nA\\n()\\n.\\nℓ\\n−\\n1  \\nW\\ne already know that \\nW\\n()\\nℓ\\n is of size \\nnn\\nℓℓ\\n×\\n−\\n1\\n and that \\nD\\n()\\nℓ\\n is of size \\nnn\\np\\nℓ\\n×\\n.\\n Each \\ncolumn of matrix \\nA\\n()\\nℓ\\n−\\n1\\n is the set of outputs of the neurons in layer \\nℓ\\n−\\n1\\n for one \\npattern vector\\n. There are \\nn\\np\\n patterns, so \\nA\\n()\\nℓ\\n−\\n1\\n is of size \\nnn\\np\\nℓ\\n−\\n1\\n×\\n.\\n From Eq. (12-\\n73) we infer that \\nA\\n post-multiplies \\nD\\n,\\n so we are also going to need \\nA\\nT\\n() ,\\nℓ\\n−\\n1\\n which \\nis of size \\nnn\\np\\n×\\nℓ\\n−\\n1\\n.\\n Finally, recall that in a matrix formulation, we construct a matrix \\nB\\n()\\nℓ\\n of size \\nnn\\np\\nℓ\\n×\\n whose columns are copies of vector \\nb\\n()\\n,\\nℓ\\n which contains all the \\nbiases in layer \\nℓ\\n. \\nNext,\\n we look at updating the biases. We know from Eq. (12-74) that each ele-\\nment \\nb\\ni\\n()\\nℓ\\n of \\nb\\n()\\nℓ\\n is updated as \\nbb\\nii i\\n() () () ,\\nℓℓ ℓ\\n=−\\nad\\n for \\nin\\n=\\n12\\n,,\\n, .\\n…\\nℓ\\n Therefore, \\nbb\\n()\\n() () .\\nℓℓ ℓ\\n=−\\na\\nÎ\\n But this is for \\none\\n pattern,\\n and the columns of \\nD\\n()\\nℓ\\n are the \\nÎ\\n()\\n’\\nℓ\\ns\\n for \\nall\\n patterns in the training set.\\n This is handled in a matrix formulation by \\nusing the \\naverage\\n of the columns of \\nD\\n()\\nℓ\\n (this is the average error over all patterns) \\nto update \\nb\\n()\\n.\\nℓ\\n \\nPutting it all together results in the following two equations for updating the \\nnetwork parameters:\\n \\nWW D A\\n() () () ( )\\nℓℓ ℓ ℓ\\n=− −\\na\\nT\\n1\\n \\n(12-80)\\nand\\n \\nbb\\n()\\n() ()\\nℓℓ ℓ\\n=−\\n=\\n∑\\na\\nÎ\\nk\\nk\\nn\\np\\n1\\n \\n(12-81)\\nwhere \\nÎ\\nk\\n()\\nℓ\\n is the \\nk\\nth column of matrix \\nD\\n()\\n.\\nℓ\\n As before, we form matrix \\nB\\n()\\nℓ\\n of size \\nnn\\np\\nℓ\\n×\\n by concatenating \\nb\\n()\\nℓ\\n \\nn\\np\\n times in the horizontal direction:\\n \\nBb\\n()\\n()\\nℓℓ\\n=\\n{}\\nconcatenate\\ntimes\\nn\\np\\n \\n(12-82)\\nAs we mentioned earlier, backpropagation consists of four principal steps: (1) \\ninputting the patterns\\n, (2) a forward pass, (3) a backpropagation pass, and (4) a \\nparameter update step. The process begins by specifying the initial weights and bias-\\nes as (small) random numbers. Table 12.3 summarizes the matrix formulations of \\nthese four steps. During training, these steps are repeated for a number of speciﬁed \\nepochs, or until a predeﬁned measure of error is deemed to be small enough. \\nDIP4E_GLOBAL_Print_Ready.indb   958\\n6/16/2017   2:17:39 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 959}),\n",
       " Document(page_content=\"12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n959\\nThere are two major types of errors in which we are interested. One is the \\nclas-\\nsiﬁcation error\\n, which we compute by counting the number of patterns that were \\nmisclassiﬁed and dividing by the total number of patterns in the training set. Mul-\\ntiplying the result by 100 gives the percentage of patterns misclassiﬁed. Subtracting \\nthe result from 1 and multiplying by 100 gives the percent correct recognition. The \\nother is the \\nmean squared error\\n (MSE), which is based on actual values of \\nE.\\n For \\nthe error deﬁned in Eq. (12-65), this value is obtained (for one pattern) by squaring \\nthe elements of a column of the matrix \\nAR\\n()\\n,\\nL\\n−\\n()\\n adding them, and dividing by \\nthe result by 2 (see Problem 12.28). Repeating this operation for all columns and \\ndividing the result by the number of patterns in \\nX\\n gives the MSE over the entire \\ntraining set.\\n \\nEXAMPLE 12.12 :  Using a fully connected neural net to solve the XOR problem.\\nFigure 12.34(a) shows the XOR classiﬁcation problem discussed previously (the coordinates were cho-\\nsen to center the patterns for convenience in indexing, but the spatial relationships are as before). Pat-\\ntern matrix \\nX\\n and class membership matrix \\nR\\n are:\\n \\nXR\\n=\\n−−\\n−−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n111\\n1\\n11 1 1\\n1100\\n0011\\n;\\nWe speciﬁed a neural network having three layers, with two nodes each (see Fig. 12.35). This is the small-\\nest network consistent with our architecture in F\\nig. 12.31. Comparing it to the minimum perceptron \\narrangements in Fig. 12.28(a), we see that our neural network performs the same basic function, in the \\nsense that it has two inputs and two outputs.\\nWe used \\na\\n=\\n10\\n.,\\n an initial set of Gaussian random weights of zero mean and standard deviation of \\n0.02,\\n and the activation function in Eq. (12-51). We then trained the network for 10,000 epochs (we \\nused a large number of epochs to get close to the values in the \\nR\\n; we discuss below solutions with fewer \\nepochs). The resulting weights and biases were:\\nStep Description\\nEquations\\nStep 1 Input patterns\\nAX\\n()\\n1\\n=\\nStep 2 Forward pass\\nF\\nor \\nℓ…\\n=\\n2,\\n, ,\\nL\\n compute: \\nZW A B\\n()\\n() ( ) () ;\\nℓℓ ℓ ℓ\\n=− +\\n1  \\nAZ\\n()\\n();\\nℓℓ\\n=\\n(\\n)\\nh\\n \\nh\\n/H11032\\nZ\\n()\\n;\\nℓ\\n(\\n)\\n and \\nDAR Z\\n()\\n() ()\\nLL h L\\n=−\\n(\\n)\\n(\\n)\\n}\\n/H11032\\nStep 3 Backpropagation\\nF\\nor \\nℓ…\\n=−\\n−\\nLL\\n12 2\\n,, , ,\\n compute \\nDW D Z\\n(\\n) () () ( )\\nℓℓ ℓ ℓ\\n=+ +\\n(\\n)\\n(\\n)\\nT\\nh\\n11\\n}\\n'\\nStep 4 Update weights and \\nbiases\\nF\\nor \\nℓ…\\n=\\n2,\\n, ,\\nL\\n let \\nWW D A\\n()\\n() () ( ) ,\\nℓℓ ℓ ℓ\\n=− −\\na\\nT\\n1  \\nbb\\n()\\n()\\n() ,\\nℓℓ ℓ\\n=−\\n=\\n∑\\na\\nÎ\\nk\\nk\\nn\\np\\n1\\n \\nand \\nBb\\n()\\n(),\\nℓℓ\\n=\\n{}\\nconcatenate\\ntimes\\nn\\np\\n where the \\nÎ\\nk\\n()\\nℓ\\n are the columns of \\nD\\n()\\nℓ\\nTABLE \\n12.3\\nMatrix formulation for training a feedforward, fully connected multilayer neural network using backpropagation. \\nSteps 1–4 are for one epoch of training. \\nX\\n, \\nR\\n, and the learning rate parameter \\na\\n,\\n are provided to the network for train-\\ning. The network is initialized by specifying weights, \\nW\\n()\\n,\\n1\\n and biases, \\nB\\n()\\n,\\n1\\n as small random \\nnumbers\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   959\\n6/16/2017   2:17:41 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 960}),\n",
       " Document(page_content='960\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nWb W\\n() ; ()\\n.\\n.\\n;(\\n22\\n4 590\\n4 486\\n3\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n4.792 4.792\\n4.486 4.486\\n)\\n);\\n(\\n)\\n.\\n.\\n=\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n9.180 9.429\\n9.178 9.427\\nb\\n3\\n4 420\\n4 419\\nFigure 12.35 shows the neural net based on these values.\\nW\\nhen presented with the four training patterns after training was completed, the results at the two \\noutputs should have been equal to the values in \\nR\\n. Instead, the values were close:\\n \\nA\\n()\\n.\\n3\\n0\\n010\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n0.987 0.990 0.010 0.010\\n0.013 0.990 0.990\\nThese weights and biases, along with the sigmoid activation function, completely specify our trained \\nneural network.\\n To test its performance with values other than the training patterns, which we know it \\nclassiﬁes correctly, we created a set of 2-D test patterns by subdividing the pattern space into increments \\nof 0.1, from \\n−\\n15\\n.\\n to 1.5 in both directions, and classiﬁed the resulting points using a forward pass through \\nx\\n1\\nx\\n2\\n4.792 4.792\\n(2)\\n4.486 4.486\\n⎡⎤\\n=\\n⎢⎥\\n⎣⎦\\nW\\n4.590\\n(2)\\n4.486\\n⎡⎤\\n=\\n⎢⎥\\n−\\n⎣⎦\\nb\\n9.180 9.429\\n(3)\\n9.178 9.427\\n−\\n⎡⎤\\n=\\n⎢⎥\\n−\\n⎣⎦\\nW\\n4.420\\n(3)\\n4.419\\n⎡⎤\\n=\\n⎢⎥\\n−\\n⎣⎦\\nb\\nFIGURE 12.35\\nNeural net used \\nto solve the XOR \\nproblem, showing \\nthe weights and \\nbiases learned \\nvia training using \\nthe equations in \\nTable 12.3. \\n1\\n−\\n1\\n–\\n1\\n1\\nx\\n1\\nx\\n2\\n0\\n1\\n0.2\\n0.4\\n0.6\\n0.8\\n1.5\\n–\\n1.5\\n0.5\\n0.5\\n–\\n0.5 –\\n0.5\\n1.5\\n1.0\\n–\\n1.0\\n−\\n0.0\\n1.0\\n1.0\\n0.0\\n–1.5\\n1\\nc\\n∈\\n2\\nc\\n∈\\nb a\\nc\\nFIGURE 12.34\\n Neural net solution to the XOR problem. (a) Four patterns in an XOR arrangement. (b) Results of \\nclassifying additional points in the range \\n−\\n15\\n.\\n to \\n15\\n.\\n in increments of 0.1. All solid points were classiﬁed as belong-\\ning to class \\nc\\n1\\n and all open circles were classiﬁed as belonging to class \\nc\\n2\\n.\\n Together, the two lines separating the \\nregions constitute the decision boundary [compare with F\\nig. 12.27(b)]. (c) Decision surface, shown as a mesh. The \\ndecision boundary is the pair of dashed, white lines in the intersection of the surface and a plane perpendicular to \\nthe vertical axis, intersecting that axis at 0.5. (Figure (c) is shown in a different perspective than (b) in order to make \\nall four patterns visible.)\\nDIP4E_GLOBAL_Print_Ready.indb   960\\n6/16/2017   2:17:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 961}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n961\\nthe network. If the activation value of output node 1 was greater than the activation value of output \\nnode 2, the pattern was assigned to class \\nc\\n1\\n;\\n otherwise, it was assigned to class \\nc\\n2\\n.\\n Fig. 12.34(b) is a plot \\nof the results\\n. Solid dots are points classiﬁed into to class \\nc\\n1\\n,\\n and white dots were classiﬁed as belong-\\ning to class \\nc\\n2\\n.\\n The boundaries between these two regions (shown as solid black lines) are precisely the \\nboundaries in F\\nig. 12.27(b). Thus, our small neural network found the simplest boundary between the \\ntwo classes, and thus performed the same function as the perceptron arrangement in Fig. 12.28(a).\\nFigure 12.34(c) shows the decision surface. This ﬁgure is analogous to Fig. 12.24(b), but it intersects \\nthe plane twice because the patterns are not linearly separable. Our decision boundary is the intersec-\\ntion of the decision surface with a plane perpendicular to the vertical axis, and intersecting that axis at \\n0.5. This is because the range of values in the output nodes is in the \\n[, ]\\n01\\n range, and we assign a pattern \\nto the class for which one the two outputs had the largest value\\n. The plane is shown shaded in the ﬁg-\\nure, and the decision boundary is shown as dashed white lines. We adjusted the viewing perspective of \\nFig. 12.34(c) so you can see all the XOR points.\\nBecause classiﬁcation in this case is based on selecting the largest output, we do not need the outputs \\nto be so close to 1 and 0 as we showed above, provided they are greater for the patterns of class \\nc\\n1\\n and \\nconversely for the patterns of class \\nc\\n2\\n.\\n This means that we can train the network using fewer epochs \\nand still achieve correct recognition.\\n For example, correct classiﬁcation of the XOR patterns can be \\nachieved using the parameters learned with as few as 150 epochs. Figure 12.36 shows the reason why this \\nis possible. By the end of the 1000th epoch, the mean squared error has decreased almost to zero, so we \\nwould expect it to decrease very little from there for 10,000 epochs. We know from the preceding results \\nthat the neural net performed ﬂawlessly using the weights learned with 10,000 epochs. Because the \\nerror for 1,000 and 10,000 epochs is close, we can expect the weights to be close as well. At 150 epochs, \\nthe error has decreased by close to 90% from its maximum, so the probability that the weights would \\nperform well should be reasonably high, which was true in this case.\\nEXAMPLE 12.13 :  Using neural nets to classify multispectral image data.\\nIn this example, we compare the recognition performance of the Bayes classiﬁer we discussed in Sec-\\ntion 12.4 and the multilayer neural nets discussed in this section. The objective here is the same as in \\nExample 12.6: to classify the pixels of multispectral image data into three pattern classes: \\nwater\\n, \\nurban\\n, \\n0 200 400 600 800 1,000\\n0\\n0.4\\n0.8\\n0.2\\n0.6\\n1.0\\n1.2\\n1.4\\nEpochs\\nMean squared error\\nFIGURE 12.36\\nMSE as a function \\nof training epochs \\nfor the XOR  \\npattern  \\narrangement.\\nDIP4E_GLOBAL_Print_Ready.indb   961\\n6/16/2017   2:17:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 962}),\n",
       " Document(page_content='962\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nand \\nvegetation\\n. Figure 12.37 shows the four multispectral images used in the experiment, the masks used \\nto extract the training and test samples, and the approach used to generate the 4-D pattern vectors.\\nAs in Example 12.6, we extracted a total of 1900 training pattern vectors and 1887 test pattern vectors \\n(see Table 12.1 for a listing of vectors by class). After preliminary runs with the training data to establish \\nthat the mean squared error was decreasing as a function of epoch, we determined that a neural net \\nwith one hidden layer of two nodes achieved stable learning with \\na\\n=\\n0\\n001 .\\n and 1,000 training epochs. \\nK\\neeping those two parameters ﬁxed, we varied the number of nodes in the internal layer, as listed in \\nTable 12.4. The objective of these preliminary runs was to determine the smallest neural net that would \\ngive the best recognition rate. As you can see from the results in the table, [4 3 3] is clearly the architec-\\nture of choice in this case. Figure 12.38 shows this neural net, along with the parameters learned during \\ntraining.\\nAfter the basic architecture was deﬁned, we kept the learning rate constant at \\na\\n=\\n0\\n001 .\\n and varied the \\nnumber of epochs to determine the best recognition rate with the architecture in F\\nig. 12.38. Table 12.5 \\nshows the results. As you can see, the recognition rate improved slowly as a function of epoch, reach-\\ning a plateau at around 50,000 epochs. In fact, as Fig. 12.39 shows, the MSE decreased quickly up to \\nabout 800 training epochs and decreased slowly after that, explaining why the correct recognition rate \\nchanged so little after about 2,000 epochs. Similar results were obtained with \\na\\n=\\n00\\n1\\n.,\\n but decreasing \\n(a) Images in spectral bands 1  4 and binary mask used to extract training samples\\nSpectral band 1\\nSpectral band 2\\nSpectral band 3\\nSpectral band 4\\n1\\n2\\n3\\n4\\nx\\nx\\nx\\nx\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nx\\n(b) Approach used to extract pattern vectors\\n–\\nFIGURE\\n \\n12.37\\n (a) Starting with the leftmost image: blue, green, red, near infrared, and binary mask images. In the \\nmask, the lower region is for water, the center region is for the urban area, and the left mask corresponds to vegeta-\\ntion. All images are of size \\n512 512\\n×\\n pixels. (b) Approach used for generating 4-D pattern vectors from a stack of \\nthe four multispectral images\\n. (Multispectral images courtesy of NASA.)\\nNetwork  \\nArchitecture\\n[4 2 3] [4 3 3] [4 4 3] [4 5 3] [4 2 2 3] [4 4 3 3] [4 4 4 3] [4 10 3 3] [4 10 10 3]\\nRecognition \\nRate\\n95.8% 96.2% 95.9% 96.1% 74.6% 90.8% 87.1% 84.9% 89.7%\\nTABLE \\n12.4\\nRecognition rate as a function of neural net architecture for \\na\\n=\\n0\\n001 .\\n and 1,000 training epochs. The network archi-\\ntecture is deﬁned by the numbers in brackets\\n. The ﬁrst and last number inside each bracket refer to the number of \\ninput and output nodes, respectively. The inner entries give the number of nodes in each hidden layer.\\nDIP4E_GLOBAL_Print_Ready.indb   962\\n6/16/2017   2:17:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 963}),\n",
       " Document(page_content='12.5\\n  \\nNeural Networks and Deep Learning\\n    \\n963\\nx\\n1\\nx\\n2\\nx\\n3\\n4\\nx\\n2.393 1.020 1.249 15.965\\n(2) 6.599 2.705 0.912 14.928\\n8.745 0.270 3.358 1.249\\n−\\n⎡⎤\\n⎢⎥\\n=− −\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nW\\n4.093 10.563 3.245\\n(3) 7.045 9.662 6.436\\n7.447 3.931 6.619\\n−−\\n⎡⎤\\n⎢⎥\\n=\\n⎢⎥\\n⎢⎥\\n−−\\n⎣⎦\\nW\\n[]\\n(2) 4.920 2.002 3.485\\nT\\n=− −\\nb\\n[]\\n(3) 3.277 14.982 1.582\\nT\\n=−\\nb\\nFIGURE 12.38\\nNeural net  \\narchitecture used to \\nclassify the  \\nmultispectral image \\ndata in Fig. 12.37 \\ninto three classes: \\nwater, urban, and \\nvegetation. The \\nparameters shown \\nwere obtained \\nin 50,000 epochs \\nof training using \\na\\n=\\n0\\n001\\n..\\n \\nTraining \\nEpochs\\n1,000 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000\\nRecognition \\nRate\\n95.3% 96.6% 96.7% 96.8% 96.9% 97.0% 97.0% 97.0% 97.0%\\nTABLE \\n12.5\\nRecognition performance on the training set as a function of training epochs. The learning rate constant was \\na\\n=\\n0\\n001 .\\n \\nin all cases\\n.\\n01234 5\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n0\\n4\\n10\\n×\\nTraining epochs (        )\\nMean squared error  \\n3\\n(1 0 )\\n×\\nFIGURE 12.39\\nMSE for the \\nnetwork  \\narchitecture in \\nFig. 12.38 as a \\nfunction of the \\nnumber of  \\ntraining epochs. \\nThe learning rate \\nparameter was \\na\\n=\\n0\\n001 .\\n in all \\ncases\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   963\\n6/16/2017   2:17:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 964}),\n",
       " Document(page_content='964\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nthis parameter to \\na\\n=\\n01\\n.\\n resulted in a drop of the best correct recognition rate to 49.1%. Based on the \\npreceding results\\n, we used \\na\\n=\\n0\\n001 .\\n and 50,000 epochs to train the network.\\nT\\nhe parameters in Fig. 12.38 were the result of training. The recognition rate for the training data \\nusing these parameters was 97%. We achieved a recognition rate of 95.6% on the test set using the same \\nparameters. The difference between these two ﬁgures, and the 96.4% and 96.2%, respectively, obtained \\nfor the same data with the Bayes classiﬁer (see Example 12.6), are statistically insigniﬁcant. \\nThe fact that our neural networks achieved results comparable to those obtained with the Bayes \\nclassiﬁer is not surprising. It can be shown (Duda, Hart, and Stork [2001]) that a three-layer neural net, \\ntrained by backpropagation using a sum of errors squared criterion, approximates the Bayes decision \\nfunctions in the limit, as the number of training samples approaches inﬁnity. Although our training sets \\nwere small, the data were well behaved enough to yield results that are close to what theory predicts. \\n12.6  DEEP CONVOLUTIONAL NEURAL NETWORKS  \\nUp to this point, we have organized pattern features as vectors. Generally, this \\nassumes that the form of those features has been specified (i.e., “engineered” by a \\nhuman designer) and extracted from images prior to being input to a neural network \\n(Example 12.13 is an illustration of this approach). But one of the strengths of neural \\nnetworks is that they are capable of learning pattern features directly from training \\ndata. What we would like to do is input a set of training images directly into a neural \\nnetwork, and have the network learn the necessary features \\non its own\\n. One way to \\ndo this would be to convert images to vectors directly by organizing the pixels based \\non a linear index (see Fig. 12.1), and then letting each element (pixel) of the linear \\nindex be an element of the vector. However, this approach does not utilize any spa-\\ntial relationships that may exist between pixels in an image, such as pixel arrange-\\nments into corners, the presence of edge segments, and other features that may help \\nto differentiate one image from another. In this section, we present a class of neural \\nnetworks called \\ndeep convolutional neural networks\\n (\\nCNNs\\n or \\nConvNets\\n for short) \\nthat accept images as inputs and are ideally suited for automatic learning and image \\nclassification. In order to differentiate between CNNs and the neural nets we stud-\\nied in Section 12.5, we will refer to the latter as “fully connected” neural networks. \\nA BASIC CNN ARCHITECTURE\\nIn the following discussion, we use a \\nLeNet\\n architecture (see references at the end of \\nthis chapter) to introduce convolutional nets. We do this for two main reasons: First, \\nthe LeNet architecture is reasonably simple to understand. This makes it ideal for \\nintroducing basic CNN concepts. Second, our real interest is in deriving the equa-\\ntions of backpropagation for convolutional networks, a task that is simplified by the \\nintuitiveness of LeNets.\\nThe CNN in Fig. 12.40 contains all the basic elements of a LeNet architecture, \\nand we use it without loss of generality. A key difference between this architecture \\nand the neural net architectures we studied in the previous section is that inputs to \\nCNNs are 2-D arrays (images), while inputs to our fully connected neural networks \\nare vectors. However, as you will see shortly, the computations performed by both \\nnetworks are very similar: (1) a sum of products is formed, (2) a bias value is added, \\n12.6\\nTo simplify the explana-\\ntion of the CNN in \\nFig. 12.40, we focus \\nattention initially on \\na single image input. \\nMultiple input images \\nare a trivial extension we \\nwill consider later in our \\ndiscussion.\\nDIP4E_GLOBAL_Print_Ready.indb   964\\n6/16/2017   2:17:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 965}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n965\\n(3) the result is passed through an activation function, and (4) the activation value \\nbecomes a single input to a following layer. \\nDespite the fact that the computations performed by CNNs and fully connected \\nneural nets are similar, there are some basic differences between the two, beyond \\ntheir input formats being 2-D versus vectors. An important difference is that CNNs \\nare capable of learning 2-D features directly from raw image data, as mentioned ear-\\nlier. Because the tools for systematically engineering comprehensive feature sets for \\ncomplex image recognition tasks do not exist, having a system that can learn its own \\nimage features from raw image data is a crucial advantage of CNNs. Another major \\ndifference is in the way in which layers are connected. In a fully connected neural net, \\nwe feed the output of \\nevery\\n neuron in a layer directly into the input of \\nevery\\n neuron in \\nthe next layer. By contrast, in a CNN we feed into every input of a layer, a \\nsingle\\n value, \\ndetermined by the convolution (hence the name \\nconvolutional\\n neural net) over a \\nspatial neighborhood in the output of the previous layer. Therefore, CNNs are not \\nfully connected in the sense deﬁned in the last section. Another difference is that the \\n2-D arrays from one layer to the next are subsampled to reduce sensitivity to transla-\\ntional variations in the input. These differences and their meaning will become clear \\nas we look at various CNN conﬁgurations in the following discussion.\\nBasics of How a CNN Operates\\nAs noted above, the type of neighborhood processing in CNNs is spatial convolu-\\ntion. We explained the mechanics of spatial convolution in Fig. 3.29, and expressed \\nit mathematically in Eq. (3-35). As that equation shows, convolution computes a \\nsum of products between pixels and a set of kernel weights. This operation is car-\\nried out at every spatial location in the input image. The result at each location \\n(,)\\nxy\\n in the input is a scalar value. Think of this value as the output of a neuron in \\na layer of a fully connected neural net.\\n If we add a bias and pass the result through \\nan activation function (see Fig. 12.29), we have a complete analogy between the \\nWe will discuss in the \\nnext subsection the exact \\nform of neural computa-\\ntions in a CNN, and show \\nthey are equivalent in \\nform to the computations \\nperformed by neurons in \\na fully connected neural \\nnet.\\nInput image\\nSubsampling\\nFeature maps\\nPooled\\nfeature\\nmaps\\nFeature\\nmaps\\nB\\nA\\nSubsampling\\nPooled\\nfeature\\nmaps\\nVectorizing\\nConvolution\\n+\\nActivation\\nBias\\n+\\nConvolution + Bias + Activation\\nFully connected\\nneural net\\n/\\nU\\nT\\nP\\nU\\nT\\nReceptive field\\nFIGURE 12.40\\n A CNN containing all the basic elements of a LeNet architecture. Points \\nA\\n and \\nB\\n are speciﬁc values \\nto be addressed later in this section. The last pooled feature maps are vectorized and serve as the input to a fully \\nconnected neural network. The class to which the input image belongs is determined by the output neuron with the \\nhighest value.\\nDIP4E_GLOBAL_Print_Ready.indb   965\\n6/16/2017   2:17:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 966}),\n",
       " Document(page_content='966\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nbasic computations performed by a CNN and those performed by the neural nets \\ndiscussed in the previous section.\\nThese remarks are summarized in Fig. 12.40, the leftmost part of which shows a \\nneighborhood at one location in the input image. In CNN terminology, these neigh-\\nborhoods are called \\nreceptive ﬁelds\\n. All a receptive ﬁeld does is select a region of \\npixels in the input image. As the ﬁgure shows, the ﬁrst operation performed by a \\nCNN is convolution, whose values are generated by moving the receptive ﬁeld over \\nthe image and, at each location, forming a sum of products of a set of weights and \\nthe pixels contained in the receptive ﬁeld. The set of weights, arranged in the shape \\nof the receptive ﬁeld, is a \\nkernel\\n, as in Chapter 3. The number of spatial increments \\nby which a receptive ﬁeld is moved is called the \\nstride\\n. Our spatial convolutions in \\nprevious chapters had a stride of one, but that is not a requirement of the equations \\nthemselves. In CNNs, an important motivations for using strides greater than one is \\ndata reduction. For example, changing the stride from one to two reduces the image \\nresolution by one-half in each spatial dimension, resulting in a three-fourths reduc-\\ntion in the amount of data per image. Another important motivation is as a substi-\\ntute for subsampling which, as we discuss below, is used to reduce system sensitivity \\nto spatial translation.\\nTo each convolution value (sum of products) we add a bias, then pass the result \\nthrough an activation function to generate a single value. Then, this value is fed to \\nthe corresponding \\n(,)\\nxy\\n location in the input of the next layer. When repeated for all \\nlocations in the input image\\n, the process just explained results in a 2-D set of values \\nthat we store in next layer as a 2-D array, called a \\nfeature map\\n. This terminology is \\nmotivated by the fact that the role performed by convolution is to extract features \\nsuch as edges, points, and blobs from the input (remember, convolution is the basis \\nof spatial ﬁltering, which we used in Chapter 3 for tasks such as smoothing, sharpen-\\ning, and computing edges in an image). The same weights and a single bias are used \\nto generate the convolution (feature map) values corresponding to all locations of \\nthe receptive ﬁeld in the input image. This is done to cause the same feature to be \\ndetected at all points in the image. Using the same weights and bias for this purpose \\nis called \\nweight \\n(or \\nparameter\\n) \\nsharing\\n.\\nFigure 12.40 shows three feature maps in the ﬁrst layer of the network. The other \\ntwo feature maps are generated in the manner just explained, but using a \\ndifferent\\n \\nset of weights and bias for each feature map. Because each set of weights and bias \\nis different, each feature map generally will contain a different set of features, all \\nextracted from the same input image. The feature maps are referred to collectively \\nas a \\nconvolutional layer\\n. Thus, the CNN in Fig. 12.40 has two convolutional layers.\\nThe process after convolution and activation is \\nsubsampling\\n (also called \\npooling\\n), \\nwhich is motivated by a model of the mammal visual cortex proposed by Hubel \\nand Wiesel [1959]. Their ﬁndings suggest that parts of the visual cortex consist of \\nsimple\\n and \\ncomplex\\n cells. The simple cells perform feature extraction, while the \\ncomplex cells combine (aggregate) those features into a more meaningful whole. In \\nthis model, a reduction in spatial resolution appears to be responsible for achieving \\ntranslational invariance. Pooling is a way of modeling this reduction in dimension-\\nality. When training a CNN with large image databases, pooling has the additional \\nIn the terminology of \\nChapter 3, a feature map \\nis a spatially ﬁltered \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   966\\n6/16/2017   2:17:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 967}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n967\\nadvantage of reducing the volume of data being processed. You can think of the \\nresults of subsampling as producing \\npooled feature maps\\n. In other words, a pooled \\nfeature map is a feature map of reduced spatial resolution. Pooling is done by subdi-\\nviding a feature map into a set of small (typically \\n22\\n×\\n)\\n regions, called \\npooling neigh-\\nborhoods\\n,\\n and replacing \\nall\\n elements in such a neighborhood by a \\nsingle\\n value. We \\nassume that pooling neighborhoods are \\nadjacent\\n (i.e., they do not overlap). There \\nare several ways to compute the pooled values; collectively, the different approaches \\nare called \\npooling methods\\n. Three common pooling methods are: (1) \\naverage pool-\\ning\\n, in which the values in each neighborhood are replaced by the average of the \\nvalues in the neighborhood; (2) \\nmax-pooling\\n, which replaces the values in a neigh-\\nborhood by the maximum value of its elements; and (3) \\nL\\n2\\n pooling, in which the \\nresulting pooled value is the square root of the sum of the neighborhood values \\nsquared. There is one pooled feature map for each feature map. The pooled feature \\nmaps are referred to collectively as a \\npooling layer\\n. In Fig. 12.40 we used \\n22\\n×\\n pool-\\ning so each resulting pooled map is one-fourth the size of the preceding feature map\\n. \\nThe use of receptive ﬁelds, convolution, parameter sharing, and pooling are charac-\\nteristics unique to CNNs.\\nBecause feature maps are the result of spatial convolution, we know from Chapter 3 \\nthat they are simply ﬁltered images. It then follows that pooled feature maps are ﬁl-\\ntered images of lower resolution. As Fig. 12.40 illustrates, the pooled feature maps \\nin the ﬁrst layer become the inputs to the next layer in the network. But, whereas \\nwe showed a single image as an input to the ﬁrst layer, we now have multiple pooled \\nfeature maps (ﬁltered images) that are inputs into the second layer. \\nTo see how these multiple inputs to the second layer are handled, focus for a \\nmoment on one pooled feature map. To generate the values for the ﬁrst feature map \\nin the second convolutional layer, we perform convolution, add a bias, and use acti-\\nvation, as before. Then, we change the kernel and bias, and repeat the procedure for \\nthe second feature map, still using the same input. We do this for every remaining \\nfeature map, changing the kernel weights and bias for each. Then, we consider the \\nnext pooled feature map input and perform the same procedure (convolution, plus \\nbias, plus activation) for every feature map in the second layer, using yet another set \\nof different kernels and biases. When we are ﬁnished, we will have generated three \\nvalues for the same location in every feature map, with one value coming from the \\ncorresponding location in each of the three inputs. The question now is: How do \\nwe combine these three individual values into one? The answer lies in the fact that \\nconvolution is a linear process, from which it follows that the three individual values \\nare combined into one by superposition (that is, by adding them).\\nIn the ﬁrst layer, we had one input image and three feature maps, so we needed \\nthree kernels to complete all required convolutions. In the second layer, we have \\nthree inputs and seven feature maps, so the total number of kernels (and biases) \\nneeded is \\n37 2 1\\n×=\\n.\\n Each feature map is pooled to generate a corresponding \\npooled feature map\\n, resulting in seven pooled feature maps. In Fig. 12.40, there are \\nonly two layers, so these seven pooled feature maps are the outputs of the last layer. \\nAs usual, the ultimate objective is to use features for classiﬁcation, so we need \\na classiﬁer. As Fig. 12.40 shows, in a CNN we perform classiﬁcation by feeding the \\nAdjacency is not a \\nrequirement of pooling \\nper se. We assume it \\nhere for simplicity \\nand because this is an \\napproach that is used \\nfrequently.\\nYou could interpret the \\nconvolution with several \\ninput images as 3-D con-\\nvolution, but with move-\\nment only in the spatial \\n(\\nx\\n and \\ny\\n) directions. The \\nresult would be identical \\nto summing individual \\nconvolutions with each \\nimage separately, as we \\ndo here. \\nDIP4E_GLOBAL_Print_Ready.indb   967\\n6/16/2017   2:17:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 968}),\n",
       " Document(page_content='968\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nvalue of the last pooled layer into a fully connected neural net, the details of which \\nyou learned in Section 12.5. But the outputs of a CNN are 2-D arrays (i.e., ﬁltered \\nimages of reduced resolution), whereas the inputs to a fully connected net are vec-\\ntors. Therefore, we have to \\nvectorize\\n the 2-D pooled feature maps in the last layer. \\nWe do this using linear indexing (see Fig. 12.1). Each 2-D array in the last layer of \\nthe CNN is converted into a vector, then all resulting vectors are concatenated (verti- \\ncally for a column) to form a single vector. This vector propagates through the neu-\\nral net, as explained in Section 12.5. In any given application, the number of outputs \\nin the fully connected net is equal to the number of pattern classes being classiﬁed. \\nAs before, the output with the highest value determines the class of the input.\\nEXAMPLE 12.14 :  Receptive ﬁelds, pooling neighborhoods, and their corresponding feature maps.\\nThe top row of Fig. 12.41 shows a numerical example of the relative sizes of feature maps and pooled \\nfeature maps as a function of the sizes of receptive ﬁelds and pooling neighborhoods. The input image \\nis of size \\n28 28\\n×\\n pixels, and the receptive ﬁeld is of size \\n55\\n×\\n.\\n If we require that the receptive ﬁeld be \\ncontained in the image during convolution,\\n you know from Section 3.4 that the resulting convolution \\narray (feature map) will be of size \\n24 24\\n×\\n.\\n If we use a pooling neighborhood of size \\n22\\n×\\n,\\n the resulting \\npooled feature maps will be of size \\n12 12\\n×\\n,\\n as the ﬁgure shows. As noted earlier, we assume that pooling \\nneighborhoods do not overlap\\n.\\nAs an analogy with fully connected neural nets, think of each element of a 2-D array in the top row \\nof Fig. 12.41 as a neuron. The outputs of the neurons in the input are pixel values. The neurons in the \\nfeature map of the ﬁrst layer have output values generated by convolving with the input image a kernel \\nwhose size and shape are the same as the receptive ﬁeld, and whose coefﬁcients are learned during train-\\ning. To each convolution value we add a bias and pass the result through an activation function to gener-\\nate the output value of the corresponding neuron in the feature map. The output values of the neurons in \\nthe pooled feature maps are generated by pooling the output values of the neurons in the feature maps.\\nThe second row in Fig. 12.41 illustrates visually how feature maps and pooled feature maps look \\nbased on the input image shown in the ﬁgure. The kernel shown is as described in the previous para-\\ngraph, and its weights (shown as intensity values) were learned from sample images using the training \\nof the CNN described later in Example 12.17. Therefore, the nature of the learned features is deter-\\nmined by the learned kernel coefﬁcients. Note that the contents of the feature maps are speciﬁc features \\ndetected by convolution. For example, some of the features emphasize edges in the the character. As \\nmentioned earlier, the pooled features are lower-resolution versions of this effect. \\nEXAMPLE 12.15 :  Graphical illustration of the functions performed by the components of a CNN.\\nFigure 12.42 shows the \\n28 28\\n×\\n image from Fig. 12.41, input into an expanded version of the CNN archi-\\ntecture from F\\nig. 12.40. The expanded CNN, which we will discuss in more detail in Example 12.17, has \\nsix feature maps in the ﬁrst layer, and twelve in the second. It uses receptive ﬁelds of size \\n55\\n×\\n,\\n and \\npooling neighborhoods of size \\n22\\n×\\n.\\n Because the receptive ﬁelds are of size \\n55\\n×\\n,\\n the feature maps in \\nthe ﬁrst layer are of size \\n24 24\\n×\\n,\\n as we explained in Example 12.14. Each feature map has its own set of \\nweights and bias\\n, \\nso we will need a total of \\n()\\n55\\n661 5 6\\n×× + =\\n parameters (six kernels with twenty-ﬁve \\nweights each,\\n and six biases) to generate the feature maps in the ﬁrst layer. The top row of Fig. 12.43(a) \\nshows the kernels with the weights learned during training of the CNN displayed as images, with intensity \\nbeing proportional to kernel values.\\nThe parameters of the \\nfully connected neural \\nnet are learned during \\ntraining of the CNN, to \\nbe discussed shortly.\\nDIP4E_GLOBAL_Print_Ready.indb   968\\n6/16/2017   2:17:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 969}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n969\\nFIGURE 12.41\\nTop row: How the \\nsizes of receptive \\nﬁelds and pooling \\nneighborhoods \\naffect the sizes of \\nfeature maps and \\npooled feature \\nmaps. \\nBottom row: An \\nimage example.  \\nThis ﬁgure is \\nexplained in more \\ndetail in Example \\n12.17. (Image \\ncourtesy of NIST.)\\nFeature map\\n(size 24 \\n/H11003\\n 24)\\nInput image\\n(size  28 \\n/H11003\\n 28)\\nPooled\\nfeature map\\n(size 12 \\n/H11003\\n 12)\\n5 \\n/H11003\\n 5 Receptive field\\n2 \\n/H11003\\n 2 Pooling neighborhood\\nKernel\\nConvolution + bias + activation\\nSubsampling\\nBecause we used pooling neighborhoods of size \\n22\\n×\\n,\\n the pooled feature maps in the ﬁrst layer of \\nF\\nig. 12.42 are of size \\n12 12\\n×\\n.\\n As we discussed earlier, the number of feature maps and pooled feature \\nmaps is the same\\n, so we will have six arrays of size \\n12 12\\n×\\n acting as inputs to the twelve feature maps \\nin the second layer (the number of feature maps generally is different from layer to layer).\\n Each fea-\\nture map will have its own set of weights and bias, so will need a total of \\n6 5 5 12 12 1812\\n×××\\n+\\n()\\n=\\n0\\n1\\n3\\n4\\n6\\n7\\n8\\n9\\n2\\n5\\nInput image\\nSubsampling\\nFeature maps\\nPooled\\nfeature\\nmaps\\nFeature\\nmaps\\nSubsampling\\nPooled\\nfeature\\nmaps\\nVectorization\\nConvolution\\n+\\nActivation\\nBias\\n+\\nConvolution + Bias + Activation\\nFully connected\\nneural net\\n0.21\\n0.17\\n0.09\\n0.10\\n0.12\\n0.39\\n0.36\\n0.19\\n0.88\\n0.42\\nFIGURE 12.42\\n Numerical example illustrating the various functions of a CNN, including recognition of an input image. \\nA sigmoid activation function was used throughout.\\nDIP4E_GLOBAL_Print_Ready.indb   969\\n6/16/2017   2:17:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 970}),\n",
       " Document(page_content='970\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nparameters to generate the feature maps in the second layer (i.e., twelve sets of six kernels with twenty-\\nﬁve weights each, plus twelve biases). The bottom part of Fig. 12.43 shows the kernels as images. Because \\nwe are using receptive ﬁelds of size \\n55\\n×\\n,\\n the feature maps in the second layer are of size \\n88\\n×\\n.\\n Using \\n22\\n×\\n pooling neighborhoods resulted in pooled feature maps of size \\n44\\n×\\n in the second layer.\\nAs we discussed earlier\\n, the pooled feature maps in the last layer have to be vectorized to be able to \\ninput them into the fully connected neural net. Each pooled feature map resulted in a column vector of \\nsize 16 1\\n×\\n.\\n There are 12 of these vectors which, when concatenated vertically, resulted in a single vector \\nof size \\n192 1\\n×\\n.\\n Therefore, our fully connected neural net has 192 input neurons. There are ten numeral \\nclasses\\n, so there are 10 output neurons. As you will see later, we obtained excellent performance by using \\na neural net with no hidden layers, so our complete neural net had a total of 192 input neurons and 10 \\noutput neurons. For the input character shown in Fig. 12.42, the highest value in the output of the fully \\nconnected neural net was in the seventh neuron, which corresponds to the class of 6’s. Therefore, the \\ninput was recognized properly. This is shown in bold text in the ﬁgure.\\nFigure 12.44 shows graphically what the feature maps look like as the input image propagates through \\nthe CNN. Consider the feature maps in the first layer. If you look at each map carefully, you will notice \\nthat it highlights a different characteristic of the input. For example, the map on the top of the first \\ncolumn highlights the two principal edges on the top of the character. The second map highlights the \\nedges of the entire inner region, and the third highlights a “blob-like” nature of the digit, almost as if it \\nhad been blurred by a lowpass kernel. The other three images show other features. Although the pooled \\nfeature maps are lower-resolution versions of the original feature maps, they still retained the key char-\\nacteristics of the features in the latter. If you look at the first two feature maps in the second layer, and \\ncompare them with the first two in the first layer, you can see that they could be interpreted as higher-\\nFIGURE 12.43\\n Top: The weights (shown as images of size \\n55\\n×\\n)\\n corresponding to the six feature maps in the ﬁrst layer \\nof the CNN in F\\nig. 12.42. Bottom: The weights corresponding to the twelve feature maps in the second layer.\\nDIP4E_GLOBAL_Print_Ready.indb   970\\n6/16/2017   2:17:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 971}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n971\\nlevel abstractions of the top part of the character, in the sense that they show an area flanked on both \\nsides by areas of opposite intensity. These abstractions are not always easy to analyze visually, but as you \\nwill see in later examples, they can be very effective. The vectorized version of the last pooled layer is \\nself-explanatory. The output of the fully connected neural net shows dark for low values and white for \\nthe highest value, indicating that the input was properly recognized as a number 6. Later in this section, \\nwe will show that the simple CNN architecture in Fig. 12.42 is capable of recognizing the correct class of \\nover 70,000 numerical samples with nearly perfect accuracy.\\nNeural Computations in a CNN\\nRecall from Fig. 12.29 that the basic computation performed by an artificial neuron \\nis a sum of products between weights and values from a previous layer. To this we \\nadd a bias and call the result the \\nnet\\n (\\ntotal\\n) \\ninput\\n to the neuron, which we denoted \\nby \\nz\\ni\\n.\\n As we showed in Eq. (12-54), the sum involved in generating \\nz\\ni\\n is a single sum. \\nThe computations performed in a CNN to generate a single value in a feature map \\nis 2-D convolution. As you learned in Chapter 3, this is a double sum of products \\nbetween the coefficients of a kernel and the corresponding elements of the image \\narray overlapped by the kernel. With reference to Fig. 12.40, let \\nw\\n denote a kernel \\nformed by arranging the weights in the shape of the receptive field we discussed \\nin connection with that figure\\n. For notational consistency with Section 12.5, let \\na\\nxy\\n,\\n \\ndenote image or pooled feature values, depending on the layer. The convolution \\nvalue at any point \\n(,)\\nxy\\n in the input is given by\\n \\nwa w\\nx,y\\n/H22841\\n=\\n−−\\n∑ ∑\\nl k xl yk\\nk l\\na\\n,,\\n \\n(12-83)\\nFIGURE 12.44\\nVisual summary \\nof an input image \\npropagating \\nthrough the CNN \\nin Fig. 12.42. Shown \\nas images are all the \\nresults of  \\nconvolution  \\n(feature maps) and \\npooling (pooled \\nfeature maps) for \\nboth layers of the \\nnetwork. (Example \\n12.17 contains more \\ndetails about this \\nﬁgure.)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\nNeural\\nnet\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\nVector\\nDIP4E_GLOBAL_Print_Ready.indb   971\\n6/16/2017   2:17:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 972}),\n",
       " Document(page_content='972\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nwhere \\nl\\n and \\nk\\n span the dimensions of the kernel. Suppose that \\nw\\n is of size \\n33\\n×\\n. \\nT\\nhen, we can then expand this equation into the following sum of products:\\n \\nwa wa w\\nww\\nx,y\\nx,y\\n/H22841/H22841\\n==\\n=+ +\\n−−\\n−−\\n−−\\n∑ ∑\\nl k xl yk\\nk l\\nxy\\nxy\\na\\naa\\n,,\\n,, , ,\\n11 1 1 12 1 2\\n/midhorizellipsis\\n+ +\\n−−\\nw\\n33 3 3\\n,,\\na\\nxy\\n \\n(12-84)\\nWe could relabel the subscripts on \\nw\\n and \\na\\n, and write instead\\n \\nwa= w w w\\nw\\nx,y\\ni\\n/H22841\\n11 2 2\\n99\\n1\\n9\\naa a\\na\\ni\\ni\\n++ +\\n=\\n=\\n∑\\n/midhorizellipsis\\n \\n(12-85)\\nThe results of Eqs. (12-84) and (12-85) are identical. If we add a bias to the latter \\nequation and call the result \\nz\\n we have\\n \\nza b\\nb\\njj\\nj\\n=+\\n=+\\n=\\n∑\\nw\\nwa\\nx,y\\n1\\n9\\n/H22841\\n \\n(12-86)\\nThe form of the first line of this equation is identical to Eq. (12-54). Therefore, we \\nconclude that if we add a bias to the spatial convolution computation performed by \\na CNN at any fixed position \\n(,)\\nxy\\n in the input, the result can be expressed in a form \\nidentical to the computation performed by an artificial neuron in a fully connected \\nneural net.\\n We need the \\nxy\\n,\\n only to account for the fact that we are working in 2-D. \\nIf we think of \\nz\\n as the net input to a neuron, the analogy with the neurons discussed \\nin Section 12.5 is completed by passing \\nz\\n through an activation function, \\nh\\n,\\n to get \\nthe output of the neuron:\\n \\nah z\\n=\\n()\\n \\n(12-87)\\nThis is exactly how the value of any point in a feature map (such as the point labeled \\nA\\n in F\\nig. 12.40) is computed.\\nNow consider point \\nB\\n in that ﬁgure. As mentioned earlier, its value is given by \\nadding three convolution equations:\\n  \\nwawawa w\\n(1)\\n(2)\\n(3)\\nlk xy lk xy lk xy\\nlk x\\na\\n,,\\n()\\n,,\\n()\\n,,\\n()\\n,\\n()\\n/H22841/H22841/H22841\\n123\\n1\\n++=\\n−\\nl\\nly k\\nk l\\nl k xl yk\\nk l\\nl k xl yk\\nk l\\naa\\n,\\n()\\n,\\n()\\n,\\n()\\n,\\n()\\n,\\n()\\n−\\n−−\\n−−\\n∑ ∑\\n∑\\n∑∑\\n+\\n+\\n1\\n22 22\\nww\\n∑ ∑\\n \\n(12-88)\\nwhere the superscripts refer to the three pooled feature maps in Fig. 12.40. The val-\\nues of \\nlkx\\n,,\\n,\\n and \\ny\\n are the same in all three equations because all three kernels are \\nof the same size and they move in unison.\\n We could expand this equation and obtain \\na sum of products that is lengthier than for point \\nA\\n in Fig. 12.40, but we could still \\nrelabel all terms and obtain a sum of products that involves only one summation, \\nexactly\\n as before.\\nDIP4E_GLOBAL_Print_Ready.indb   972\\n6/16/2017   2:17:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 973}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n973\\nThe preceding result tells us that the equations used to obtain the value of an \\nelement of any feature map in a CNN can be expressed in the form of the computa-\\ntion performed by an artiﬁcial neuron. This holds for any feature map, regardless \\nof how many convolutions are involved in the computation of the elements of that \\nfeature map, in which case we would simply be dealing with the sum of more con-\\nvolution equations. The implication is that we can use the basic form of Eqs. (12-86) \\nand (12-87) to describe how the value of an element in any feature map of a CNN \\nis obtained. This means we do not have to account explicitly for the number of dif-\\nferent pooled feature maps (and hence the number of different kernels) used in a \\npooling layer. The result is a signiﬁcant simpliﬁcation of the equations that describe \\nforward and backpropagation in a CNN.\\nMultiple Input Images\\nThe values of \\na\\nxy\\n,\\n just discussed are pixel values in the first layer but, in layers past \\nthe first, \\na\\nxy\\n,\\n denotes values of pooled features. However, our equations do not dif-\\nferentiate based on what these variables actually represent. For example, suppose \\nwe replace the input to Fig. 12.40 with three images, such as the three components \\nof an RGB image. The equations for the value of point \\nA\\n in the figure would now \\nhave  the same form as those we stated for point \\nB\\n—only the weights and biases \\nwould be different. Thus, the results in the previous discussion for one input image \\nare applicable directly to multiple input images. We will give an example of a CNN \\nwith three input images later in our discussion.\\nTHE EQUATIONS OF A FORWARD PASS THROUGH A CNN\\nWe concluded in the preceding discussion that we can express the result of convolv-\\ning a kernel, \\nw\\n, and an input array with values \\na\\nxy\\n,\\n, as\\n \\nza b\\nb\\nx y\\nl k xl yk\\nk l\\n,, ,\\n=+\\n=+\\n−−\\n∑ ∑\\nw\\nwa\\nx,y\\n/H22841\\n \\n(12-89)\\nwhere \\nl\\n and \\nk\\n span the dimensions of the kernel, \\nx\\n and \\ny\\n span the dimensions of the \\ninput,\\n and \\nb\\n is a bias. The corresponding value of \\na\\nxy\\n,\\n is\\n \\nah z\\nxy xy\\n,,\\n=\\n()\\n \\n(12-90)\\nBut this \\na\\nxy\\n,\\n is different from the one we used to compute Eq. (12-89), in which \\na\\nxy\\n,\\n \\nrepresents values from the previous layer. Thus, we are going to need additional \\nnotation to differentiate between layers. As in fully connected neural nets, we use \\nℓ\\n \\nfor this purpose\\n, and write Eqs. (12-89) and (12-90) as\\n \\nza b\\nb\\nxy\\nl k xl yk\\nk l\\n,\\n() () ( ) ()\\n()( )\\n,,\\nℓℓ ℓ ℓ\\nℓℓ ℓ\\n=−\\n+\\n=− +\\n−−\\n∑ ∑\\nw\\nw( ) a\\nx,y\\n1\\n1\\n/H22841\\n \\n(12-91)\\nAs noted earlier, a kernel \\nis formed by organizing \\nthe weights in the shape of \\na corresponding receptive \\nﬁeld. Also keep in mind \\nthat \\nw\\n and \\na\\nx,y\\n represent \\nall\\n the weights and  \\ncorresponding values in \\na set of input images or \\npooled features.\\nDIP4E_GLOBAL_Print_Ready.indb   973\\n6/16/2017   2:17:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 974}),\n",
       " Document(page_content='974\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nand\\n \\nah z\\nxy\\nxy\\n,,\\n() ()\\nℓℓ\\n=\\n()\\n \\n(12-92)\\nfor \\nℓ…\\n=\\n12\\n,,\\n, ,\\nL\\nc\\n where \\nL\\nc\\n is the number of convolutional layers, and \\na\\nxy\\n,\\n()\\nℓ\\n \\ndenotes the values of pooled features in convolutional layer \\nℓ\\n.\\n When \\nℓ\\n=\\n1,\\n \\n \\na\\nxy\\n,\\n()\\n0\\n=\\n{}\\nvalues of pixels in the input image(s)\\n \\n(12-93)\\nWhen \\nℓ\\n=\\nL\\nc\\n,\\n \\n()\\n,\\naL\\nxy c\\n=\\nvalues of pooled features in last layer of the CN\\nNN\\n{}\\n \\n(12-94)\\n \\nNote that \\nℓ\\n starts at 1 instead of 2, as we did in Section 12.5. The reason is that we are \\nnaming layers\\n, as in “convolutional layer \\nℓ\\n.\\n” It would be confusing to start at convo-\\nlutional layer 2.\\n Finally, we note that the pooling does not require any convolutions. \\nThe only function of pooling is to reduce the spatial dimensions of the feature map \\npreceding it, so we do not include explicit pooling equations here.\\nEquations (12-91) through (12-94) are all we need to compute all values in a \\nforward pass through the convolutional section of a CNN. As described in Fig. 12.40, \\nthe values of the pooled features of the last layer are vectorized and fed into a fully \\nconnected feedforward neural network, whose forward propagation is explained in \\nEqs. (12-54) and (12-55) or, in matrix form, in Table 12.2.\\nTHE EQUATIONS OF BACKPROPAGATION USED TO TRAIN CNN\\nS\\nAs you saw in the previous section, the feedforward equations of a CNN are similar \\nto those of a fully connected neural net, but with multiplication replaced by convo-\\nlution, and notation that reflects the fact that CNNs are not fully connected in the \\nsense defined in Section 12.5. As you will see in this section, the equations of back-\\npropagation also are similar in many respects to those in fully connected neural nets.\\nAs in the derivation of backpropagation in Section 12.5, we start with the deﬁni-\\ntion of how the output error of our CNN changes with respect to each neuron in the \\nnetwork. The form of the error is the same as for fully connected neural nets, but \\nnow it is a function of \\nx\\n and \\ny\\n instead of \\nj\\n:\\n \\nd\\nxy\\nxy\\nE\\nz\\n,\\n,\\n()\\n()\\nℓ\\nℓ\\n=\\n∂\\n∂\\n \\n(12-95)\\nAs in Section 12.5, we want to relate this quantity to \\nd\\nxy\\n() ,\\nℓ\\n+\\n1\\n which we again do \\nusing the chain rule:\\n \\nd\\nxy\\nxy\\nu\\nu\\nu\\nxy\\nE\\nz\\nE\\nz\\nz\\nz\\n,\\n,,\\n,\\n,\\n()\\n() ( )\\n()\\n()\\nℓ\\nℓℓ\\nℓ\\nℓ\\n=\\n∂\\n∂\\n=\\n∂\\n∂+\\n∂+\\n∂\\n∑ ∑\\nv\\nv\\nv\\n1\\n1\\n \\n(12-96)\\nDIP4E_GLOBAL_Print_Ready.indb   974\\n6/16/2017   2:17:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 975}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n975\\nwhere \\nu\\n and \\nv\\n are any two variables of summation over the range of possible values \\nof \\nz\\n.\\n As noted in Section 12.5, these summations result from applying the chain rule. \\nBy deﬁnition, the ﬁrst term of the double summation of Eq. (12-96) is \\nd\\nxy\\n,\\n() .\\nℓ\\n+\\n1  \\nSo\\n, we can write this equation as\\n \\ndd\\nxy\\nxy\\nu\\nu\\nu\\nxy\\nE\\nz\\nz\\nz\\n,\\n,\\n,\\n,\\n,\\n()\\n()\\n()\\n()\\n()\\nℓ\\nℓ\\nℓ\\nℓ\\nℓ\\n=\\n∂\\n∂\\n=+\\n∂+\\n∂\\n∑ ∑\\nv\\nv\\nv\\n1\\n1\\n \\n(12-97)\\nSubstituting Eq. (12-92) into Eq. (12-91), and using the resulting \\nz\\nu,v\\n in Eq. (12-97), \\nwe obtain\\n \\ndd\\nxy\\nu\\nu\\nxy\\nlk\\nk l\\nul k\\nz\\nhz\\n,,\\n,\\n,,\\n() ( )\\n()\\n() ( )\\nℓℓ\\nℓ\\nℓℓ\\n=+\\n∂\\n∂\\n+\\n(\\n)\\n+\\n∑\\n∑∑\\n∑\\n−−\\nv\\nv\\nv\\nw\\n11\\nb b\\n()\\nℓ\\n+\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n(12-98)\\nThe derivative of the expression inside the brackets is zero unless \\nul x\\n−=\\n and \\nv\\n−=\\nky\\n,\\n and because the derivative of \\nb\\n()\\nℓ\\n+\\n1\\n with respect to \\nz\\nxy\\n,\\n()\\nℓ\\n is zero. But, if \\nul x\\n−=\\n and \\nv\\n−=\\nky\\n,\\n then \\nlux\\n=−\\n and \\nky\\n=−\\nv\\n.\\n Therefore, taking the indicated \\nderivative of the expression in brackets\\n, we can write Eq. (12-98) as\\n \\ndd\\nxy\\nu\\nu\\nxy\\ny x\\nxy\\nhz\\n,, ,,\\n( ) () () ( )\\nℓℓ ℓ ℓ\\n=+ +\\n()\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n∑\\n∑∑\\n∑\\n−−\\n− −\\nv\\nv\\nuv\\nv u\\nw\\n11\\n/H11032\\n⎥ ⎥\\n \\n(12-99)\\nValues of \\nx\\n, \\ny\\n, \\nu\\n,\\n and \\nv\\n are specified outside of the terms inside the brackets. Once the \\nvalues of these variables are fixed,\\n \\nu\\n−\\nx\\n and \\nv\\n−\\ny\\n inside the brackets are simply two \\nconstants\\n.\\n Therefore, the double summation evaluates to \\nw\\nuv\\n−−\\n+\\n()\\nxy\\nx y\\nhz\\n,,\\n() ( ) ,\\nℓℓ\\n1\\n/H11032\\n \\nand we can write Eq.\\n (12-99) as \\n \\ndd\\nd\\nxy\\nu\\nu\\nxy\\nx y\\nxy\\nhz\\nhz\\n,, ,,\\n,\\n( ) () () ( )\\n()\\nℓℓℓ ℓ\\nℓ\\n=+ +\\n()\\n=\\n()\\n∑ ∑\\n−−\\nv\\nv\\nuv\\nw\\n11\\n/H11032\\n/H11032\\nu u\\nu\\nxy\\n,,\\n() ()\\nv\\nv\\nuv\\nw\\nℓℓ\\n++\\n∑ ∑\\n−−\\n11\\n \\n(12-100)\\nThe double sum expression in the second line of this equation is in the form of a con-\\nvolution,\\n but the displacements are the negatives of those in Eq. (12-91). Therefore, \\nwe can write Eq. (12-100) as\\n \\ndd\\nxy\\nxy xy\\nx y\\nhz\\n,, , ,\\n() () ( ) ( )\\nℓℓ ℓ ℓ\\n=\\n()\\n++\\n⎡\\n⎣\\n⎤\\n⎦\\n−−\\n/H11032\\n11\\n/H22841\\nw\\n \\n(12-101)\\nThe negatives in the subscripts indicate that \\nw\\n is \\nreflected\\n about both spatial axes. \\nThis is the same as rotating \\nw\\n by \\n180\\n°,\\n as we explained in connection with Eq. (3-35). \\nUsing this fact,\\n we finally arrive at an expression for the error at a layer \\nℓ\\n by writing \\nEq.\\n (12-101) equivalently as\\n \\ndd\\nxy\\nxy xy\\nxy\\nhz\\n,, ,\\n,\\n() () ( )\\n( )\\nℓℓ ℓ ℓ\\n=\\n()\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11032\\n11\\n180\\n/H22841\\nrot\\nw\\n \\n(12-102)\\nThe 180° rotation is \\nfor \\neach\\n 2-D kernel in \\na layer. \\nDIP4E_GLOBAL_Print_Ready.indb   975\\n6/16/2017   2:17:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 976}),\n",
       " Document(page_content='976\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nBut the kernels do not depend on \\nx\\n and \\ny\\n, so we can write this equation as\\n \\ndd\\nxy\\nxy xy\\nhz\\n,, ,\\n() () ( ) ( )\\nℓℓ ℓ ℓ\\n=\\n()\\n++\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11032\\n11\\n180\\n/H22841\\nrot\\nw\\n \\n(12-103)\\nAs in Section 12.5, our ﬁnal objective is to compute the change in \\nE\\n with respect \\nto the weights and biases\\n. Following a similar procedure as above, we obtain\\n \\n∂\\n∂\\n=\\n∂\\n∂\\n∂\\n∂\\n=\\n∂\\n∂\\n∑ ∑\\n∑ ∑\\nEE\\nz\\nz\\nz\\nxy\\ny x\\nxy\\nxy\\ny x\\nxy\\nww\\nw\\nl,k\\nl,k\\nl\\n,\\n,\\n,\\n,\\n()\\n()\\n()\\n()\\nℓ\\nℓ\\nℓ\\nℓ\\nd\\n,, k\\nl,k\\nl,k\\nk\\nw\\nw\\nk\\n=\\n∂\\n∂\\n−\\n()\\n+\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n∑\\n∑∑\\n∑\\n−−\\nd\\nxy\\ny\\nxl\\nhz\\nb\\nxl y\\n,\\n()\\n() ( ) ()\\n,\\nℓℓ ℓ ℓ\\n1\\n⎥ ⎥\\n=−\\n()\\n=−\\n∑ ∑\\n∑ ∑\\n−−\\n−−\\nd\\nd\\nxy\\ny x\\nxy\\ny x\\nhz\\na\\nxl y\\nxl y\\n,\\n,\\n() ( )\\n() ( )\\n,\\n,\\nℓℓ\\nℓℓ\\nk\\nk\\n1\\n1\\n \\n(12-104)\\nwhere the last line follows from Eq. (12-92). This line is in the form of a convolution \\nbut,\\n comparing it to Eq. (12-91), we see there is a sign reversal between the summa-\\ntion variables and their corresponding subscripts. To put it in the form of a convolu-\\ntion, we write the last line of Eq. (12-104) as\\n \\n∂\\n∂\\n=−\\n=−\\n∑ ∑\\n−− − −\\n−−\\nE\\na\\na\\nxy\\ny x\\nlk l k\\nlx ky\\nw\\nl,k\\nd\\nd\\n,\\n,,\\n() ( )\\n() ( )\\n() , ( )\\nℓℓ\\nℓℓ\\n1\\n1\\n/H22841\\n=\\n=−\\n()\\nd\\nlk\\na\\n,\\n() ( )\\nℓℓ\\n/H22841\\nrot180 1\\n \\n(12-105)\\nSimilarly (see Problem 12.32),\\n \\n∂\\n∂\\n=\\n∑ ∑\\nE\\nb\\nxy\\ny x\\n()\\n()\\n,\\nℓ\\nℓ\\nd\\n \\n(12-106)\\nUsing the preceding two expressions in the gradient descent equations (see \\nSection 12.5),\\n it follows that\\n \\nww\\nw\\nw\\nl,k\\nlk lk\\nlk lk\\nE\\na\\n,,\\n,,\\n() ()\\n() ()\\n()\\nℓℓ\\nℓℓ ℓ\\n=−\\n∂\\n∂\\n=−\\n−\\n()\\na\\nad\\n/H22841\\nrot180 1\\n \\n(12-107)\\nDIP4E_GLOBAL_Print_Ready.indb   976\\n6/16/2017   2:17:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 977}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n977\\nand\\n \\nbb\\nE\\nb\\nb\\nxy\\ny x\\n() ()\\n()\\n()\\n()\\n,\\nℓℓ\\nℓ\\nℓℓ\\n=−\\n∂\\n∂\\n=−\\n∑ ∑\\na\\nad\\n \\n(12-108)\\nEquations (12-107) and (12-108) update the weights and bias of each convolution \\nlayer in a CNN\\n. As we have mentioned before, it is understood that the \\nw\\nlk\\n,\\n repre-\\nsents \\nall\\n the weights of a layer. The variables \\nl\\n and \\nk\\n span the spatial dimensions of \\nthe 2-D kernels, all of which are of the same size. \\nIn a forward pass, we went from a convolution layer to a pooled layer. In back-\\npropagation, we are going in the opposite direction. But the pooled feature maps \\nare smaller than their corresponding feature maps (see Fig. 12.40). Therefore, when \\ngoing in the reverse direction, we \\nupsample\\n (e.g., by pixel replication) each pooled \\nfeature map to match the size of the feature map that generated it. Each pooled \\nfeature map corresponds to a unique feature map, so the path of backpropagation \\nis clearly deﬁned.\\nWith reference to Fig. 12.40, backpropagation starts at the output of the fully con-\\nnected neural net. We know from Section 12.5 how to update the weights of this net-\\nwork. When we get to the “interface” between the neural net and the CNN, we have \\nto reverse the vectorization method used to generate input vectors. That is, before \\nwe can proceed with backpropagation using Eqs. (12-107) and (12-108), we have to \\nregenerate\\n the individual pooled feature maps from the single vector propagated \\nback by the fully connected neural net. \\nWe summarized in Table 12.3 the backpropagation steps for a fully connected \\nneural net. Table 12.6 summarizes the steps for performing backpropagation in the \\nCNN architecture in Fig. 12.40. The procedure is repeated for a speciﬁed number of \\nStep Description\\nEquations\\nStep 1\\nInput images\\na\\n()\\n0\\n = the set of image pixels in the input to layer 1\\nStep 2 Forward pass\\nFor each neuron corresponding to location \\n(,)\\nxy\\n in each feature map in layer \\nℓ\\n \\ncompute:\\n \\nzb\\nxy\\nxy\\na\\n,,\\n() ) ) ()\\n(\\nℓℓℓ ℓ\\n=− +\\nw(\\n/H22841\\n1  and \\nah z\\nxy\\nxy\\n,,\\n() ();\\nℓℓ\\n=\\n(\\n)\\n \\nℓ…\\n=\\n12\\n,,\\n,\\nL\\nc\\nStep 3 Backpropagation\\nFor each neuron in each feature map in layer \\nℓ\\n compute: \\ndd\\nxy\\nxy xy\\nhz\\n,, ,\\n() () ( ) ( ) ;\\nℓℓ ℓ ℓ\\n=\\n(\\n)\\n++\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11032\\n11\\n180\\n/H22841\\nrot\\nw\\n \\nℓ\\n…\\n=− −\\nLL\\ncc\\n12 1\\n,, ,\\nStep 4 Update parameters Update the weights and bias for each feature map using\\n \\n \\n \\nww\\nlk lk\\nlk\\na\\n,, ,\\n() () () ( )\\nℓℓ ℓ ℓ\\n=− −\\n(\\n)\\nad\\n/H22841\\nrot180 1\\n and \\n \\nbb\\nxy\\ny x\\n() ()\\n() ;\\n,\\nℓℓ ℓ\\n=−\\n∑ ∑\\nad\\n \\nℓ…\\n=\\n12\\n,,\\n,\\nL\\nc\\nTABLE \\n12.6\\nThe principal steps used to train a CNN. The network is initialized with a set of small random weights and biases. \\nIn backpropagation, a vector arriving (from the fully connected net) at the output pooling layer must be converted \\nto 2-D arrays of the same size as the pooled feature maps in that layer. Each pooled feature map is upsampled to \\nmatch the size of its corresponding feature map. The steps in the table are for one epoch of training.\\nDIP4E_GLOBAL_Print_Ready.indb   977\\n6/16/2017   2:17:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 978}),\n",
       " Document(page_content='978\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nepochs, or until the output error of the neural net reaches an acceptable value. The \\nerror is computed exactly as we did in Section 12.5. It can be the mean squared error, \\nor the recognition error. Keep in mind that the weights in \\nw\\n()\\nℓ\\n and the bias value \\nb\\n()\\nℓ\\n are \\ndifferent\\n for \\neac\\nh\\n feature map in layer \\nℓ\\n. \\nEXAMPLE 12.16 :  Teaching a CNN to recognize some simple images.\\nWe begin our illustrations of CNN performance by teaching the CNN in Fig. 12.45 to recognize the small \\n66\\n×\\n images in Fig. 12.46. As you can see on the left of this ﬁgure, there are three samples each of images \\nof a horizontal stripe\\n, a small centered square, and a vertical stripe. These images were used as the train-\\ning set. On the right are noisy samples of images in these three categories. These were used as the test set.\\nFully connected\\ntwo-layer neural net\\nTwo feature maps \\nof size 4 \\n/H11003 \\n4\\nImage of size 6 \\n/H11003 \\n6\\nTwo pooled \\nfeature maps \\nof size 2 \\n/H11003 \\n2\\nVectorization\\n8\\n \\ninput neurons\\n3 output\\n neurons\\nFIGURE 12.45\\nCNN with one \\nconvolutional \\nlayer used to \\nlearn to recognize \\nthe images in Fig. \\n12.46.\\nTraining Image Set\\nTest Image Set\\nFIGURE 12.46\\n Left: Training images. Top row: Samples of a dark horizontal stripe. Center row: Samples of a centered \\ndark square. Bottom row: Samples of a dark vertical stripe. Right: Noisy samples of the three categories on the left, \\ncreated by adding Gaussian noise of zero mean and unit variance to the samples on the left. (All images are 8-bit \\ngrayscale images.)\\nDIP4E_GLOBAL_Print_Ready.indb   978\\n6/16/2017   2:18:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 979}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n979\\nAs Fig. 12.45 shows, the inputs to our system are single images. We used a receptor ﬁeld of size \\n33\\n×\\n, \\nwhich resulted in feature maps of size \\n44\\n×\\n.\\n There are two feature maps, which means we need two \\nkernels of size \\n33\\n×\\n,\\n and two biases. The pooled feature maps were generated using average pooling in \\nneighborhoods of size \\n22\\n×\\n.\\n This resulted in two pooled feature maps of size \\n22\\n×\\n,\\n because the feature \\nmaps are of size \\n44\\n×\\n.\\n The two pooled maps contain eight total elements which were organized as an \\n8-D column vector to vectorize the output of the last layer\\n. (We used linear indexing of each image, then \\nconcatenated the two resulting 4-D vectors into a single 8-D vector.) This vector was then fed into the \\nfully connected neural net on the right, which consists of the input layer and a three-neuron output layer, \\none neuron per class. Because this network has no hidden layers, it implements linear decision functions \\n(see Problem 12.18). To train the system, we used \\na\\n=\\n10\\n.\\n and ran the system for 400 epochs. Figure 12.47 \\nis a plot of the MSE as a function of epoch.\\n Perfect recognition of the training set was achieved after \\napproximately 100 epochs of training, despite the fact that the MSE was relatively high there. Recogni-\\ntion of the test set was 100% as well. The kernel and bias values learned by the system were: \\n \\nw\\n1\\n=\\n−\\n−\\n−\\n3.0132 1.1808 0.0945\\n0.9718 0.7087 0.9093\\n0.7193 0.0230 0.883 33\\n0.7388 1.8832 4.1077\\n1.0027 0.390\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=−\\n=\\n−\\n−\\n,.\\nb\\n12\\n0 2990\\nw\\n8 8 2.0357\\n1.2164 1.1853 0.1987\\n−−−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=−\\n,.\\nb\\n2\\n0 2834\\nIt is important that the CNN learned these parameters automatically from the raw training images. No \\nfeatures in the sense discussed in Chapter 11 were employed.\\n \\nEXAMPLE 12.17 :  Using a large training set to teach a CNN to recognize handwritten numerals.\\nIn this example, we look at a more practical application using a database containing 60,000 training and \\n10,000 test images of handwritten numeric characters. The content of this database, called the \\nMNIST \\ndatabase\\n, is similar to a database from NIST (National Institute of Standards and Technology). The \\nformer is a “cleaned up” version of the latter, in which the characters have been centered and for-\\nmatted into grayscale images of size \\n28 28\\n×\\n pixels. Both databases are freely available online. Figure \\n12.48 shows examples of typical numeric characters available in the databases\\n. As you can see, there is \\n100 200 300 400\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0\\nEpochs\\n Mean squared error\\nFIGURE 12.47\\nTraining MSE as a \\nfunction of epoch \\nfor the images in \\nFig. 12.46. Perfect \\nrecognition of the \\ntraining and test \\nsets was achieved \\nafter approxi-\\nmately 100 \\nepochs, despite \\nthe fact that the \\nMSE was rela-\\ntively high there.\\nDIP4E_GLOBAL_Print_Ready.indb   979\\n6/16/2017   2:18:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 980}),\n",
       " Document(page_content='980\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nsigniﬁcant variability in the characters—and this is just a small sampling of the 70,000 characters avail-\\nable for experimentation.\\nFigure 12.49 shows the architecture of the CNN we trained to recognize the ten digits in the MNIST \\ndatabase. We trained the system for 200 epochs using \\na\\n=\\n10\\n..\\n Figure 12.50 shows the training MSE as a \\nfunction of epoch for the 60,000 training images in the MNIST database\\n.\\nTraining was done using mini batches of 50 images at a time to improve the learning rate (see the dis-\\ncussion in Section 12.7). We also classiﬁed all images of the training set and all images of the test set after \\neach epoch of training. The objective of doing this was to see how quickly the system was learning the \\ncharacteristics of the data. Figure 12.51 shows the results. A high level of correct recognition performance \\nwas achieved after relatively few epochs for both data sets, with approximately 98% correct recognition \\nachieved after about 40 epochs. This is consistent with the training MSE in Fig. 12.50, which dropped \\nquickly, then began a slow descent after about 40 epochs. Another 160 epochs of training were required \\nfor the system to achieve recognition of about 99.9%. These are impressive results for such a small CNN.\\n6 feature maps \\nof size 24 \\n/H11003 \\n24\\n6 pooled \\nfeature \\nmaps of \\nsize 12 \\n/H11003 \\n12\\nImage of size 28 \\n/H11003 \\n28\\n12\\nfeature \\nmaps of \\nsize 8 \\n/H11003 \\n8\\n12\\npooled \\nfeature\\nmaps of \\nsize 4 \\n/H11003 \\n4\\nFully connected\\ntwo-la\\ny\\ner neural ne\\nt\\n10 \\noutput\\n neuron\\ns\\nVectorization\\n192 input neurons\\nFIGURE 12.49\\n CNN used to recognize the ten digits in the MNIST database. The system was trained with 60,000 \\nnumerical character images of the same size as the image shown on the left. This architecture is the same as the \\narchitecture we used in Fig. 12.42. (Image courtesy of NIST.)\\nFIGURE 12.48\\nSamples  \\nsimilar to those \\navailable in the \\nNIST and MNIST  \\ndatabases. Each \\ncharacter  \\nsubimage is \\nof size \\n28 28\\n×\\n \\npixels\\n.(Individual \\nimages courtesy \\nof NIST.)\\nDIP4E_GLOBAL_Print_Ready.indb   980\\n6/16/2017   2:18:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 981}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n981\\nFigure 12.52 shows recognition performance on each digit class for both the training and test sets. The \\nmost revealing feature of these two graphs is that the CNN did equally as well on both sets of data. This \\nis a good indication that the training was successful, and that it generalized well to digits it had not seen \\nbefore. This is an example of the neural network not “over-ﬁtting” the data in the training set.\\nFigure 12.53 shows the values of the kernels for the ﬁrst feature map, displayed as intensities. There \\nis one input image and six feature maps, so six kernels are required to generate the feature maps of \\nthe ﬁrst layer. The dimensions of the kernels are the same as the receptive ﬁeld, which we set at \\n55\\n×\\n. \\nT\\nhus, the ﬁrst image on the left in Fig. 12.53 is the \\n55\\n×\\n kernel corresponding to the ﬁrst feature map. \\nF\\nigure 12.54 shows the kernels for the second layer. In this layer, we have six inputs (which are the \\npooled maps of the ﬁrst layer) and twelve feature maps, so we need a total of \\n61 2 7 2\\n×=\\n kernels and \\nbiases to generate the twelve feature maps in the second layer\\n. Each column of Fig. 12.54 shows the six \\nFIGURE 12.50\\nTraining mean \\nsquared error \\nas a function of \\nepoch for the \\n60,000 training \\ndigit images in the \\nMNIST database.\\n0\\n0.1\\n0.2\\n0.3\\nTraining MSE\\nEpoch\\n0 40 80 120 160 200\\nTraining accuracy (\\n/H11003\\n100%)\\nEpoch\\n0.86\\n0.88\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n0 40 80 120 160 200\\nTesting accuracy (\\n/H11003\\n100%)\\nEpoch\\n0 40 80 120 160 200\\n0.86\\n0.88\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\nb a\\nFIGURE 12.51\\n (a) Training accuracy (percent correct recognition of the training set) as a function of epoch for the \\n60,000 training images in the MNIST database. The maximum achieved was 99.36% correct recognition. (b) Accu-\\nracy as a function of epoch for the 10,000 test images in the MNIST database. The maximum correct recognition \\nrate was 99.13%. \\nDIP4E_GLOBAL_Print_Ready.indb   981\\n6/16/2017   2:18:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 982}),\n",
       " Document(page_content='982\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\n55\\n×\\n kernels corresponding to one of the feature maps in the second layer. We used \\n22\\n×\\n pooling in \\nboth layers\\n, resulting in a 50% reduction of each of the two spatial dimensions of the feature maps.\\nFinally, it is of interest to visualize how one input image proceeds through the network, using the \\nkernels learned during training. Figure 12.55 shows an input digit image from the test set, and the com-\\nputations performed by the CNN at each layer. As before, we display numerical results as intensities. \\nConsider the results of convolution in the ﬁrst layer. If you look at each resulting feature map care-\\nfully, you will notice that it highlights a different characteristic of the input. For example, the feature map \\non the top of the ﬁrst column highlights the two vertical edges on the top of the character. The second \\nhighlights the edges of the entire inner region, and the third highlights a “blob-like”feature of the digit, \\nas if it had been blurred by a lowpass kernel. The other three feature maps show other features. If you \\nnow look at the ﬁrst two feature maps in the second layer, and compare them with the ﬁrst feature map \\nin the ﬁrst layer, you can see that they could be interpreted as higher-level abstractions of the top of the \\ncharacter, in the sense that they show a dark area ﬂanked on each side by white areas. Although these \\nabstractions are not always easy to analyze visually, this example clearly demonstrates that they can be \\nvery effective. And, remember the important fact that our simple system learned these features auto-\\nmatically from 60,000 training images. This capability is what makes convolutional networks so powerful \\nwhen it comes to image pattern classiﬁcation. In the next example, we will consider even more complex \\nimages, and show some of the limitations of our simple CNN architecture. \\nEXAMPLE 12.18 : Using a large image database to teach a CNN to recognize natural images.\\nIn this example, we trained the same CNN architecture as in Fig. 12.49, but using the RGB color images \\nin Fig. 12.56. These images are representative of those found in the CIFAR-10 database, a popular data-\\nbase used to test the performance of image classiﬁcation systems. Our objective was to test the limita-\\ntions of the CNN architecture in Fig. 12.49 by training it with data that is signiﬁcantly more complex \\nthan the MNIST images in Example 12.17. The only difference between the architecture needed to \\nb a\\nFIGURE 12.52\\n (a) Recognition accuracy of training set by image class. Each bar shows a number between 0 and 1. \\nWhen multiplied by 100%, these numbers give the correct recognition percentage for that class. (b) Recognition \\nresults per class in the test set. In both graphs the recognition rate is above 98%. \\nTraining set\\n0123 456789\\nClass (digit number)\\n1.00\\n0.99\\n0.98\\n0.97\\n0.96\\n0.95\\n0.94\\n0.93\\nRecognition accuracy\\nTest set\\n0123 456789\\nClass (digit number)\\n1.00\\n0.99\\n0.98\\n0.97\\n0.96\\n0.95\\n0.94\\n0.93\\nRecognition accuracy\\nDIP4E_GLOBAL_Print_Ready.indb   982\\n6/16/2017   2:18:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 983}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n983\\nFIGURE 12.54\\n Kernels of the second layer after 200 epochs of training, displayed as images of size \\n55\\n×\\n.\\n There are six \\ninputs (pooled feature maps) into the second layer\\n. Because there are twelve feature maps in the second layer, the \\nCNN learned the weights of \\n61 2 7 2\\n×=\\n kernels.\\nFIGURE 12.53\\nKernels of the \\nﬁrst layer after \\n200 epochs of \\ntraining, shown as \\nimages.\\nFIGURE 12.55\\nResults of a for-\\nward pass for one \\ndigit image through \\nthe CNN in Fig. \\n12.49 after training. \\nThe feature maps \\nwere generated \\nusing the kernels \\nfrom Figs. 12.53 and \\n12.54, followed by \\npooling. The neural \\nnet is the two-layer \\nneural network \\nfrom Fig. 12.49. The \\noutput high value \\n(in white) indicates \\nthat the CNN rec-\\nognized the input \\nproperly. (This \\nﬁgure is the same \\nas Fig. 12.44.)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\nNeural\\nnet\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\nVector\\nDIP4E_GLOBAL_Print_Ready.indb   983\\n6/16/2017   2:18:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 984}),\n",
       " Document(page_content='984\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nAirplane\\nAutomobile\\nBird\\nCat\\nDeer\\nDog\\nFrog\\nHorse\\nShip\\nTruck\\nFIGURE 12.56\\nMini images \\nof size \\n32 32\\n×\\n \\npixels\\n,  \\nrepresentative of \\nthe 50,000  \\ntraining and \\n10,000 test images \\nin the CIFAR-10 \\ndatabase (the 10 \\nstands for ten \\nclasses). The class \\nnames are shown \\non the right. \\n(Images courtesy \\nof Pearson  \\nEducation.)\\n0 100 200 300 400 500\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nEpoch\\nTraining MSE\\nFIGURE 12.57\\nTraining mean \\nsquared error \\nas a function of \\nthe number of \\nepochs for a train-\\ning set of 50,000 \\nCIFAR-10 images.\\nDIP4E_GLOBAL_Print_Ready.indb   984\\n6/16/2017   2:18:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 985}),\n",
       " Document(page_content='12.6\\n  \\nDeep Convolutional Neural Networks\\n    \\n985\\nprocess the CIFAR-10 images, and the architecture in Fig. 12.49, is that the CIFAR-10 images are RGB \\ncolor images, and hence have three channels. We worked with these input images using the approach \\nexplained in the subsection entitled Multiple Input Images, on page 973. \\nWe trained the modiﬁed CNN for 500 epochs using the 50,000 training images of the CIFAR-10 data-\\nbase. Figure 12.57 is a plot of the mean squared error as a function of epoch during the training phase. \\nObserve that the MSE begins to plateau at a value of approximately 0.25. In contrast, the MSE plot in \\nFig. 12.50 for the MNIST data achieved a much lower ﬁnal value. This is not unexpected, given that the \\nCIFAR-10 images are signiﬁcantly more complex, both in the objects of interest as well as their back-\\ngrounds. The lower expected recognition performance of the training set is conﬁrmed by the training-\\naccuracy plotted in Fig. 12.58(a) as a function of epoch. The recognition rate leveled-off around 68% for \\nthe training data and about 61% for the test data. Although these results are not nearly as good as those \\nobtained for the MNIST data, they are consistent with what we would expect from a very basic network. \\nIt is possible to achieve over 96% accuracy on this database (see Graham [2015]), but that requires a \\nmore complex network and a different pooling strategy.\\nFigure 12.59 shows the recognition accuracy per class for the training and test image sets. With a few \\nexceptions, the highest recognition rate in both the training and test sets was achieved for engineered \\nobjects, and the lowest was for small animals. Frogs were an exception, caused most likely by the fact \\nthat frog size and shape are more consistent than they are, for example, in dogs and birds. As you can \\nsee in Fig. 12.59, if the small animals were removed from the list, recognition performance on the rest of \\nthe images would have been considerably higher.\\nFigures 12.60 and Fig. 12.61 show the kernels of the ﬁrst and second layers. Note that each column \\nin Fig. 12.60 has three \\n55\\n×\\n kernels. This is because there are three input channels to the CNN in this \\nexample\\n. If you look carefully at the columns in Fig. 12.60, you can detect a similarity in the arrangement \\nand values of the coefﬁcients. Although it is not obvious what the kernels are detecting, it is clear that \\nthey are consistent in each column, and that all columns are quite different from each other, indicating \\na capability to detect different features in the input images. We show Fig. 12.61 for completeness only, \\n0 100 200 300 400 500\\n0.2\\n0\\n0.4\\n0.6\\n0.8\\n1.0\\nTraining accuracy (\\n/H11003\\n100%)\\nTesting accuracy (\\n/H11003\\n100%)\\n0.2\\n0\\n0.4\\n0.6\\n0.8\\n1.0\\n0 100 200 300 400 500\\nEpoch\\nEpoch\\nb a\\nFIGURE 12.58\\n (a) Training accuracy (percent correct recognition of the training set) as a function of epoch for the \\n50,000 training images in the CIFAR-10 database. (b) Accuracy as a function of epoch for the 10,000 CIFAR-10 \\ntest images.\\nDIP4E_GLOBAL_Print_Ready.indb   985\\n6/16/2017   2:18:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 986}),\n",
       " Document(page_content='986\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nas there is little we can infer that deep into the network, especially at this small scale, and considering \\nthe complexity of the images in the training set. Finally, Fig. 12.62 shows a complete recognition pass \\nthrough the CNN using the weights in Figs. 12.60 and 12.61. The input shows the three color channels \\nof the RGB image in the seventh column of the ﬁrst row in Fig. 12.56. The feature maps in the ﬁrst \\ncolumn, show the various features extracted from the input. The second column shows the pooling \\nresults, zoomed to the size of the features maps for clarity. The third and fourth columns show the results \\nin the second layer, and the ﬁfth column shows the vectorized output. Finally, the last column shows the \\nresult of recognition, with white representing a high output, and the others showing much smaller values. \\nThe input image was properly recognized as belonging to class 1.\\n0.69\\n0.85\\n0.55\\n0.50\\n0.54\\n0.56\\n0.79\\n0.68\\n0.79\\n0.77\\nClass\\nTraining set\\n0.65\\n0.81\\n0.48\\n0.43\\n0.47\\n0.50\\n0.76\\n0.64\\n0.72\\n0.69\\nClass\\nTest set\\n123456789 1 0 123456789 1 0\\nRecognition accuracy\\n1.0\\n0.0\\n0.9\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nRecognition accuracy\\n1.0\\n0.0\\n0.9\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nb a\\nFIGURE 12.59\\n (a) CIFAR-10 recognition rate of training set by image class. Each bar shows a number between 0 and 1. \\nWhen multiplied by 100%, these numbers give the correct recognition percentage for that class. (b) Recognition \\nresults per class in the test set.\\nFIGURE 12.60\\nWeights of the \\nkernels of the ﬁrst \\nconvolution layer \\nafter 500 epochs \\nof training. \\nDIP4E_GLOBAL_Print_Ready.indb   986\\n6/16/2017   2:18:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 987}),\n",
       " Document(page_content='12.7\\n  \\nAdditional Details of Implementation\\n    \\n987\\n12.7  SOME ADDITIONAL DETAILS OF IMPLEMENTATION  \\nWe mentioned in the previous section that neural (including convolutional) nets \\nhave the ability to learn features directly from training data, thus reducing the need \\nfor “engineered” features. While this is a significant advantage, it does not imply that \\nthe design of a neural network is free of human input. On the contrary, designing \\ncomplex neural networks requires significant skill and experimentation.\\nIn the last two sections, our focus was on the development of fundamental con-\\ncepts in neural nets, with an emphasis on the derivation of backpropagation for both \\nfully connected and convolutional nets. Backpropagation is the backbone of neural \\nnet design, but there are other important considerations that inﬂuence how well \\na neural net learns, and then generalizes to patterns it has not seen before. In this \\nsection, we discuss brieﬂy some important aspects in the design of fully connected \\nand convolutional neural networks.\\nOne of the ﬁrst questions when designing a neural net architecture is how many \\nlayers to specify for the network. Theoretically, the \\nuniversality approximation \\ntheorem\\n (Cybenco [1989]) tells us that, under mild conditions, arbitrarily complex \\ndecision functions can be \\napproximated\\n by a continuous feedforward neural network \\nwith a single hidden layer. Although the theorem does not tell us how to compute \\nthe parameters of that single hidden layer, it does indicate that structurally simple \\nneural nets can be very powerful. You have seen this in some of the examples in the \\nlast two sections. Experimental evidence suggests that deep neural nets (i.e., net-\\nworks with two or more hidden layers) are better than a single hidden layer network \\nat learning abstract representations, which typically is the main point of learning. \\nThere is no such thing as an algorithm to determine the “optimum” number of lay-\\ners to use in a neural network. Therefore, specifying the number of layers generally \\n12.7\\nFIGURE 12.61\\n Weights of the kernels of the second convolution layer after 500 epochs of training. The interpretation \\nof these kernels is the same as in Fig. 12.54.\\nDIP4E_GLOBAL_Print_Ready.indb   987\\n6/16/2017   2:18:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 988}),\n",
       " Document(page_content=\"988\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nFIGURE 12.62\\nGraphical  \\nillustration of \\na forward pass \\nthrough the \\ntrained CNN. \\nThe purpose \\nwas to recognize \\none input image \\nfrom the set in \\nFig. \\n12.56\\n. As the \\noutput shows, the \\nimage was  \\nrecognized  \\ncorrectly as \\nbelonging to class \\n1, the class of \\nairplanes.  \\n(Original image \\ncourtesy of  \\nPearson  \\nEducation.)\\nB\\n'\\nR\\nRGB components\\nof an input color\\nimage\\nVector\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\nFeature\\nmaps\\nPooled\\nfeature\\nmaps\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nNeural\\nnet\\nis determined by a combination of experience and experimentation. “Starting small” \\nis a logical approach to this problem. The more layers a network has, the higher the \\nprobability that backpropagation will run into problems such as so-called \\nvanishing \\ngradients,\\n where gradient values are so small that gradient descent ceases to be \\neffective. In convolutional networks, we have the added issue that the size of the \\ninputs decreases as the images propagate through the network. There are two causes \\nfor this. The ﬁrst is a natural size reduction caused by convolution itself, with the \\namount of reduction being proportional to the size of the receptive ﬁelds. One solu-\\ntion is to use \\npadding\\n prior to performing convolution operations, as we discussed in \\nSection 3.4. The second (and most signiﬁcant) cause of size reduction is pooling. The \\nminimum pooling neighborhood is of size \\n22\\n×\\n,\\n which reduces the size of feature \\nmaps by three-quarters at each layer\\n. A solution that helps is to \\nupsample\\n the input \\nDIP4E_GLOBAL_Print_Ready.indb   988\\n6/16/2017   2:18:05 PM\\nwww.EBooksWorld.ir\", metadata={'source': 'imagepro.pdf', 'page': 989}),\n",
       " Document(page_content='12.7\\n  \\nAdditional Details of Implementation\\n    \\n989\\nimages, but this must done with care because the relative sizes of features of interest \\nwould increase proportionally, thus inﬂuencing the size selected for receptive ﬁelds.\\nAfter the number of layers has been speciﬁed, the next task is to specify the num-\\nber of neurons per layer. We always know how many neurons are needed in the ﬁrst \\nand last layers, but the number of neurons for internal layer is also an open question \\nwith no theoretical “best” answer. If the objective is to keep the number of layers as \\nsmall as possible, the power of the network is increased to some degree by increas-\\ning the number of neurons per layer. \\nThe main aspects of specifying the architecture of a neural network are com-\\npleted by specifying the activation function. In this chapter, we worked with sigmoid \\nfunctions for consistency between examples, but there are applications in which\\n \\nhyperbolic tangent and ReLU activation functions are superior in terms of improv-\\ning training performance\\n.\\nOnce a network architecture has been speciﬁed, training is the central aspect of \\nmaking the architecture useful. Although the networks we discussed in this chapter \\nare relatively simple, networks applied to very large-scale problems can have mil-\\nlions of nodes and require large blocks of time to train. When available, the param-\\neters of a \\npretrained\\n network are an ideal starting point for further training, or for \\nvalidating recognition performance. Another central theme in training neural nets is \\nthe use of GPUs to accelerate matrix operations.\\nAn issue often encountered in training is \\nover-ﬁtting\\n, in which recognition of the \\ntraining set is acceptable, but the recognition rate on samples not used for training is \\nmuch lower. That is, the net is not able to \\ngeneralize\\n what it learned and apply it to \\ninputs it has not encountered before. When additional training data is not available, \\nthe most common approach is to artiﬁcially enlarge the training set using transfor-\\nmations such as geometric distortions and intensity variations. The transformations \\nare carried out while preserving the class membership of the transformed patterns. \\nAnother major approach is to use \\ndropout\\n, a technique that randomly drops nodes \\nwith their connections from a neural network during training. The idea is to change \\nthe architecture slightly to prevent the net from adapting too much to a ﬁxed set of \\nparameters (see Srivastava et al. [2014]).\\nIn addition to computational speed, another important aspect of training is efﬁ-\\nciency. Simple things, such as shufﬂing the input patterns at the beginning of each \\ntraining epoch can reduce or eliminate the possibility of “cycling,” in which param-\\neter values repeat at regular intervals. \\nStochastic gradient descent\\n is another impor-\\ntant training reﬁnement in which, instead of using the entire training set, samples \\nare selected randomly and input into the network. You can think of this as dividing \\nthe training set into \\nmini-batches,\\n and then choosing a single sample from each mini-\\nbatch. This approach often results in speedier convergence during training.\\nIn addition to the above topics, a paper by LeCun et al. [2012] is an excellent over-\\nview of the types of considerations introduced in the preceding discussion. In fact, \\nthe breath spanned by these topics is extensive enough to be the subject of an entire \\nbook (see Montavon et al. [2012]). The neural net architectures we discussed were \\nby necessity limited in scope. You can get a good idea of the practical requirements \\nof implementing practical networks by reading a paper by Krizhevsky, Sutskever, \\nand Hinton [2012], which summarizes the design and implementation of a large-\\nscale, deep convolutional neural network. There are a multitude of designs that have \\nDIP4E_GLOBAL_Print_Ready.indb   989\\n6/16/2017   2:18:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 990}),\n",
       " Document(page_content='990\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n12.1 \\nDo the following:\\n(a) * \\nCompute the decision functions of a mini-\\nmum distance classiﬁer for the patterns in\\n \\nFig. 12.10. You may obtain the required mean \\nvectors by (careful) inspection.\\n(b) \\nSketch the decision boundary implemented \\nby the decision functions in (a).\\n12.2 * \\nShow that Eqs. (12-3) and (12-4) perform the \\nsame function in terms of pattern classiﬁcation.\\n12.3 \\nShow that the boundary given by Eq. (12-8) is \\nthe perpendicular bisector of the line joining the \\nn\\n-dimensional points \\nm\\ni\\n and \\nm\\nj\\n.\\n12.4 * \\nShow how the minimum distance classiﬁer dis-\\ncussed in connection with F\\nig. 12.11 could be \\nimplemented by using \\nN\\nc\\n resistor banks (\\nN\\nc\\n is \\nthe number of classes), a summing junction at \\neach bank (for summing currents), and a maxi-\\nmum selector capable of selecting the maximum \\nvalue of \\nN\\nc\\n decision functions in order to deter-\\nmine the class membership of a given input.\\n12.5 * \\nShow that the correlation coefﬁcient of Eq. (12-10) \\nhas values in the range \\n[, ] .\\n−\\n11\\n (\\nHint\\n:\\n Express \\ng\\n in \\nvector form.)\\n12.6 \\nShow that the distance measure \\nDab\\n(,\\n)\\n in Eq. \\n(12-12) satisﬁes the properties in Eq.\\n (12-13).\\n12.7 * \\nShow that \\nba\\n=\\n()\\n−\\nmax ,\\nab\\n in Eq. (12-14) is 0 \\nif and only if \\na\\n and \\nb\\n are identical strings\\n.\\n12.8 \\nCarry out the manual computations that resulted \\nin the mean vector and covariance matrices in \\nExample 12.5.\\n12.9 * \\nThe following pattern classes have Gaussian prob-\\nability density functions:\\nSummary, References, and Further Reading\\n \\nBackground material for Sections 12.1 through 12.4 are the books by Theodoridis and Koutroumbas [2006], by \\nDuda, Hart, and Stork [2001], and by Tou and Gonzalez [1974]. For additional reading on the material on match-\\ning shape numbers see Bribiesca and Guzman [1980]. On string matching, see Sze and Yang [1981]. A signiﬁcant \\nportion of this chapter was devoted to neural networks. This is a reﬂection of the fact that neural nets, and in \\nparticular convolutional neural nets, have made signiﬁcant strides in the past decade in solving image pattern \\nclassiﬁcations problems. As in the rest of the book, our presentation of this topic focused on fundamentals, but \\nthe topics covered were thoroughly developed. What you have learned in this chapter is a solid foundation for \\nmuch of the work being conducted in this area. As we mentioned earlier, the literature on neural nets is vast, and \\nquickly growing. As a starting point, a basic book by Nielsen [2015] provides an excellent introduction to the topic. \\nThe more advanced book by Goodfellow, Bengio, and Courville [2016] provides more depth into the mathemati-\\ncal underpinning of neural nets. Two classic papers worth reading are by Rumelhart, Hinton, and Williams [1986], \\nand by LeCun, Bengio, and Haffner [1998]. The LeNet architecture we discussed in Section 12.6 was introduced in \\nthe latter reference, and it is still a foundation for image pattern classiﬁcation. A recent survey article by LeCun, \\nBengio, and Hinton [2015] gives an interesting perspective on the scope of applicability of neural nets in general. \\nThe paper by Krizhevsky, Sutskever, and Hinton [2012] was one of the most important catalysts leading to the \\nsigniﬁcant increase in the present interest on convolutional networks, and on their applicability to image pattern \\nclassiﬁcation. This paper is also a good overview of the details and techniques involved in implementing a large-\\nscale convolutional neural network. For details on the software aspects of many of the examples in this chapter, see \\nGonzalez, Woods, and Eddins [2009].\\nbeen implemented over the past decade, including commercial and free implemen-\\ntations. A quick internet search will reveal a multitude of available architectures. \\nDIP4E_GLOBAL_Print_Ready.indb   990\\n6/16/2017   2:18:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 991}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n991\\nc\\nTTTT\\n1\\n00 20 22 02\\n: { (,), (,), (,), (,)}\\nc\\nTTTT\\n2\\n44 64 66 46\\n: { (,), (,), (,), (,)}\\n(a) \\nAssume that \\nPc Pc\\n() ()\\n12\\n12\\n==\\n and \\nobtain \\nthe equation of the Bayes decision boundary \\nbetween these two classes\\n.\\n(b) \\nSketch the boundary.\\n12.10 \\nRepeat Problem 12.9, but use the following pat-\\ntern classes:\\nc\\nT TTT\\n1\\n10 0 1 10 01\\n: { ( ,), (, ), (,), (,)}\\n−−\\nc\\nT TTT\\n2\\n20 0 2 20 02\\n: { ( ,), (, ), (,), (,)}\\n−−\\nNote that the classes are not linearly separable.\\n12.11 \\nWith reference to the results in Table 12.1, com-\\npute the overall correct recognition rate for the \\npatterns of the training set.\\n Repeat for the pat-\\nterns of the test set.\\n12.12 * \\nWe derived the Bayes decision functions\\nd\\npc P cj N\\njj j\\nc\\nxx\\n(\\n)\\n=\\n(\\n)\\n(\\n)\\n=\\n,, , ,\\n12\\n…\\nusing a 0-1 loss function. Prove that these deci-\\nsion functions minimize the probability of error\\n. \\n(\\nHint:\\n The probability of error \\npe\\n()\\n is \\n1\\n−\\npc\\n()\\n,\\n \\nwhere \\npc\\n()\\n is the probability of being correct. \\nF\\nor a pattern vector \\nx\\n belonging to class \\nc\\ni\\n,\\n \\npc pc\\ni\\nxx\\n(\\n)\\n=\\n()\\n.\\n Find \\npc\\n()\\n and show that \\npc\\n()\\n is \\nmaximum [\\npe\\n()\\n is minimum] when \\npc P c\\nii\\n() ( )\\nx\\n \\nis maximum.)\\n12.13 \\nFinish the computations started in Example 12.7.\\n12.14 * \\nThe perceptron algorithm given in Eqs. (12-44) \\nthrough (12-46) can be expressed in a more con-\\ncise form by multiplying the patterns of class \\nc\\n2\\n by \\n−\\n1,\\n in which case the correction steps \\nin the algorithm become \\nww\\n()\\n( ) ,\\nkk\\n+=\\n1\\n if \\nw\\nT\\nkk\\n()() ,\\ny\\n>\\n0\\n and \\nww\\n() ( )\\nkk k\\n+= +\\n()\\n1\\na\\ny\\n \\notherwise, where we use \\ny\\n instead of \\nx\\n to make it \\nclear that the patterns of class \\nc\\n2\\n were multiplied \\nby \\n−\\n1.\\n This is one of several perceptron algo-\\nrithm formulations that can be derived starting \\nfrom the general gradient descent equation\\nww\\nw\\nw\\nww\\nkk\\nJ\\nk\\n+\\n(\\n)\\n=\\n(\\n)\\n−\\n∂\\n(\\n)\\n∂\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n()\\n1\\na\\n,\\ny\\nwhere \\na\\n>\\n0,\\n \\nJ\\n(,\\n)\\nw\\ny\\n is a criterion function, and \\nthe partial derivative is evaluated at \\nww\\n=\\n() .\\nk\\n \\nShow that the perceptron algorithm in the prob-\\nlem statement can be obtained from this general \\ngradient descent procedure by using the criterion \\nfunction\\nJ\\nTT\\n(, )\\nww w\\nyy y\\n=−\\n(\\n)\\n1\\n2\\n(\\nHint:\\n \\nThe partial derivative of \\nw\\nT\\ny\\n with respect \\nto \\nw\\n is \\ny\\n.)\\n12.15 * \\nProve that the perceptron training algorithm giv-\\ne\\nn in Eqs. (12-44) through (12-46) converges in \\na ﬁnite number of steps if the training pattern \\nsets are linearly separable. [\\nHint:\\n Multiply the \\npatterns of class \\nc\\n2\\n by \\n−\\n1\\n and consider a non-\\nnegative threshold,\\n \\nT\\n0\\n so that the perceptron \\ntraining algorithm (with \\na\\n=\\n1)\\n is expressed in \\nthe form \\nww\\n()\\n( ) ,\\nkk\\n+=\\n1\\n if \\nw\\nT\\nkk T\\n()() ,\\ny\\n>\\n0\\n \\nand \\nww\\n( ) () ()\\nkk k\\n+= +\\n1\\na\\ny\\n otherwise. You \\nmay need to use the Cauchy-Schwartz inequality: \\nab a b\\n22\\n2\\n≥\\n() . ]\\nT\\n12.16 \\nDerive equations of the derivatives of the follow-\\ning activation functions:\\n(a) \\nThe sigmoid activation function in Fig. 12.30(a).\\n(b) \\nThe hyperbolic tangent activation function \\nin F\\nig. 12.30(b).\\n(c) * \\nThe ReLU activation function in Fig. 12.30(c).\\n12.17 * \\nSpecify the structure, weights, and bias(es) of the \\nsmallest neural network capable of performing \\nexactly\\n the same function as a minimum distance \\nclassiﬁer for two pattern classes in \\nn\\n-dimensional \\nspace\\n. You may assume \\nthat the classes are tightly \\ngrouped and are linearly separable.\\n12.18 \\nWhat is the decision boundary implemented by \\na neural network with \\nn\\n inputs\\n, a single output \\nneuron, and no hidden layers? Explain.\\n12.19 \\nSpecify the structure, weights, and bias of a neu-\\nral network capable of performing exactly the \\nsame function as a Bayes classiﬁer for two pat-\\ntern classes in \\nn\\n-dimensional space\\n. The classes \\nare Gaussian with different means but equal \\ncovariance matrices.\\n12.20 \\nAnswer the following:\\n(a) * \\nUnder what conditions are the neural net-\\nworks in Problems 12.17 and 12.19 identical?\\n(b) \\nSuppose you specify a neural net architecture \\nidentical to the one in Problem 12.17.\\n Would \\ntraining by backpropagation yield the same \\nDIP4E_GLOBAL_Print_Ready.indb   991\\n6/16/2017   2:18:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 992}),\n",
       " Document(page_content='992\\n    \\nChapter\\n \\n12\\n  \\nImage Pattern Classiﬁcation\\nweights and bias as that network if trained \\nwith a sufﬁciently large number of samples?\\n \\nExplain.\\n12.21 \\nTwo pattern classes in two dimensions are distrib-\\nuted in such a way that the patterns of class \\nc\\n1\\n lie \\nrandomly along a circle of radius \\nr\\n1\\n.\\n Similarly, the \\npatterns of class \\nc\\n2\\n lie randomly along a circle of \\nradius \\nr\\n2\\n,\\n where \\nrr\\n21\\n2\\n=\\n.\\n Specify the structure of \\na neural network with the minimum number of \\nlayers and nodes needed to classify properly the \\npatterns of these two classes\\n.\\n12.22 * \\nIf two classes are linearly separable, we can train \\na perceptron starting with weights and a bias that \\nare all zero\\n, and we would still get a solution. Can \\nyou do the same when training a neural network \\nby backpropagation? Explain.\\n12.23 \\nLabel the outputs, weights, and biases for every \\nnode in the following neural network using the \\ngeneral notation introduced in F\\nig. 12.31.\\n1\\n1\\n1\\n1\\n12.24 \\nAnswer the following:\\n(a) \\nThe last element of the input vector in Fig. \\n12.32 is 1.\\n Is this vector augmented? Explain.\\n(b) \\nRepeat the calculations in Fig. 12.32, but \\nusing weight matrices that are 100 times the \\nvalues of those used in the ﬁgure\\n. \\n(c) * \\nWhat can you conclude in general from your \\nresults in (b)?\\n12.25 \\nAnswer the following:\\n(a) * \\nThe chain rule in Eq. (12-70) shows three \\nterms\\n. However, you are probably more famil-\\niar with chain rule expressions that have two \\nterms. Show that if you start with the expres-\\nsion\\nd\\nj\\nji\\ni\\nj\\ni\\nE\\nz\\nE\\nz\\nz\\nz\\n()\\n() ( )\\n()\\n()\\nℓ\\nℓℓ\\nℓ\\nℓ\\n=\\n∂\\n∂\\n=\\n∂\\n∂+\\n∂+\\n∂\\n∑\\n1\\n1\\nyou can arrive at the result in Eq. (12-70).\\n(b) \\nShow how the middle term in the third line \\nof Eq.\\n (12-70) follows from the middle term \\nin the second.\\n12.26 \\nShow the validity of Eq. (12-72). (\\nHint:\\n Use the \\nchain rule\\n.)\\n12.27 * \\nShow that the dimensions of matrix \\nD\\n()\\nℓ\\n in Eq. \\n(12-79) are \\nnn\\np\\nℓ\\n×\\n.\\n (\\nHint:\\n Some of the parameters \\nin that equation are computed in forward propaga-\\ntion, so you already know their dimensions.)\\n12.28 \\nWith reference to the discussion following Eq. \\n(12-82),\\n explain why the error for one pattern is \\nobtained by squaring the elements of one column \\nof matrix \\nAR\\n()\\n,\\nL\\n−\\n(\\n)\\n adding them, and dividing \\nthe result by 2.\\n12.29 * \\nThe matrix formulation in Table 12.3 contains all \\npatterns as columns of a single matrix \\nX\\n.\\n This is \\nideal in terms of speed and economy of imple-\\nmentation. It is also well suited when training \\nis done using mini-batches. However, there are \\napplications in which the large number of train-\\ning vectors is too large to hold in memory, and \\nit becomes more practical to loop through each \\npattern using the vector formulation. Compose \\na table similar to Table 12.3, but using individual \\npatterns, \\nx\\n, instead of matrix \\nX\\n.\\n12.30 \\nConsider a CNN whose inputs are RGB color \\nimages of size \\n512 512\\n×\\n pixels. The network has \\ntwo convolutional layers\\n. Using this information, \\nanswer the following:\\n(a) * \\nYou are told that the spatial dimensions \\nof the feature maps in the ﬁrst layer are \\n504 504\\n×\\n,\\n and that there are 12 feature \\nmaps in the ﬁrst layer\\n. Assuming that no \\npadding is used, and that the kernels used \\nare square, and of an odd size, what are the \\nspatial dimensions of these kernels?\\n(b) \\nIf subsampling is done using neighborhoods \\nof size \\n22\\n×\\n,\\n what are the spatial dimensions \\nof the pooled feature maps in the ﬁrst layer?\\n(c) \\nWhat is the depth (number) of the pooled \\nfeature maps in the ﬁrst layer?\\n(d) \\nThe spatial dimensions of the convolution \\nkernels in the second layer are \\n33\\n×\\n.\\n Assum-\\ning no padding\\n, what are the sizes of the fea-\\nture maps in the second layer?\\n(e) \\nYou are told that the number of feature maps \\nDIP4E_GLOBAL_Print_Ready.indb   992\\n6/16/2017   2:18:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 993}),\n",
       " Document(page_content=' \\n  \\nProblems\\n    \\n993\\nin the second layer is 6, and that the size of \\nthe pooling neighborhoods is again \\n22\\n×\\n.\\n \\nWhat are the dimensions of the vectors that \\nresult from vectorizing the last layer of the \\nCNN? Assume that vectorization is done \\nusing linear indexing. \\n12.31 \\nSuppose the input images to a CNN are padded \\nto compensate for the size reduction caused by \\nconvolution and subsampling (pooling).\\n \\nLet \\nP\\n \\ndenote the thickness o\\nf \\nthe padding border, let \\nV\\n \\ndenote the width of the (square) input images\\n, let \\nS\\n denote the stride, and let \\nF\\n denote the width of \\nthe (square) receptive ﬁeld.\\n \\n(a) \\nShow that the number, \\nN\\n,\\n of neurons in \\neach row in the resulting feature map is\\n \\nN\\nVP F\\nS\\n=\\n+−\\n+\\n2\\n1\\n(b) * \\nHow would you interpret a result using this \\nequation that is not an integer?\\n12.32 * \\nShow the validity of Eq. (12-106).\\n12.33 \\nAn experiment produces binary images of blobs \\nthat are nearly elliptical in shape\\n, as the following \\nexample image shows. The blobs are of three siz-\\nes, with the average values of the principal axes \\nof the ellipses being (1.3, 0.7), (1.0, 0.5), and (0.75, \\n0.25). The dimensions of these axes vary \\n±\\n10% \\nabout their average values\\n. \\nDevelop an image processing system capable of \\nrejecting incomplete or overlapping ellipses, then \\nclassifying the remaining single ellipses into one \\nof the three given size classes. Show your solu-\\ntion in block diagram form, giving speciﬁc details \\nregarding the operation of each block. Solve the \\nclassiﬁcation problem using a minimum distance \\nclassiﬁer, indicating clearly how you would go \\nabout obtaining training samples, and how you \\nwould use these samples to train the classiﬁer.\\n12.34 \\nA factory mass-produces small American ﬂags \\nfor sporting events\\n. The quality assurance team \\nhas observed that, during periods of peak pro-\\nduction, some printing machines have a tendency \\nto drop (randomly) between one and three stars \\nand one or two entire stripes. Aside from these \\nerrors, the ﬂags are perfect in every other way. \\nAlthough the ﬂags containing errors represent a \\nsmall percentage of total production, the plant \\nmanager decides to solve the problem. After \\nmuch investigation, she concludes that automatic \\ninspection using image processing techniques is \\nthe most economical approach. The basic speciﬁ-\\ncations are as follows: The ﬂags are approximate-\\nly 7.5 cm by 12.5 cm in size. They move length-\\nwise down the production line (individually, but \\nwith a \\n±\\n15%\\n variation in orientation) at approxi-\\nmately 50 cm/s\\n, with a separation between ﬂags of \\napproximately 5 cm. In all cases, “approximately” \\nmeans \\n±\\n5%.\\n The plant manager employs you to \\ndesign an image processing system for each pro-\\nduction line\\n. You are told that cost and simplicity \\nare important parameters in determining the via-\\nbility of your approach. Design a complete sys-\\ntem based on the model of Fig. 1.23. Document \\nyour solution (including assumptions and speci-\\nﬁcations) in a brief (but clear) written report \\naddressed to the plant manager.\\n \\nYou can use any \\nof the methods discussed in the book.\\nDIP4E_GLOBAL_Print_Ready.indb   993\\n6/16/2017   2:18:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 994}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 995}),\n",
       " Document(page_content='Bibliography\\nAbidi, M. A. and Gonzalez, R. C. (eds.) [1992]. \\nData Fusion in Robotics and Machine \\nIntelligence\\n, Academic Press, New York.\\nAbramson, N. [1963]. \\nInformation Theory and Coding\\n, McGraw-Hill, New York.\\nAchanta, R., et al. [2012]. “SLIC Superpixels Compared to State-of-the-Art Superpix-\\nel Methods,” \\nIEEE Trans. Pattern Anal. Mach. Intell\\n. vol. 34, no. 11, pp. 2274–2281.\\nAhmed, N., Natarajan, T., and Rao, K. R. [1974]. “Discrete Cosine Transforms,” \\nIEEE \\nTrans. Comp.\\n, vol. C-23, pp. 90–93.\\nAndrews, H. C. [1970]. \\nComputer Techniques in Image Processing\\n, Academic Press, \\nNew York.\\nAndrews, H. C. and Hunt, B. R. [1977]. \\nDigital Image Restoration\\n, Prentice Hall, \\nEnglewood Cliffs, NJ.\\nAntonini, M., Barlaud, M., Mathieu, P ., and Daubechies, I. [1992].“Image Coding \\nUsing Wavelet Transform,” \\nIEEE Trans. Image Process.\\n, vol. 1, no. 2, pp. 205–220.\\nAscher, R.N. and Nagy, G. [1974].“A Means for Achieving a High Degree of Com-\\npaction on Scan-Digitized Printed Text,” \\nIEEE Transactions on Comp.\\n, C-23, \\npp. 1174–1179.\\nBallard, D. H. [1981]. “Generalizing the Hough Transform to Detect Arbitrary \\nShape\\ns,” \\nPattern Recognition\\n, vol. 13, no. 2, pp. 111–122.\\nBallard, D. H. and Brown, C. M. [1982]. \\nComputer Vision\\n, Prentice Hall, Englewood \\nCliffs, NJ.\\nBasart, J. P ., Chacklackal, M. S., and Gonzalez, R. C. [1992]. “Introduction to Gray-\\nScale Morphology,” in \\nAdvances in Image Analysis\\n, Y. Mahdavieh and R. C. Gon-\\nzalez (eds.), SPIE Press, Bellingham,WA, pp. 306–354.\\nBasart, J. P . and Gonzalez, R. C. [1992]. “Binary Morphology,” in \\nAdvances in Image \\nAnalysis\\n, Y. Mahdavieh and R. C. Gonzalez (eds.), SPIE Press, Bellingham, WA, \\npp. 277–305.\\nBell, E.T. [1965]. \\nMen of Mathematics\\n, Simon & Schuster, New York.\\nBerger, T. [1971]. \\nRate Distortion Theory\\n, Prentice Hall, Englewood Cliffs, N.J.\\nBeucher, S. and Meyer, F. [1992]. “The Morphological Approach of Segmentation: \\nThe Watershed Transformation,” in \\nMathematical Morphology in Image Process-\\ning\\n, E. Dougherty (ed.), Marcel Dekker, New York. \\nBeyerer, J., Puente Leon, F. and Frese, C. [2016]. \\nMachine Vision—Automated Visu-\\nal Inspection: Theory, Practice, and Applications\\n, Springer-Verlag, Berlin, Ger-\\nmaNew York.\\nBlahut, R. E. [1987]. \\nPrinciples and Practice of Information Theory\\n, Addison-Wesley, \\nReading, MA.\\nBleau, A. and Leon, L. J. [2000]. “Watershed-Based Segmentation and Region \\nMerging,” \\nComputer Vision and Image Understanding\\n, vol. 77, no. 3, pp. 317–370.\\nBlum, H. [1967]. “A Transformation for Extracting New Descriptors of Shape,” in \\nModels for the Perception of Speech and Visual Form\\n, Wathen-Dunn,W. (ed.), \\nMIT Press, Cambridge, MA.\\nDIP4E_GLOBAL_Print_Ready.indb   995\\n7/12/2017   10:43:36 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 996}),\n",
       " Document(page_content='996\\n    \\nBibliography\\nBorn, M. and Wolf, E. [1999]. \\nPrinciples of Optics: Electromagnetic Theory of Prop-\\nagation, Interference and Diffraction of Light\\n, 7th ed., Cambridge University \\nPress,Cambridge, UK.\\nBracewell, R.  N. [1995]. \\nTwo-Dimensional Imaging\\n, Prentice Hall, Upper Saddle \\nRiver,  NJ.\\nBracewell, R. N. [2003]. \\nFourier Analysis and Imaging\\n, Springer, New York.\\nBribiesca, E. [1992]. “A Geometric Structure for Two-Dimensional Shapes and \\nThree Dimensional Surfaces,” \\nPattern Recognition\\n, vol. 25, pp. 483–496.\\nBribiesca, E. [2013]. “A Measure of Tortuosity Based on Chain Coding,” \\nPattern Rec-\\nognition\\n, vol. 46, pp. 716–724.\\nBribiesca, E. and Guzman, A. [1980]. “How to Describe Pure Form and How to \\nMeasure Differences in Shape Using Shape Numbers,” \\nPattern Recognition\\n, \\nvol. 12, no. 2, pp. 101–112.\\nBrigham, E. O. [1988]. \\nThe Fast Fourier Transform and its Applications\\n, Prentice Hall, \\nUpper Saddle River, NJ.\\nBronson, R. and Costa, G. B. [2009]. \\nMatrix Methods: Applied Linear Algebra\\n, 3rd ed., \\nAcademic Press/Elsevier, Burlington, MA.\\nBurrus, C. S., Gopinath, R. A., and Guo, H. [1998]. \\nIntroduction to Wavelets and \\nWavelet Transforms\\n, Prentice Hall, Upper Saddle River,  NJ,  pp. 250–251.\\nBuzug, T. M. [2008]. \\nComputed Tomography: From Photon Statistics to Modern \\nCone-Beam CT\\n, Springer-Verlag, Berlin, Germany.\\nCannon, T. M. [1974]. “Digital Image Deblurring by Non-Linear Homomorphic Fil-\\ntering,” Ph.D. thesis, University of Utah.\\nCanny, J. [1986]. “A Computational Approach for Edge Detection,” \\nIEEE Trans. Pat-\\ntern Anal. Machine Intell\\n., vol. 8, no. 6, pp. 679–698.\\nCaselles, V ., Kimmel, R., and Sapiro, G. [1997]. “Geodesic Active Contours,” \\nInt’l J. \\nComp. Vision\\n, vol. 22, no. 1, pp. 61–79.\\nCastleman, K. R. [1996]. \\nDigital Image Processing\\n, 2nd ed.,  Prentice Hall, Upper Sad-\\ndle River, NJ.\\nChakrabarti, I., et al. [2015]. \\nMotion Estimation for Video Coding\\n, Springer Int’l Pub-\\nlishing, Cham, Switzerland.\\nChampeney, D. C. [1987]. \\nA Handbook of Fourier Theorems\\n, Cambridge University \\nPress, London, UK.\\nChan, T. F. and Vese, L. A. [2001]. “Active Contours Without Edges,” \\nIEEE Trans. \\nImage Process\\n., vol. 10, no. 2, pp. 266–277.\\nCheng, Y., Hu, X., Wang, J., Wang, Y., and Tamura, S. [2015]. “Accurate Vessel Seg-\\nmentation with Constrained B-Snake,” \\nIEEE Trans. Image Process.\\n vol. 24, no. 8, \\npp. 2440-2455.\\nChoromanska, A., et al. [2015]. “The Loss Surfaces of Multilayer Networks,” \\nProc. \\n18th Int’l Conference Artiﬁcial Intell. and Statistics\\n (AISTATS), vol. 38, pp. 192–204.\\nClarke, R. J. [1985]. \\nTransform Coding of Images\\n, Academic Press, New York.\\nCohen, A., Daubechies, I., and Feauveau, J.-C. [1992]. “Biorthogonal Bases of Com-\\npactly Supported Wavelets,” \\nCommun. Pure and Appl. Math.\\n, vol. 45, pp. 485–560.\\nCoifman, R. R. and Wickerhauser, M. V . [1992]. “Entropy-Based Algorithms for Best \\nBasis Selection,” \\nIEEE Tran. Information Theory\\n, vol. 38, no. 2, pp. 713–718.\\nDIP4E_GLOBAL_Print_Ready.indb   996\\n7/12/2017   10:43:36 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 997}),\n",
       " Document(page_content='Bibliography\\n    \\n997\\nColtuc, D., Bolon, P ., and Chassery, J-M [2006]. “Exact Histogram Speciﬁcation,” \\nIEEE Trans. Image Process\\n., vol. 15, no. 5, pp. 1143–1152.\\nCornsweet, T. N. [1970]. \\nVisual Perception\\n, Academic Press, New York.\\nCox, I., Kilian, J., Leighton, F., and Shamoon, T. [1997]. “Secure Spread Spectrum \\nWatermarking for Multimedia,” \\nIEEE Trans. Image Process.\\n, vol. 6, no. 12, \\npp. 1673–1687.\\nCox, I., Miller, M., and Bloom, J. [2001]. \\nDigital Watermarking\\n, Morgan Kaufmann \\n(Elsevier), New York.\\nCybenco, G. [1989]. “Approximation by Superposition of a Sigmoidal Function,” \\nMath. Control Signals Systems\\n, vol. 2, no. 4, pp. 303–314.\\nD. N. Joanes and C. A. Gill. [1998]. “Comparing Measures of Sample Skewness and \\nKurtosis”. \\nThe Statistician\\n, vol 47, no. 1, pp. 183–189.\\nDanielsson, P . E. and Seger, O. [1990]. “Generalized and Separable Sobel Opera-\\ntors,” in \\nMachine Vision for Three-Dimensional Scenes\\n, Herbert Freeman (ed.), \\nAcademic Press, New York.\\nDaubechies, I. [1988]. “Orthonormal Bases of Compactly Supported Wavelet\\ns,” \\nCommun. On Pure and Appl. Math.\\n, vol. 41, pp. 909–996.\\nDaubechies, I. [1990]. “The Wavelet Transform, Time-Frequency Localization \\nand Signal Analysis,” \\nIEEE Transactions on Information Theory\\n, vol. 36, no. 5, \\npp. 961–1005.\\nDaubechies, I. [1992]. \\nTen Lectures on Wavelets\\n, Society for Industrial and Applied \\nMathematics, Philadelphia, PA.\\nDaubechies, I. [1993].“Orthonormal Bases of Compactly Supported Wave-\\nlets II,Variations on a Theme,” \\nSIAM J. Mathematical Analysis\\n, vol. 24, no. 2, \\npp. 499–519.\\nDaubechies, I. [1996]. “Where Do We Go from Here?—A Personal Point of View,” \\nProc. IEEE\\n, vol. 84, no. 4, pp. 510–513.\\nDelgado-Gonzalo, R., Uhlmann, V ., and Unser, M. [2015]. “Snakes on a Plane: A \\nPerfect Snap for Bioimage Analysis,” \\nIEEE Signal Proc. Magazine\\n, vol. 32, no. 1, \\npp. 41– 48.\\nde Moura, C. A. and Kubrusky, C. S. (eds.) [2013]. \\nThe Courant-Friedrichs-Lewy \\n(CLF) Condition\\n, Springer, New York.\\nDrew, M. S., Wei, J., and Li, Z.-N. [1999].  “Illumination Invariant Image Retrieval \\nand Video Segmentation,” \\nPattern Recognition\\n., vol. 32, no. 8, pp. 1369–1388.\\nDuda, R. O., Hart, P . E., and Stork, D. G. [2001]. \\nPattern Classiﬁcation\\n, John Wiley & \\nSons, New York.\\nEng, H.-L. and Ma, K.-K. [2001]. “Noise Adaptive Soft-Switching Median Filter,” \\nIEEE Trans. Image Process.\\n, vol. 10, no. 2, pp. 242–251.\\nEng, H.-L. and Ma, K.-K. [2006]. “A Switching Median Filter With Boundary Dis-\\ncriminitative Noise Detection for Extremely Corrupted Image\\ns,” \\nIEEE Trans. \\nImage Process\\n., vol. 15, no. 6, pp. 1506–1516.\\nFederal Bureau of Investigation [1993]. \\nWSQ Gray-Scale Fingerprint Image Com-\\npression Speciﬁcation\\n, IAFIS-IC-0110v2,Washington, DC.\\nDIP4E_GLOBAL_Print_Ready.indb   997\\n7/12/2017   10:43:36 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 998}),\n",
       " Document(page_content='998\\n    \\nBibliography\\nFeng, J., Cao, Z, and Pi, Y. [2013]. “Multiphase SAR Image Segmentation With \\nStatistical-Model-Based Active Contours,” \\nIEEE Trans. Geoscience and Remote \\nSensing\\n, vol. 51, no. 7, pp. 4190 – 4199.\\nFisher, R. A. [1936]. “The Use of Multiple Measurements in Taxonomic Problem\\ns,” \\nAnn. Eugenics\\n, vol. 7, Part 2, pp. 179–188. (Also in \\nContributions to Mathematical \\nStatistics\\n, John Wiley & Sons, New York, 1950.)\\nFlusser, J. [2000]. “On the Independence of Rotation Moment Invariant\\ns,” \\nPattern \\nRecognition\\n, vol. 33, pp. 1405–1410.\\nFreeman, A. (translator) [1878].  \\nJ. Fourier, The Analytical Theory of Heat\\n, Cam-\\nbridge University Press, London, UK.\\nFreeman, H. [1961]. “On the Encoding of Arbitrary Geometric Conﬁguration\\ns,” \\nIEEE Trans. Elec. Computers\\n, vol. EC-10, pp. 260–268.\\nFreeman, H. [1974]. “Computer Processing of Line Drawing\\ns,” \\nComput. Surveys\\n, \\nvol. 6, pp. 57–97.\\nFrendendall, G. L. and Behrend,W. L. [1960]. “Picture Quality—Procedures for \\nEvaluating Subjective Effects of Interference,” \\nProc. IRE\\n, vol. 48, pp. 1030–1034.\\nFukunaga, K. [1972]. \\nIntroduction to Statistical Pattern Recognition\\n, Academic Press, \\nNew York.\\nFurht, B., Greenberg, J., and Westwater, R. [1997]. \\nMotion Estimation Algorithms for \\nVideo Compression\\n, Kluwer Academic Publishers, Boston.\\nGaetano, R., Masi, G., Poggi, G., Verdoliva, L., and Scarpa, G. [2015]. “Marker-\\nControlled Watershed-Based Segmentation of Multiresolution Remote Sensing \\nImage\\ns,” \\nIEEE Trans. Geo Sci and Remote Sensing\\n, vol. 53, no. 6, pp. 2987–3004.\\nGallager, R. and Voorhis, D.V . [1975]. “Optimal Source Codes for Geometrically Dis-\\ntributed Integer Alphabets,” \\nIEEE Trans. Inform. Theory\\n, vol. IT-21, pp. 228–230.\\nGiardina, C. R. and Dougherty, E. R. [1988]. \\nMorphological Methods in Image and \\nSignal Processing\\n, Prentice Hall, Upper Saddle River, NJ.\\nGolomb, S.W. [1966]. “Run-Length Encoding\\ns,” \\nIEEE Trans. Inform.Theory\\n, vol. \\nIT-12, pp. 399–401.\\nGonzalez, R. C., Edwards, J. J., and Thomason, M. G. [1976]. “An Algorithm for the \\nInference of Tree Grammars,” \\nInt. J. Comput. Info. Sci\\n., vol. 5, no. 2, pp. 145–163.\\nGonzalez, R. C., Woods, R. E., and Eddins, S. L. [2004]. \\nDigital Image Processing \\nUsing MATLAB\\n, Prentice Hall, Upper Saddle River, NJ.\\nGonzalez, R. C., Woods, R. E., and Eddins, S. L. [2009]. \\nDigital Image Processing \\nUsing MATLAB\\n, 3rd ed., Gatesmark Publishing, Knoxville, TN.\\nGonzalez, R. C. [1985]. “Industrial Computer Vision,” in \\nAdvances in Information \\nSystems Science\\n, Tou, J. T. (ed.), Plenum, New York, pp. 345–385.\\nGonzalez, R. C. [1986]. “Image Enhancement and Restoration,” in \\nHandbook of \\nPattern Recognition and Image Processing\\n, Young, T. Y., and Fu, K. S. (eds.), Aca-\\ndemic Press, New York, pp. 191–213.\\nGonzalez, R. C. and Fittes, B. A. [1977]. “Gray-Level Transformations for Interactive \\nImage Enhancement,” \\nMechanism and Machine Theory\\n, vol. 12, pp. 111–122\\nGonzalez, R. C. and Safabakhsh, R. [1982].“Computer Vision Techniques for Indus-\\ntrial Applications,” \\nIEEE\\n \\nComputer\\n, vol. 15, no. 12, pp. 17–32.\\nDIP4E_GLOBAL_Print_Ready.indb   998\\n7/12/2017   10:43:36 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 999}),\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('imagepro.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GLOBAL\\n \\nEDITION\\nThis is a special edition of an established \\ntitle widely used by colleges and universities \\nthroughout the world. Pearson published this \\nexclusive edition for the benefit of students \\noutside the United States and Canada. If \\nyou purchased this book within the United \\nStates or Canada, you should be aware that \\nit has been imported without the approval of \\nthe Publisher or Author. The Global Edition \\nis not supported in the United States and \\nCanada.\\nPearson Global Edition\\nGLOBAL\\n \\nEDITION\\nFor these Global Editions, the editorial team at Pearson has \\ncollaborated with educators across the world to address a \\nwide range of subjects and requirements, equipping students \\nwith the best possible learning tools. This Global Edition \\npreserves the cutting-edge approach and pedagogy of the \\noriginal, but also features alterations, customization, and \\nadaptation from the North American version.\\nD\\nigital Image Processing\\n FOURTH EDITION\\n Rafael C. Gonzalez • Richard E. Woods', metadata={'source': 'imagepro.pdf', 'page': 0}),\n",
       " Document(page_content='Digital Image Processing\\nGonzalez \\nWoods\\nFOURTH\\n \\nEDITION\\nGLOBAL\\n \\nEDITION\\nGonzalez_04_1292223049_Final.indd   1\\n11/08/17   5:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 0}),\n",
       " Document(page_content='Support Package for \\nDigital\\n \\nImage Processing\\nYour new textbook provides access to support packages that may include reviews in areas \\nlike probability and vectors, tutorials on topics relevant to the material in the book, an image \\ndatabase, and more. Refer to the Preface in the textbook for a detailed list of resources.\\nFollow the instructions below to register for the Companion Website for Rafael C. Gonzalez and \\nRichard E. Woods’ \\nDigital Image Processing\\n, Fourth Edition, Global Edition.\\n1.\\n \\nGo to \\nwww.ImageProcessingPlace.com\\n2.\\n \\nFind the title of your te\\nxtbook.\\n3.\\n  \\nCl\\nick Support Materials and follow the on-screen instructions to create a login name and \\npassword.\\nUse the login name and password you created during registration to start using the \\ndigital resources that accompany your textbook.\\nIMPORTANT:\\nThis serial code can only be used once. This subscription is not transferrable.\\nGonzalez_04_1292223049_ifc_Final.indd   1\\n11/08/17   5:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 1}),\n",
       " Document(page_content='Processing\\nigital Image\\n4\\nD\\nFOURTH\\nEDITION\\nRafael C. Gonzalez\\nUniversity of Tennessee\\nRichard E. Woods\\nInterapptics\\n330 Hudson Street, New York, NY 10013\\nGlobal Edition\\nDIP4E_GLOBAL_Print_Ready.indb   1\\n7/6/2017   10:55:08 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 2}),\n",
       " Document(page_content='Senior Vice President Courseware Portfolio Management:\\n Marcia J. Horton\\nDirector, Portfolio Management: Engineering, Computer Science & Global Editions:\\n Julian Partridge\\nPortfolio Manager:\\n Julie Bai \\nField Marketing Manager:\\n Demetrius Hall \\nProduct Marketing Manager:\\n Yvonne Vannatta \\nMarketing Assistant:\\n Jon Bryant \\nContent Managing Producer, ECS and Math:\\n Scott Disanno \\nContent Producer:\\n Michelle Bayman \\nProject Manager:\\n Rose Kernan\\nAssistant Project Editor, Global Editions:\\n Vikash Tiwari\\nOperations Specialist:\\n Maura Zaldivar-Garcia \\nManager, Rights and Permissions:\\n Ben Ferrini \\nSenior Manufacturing Controller, Global Editions:\\n Trudy Kimber \\nMedia Production Manager, Global Editions:\\n Vikram Kumar\\nCover Designer:\\n Lumina Datamatics \\nCover Photo:\\n \\nCT image\\n—© zhuravliki.123rf.com/Pearson Asset Library; \\nGram-negative bacteria\\n—© royaltystockphoto.com/\\nShutterstock.com; \\nOrion Nebula\\n—© creativemarc/Shutterstock.com; \\nFingerprints\\n—© Larysa Ray/Shutterstock.com; \\nCancer', metadata={'source': 'imagepro.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GLOBAL\\n \\nEDITION\\nThis is a special edition of an established \\ntitle widely used by colleges and universities \\nthroughout the world. Pearson published this \\nexclusive edition for the benefit of students \\noutside the United States and Canada. If \\nyou purchased this book within the United \\nStates or Canada, you should be aware that \\nit has been imported without the approval of \\nthe Publisher or Author. The Global Edition \\nis not supported in the United States and \\nCanada.\\nPearson Global Edition\\nGLOBAL\\n \\nEDITION\\nFor these Global Editions, the editorial team at Pearson has \\ncollaborated with educators across the world to address a \\nwide range of subjects and requirements, equipping students \\nwith the best possible learning tools. This Global Edition \\npreserves the cutting-edge approach and pedagogy of the \\noriginal, but also features alterations, customization, and \\nadaptation from the North American version.\\nD\\nigital Image Processing\\n FOURTH EDITION\\n Rafael C. Gonzalez • Richard E. Woods', metadata={'source': 'imagepro.pdf', 'page': 0}),\n",
       " Document(page_content='Digital Image Processing\\nGonzalez \\nWoods\\nFOURTH\\n \\nEDITION\\nGLOBAL\\n \\nEDITION\\nGonzalez_04_1292223049_Final.indd   1\\n11/08/17   5:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 0}),\n",
       " Document(page_content='Support Package for \\nDigital\\n \\nImage Processing\\nYour new textbook provides access to support packages that may include reviews in areas \\nlike probability and vectors, tutorials on topics relevant to the material in the book, an image \\ndatabase, and more. Refer to the Preface in the textbook for a detailed list of resources.\\nFollow the instructions below to register for the Companion Website for Rafael C. Gonzalez and \\nRichard E. Woods’ \\nDigital Image Processing\\n, Fourth Edition, Global Edition.\\n1.\\n \\nGo to \\nwww.ImageProcessingPlace.com\\n2.\\n \\nFind the title of your te\\nxtbook.\\n3.\\n  \\nCl\\nick Support Materials and follow the on-screen instructions to create a login name and \\npassword.\\nUse the login name and password you created during registration to start using the \\ndigital resources that accompany your textbook.\\nIMPORTANT:\\nThis serial code can only be used once. This subscription is not transferrable.\\nGonzalez_04_1292223049_ifc_Final.indd   1\\n11/08/17   5:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 1}),\n",
       " Document(page_content='Processing\\nigital Image\\n4\\nD\\nFOURTH\\nEDITION\\nRafael C. Gonzalez\\nUniversity of Tennessee\\nRichard E. Woods\\nInterapptics\\n330 Hudson Street, New York, NY 10013\\nGlobal Edition\\nDIP4E_GLOBAL_Print_Ready.indb   1\\n7/6/2017   10:55:08 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 2}),\n",
       " Document(page_content='Senior Vice President Courseware Portfolio Management:\\n Marcia J. Horton\\nDirector, Portfolio Management: Engineering, Computer Science & Global Editions:\\n Julian Partridge\\nPortfolio Manager:\\n Julie Bai \\nField Marketing Manager:\\n Demetrius Hall \\nProduct Marketing Manager:\\n Yvonne Vannatta \\nMarketing Assistant:\\n Jon Bryant \\nContent Managing Producer, ECS and Math:\\n Scott Disanno \\nContent Producer:\\n Michelle Bayman \\nProject Manager:\\n Rose Kernan\\nAssistant Project Editor, Global Editions:\\n Vikash Tiwari\\nOperations Specialist:\\n Maura Zaldivar-Garcia \\nManager, Rights and Permissions:\\n Ben Ferrini \\nSenior Manufacturing Controller, Global Editions:\\n Trudy Kimber \\nMedia Production Manager, Global Editions:\\n Vikram Kumar\\nCover Designer:\\n Lumina Datamatics \\nCover Photo:\\n \\nCT image\\n—© zhuravliki.123rf.com/Pearson Asset Library; \\nGram-negative bacteria\\n—© royaltystockphoto.com/\\nShutterstock.com; \\nOrion Nebula\\n—© creativemarc/Shutterstock.com; \\nFingerprints\\n—© Larysa Ray/Shutterstock.com; \\nCancer', metadata={'source': 'imagepro.pdf', 'page': 3}),\n",
       " Document(page_content='Cancer \\ncells\\n—© Greenshoots Communications/Alamy Stock Photo\\nMATLAB is a registered trademark of The MathWorks, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098.\\nPearson Education Limited\\nEdinburgh Gate\\nHarlow\\nEssex CM20 2JE\\nEngland\\nand Associated Companies throughout the world\\nVisit us on the World Wide Web at: \\nwww.pearsonglobaleditions.com\\n© Pearson Education Limited 2018\\nThe rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them \\nin accordance with the Copyright, Designs and Patents Act 1988.\\nAuthorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4\\n, \\nby Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education © 2018.\\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by', metadata={'source': 'imagepro.pdf', 'page': 3}),\n",
       " Document(page_content='any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the pub\\n-\\nlisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron \\nHouse, 6–10 Kirby Street, London EC1N 8TS.\\nAll trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the \\nauthor or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliatio\\nn \\nwith or endorsement of this book by such owners.\\nBritish Library Cataloguing-in-Publication Data\\nA catalogue record for this book is available from the British Library\\n10 9 8 7 6 5 4 3 2 1\\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-22304-9\\nTypeset by Richard E. Woods\\nPrinted and bound in Malaysia\\nDIP4E_GLOBAL_Print_Ready.indb   2\\n7/6/2017   10:55:08 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 3}),\n",
       " Document(page_content='To Connie, Ralph, and Rob \\nand\\nTo Janice, David, and Jonathan\\nDIP4E_GLOBAL_Print_Ready.indb   3\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 4}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 5}),\n",
       " Document(page_content='Contents\\nPreface  9\\nAcknowledgments  12\\nThe Book Website  13\\nThe DIP4E Support Packages  13\\nAbout the Authors  14\\n1\\n Introduction  17\\nWhat is Digital Image Processing?  18\\nThe Origins of Digital Image Processing  19\\nExamples of Fields that Use Digital Image Processing  23\\nFundamental Steps in Digital Image Processing  41\\nComponents of an Image Processing System  44\\n2\\n Digital Image Fundamentals  47\\nElements of Visual Perception  48\\nLight and the Electromagnetic Spectrum  54\\nImage Sensing and Acquisition  57\\nImage Sampling and Quantization  63\\nSome Basic Relationships Between Pixels  79\\nIntroduction to the Basic Mathematical Tools Used in Digital Image \\nProcessing  \\n83\\n3\\n Intensity Transformations and Spatial  \\nFiltering  119\\nBackground  120\\nSome Basic Intensity Transformation Functions  122\\nHistogram Processing  133\\nFundamentals of Spatial Filtering  153\\nSmoothing (Lowpass) Spatial Filters  164\\nSharpening (Highpass) Spatial Filters  175', metadata={'source': 'imagepro.pdf', 'page': 6}),\n",
       " Document(page_content='Highpass, Bandreject, and Bandpass Filters from Lowpass Filters  188\\nCombining Spatial Enhancement Methods  191\\nDIP4E_GLOBAL_Print_Ready.indb   5\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 6}),\n",
       " Document(page_content='6\\n    \\nContents\\n4\\n Filtering in the Frequency \\nDomain  203\\nBackground  204\\nPreliminary Concepts  207\\nSampling and the Fourier Transform of Sampled  \\nFunctions  215\\nThe Discrete Fourier Transform of One Variable  225\\nExtensions to Functions of Two Variables  230\\nSome Properties of the 2-D DFT and IDFT  240\\nThe Basics of Filtering in the Frequency Domain  260\\nImage Smoothing Using Lowpass Frequency Domain  \\nFilters  272\\nImage Sharpening Using Highpass Filters  284\\nSelective Filtering  296\\nThe Fast Fourier Transform  303\\n5\\n Image Restoration \\nand Reconstruction  317\\nA Model of the Image Degradation/Restoration  \\nprocess  318\\nNoise Models  318\\nRestoration in the Presence of Noise Only—Spatial Filtering  327\\nPeriodic Noise Reduction Using Frequency Domain Filtering  340\\nLinear, Position-Invariant Degradations  348\\nEstimating the Degradation Function  352\\nInverse Filtering  356\\nMinimum Mean Square Error (Wiener) Filtering  358\\nConstrained Least Squares Filtering  363', metadata={'source': 'imagepro.pdf', 'page': 7}),\n",
       " Document(page_content='Geometric Mean Filter  367\\nImage Reconstruction from Projections  368\\n6\\n Color Image Processing  399\\nColor Fundamentals  400\\nColor Models  405\\nPseudocolor Image Processing  420\\nBasics of Full-Color Image Processing  429\\nColor Transformations  430\\nDIP4E_GLOBAL_Print_Ready.indb   6\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 7}),\n",
       " Document(page_content='Contents\\n    \\n7\\nColor Image Smoothing and Sharpening  442\\nUsing Color in Image Segmentation  445\\nNoise in Color Images  452\\nColor Image Compression  455\\n7\\n Wavelet and Other Image Transforms  463\\nPreliminaries  464\\nMatrix-based Transforms  466\\nCorrelation  478\\nBasis Functions in the Time-Frequency Plane  479\\nBasis Images  483\\nFourier-Related Transforms  484\\nWalsh-Hadamard Transforms  496\\nSlant Transform  500\\nHaar Transform  502\\nWavelet Transforms  \\n504\\n8\\n Image Compression and  \\nWatermarking  539\\nFundamentals  540\\nHuffman Coding  553\\nGolomb Coding  556\\nArithmetic Coding  561\\nLZW Coding  564\\nRun-length Coding  566\\nSymbol-based Coding  572\\nBit-plane Coding  575\\nBlock Transform Coding  576\\nPredictive Coding  \\n594\\nWavelet Coding  \\n614\\nDigital Image Watermarking  \\n624\\n9\\n Morphological Image Processing  635\\nPreliminaries  636\\nErosion and Dilation  638\\nOpening and Closing  644\\nThe Hit-or-Miss Transform  648\\nDIP4E_GLOBAL_Print_Ready.indb   7\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 8}),\n",
       " Document(page_content='8\\n    \\nContents\\nSome Basic Morphological Algorithms  652\\nMorphological Reconstruction  667\\nSummary of Morphological Operations on Binary Images  673\\nGrayscale Morphology  674\\n10\\n Image Segmentation  699\\nFundamentals  \\n700\\nPoint, Line, and Edge Detection  \\n701\\nThresholding  \\n742\\nSegmentation by Region Growing and by Region Splitting and \\nMerging  \\n764\\nRegion Segmentation Using Clustering and  \\nSuperpixels  \\n770\\nRegion Segmentation Using Graph Cuts\\n  \\n777\\nSegmentation Using Morphological Watersheds\\n  \\n786\\nThe Use of Motion in Segmentation  \\n796\\n11\\n Feature Extraction   811\\nBackground  \\n812\\nBoundary Preprocessing  \\n814\\nBoundary Feature Descriptors  \\n831\\nRegion Feature Descriptors  \\n840\\nPrincipal Components as Feature Descriptors  \\n859\\nWhole-Image Features  \\n868\\nScale-Invariant Feature Transform (SIFT)  \\n881\\n12\\n Image Pattern Classification  903\\nBackground  \\n904\\nPatterns and Pattern Classes  \\n906\\nPattern Classification by Prototype Matching  \\n910\\nOptimum (Bayes) Statistical Classifiers', metadata={'source': 'imagepro.pdf', 'page': 9}),\n",
       " Document(page_content='923\\nNeural Networks and Deep Learning  \\n931\\nDeep Convolutional Neural Networks  \\n964\\nSome Additional Details of Implementation  \\n987\\nBibliography  995\\nIndex  1009\\nDIP4E_GLOBAL_Print_Ready.indb   8\\n7/12/2017   10:23:39 AM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 9}),\n",
       " Document(page_content='Preface\\nWhen something can be read without effort, great effort has gone into its writing.\\nEnrique Jardiel Poncela\\nThis edition of \\nDigital Image Processing\\n is a major revision of the book.\\n As in \\nthe 1977 and 1987 editions by Gonzalez and Wintz, and the 1992, 2002, and 2008  \\neditions by Gonzalez and Woods, this sixth-generation edition was prepared \\nwith students and instructors in mind. The principal objectives of the book \\ncontinue to be to provide an introduction to basic concepts and methodologies \\napplicable to digital image processing, and to develop a foundation that can \\nbe used as the basis for further study and research in this field. To achieve \\nthese objectives, we focused again on material that we believe is fundamental \\nand whose scope of application is not limited to the solution of specialized \\nproblems. The mathematical complexity of the book remains at a level well \\nwithin the grasp of college seniors and first-year graduate students who have', metadata={'source': 'imagepro.pdf', 'page': 10}),\n",
       " Document(page_content='introductory preparation in mathematical analysis, vectors, matrices, probability, \\nstatistics, linear systems, and computer programming. The book website pro-\\nvides tutorials to support readers needing a review of this background material.  \\nOne of the principal reasons this book has been the world leader in its ﬁeld for \\n40 years is the level of attention we pay to the changing educational needs of our \\nreaders. The present edition is based on an extensive survey that involved faculty, \\nstudents, and independent readers of the book in 150 institutions from 30 countries. \\nThe survey revealed a need for coverage of new material that has matured since the \\nlast edition of the book. The principal ﬁndings of the survey indicated a need for: \\n• \\nExpanded coverage of the fundamentals of spatial filtering.\\n• \\nA more comprehensive and cohesive coverage of image transforms.\\n• \\nA more complete presentation of finite differences, with a focus on edge detec-\\ntion.\\n•', metadata={'source': 'imagepro.pdf', 'page': 10}),\n",
       " Document(page_content='tion.\\n• \\nA discussion of clustering, superpixels, and their use in region segmentation. \\n• \\nCoverage of maximally stable extremal regions.\\n• \\nExpanded coverage of feature extraction to include the Scale Invariant Feature \\nT\\nransform (SIFT).\\n• \\nExpanded coverage of neural networks to include deep neural networks, back-\\npropagation,\\n deep learning, and, especially, deep convolutional neural networks. \\n• \\nMore homework exercises at the end of the chapters.\\nT\\nhe new and reorganized material that resulted in the present edition is our \\nattempt at providing a reasonable balance between rigor, clarity of presentation, \\nand the ﬁndings of the survey. In addition to new material, earlier portions of the \\ntext were updated and clariﬁed. This edition contains 241 new images, 72 new draw-\\nings, and 135 new exercises.\\nDIP4E_GLOBAL_Print_Ready.indb   9\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 10}),\n",
       " Document(page_content='10\\n    \\nPreface\\nNew to This Edition\\nThe highlights of this edition are as follows.\\nChapter 1:\\n Some figures were updated, and parts of the text were rewritten to cor-\\nrespond to changes in later chapters\\n.\\nChapter 2:\\n Many of the sections and examples were rewritten for clarity. We \\nadded 14 new exercises\\n. \\nChapter 3:\\n Fundamental concepts of spatial ﬁltering were rewritten to include a \\ndiscussion on separable ﬁlter kernels\\n, expanded coverage of the properties of low-\\npass Gaussian kernels, and expanded coverage of highpass, bandreject, and band-\\npass ﬁlters, including numerous new examples that illustrate their use. In addition to \\nrevisions in the text, including 6 new examples, the chapter has 59 new images, 2 new \\nline drawings, and 15 new exercises.\\nChapter 4:\\n Several of the sections of this chapter were revised to improve the clar-\\nity of presentation.\\n We replaced dated graphical material with 35 new images and 4 \\nnew line drawings. We added 21 new exercises. \\nChapter 5:', metadata={'source': 'imagepro.pdf', 'page': 11}),\n",
       " Document(page_content='Chapter 5:\\n Revisions to this chapter were limited to clariﬁcations and a few cor-\\nrections in notation.\\n We added 6 new images and 14 new exercises, \\nChapter 6:\\n Several sections were clarified, and the explanation of the CMY and \\nCMYK color models was expanded,\\n including 2 new images.\\nChapter 7:\\n This is a new chapter that brings together wavelets, several new trans-\\nforms\\n, and many of the image transforms that were scattered throughout the book. \\nThe emphasis of this new chapter is on the presentation of these transforms from a \\nunified point of view.  We added 24 new images, 20 new drawings, and 25 new exer-\\ncises. \\nChapter 8:\\n The material was revised with numerous clarifications and several \\nimprovements to the presentation.\\nChapter 9:\\n Revisions of this chapter included a complete rewrite of several sec-\\ntions\\n, including redrafting of several line drawings. We added 16 new exercises\\nChapter 10:\\n Several of the sections were rewritten for clarity. We updated the', metadata={'source': 'imagepro.pdf', 'page': 11}),\n",
       " Document(page_content='chapter by adding coverage of finite differences\\n, \\nK\\n-means clustering, superpixels, \\nand graph cuts. The new topics are illustrated with 4 new examples. In total, we \\nadded 29 new images, 3 new drawings, and 6 new exercises.\\nChapter 11:\\n The chapter was updated with numerous topics, beginning with a more \\ndetailed classification of feature types and their uses\\n. In addition to improvements in \\nthe clarity of presentation, we added coverage of slope change codes, expanded the \\nexplanation of skeletons, medial axes, and the distance transform, and added sev-\\neral new basic descriptors of compactness, circularity, and eccentricity. New mate-\\nrial includes coverage of the Harris-Stephens corner detector, and a presentation of \\nmaximally stable extremal regions. A major addition to the chapter is a comprehen-\\nsive discussion dealing with the Scale-Invariant Feature Transform (SIFT). The new \\nmaterial is complemented by 65 new images, 15 new drawings, and 12 new exercises.', metadata={'source': 'imagepro.pdf', 'page': 11}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   10\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 11}),\n",
       " Document(page_content='Preface\\n    \\n11\\nChapter 12:\\n This chapter underwent a major revision to include an extensive \\nrewrite of neural networks and deep learning\\n, an area that has grown significantly \\nsince the last edition of the book. We added a comprehensive discussion on fully \\nconnected, deep neural networks that includes derivation of backpropagation start-\\ning from basic principles. The equations of backpropagation were expressed in “tra-\\nditional” scalar terms, and then generalized into a compact set of matrix equations \\nideally suited for implementation of deep neural nets. The effectiveness of fully con-\\nnected networks was demonstrated with several examples that included a compari-\\nson with the Bayes classifier. One of the most-requested topics in the survey was \\ncoverage of deep convolutional neural networks. We added an extensive section on \\nthis, following the same blueprint we used for deep, fully connected nets. That is, we', metadata={'source': 'imagepro.pdf', 'page': 12}),\n",
       " Document(page_content='derived the equations of backpropagation for convolutional nets, and showed how \\nthey are different from “traditional” backpropagation. We then illustrated the use of \\nconvolutional networks with simple images, and applied them to large image data-\\nbases of numerals and natural scenes.  The written material is complemented by 23 \\nnew images, 28 new drawings, and 12 new exercises.\\nAlso for the first time, we have created student and faculty support packages that \\ncan be downloaded from the book website. The \\nStudent Support Package\\n contains \\nmany of the original images in the book and answers to selected exercises The \\nFac-\\nulty Support Package\\n contains solutions to all exercises, teaching suggestions, and all \\nthe art in the book in the form of modifiable PowerPoint slides. One support pack-\\nage is made available with every new book, free of charge. \\nThe book website, established during the launch of the 2002 edition, continues to', metadata={'source': 'imagepro.pdf', 'page': 12}),\n",
       " Document(page_content='be a success, attracting more than 25,000 visitors each month. The site was upgraded \\nfor the launch of this edition. For more details on site features and content, see \\nThe \\nBook Website\\n, following the \\nAcknowledgments\\n section.\\nThis edition of \\nDigital Image Processing\\n is a reﬂection of how the educational \\nneeds of our readers have changed since 2008. As is usual in an endeavor such as \\nthis, progress in the ﬁeld continues after work on the manuscript stops. One of the \\nreasons why this book has been so well accepted since it ﬁrst appeared in 1977 is its \\ncontinued emphasis on fundamental concepts that retain their relevance over time. \\nThis approach, among other things, attempts to provide a measure of stability in a \\nrapidly evolving body of knowledge. We have tried to follow the same principle in \\npreparing this edition of the book.\\nR.C.G.\\nR.E.W.\\nDIP4E_GLOBAL_Print_Ready.indb   11\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 12}),\n",
       " Document(page_content='12\\n    \\nAcknowledgments\\nAcknowledgments\\nWe are indebted to a number of individuals in academic circles, industry, and gov-\\nernment who have contributed to this edition of the book. In particular, we wish \\nto extend our appreciation to Hairong Qi and her students, Zhifei Zhang and \\nChengcheng Li, for their valuable review of the material on neural networks, and for \\ntheir help in generating examples for that material. We also want to thank Ernesto \\nBribiesca Correa for providing and reviewing material on slope chain codes, and \\nDirk Padfield for his many suggestions and review of several chapters in the book. \\nWe appreciate Michel Kocher’s many thoughtful comments and suggestions over \\nthe years on how to improve the book. Thanks also to Steve Eddins for his sugges-\\ntions on MATLAB and related software issues.\\nNumerous individuals have contributed to material carried over from the previ-\\nous to the current edition of the book. Their contributions have been important in so', metadata={'source': 'imagepro.pdf', 'page': 13}),\n",
       " Document(page_content='many different ways that we ﬁnd it difﬁcult to acknowledge them in any other way \\nbut alphabetically. We thank Mongi A. Abidi, Yongmin Kim, Bryan Morse, Andrew \\nOldroyd, Ali M. Reza, Edgardo Felipe Riveron, Jose Ruiz Shulcloper, and Cameron \\nH.G. Wright for their many suggestions on how to improve the presentation and/or \\nthe scope of coverage in the book. We are also indebted to Naomi Fernandes at the \\nMathWorks for providing us with MATLAB software and support that were impor-\\ntant in our ability to create many of the examples and experimental results included \\nin this edition of the book.\\nA signiﬁcant percentage of the new images used in this edition (and in some \\ncases their history and interpretation) were obtained through the efforts of indi-\\nviduals whose contributions are sincerely appreciated. In particular, we wish to \\nacknowledge the efforts of Serge Beucher, Uwe Boos, Michael E. Casey, Michael \\nW. Davidson, Susan L. Forsburg, Thomas R. Gest, Daniel A. Hammer, Zhong He,', metadata={'source': 'imagepro.pdf', 'page': 13}),\n",
       " Document(page_content='Roger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, \\nRhonda Knighton, Don P . Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, \\nMichael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David \\nK. Wehe, and Robert A. West. We also wish to acknowledge other individuals and \\norganizations cited in the captions of numerous ﬁgures throughout the book for \\ntheir permission to use that material. \\nWe also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for \\ntheir support and signiﬁcant patience during the production of the book.\\nR.C.G.\\nR.E.W.\\nDIP4E_GLOBAL_Print_Ready.indb   12\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 13}),\n",
       " Document(page_content='The Book Website\\nwww.ImageProcessingPlace.com\\nDigital Image Processing\\n is a completely self-contained book.\\n However, the compan-\\nion website offers additional support in a number of important areas.\\nFor the Student or Independent Reader the site contains\\n• \\nReviews in areas such as probability, statistics, vectors, and matrices.\\n• \\nA Tutorials section containing dozens of tutorials on topics relevant to the mate-\\nrial in the book.\\n• \\nAn image database containing all the images in the book, as well as many other \\nimage databases\\n.\\nFor the Instructor the site contains\\n• \\nAn Instructor’s Manual with complete solutions to all the problems.\\n• \\nClassroom presentation materials in modifiable PowerPoint format.\\n• \\nMaterial removed from previous editions, downloadable in convenient PDF \\nformat.\\n• \\nNumerous links to other educational resources.\\nF\\nor the Practitioner the site contains additional specialized topics such as\\n• \\nLinks to commercial sites.\\n• \\nSelected new references.\\n•', metadata={'source': 'imagepro.pdf', 'page': 14}),\n",
       " Document(page_content='• \\nLinks to commercial image databases.\\nT\\nhe website is an ideal tool for keeping the book current between editions by includ-\\ning new topics, digital images, and other relevant material that has appeared after \\nthe book was published. Although considerable care was taken in the production \\nof the book, the website is also a convenient repository for any errors discovered \\nbetween printings. \\nThe DIP4E Support Packages\\nIn this edition, we created support packages for students and faculty to organize \\nall the classroom support materials available for the new edition of the book into \\none easy download. The Student Support Package contains many of the original \\nimages in the book, and answers to selected exercises, The Faculty Support Package \\ncontains solutions to all exercises, teaching suggestions, and all the art in the book \\nin modifiable PowerPoint slides. One support package is made available with every', metadata={'source': 'imagepro.pdf', 'page': 14}),\n",
       " Document(page_content='new book, free of charge. Applications for the support packages are submitted at \\nthe book website.\\nDIP4E_GLOBAL_Print_Ready.indb   13\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 14}),\n",
       " Document(page_content='About the Authors\\nRAFAEL C. GONZALEZ\\nR. C. Gonzalez received the B.S.E.E. degree from the University of Miami in 1965 \\nand the M.E. and Ph.D. degrees in electrical engineering from the University of \\nFlorida, Gainesville, in 1967 and 1970, respectively. He joined the Electrical and \\nComputer Science Department at the University of Tennessee, Knoxville (UTK) in \\n1970, where he became Associate Professor in 1973, Professor in 1978, and Distin-\\nguished Service Professor in 1984. He served as Chairman of the department from \\n1994 through 1997. He is currently a Professor Emeritus at UTK.\\nGonzalez is the founder of the Image & Pattern Analysis Laboratory and the \\nRobotics & Computer Vision Laboratory at the University of Tennessee. He also \\nfounded Perceptics Corporation in 1982 and was its president until 1992. The last \\nthree years of this period were spent under a full-time employment contract with \\nWestinghouse Corporation, who acquired the company in 1989.', metadata={'source': 'imagepro.pdf', 'page': 15}),\n",
       " Document(page_content='Under his direction, Perceptics became highly successful in image processing, \\ncomputer vision, and laser disk storage technology. In its initial ten years, Perceptics \\nintroduced a series of innovative products, including: The world’s ﬁrst commercially \\navailable computer vision system for automatically reading license plates on moving \\nvehicles; a series of large-scale image processing and archiving systems used by the \\nU.S. Navy at six different manufacturing sites throughout the country to inspect the \\nrocket motors of missiles in the Trident II Submarine Program; the market-leading \\nfamily of imaging boards for advanced Macintosh computers; and a line of trillion-\\nbyte laser disk products.\\nHe is a frequent consultant to industry and government in the areas of pattern \\nrecognition, image processing, and machine learning. His academic honors for work \\nin these ﬁelds include the 1977 UTK College of Engineering Faculty Achievement', metadata={'source': 'imagepro.pdf', 'page': 15}),\n",
       " Document(page_content='Award; the 1978 UTK Chancellor’s Research Scholar Award; the 1980 Magnavox \\nEngineering Professor Award; and the 1980 M.E. Brooks Distinguished Professor \\nAward. In 1981 he became an IBM Professor at the University of Tennessee and \\nin 1984 he was named a Distinguished Service Professor there. He was awarded a \\nDistinguished Alumnus Award by the University of Miami in 1985, the Phi Kappa \\nPhi Scholar Award in 1986, and the University of Tennessee’s Nathan W. Dougherty \\nAward for Excellence in Engineering in 1992.\\nHonors for industrial accomplishment include the 1987 IEEE Outstanding Engi-\\nneer Award for Commercial Development in Tennessee; the 1988 Albert Rose \\nNational Award for Excellence in Commercial Image Processing; the 1989 B. Otto \\nWheeley Award for Excellence in Technology Transfer; the 1989 Coopers and \\nLybrand Entrepreneur of the Year Award; the 1992 IEEE Region 3 Outstanding \\nEngineer Award; and the 1993 Automated Imaging Association National Award for', metadata={'source': 'imagepro.pdf', 'page': 15}),\n",
       " Document(page_content='Technology Development.\\nGonzalez is author or co-author of over 100 technical articles, two edited books, \\nand four textbooks in the ﬁelds of pattern recognition, image processing, and robot-\\nics. His books are used in over 1000 universities and research institutions throughout \\nDIP4E_GLOBAL_Print_Ready.indb   14\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 15}),\n",
       " Document(page_content='the world. He is listed in the prestigious \\nMarquis Who’s Who in America\\n, \\nMarquis \\nWho’s Who in Engineering\\n, \\nMarquis Who’s Who in the World\\n, and in 10 other national \\nand international biographical citations. He is the co-holder of two U.S. Patents, and \\nhas been an associate editor of the \\nIEEE Transactions on Systems, Man and Cyber-\\nnetics\\n, and the \\nInternational Journal of Computer and Information Sciences\\n. He is a \\nmember of numerous professional and honorary societies, including Tau Beta Pi, Phi \\nKappa Phi, Eta Kappa Nu, and Sigma Xi. He is a Fellow of the IEEE.\\nRICHARD E. WOODS\\nR. E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical Engineering from \\nthe University of Tennessee, Knoxville in 1975, 1977, and 1980, respectively. He \\nbecame an Assistant Professor of Electrical Engineering and Computer Science in \\n1981 and was recognized as a Distinguished Engineering Alumnus in 1986.\\nA veteran hardware and software developer, Dr. Woods has been involved in', metadata={'source': 'imagepro.pdf', 'page': 16}),\n",
       " Document(page_content='the founding of several high-technology startups, including Perceptics Corporation, \\nwhere he was responsible for the development of the company’s quantitative image \\nanalysis and autonomous decision-making products; MedData Interactive, a high-\\ntechnology company specializing in the development of handheld computer systems \\nfor medical applications; and Interapptics, an internet-based company that designs \\ndesktop and handheld computer applications.\\nDr. Woods currently serves on several nonproﬁt educational and media-related \\nboards, including Johnson University, and was recently a summer English instructor \\nat the Beijing Institute of Technology. He is the holder of a U.S. Patent in the area \\nof digital image processing and has published two textbooks, as well as numerous \\narticles related to digital signal processing. Dr. Woods is a member of several profes-\\nsional societies, including Tau Beta Pi, Phi Kappa Phi, and the IEEE.\\nDIP4E_GLOBAL_Print_Ready.indb   15', metadata={'source': 'imagepro.pdf', 'page': 16}),\n",
       " Document(page_content='6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 16}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 17}),\n",
       " Document(page_content='171\\nIntroduction\\nOne picture is worth more than ten thousand words.\\nAnonymous\\nPreview\\nInterest in digital image processing methods stems from two principal application areas: improvement \\nof pictorial information for human interpretation, and processing of image data for tasks such as storage, \\ntransmission, and extraction of pictorial information. This chapter has several objectives: (1) to deﬁne \\nthe scope of the ﬁeld that we call image processing; (2) to give a historical perspective of the origins of \\nthis ﬁeld; (3) to present an overview of the state of the art in image processing by examining some of \\nthe principal areas in which it is applied; (4) to discuss brieﬂy the principal approaches used in digital \\nimage processing; (5) to give an overview of the components contained in a typical, general-purpose \\nimage processing system; and (6) to provide direction to the literature where image processing work is', metadata={'source': 'imagepro.pdf', 'page': 18}),\n",
       " Document(page_content='reported. The material in this chapter is extensively illustrated with a range of images that are represen-\\ntative of the images we will be using throughout the book.\\nUpon completion of this chapter, readers should:\\n Understand the concept of a digital image.\\n Have a broad overview of the historical under-\\npinnings of the ﬁeld of digital image process-\\ning.\\n Understand the deﬁnition and scope of digi-\\ntal image processing.\\n Know the fundamentals of the electromag-\\nnetic spectrum and its relationship to image \\ngeneration.\\n Be aware of the different ﬁelds in which digi-\\ntal image processing methods are applied.\\n Be familiar with the basic processes involved \\nin image processing.\\n Be familiar with the components that make \\nup a general-purpose digital image process-\\ning system.\\n Be familiar with the scope of the literature \\nwhere image processing work is reported.\\nDIP4E_GLOBAL_Print_Ready.indb   17\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 18}),\n",
       " Document(page_content='18\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\nAn image may be defined as a two-dimensional function, \\nfx y\\n(,\\n) ,\\n where \\nx\\n and \\ny\\n are \\nspatial\\n (plane) coordinates\\n, and the amplitude of \\nf\\n at any pair of coordinates \\n(,)\\nxy\\n \\nis called the \\nintensity\\n or \\ngra\\ny level\\n of the image at that point. When \\nx\\n, \\ny\\n, and the \\nintensity values of \\nf\\n are all finite, discrete quantities, we call the image a \\ndigital image\\n. \\nThe field of \\ndigital image processing\\n refers to processing digital images by means of \\na digital computer. Note that a digital image is composed of a finite number of ele-\\nments, each of which has a particular location and value. These elements are called \\npicture elements\\n, \\nimage elements\\n, \\npels\\n, and \\npixels\\n. \\nPixel\\n is the term used most widely \\nto denote the elements of a digital image. We will consider these definitions in more \\nformal terms in Chapter 2.', metadata={'source': 'imagepro.pdf', 'page': 19}),\n",
       " Document(page_content='Vision is the most advanced of our senses, so it is not surprising that images \\nplay the single most important role in human perception. However, unlike humans, \\nwho are limited to the visual band of the electromagnetic (EM) spectrum, imaging \\nmachines cover almost the entire EM spectrum, ranging from gamma to radio waves. \\nThey can operate on images generated by sources that humans are not accustomed \\nto associating with images. These include ultrasound, electron microscopy, and com-\\nputer-generated images. Thus, digital image processing encompasses a wide and var-\\nied ﬁeld of applications.\\nThere is no general agreement among authors regarding where image process-\\ning stops and other related areas, such as \\nimage analysis\\n and \\ncomputer vision\\n, start. \\nSometimes, a distinction is made by deﬁning image processing as a discipline in \\nwhich both the input and output of a process are images. We believe this to be a', metadata={'source': 'imagepro.pdf', 'page': 19}),\n",
       " Document(page_content='limiting and somewhat artiﬁcial boundary. For example, under this deﬁnition, even \\nthe trivial task of computing the average intensity of an image (which yields a sin-\\ngle number) would not be considered an image processing operation. On the other \\nhand, there are ﬁelds such as computer vision whose ultimate goal is to use comput-\\ners to emulate human vision, including learning and being able to make inferences \\nand take actions based on visual inputs. This area itself is a branch of \\nartiﬁcial intel-\\nligence\\n (AI) whose objective is to emulate human intelligence. The ﬁeld of AI is in its \\nearliest stages of infancy in terms of development, with progress having been much \\nslower than originally anticipated. The area of image analysis (also called \\nimage \\nunderstanding\\n) is in between image processing and computer vision.\\nThere are no clear-cut boundaries in the continuum from image processing at \\none end to computer vision at the other. However, one useful paradigm is to con-', metadata={'source': 'imagepro.pdf', 'page': 19}),\n",
       " Document(page_content='sider three types of computerized processes in this continuum: low-, mid-, and high-\\nlevel processes. Low-level processes involve primitive operations such as image \\npreprocessing to reduce noise, contrast enhancement, and image sharpening. A low-\\nlevel process is characterized by the fact that both its inputs and outputs are images. \\nMid-level processing of images involves tasks such as segmentation (partitioning \\nan image into regions or objects), description of those objects to reduce them to a \\nform suitable for computer processing, and classiﬁcation (recognition) of individual \\nobjects. A mid-level process is characterized by the fact that its inputs generally \\nare images, but its outputs are attributes extracted from those images (e.g., edges, \\ncontours, and the identity of individual objects). Finally, higher-level processing \\n1.1\\nDIP4E_GLOBAL_Print_Ready.indb   18\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 19}),\n",
       " Document(page_content='1.2\\n  \\nThe Origins of Digital Image Processing\\n    \\n19\\ninvolves “making sense” of an \\nensemble of recognized objects, as in image analysis, \\nand, at the far end of the continuum, performing the cognitive functions normally \\nassociated with human vision.\\nBased on the preceding comments, we see that a logical place of overlap between \\nimage processing and image analysis is the area of recognition of individual regions \\nor objects in an image. Thus, what we call in this book \\ndigital image processing\\n encom-\\npasses processes whose inputs and outputs are images and, in addition, includes pro-\\ncesses that extract attributes from images up to, and including, the recognition of \\nindividual objects. As an illustration to clarify these concepts, consider the area of \\nautomated analysis of text. The processes of acquiring an image of the area con-\\ntaining the text, preprocessing that image, extracting (segmenting) the individual', metadata={'source': 'imagepro.pdf', 'page': 20}),\n",
       " Document(page_content='characters, describing the characters in a form suitable for computer processing, and \\nrecognizing those individual characters are in the scope of what we call digital image \\nprocessing in this book. Making sense of the content of the page may be viewed as \\nbeing in the domain of image analysis and even computer vision, depending on the \\nlevel of complexity implied by the statement “making sense of.” As will become \\nevident shortly, digital image processing, as we have deﬁned it, is used routinely in a \\nbroad range of areas of exceptional social and economic value. The concepts devel-\\noped in the following chapters are the foundation for the methods used in those \\napplication areas.\\n1.2 THE ORIGINS OF DIGITAL IMAGE PROCESSING  \\nOne of the earliest applications of digital images was in the newspaper industry, \\nwhen pictures were first sent by submarine cable between London and New York. \\nIntroduction of the Bartlane cable picture transmission system in the early 1920s', metadata={'source': 'imagepro.pdf', 'page': 20}),\n",
       " Document(page_content='reduced the time required to transport a picture across the Atlantic from more than \\na week to less than three hours. Specialized printing equipment coded pictures for \\ncable transmission, then reconstructed them at the receiving end. Figure 1.1 was \\ntransmitted in this way and reproduced on a telegraph printer fitted with typefaces \\nsimulating a halftone pattern. \\nSome of the initial problems in improving the visual quality of these early digital \\npictures were related to the selection of printing procedures and the distribution of \\n1.2\\nFIGURE 1.1\\n A digital picture produced in 1921 from a coded tape by a telegraph printer with \\nspecial typefaces. (McFarlane.) [References in the bibliography at the end of the book are \\nlisted in alphabetical order by authors’ last names.]\\nDIP4E_GLOBAL_Print_Ready.indb   19\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 20}),\n",
       " Document(page_content='20\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nintensity levels. The printing method used to obtain Fig. 1.1 was abandoned toward \\nthe end of 1921 in favor of a technique based on photographic reproduction made \\nfrom tapes perforated at the telegraph receiving terminal. Figure 1.2 shows an image \\nobtained using this method. The improvements over Fig. 1.1 are evident, both in \\ntonal quality and in resolution.\\nThe early Bartlane systems were capable of coding images in ﬁve distinct levels \\nof gray. This capability was increased to 15 levels in 1929. Figure 1.3 is typical of the \\ntype of images that could be obtained using the 15-tone equipment. During this \\nperiod, introduction of a system for developing a ﬁlm plate via light beams that were \\nmodulated by the coded picture tape improved the reproduction process consider-\\nably.\\nAlthough the examples just cited involve digital images, they are not considered \\ndigital image processing results in the context of our deﬁnition, because digital com-', metadata={'source': 'imagepro.pdf', 'page': 21}),\n",
       " Document(page_content='puters were not used in their creation. Thus, the history of digital image processing \\nis intimately tied to the development of the digital computer. In fact, digital images \\nrequire so much storage and computational power that progress in the ﬁeld of digi-\\ntal image processing has been dependent on the development of digital computers \\nand of supporting technologies that include data storage, display, and transmission.\\nFIGURE 1.2\\nA digital picture \\nmade in 1922 \\nfrom a tape \\npunched after \\nthe signals had \\ncrossed the  \\nAtlantic twice. \\n(McFarlane.)\\nFIGURE 1.3\\nUnretouched \\ncable picture of \\nGenerals Pershing \\n(right) and Foch,  \\ntransmitted in \\n1929 from  \\nLondon to New \\nYork by 15-tone \\nequipment. \\n(McFarlane.)\\nDIP4E_GLOBAL_Print_Ready.indb   20\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 21}),\n",
       " Document(page_content='1.2\\n  \\nThe Origins of Digital Image Processing\\n    \\n21\\nThe concept of a computer dates back to the invention of the abacus in Asia \\nMinor, more than 5000 years ago. More recently, there have been developments \\nin the past two centuries that are the foundation of what we call a computer today. \\nHowever, the basis for what we call a \\nmodern\\n digital computer dates back to only \\nthe 1940s, with the introduction by John von Neumann of two key concepts: (1) a \\nmemory to hold a stored program and data, and (2) conditional branching. These \\ntwo ideas are the foundation of a central processing unit (CPU), which is at the heart \\nof computers today. Starting with von Neumann, there were a series of key advanc-\\nes that led to computers powerful enough to be used for digital image processing. \\nBrieﬂy, these advances may be summarized as follows: (1) the invention of the tran-\\nsistor at Bell Laboratories in 1948; (2) the development in the 1950s and 1960s of', metadata={'source': 'imagepro.pdf', 'page': 22}),\n",
       " Document(page_content='the high-level programming languages COBOL (Common Business-Oriented Lan-\\nguage) and FORTRAN (Formula Translator); (3) the invention of the integrated \\ncircuit (IC) at Texas Instruments in 1958; (4) the development of operating systems \\nin the early 1960s; (5) the development of the microprocessor (a single chip consist-\\ning of a CPU, memory, and input and output controls) by Intel in the early 1970s; \\n(6) the introduction by IBM of the personal computer in 1981; and (7) progressive \\nminiaturization of components, starting with large-scale integration (LI) in the late \\n1970s, then very-large-scale integration (VLSI) in the 1980s, to the present use of \\nultra-large-scale integration (ULSI) and experimental nonotechnologies. Concur-\\nrent with these advances were developments in the areas of mass storage and display \\nsystems, both of which are fundamental requirements for digital image processing.\\nThe ﬁrst computers powerful enough to carry out meaningful image processing', metadata={'source': 'imagepro.pdf', 'page': 22}),\n",
       " Document(page_content='tasks appeared in the early 1960s. The birth of what we call digital image processing \\ntoday can be traced to the availability of those machines, and to the onset of the \\nspace program during that period. It took the combination of those two develop-\\nments to bring into focus the potential of digital image processing for solving prob-\\nlems of practical signiﬁcance. Work on using computer techniques for improving \\nimages from a space probe began at the Jet Propulsion Laboratory (Pasadena, Cali-\\nfornia) in 1964, when pictures of the moon transmitted by \\nRanger 7\\n were processed \\nby a computer to correct various types of image distortion inherent in the on-board \\ntelevision camera. Figure 1.4 shows the ﬁrst image of the moon taken by \\nRanger \\n7\\n on July 31, 1964 at 9:09 A.M. Eastern Daylight Time (EDT), about 17 minutes \\nbefore impacting the lunar surface (the markers, called \\nreseau marks\\n, are used for', metadata={'source': 'imagepro.pdf', 'page': 22}),\n",
       " Document(page_content=', are used for \\ngeometric corrections, as discussed in Chapter 2).This also is the ﬁrst image of the \\nmoon taken by a U.S. spacecraft. The imaging lessons learned with \\nRanger 7\\n served \\nas the basis for improved methods used to enhance and restore images from the Sur-\\nveyor missions to the moon, the \\nMariner\\n series of ﬂyby missions to Mars, the \\nApollo \\nmanned ﬂights to the moon, and others.\\nIn parallel with space applications, digital image processing techniques began in \\nthe late 1960s and early 1970s to be used in medical imaging, remote Earth resourc-\\nes observations, and astronomy. The invention in the early 1970s of \\ncomputerized \\naxial tomography\\n (CAT), also called \\ncomputerized tomography\\n (CT) for short, is \\none of the most important events in the application of image processing in medical \\ndiagnosis. Computerized axial tomography is a process in which a ring of detectors \\nDIP4E_GLOBAL_Print_Ready.indb   21\\n6/16/2017   2:01:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 22}),\n",
       " Document(page_content='22\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nencircles an object (or patient) and an X-ray source, concentric with the detector \\nring, rotates about the object. The X-rays pass through the object and are collected \\nat the opposite end by the corresponding detectors in the ring. This procedure is \\nrepeated the source rotates. Tomography consists of algorithms that use the sensed \\ndata to construct an image that represents a “slice” through the object. Motion of \\nthe object in a direction perpendicular to the ring of detectors produces a set of \\nsuch slices, which constitute a three-dimensional (3-D) rendition of the inside of the \\nobject. Tomography was invented independently by Sir Godfrey N. Hounsﬁeld and \\nProfessor Allan M. Cormack, who shared the 1979 Nobel Prize in Medicine for their \\ninvention. It is interesting to note that X-rays were discovered in 1895 by Wilhelm \\nConrad Roentgen, for which he received the 1901 Nobel Prize for Physics. These two', metadata={'source': 'imagepro.pdf', 'page': 23}),\n",
       " Document(page_content='inventions, nearly 100 years apart, led to some of the most important applications of \\nimage processing today.\\nFrom the 1960s until the present, the ﬁeld of image processing has grown vigor-\\nously. In addition to applications in medicine and the space program, digital image \\nprocessing techniques are now used in a broad range of applications. Computer pro-\\ncedures are used to enhance the contrast or code the intensity levels into color for \\neasier interpretation of X-rays and other images used in industry, medicine, and the \\nbiological sciences. Geographers use the same or similar techniques to study pollu-\\ntion patterns from aerial and satellite imagery. Image enhancement and restoration \\nprocedures are used to process degraded images of unrecoverable objects, or experi-\\nmental results too expensive to duplicate. In archeology, image processing meth-\\nods have successfully restored blurred pictures that were the only available records', metadata={'source': 'imagepro.pdf', 'page': 23}),\n",
       " Document(page_content='of rare artifacts lost or damaged after being photographed. In physics and related \\nﬁelds, computer techniques routinely enhance images of experiments in areas such \\nas high-energy plasmas and electron microscopy. Similarly successful applications \\nof image processing concepts can be found in astronomy, biology, nuclear medicine, \\nlaw enforcement, defense, and industry.\\nFIGURE 1.4\\nThe ﬁrst picture \\nof the moon by \\na U.S. spacecraft. \\nRanger 7\\n took \\nthis image on \\nJuly 31, 1964 at \\n9:09 A.M. EDT, \\nabout 17 minutes \\nbefore impacting \\nthe lunar surface. \\n(Courtesy of \\nNASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   22\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 23}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n23\\nThese examples illustrate processing results intended for human interpretation. \\nThe second major area of application of digital image processing techniques men-\\ntioned at the beginning of this chapter is in solving problems dealing with machine \\nperception. In this case, interest is on procedures for extracting information from \\nan image, in a form suitable for computer processing. Often, this information bears \\nlittle resemblance to visual features that humans use in interpreting the content \\nof an image. Examples of the type of information used in machine perception are \\nstatistical moments, Fourier transform coefﬁcients, and multidimensional distance \\nmeasures. Typical problems in machine perception that routinely utilize image pro-\\ncessing techniques are automatic character recognition, industrial machine vision \\nfor product assembly and inspection, military recognizance, automatic processing of', metadata={'source': 'imagepro.pdf', 'page': 24}),\n",
       " Document(page_content='ﬁngerprints, screening of X-rays and blood samples, and machine processing of aer-\\nial and satellite imagery for weather prediction and environmental assessment. The \\ncontinuing decline in the ratio of computer price to performance, and the expansion \\nof networking and communication bandwidth via the internet, have created unprec-\\nedented opportunities for continued growth of digital image processing. Some of \\nthese application areas will be illustrated in the following section.\\n1.3 EXAMPLES OF FIELDS THAT USE DIGITAL IMAGE PROCESSING  \\nToday, there is almost no area of technical endeavor that is not impacted in some \\nway by digital image processing. We can cover only a few of these applications in the \\ncontext and space of the current discussion. However, limited as it is, the material \\npresented in this section will leave no doubt in your mind regarding the breadth and \\nimportance of digital image processing. We show in this section numerous areas of', metadata={'source': 'imagepro.pdf', 'page': 24}),\n",
       " Document(page_content='application, each of which routinely utilizes the digital image processing techniques \\ndeveloped in the following chapters. Many of the images shown in this section are \\nused later in one or more of the examples given in the book. Most images shown are \\ndigital images. \\nThe areas of application of digital image processing are so varied that some form \\nof organization is desirable in attempting to capture the breadth of this ﬁeld. One \\nof the simplest ways to develop a basic understanding of the extent of image pro-\\ncessing applications is to categorize images according to their source (e.g., X-ray, \\nvisual, infrared, and so on).The principal energy source for images in use today is \\nthe electromagnetic energy spectrum. Other important sources of energy include \\nacoustic, ultrasonic, and electronic (in the form of electron beams used in electron \\nmicroscopy). Synthetic images, used for modeling and visualization, are generated', metadata={'source': 'imagepro.pdf', 'page': 24}),\n",
       " Document(page_content='by computer. In this section we will discuss brieﬂy how images are generated in \\nthese various categories, and the areas in which they are applied. Methods for con-\\nverting images into digital form will be discussed in the next chapter.\\nImages based on radiation from the EM spectrum are the most familiar, espe-\\ncially images in the X-ray and visual bands of the spectrum. Electromagnetic waves \\ncan be conceptualized as propagating sinusoidal waves of varying wavelengths, or \\nthey can be thought of as a stream of massless particles, each traveling in a wavelike \\npattern and moving at the speed of light. Each massless particle contains a certain \\namount (or bundle) of energy. Each bundle of energy is called a \\nphoton\\n. If spectral \\n1.3\\nDIP4E_GLOBAL_Print_Ready.indb   23\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 24}),\n",
       " Document(page_content='24\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nbands are grouped according to energy per photon, we obtain the spectrum shown \\nin Fig. 1.5, ranging from gamma rays (highest energy) at one end to radio waves \\n(lowest energy) at the other. The bands are shown shaded to convey the fact that \\nbands of the EM spectrum are not distinct, but rather transition smoothly from one \\nto the other.\\nGAMMA-RAY IMAGING\\nMajor uses of imaging based on gamma rays include nuclear medicine and astro-\\nnomical observations. In nuclear medicine, the approach is to inject a patient with a \\nradioactive isotope that emits gamma rays as it decays. Images are produced from \\nthe emissions collected by gamma-ray detectors. Figure 1.6(a) shows an image of a \\ncomplete bone scan obtained by using gamma-ray imaging. Images of this sort are \\nused to locate sites of bone pathology, such as infections or tumors. Figure 1.6(b) \\nshows another major modality of nuclear imaging called \\npositron emission tomogra-\\nphy', metadata={'source': 'imagepro.pdf', 'page': 25}),\n",
       " Document(page_content='phy\\n (PET). The principle is the same as with X-ray tomography, mentioned briefly \\nin Section 1.2. However, instead of using an external source of X-ray energy, the \\npatient is given a radioactive isotope that emits positrons as it decays. When a pos-\\nitron meets an electron, both are annihilated and two gamma rays are given off. \\nThese are detected and a tomographic image is created using the basic principles of \\ntomography. The image shown in Fig. 1.6(b) is one sample of a sequence that con-\\nstitutes a 3-D rendition of the patient. This image shows a tumor in the brain and \\nanother in the lung, easily visible as small white masses.\\nA star in the constellation of Cygnus exploded about 15,000 years ago, generat-\\ning a superheated, stationary gas cloud (known as the Cygnus Loop) that glows in \\na spectacular array of colors. Figure 1.6(c) shows an image of the Cygnus Loop in \\nthe gamma-ray band. Unlike the two examples in Figs. 1.6(a) and (b), this image was', metadata={'source': 'imagepro.pdf', 'page': 25}),\n",
       " Document(page_content='obtained using the natural radiation of the object being imaged. Finally, Fig. 1.6(d) \\nshows an image of gamma radiation from a valve in a nuclear reactor. An area of \\nstrong radiation is seen in the lower left side of the image.\\nX-RAY IMAGING\\nX-rays are among the oldest sources of EM radiation used for imaging. The best \\nknown use of X-rays is medical diagnostics, but they are also used extensively in \\nindustry and other areas, such as astronomy. X-rays for medical and industrial imag-\\ning are generated using an X-ray tube, which is a vacuum tube with a cathode and \\nanode. The cathode is heated, causing free electrons to be released. These electrons \\nflow at high speed to the positively charged anode. When the electrons strike a \\n10\\n/H11002\\n9\\n10\\n/H11002\\n8\\n10\\n/H11002\\n7\\n10\\n/H11002\\n6\\n10\\n/H11002\\n5\\n10\\n/H11002\\n4\\n10\\n/H11002\\n3\\n10\\n/H11002\\n2\\n10\\n0\\n10\\n/H11002\\n1\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\nEnergy of one photon (electron volts)\\nGamma rays X-rays Ultraviolet Visible Infrared Microwaves', metadata={'source': 'imagepro.pdf', 'page': 25}),\n",
       " Document(page_content='Radio waves\\nFIGURE 1.5\\n The electromagnetic spectrum arranged according to energy per photon.\\nDIP4E_GLOBAL_Print_Ready.indb   24\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 25}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n25\\nnucleus, energy is released in the form of X-ray radiation. The energy (penetrat-\\ning power) of X-rays is controlled by a voltage applied across the anode, and by a \\ncurrent applied to the filament in the cathode. Figure 1.7(a) shows a familiar chest \\nX-ray generated simply by placing the patient between an X-ray source and a film \\nsensitive to X-ray energy. The intensity of the X-rays is modified by absorption as \\nthey pass through the patient, and the resulting energy falling on the film develops it, \\nmuch in the same way that light develops photographic film. In digital radiography, \\nb a\\nd c\\nFIGURE 1.6\\nExamples of \\ngamma-ray  \\nimaging.  \\n(a) Bone scan.  \\n(b) PET image. \\n(c) Cygnus Loop. \\n(d) Gamma radia-\\ntion (bright spot) \\nfrom a reactor \\nvalve.  \\n(Images  \\ncourtesy of  \\n(a) G.E. Medical \\nSystems; (b) Dr. \\nMichael E. Casey, \\nCTI PET Systems; \\n(c) NASA;  \\n(d) Professors \\nZhong He and \\nDavid K. Wehe,  \\nUniversity of', metadata={'source': 'imagepro.pdf', 'page': 26}),\n",
       " Document(page_content='University of \\nMichigan.) \\nDIP4E_GLOBAL_Print_Ready.indb   25\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 26}),\n",
       " Document(page_content='26\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\ndigital images are obtained by one of two methods: (1) by digitizing X-ray films; or; \\n(2) by having the X-rays that pass through the patient fall directly onto devices (such \\nas a phosphor screen) that convert X-rays to light. The light signal in turn is captured \\nby a light-sensitive digitizing system. We will discuss digitization in more detail in \\nChapters 2 and 4.\\nb\\na\\nd\\nc\\ne\\nFIGURE 1.7\\nExamples of \\nX-ray imaging.  \\n(a) Chest X-ray. \\n(b) Aortic  \\nangiogram.  \\n(c) Head CT.  \\n(d) Circuit boards. \\n(e) Cygnus Loop. \\n(Images courtesy \\nof (a) and (c) Dr. \\nDavid R. Pickens, \\nDept. of  \\nRadiology & \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center; \\n(b) Dr. Thomas \\nR. Gest, Division \\nof Anatomical \\nSciences, Univ. of \\nMichigan Medical \\nSchool;  \\n(d) Mr. Joseph \\nE. Pascente, Lixi, \\nInc.; and  \\n(e) NASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   26\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 27}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n27\\nAngiography is another major application in an area called contrast enhancement \\nradiography. This procedure is used to obtain images of blood vessels, called \\nangio-\\ngrams\\n. A catheter (a small, ﬂexible, hollow tube) is inserted, for example, into an \\nartery or vein in the groin. The catheter is threaded into the blood vessel and guided \\nto the area to be studied. When the catheter reaches the site under investigation, \\nan X-ray contrast medium is injected through the tube. This enhances the contrast \\nof the blood vessels and enables a radiologist to see any irregularities or blockages. \\nFigure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen \\nbeing inserted into the large blood vessel on the lower left of the picture. Note the \\nhigh contrast of the large vessel as the contrast medium ﬂows up in the direction of', metadata={'source': 'imagepro.pdf', 'page': 28}),\n",
       " Document(page_content='the kidneys, which are also visible in the image. As we will discuss further in Chapter 2, \\nangiography is a major area of digital image processing, where image subtraction is \\nused to further enhance the blood vessels being studied.\\nAnother important use of X-rays in medical imaging is computerized axial tomog-\\nraphy (CAT). Due to their resolution and 3-D capabilities, CAT scans revolution-\\nized medicine from the moment they ﬁrst became available in the early 1970s. As \\nnoted in Section 1.2, each CAT image is a “slice” taken perpendicularly through \\nthe patient. Numerous slices are generated as the patient is moved in a longitudinal \\ndirection. The ensemble of such images constitutes a 3-D rendition of the inside of \\nthe body, with the longitudinal resolution being proportional to the number of slice \\nimages taken. Figure 1.7(c) shows a typical CAT slice image of a human head.\\nTechniques similar to the ones just discussed, but generally involving higher', metadata={'source': 'imagepro.pdf', 'page': 28}),\n",
       " Document(page_content='energy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray \\nimage of an electronic circuit board. Such images, representative of literally hundreds \\nof industrial applications of X-rays, are used to examine circuit boards for ﬂaws in \\nmanufacturing, such as missing components or broken traces. Industrial CAT scans \\nare useful when the parts can be penetrated by X-rays, such as in plastic assemblies, \\nand even large bodies, such as solid-propellant rocket motors. Figure 1.7(e) shows an \\nexample of X-ray imaging in astronomy. This image is the Cygnus Loop of Fig. 1.6(c), \\nbut imaged in the X-ray band.\\nIMAGING IN THE ULTRAVIOLET BAND\\nApplications of ultraviolet “light” are varied. They include lithography, industrial \\ninspection, microscopy, lasers, biological imaging, and astronomical observations. \\nWe illustrate imaging in this band with examples from microscopy and astronomy.\\nUltraviolet light is used in ﬂuorescence microscopy, one of the fastest growing', metadata={'source': 'imagepro.pdf', 'page': 28}),\n",
       " Document(page_content='areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the \\nnineteenth century, when it was ﬁrst observed that the mineral ﬂuorspar ﬂuoresces \\nwhen ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but \\nwhen a photon of ultraviolet radiation collides with an electron in an atom of a ﬂuo-\\nrescent material, it elevates the electron to a higher energy level. Subsequently, the \\nexcited electron relaxes to a lower level and emits light in the form of a lower-energy \\nphoton in the visible (red) light region. Important tasks performed with a ﬂuores-\\ncence microscope are to use an excitation light to irradiate a prepared specimen, \\nand then to separate the much weaker radiating ﬂuorescent light from the brighter \\nDIP4E_GLOBAL_Print_Ready.indb   27\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 28}),\n",
       " Document(page_content='28\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nexcitation light. Thus, only the emission light reaches the eye or other detector. The \\nresulting ﬂuorescing areas shine against a dark background with sufﬁcient contrast \\nto permit detection. The darker the background of the nonﬂuorescing material, the \\nmore efﬁcient the instrument.\\nFluorescence microscopy is an excellent method for studying materials that can be \\nmade to ﬂuoresce, either in their natural form (primary ﬂuorescence) or when treat-\\ned with chemicals capable of ﬂuorescing (secondary ﬂuorescence). Figures 1.8(a) \\nand (b) show results typical of the capability of ﬂuorescence microscopy. Figure \\n1.8(a) shows a ﬂuorescence microscope image of normal corn, and Fig. 1.8(b) shows \\ncorn infected by “smut,” a disease of cereals, corn, grasses, onions, and sorghum that \\ncan be caused by any one of more than 700 species of parasitic fungi. Corn smut is \\nparticularly harmful because corn is one of the principal food sources in the world.', metadata={'source': 'imagepro.pdf', 'page': 29}),\n",
       " Document(page_content='As another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy \\nregion of the ultraviolet band.\\nIMAGING IN THE VISIBLE AND INFRARED BANDS\\nConsidering that the visual band of the electromagnetic spectrum is the most famil-\\niar in all our activities, it is not surprising that imaging in this band outweighs by far \\nall the others in terms of breadth of application. The infrared band often is used in \\nconjunction with visual imaging, so we have grouped the visible and infrared bands \\nin this section for the purpose of illustration. We consider in the following discus-\\nsion applications in light microscopy, astronomy, remote sensing, industry, and law \\nenforcement.\\nFigure 1.9 shows several examples of images obtained with a light microscope. \\nThe examples range from pharmaceuticals and microinspection to materials char-\\nacterization. Even in microscopy alone, the application areas are too numerous to', metadata={'source': 'imagepro.pdf', 'page': 29}),\n",
       " Document(page_content='detail here. It is not difﬁcult to conceptualize the types of processes one might apply \\nto these images, ranging from enhancement to measurements.\\nb a\\nc\\nFIGURE 1.8\\n Examples of ultraviolet imaging. (a) Normal corn. (b) Corn infected by smut. (c) Cygnus Loop. (Images \\n(a) and (b) courtesy of Dr. Michael W. Davidson, Florida State University, (c) NASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   28\\n6/16/2017   2:01:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 29}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n29\\nAnother major area of visual processing is remote sensing, which usually includes \\nseveral bands in the visual and infrared regions of the spectrum. Table 1.1 shows the \\nso-called \\nthematic bands\\n in NASA’s LANDSAT satellites. The primary function of \\nLANDSAT is to obtain and transmit images of the Earth from space, for purposes \\nof monitoring environmental conditions on the planet. The bands are expressed in \\nterms of wavelength, with \\n1\\nm\\nm\\n being equal to \\n10\\n6\\n−\\n m (we will discuss the wave-\\nlength regions of the electromagnetic spectrum in more detail in Chapter 2). Note \\nthe characteristics and uses of each band in Table 1.1.\\nIn order to develop a basic appreciation for the power of this type of multispec-\\ntral imaging, consider Fig. 1.10, which shows one image for each of the spectral bands \\nin Table 1.1. The area imaged is Washington D.C., which includes features such as', metadata={'source': 'imagepro.pdf', 'page': 30}),\n",
       " Document(page_content='buildings, roads, vegetation, and a major river (the Potomac) going though the city. \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 1.9\\nExamples of light  \\nmicroscopy images.  \\n(a) Taxol (antican-\\ncer agent), magni-\\nﬁed \\n250\\n×\\n. \\n(b) Cholesterol—\\n40\\n×\\n.  \\n(c) Microproces-\\nsor—\\n60\\n×\\n.  \\n(d) Nickel oxide \\nthin ﬁlm—\\n600\\n×\\n. \\n(e) Surface of audio \\nCD—\\n1750\\n×\\n.  \\n(f) Organic super\\n-\\nconductor— \\n450\\n×\\n. \\n(Images courtesy of \\nDr\\n. Michael W.  \\nDavidson, Florida \\nState University.) \\nDIP4E_GLOBAL_Print_Ready.indb   29\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 30}),\n",
       " Document(page_content='30\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nImages of population centers are used over time to assess population growth and \\nshift patterns, pollution, and other factors affecting the environment. The differenc-\\nes between visual and infrared image features are quite noticeable in these images. \\nObserve, for example, how well deﬁned the river is from its surroundings in Bands \\n4 and 5.\\nWeather observation and prediction also are major applications of multispectral \\nimaging from satellites. For example, Fig. 1.11 is an image of Hurricane Katrina, one \\nof the most devastating storms in recent memory in the Western Hemisphere. This \\nimage was taken by a National Oceanographic and Atmospheric Administration \\n(NOAA) satellite using sensors in the visible and infrared bands. The eye of the hur-\\nricane is clearly visible in this image.\\nBand No. Name\\nWavelength \\n(\\nM\\nm)\\nCharacteristics and Uses\\n1 Visible blue\\n0.45– 0.52 Maximum water penetration\\n2 Visible green\\n0.53– 0.61 Measures plant vigor', metadata={'source': 'imagepro.pdf', 'page': 31}),\n",
       " Document(page_content='3 Visible red\\n0.63– 0.69 Vegetation discrimination\\n4 Near infrared\\n0.78– 0.90 Biomass and shoreline mapping\\n5 Middle infrared 1.55–1.75 Moisture content: soil/vegetation\\n6 Thermal infrared 10.4–12.5 Soil moisture; thermal mapping\\n7 Short-wave infrared 2.09–2.35 Mineral mapping\\nTABLE \\n1.1\\nThematic bands \\nof NASA’s \\nLANDSAT  \\nsatellite.\\n123\\n4567\\nFIGURE 1.10\\n LANDSAT satellite images of the Washington, D.C. area. The numbers refer to the thematic bands in \\nTable 1.1. (Images courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   30\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 31}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n31\\nFigures 1.12 and 1.13 show an application of infrared imaging. These images are \\npart of the Nighttime Lights of the World data set, which provides a global inventory \\nof human settlements. The images were generated by an infrared imaging system \\nmounted on a NOAA/DMSP (Defense Meteorological Satellite Program) satel-\\nlite. The infrared system operates in the band 10.0 to 13.4 \\nm\\nm,\\n and has the unique \\ncapability to observe faint sources of visible\\n, near infrared emissions present on the  \\nEarth’s surface, including cities, towns, villages, gas ﬂares, and ﬁres. Even without \\nformal training in image processing, it is not difﬁcult to imagine writing a computer \\nprogram that would use these images to estimate the relative percent of total electri-\\ncal energy used by various regions of the world.\\nA major area of imaging in the visible spectrum is in automated visual inspection', metadata={'source': 'imagepro.pdf', 'page': 32}),\n",
       " Document(page_content='of manufactured goods. Figure 1.14 shows some examples. Figure 1.14(a) is a con-\\ntroller board for a CD-ROM drive. A typical image processing task with products \\nsuch as this is to inspect them for missing parts (the black square on the top, right \\nquadrant of the image is an example of a missing component).\\nFigure 1.14(b) is an imaged pill container. The objective here is to have a machine \\nlook for missing, incomplete, or deformed pills. Figure 1.14(c) shows an application \\nin which image processing is used to look for bottles that are not ﬁlled up to an \\nacceptable level. Figure 1.14(d) shows a clear plastic part with an unacceptable num-\\nber of air pockets in it. Detecting anomalies like these is a major theme of industrial \\ninspection that includes other products, such as wood and cloth. Figure 1.14(e) shows \\na batch of cereal during inspection for color and the presence of anomalies such as \\nburned ﬂakes. Finally, Fig. 1.14(f) shows an image of an intraocular implant (replace-', metadata={'source': 'imagepro.pdf', 'page': 32}),\n",
       " Document(page_content='ment lens for the human eye). A “structured light” illumination technique was used \\nto highlight deformations toward the center of the lens, and other imperfections. For \\nexample, the markings at 1 o’clock and 5 o’clock are tweezer damage. Most of the \\nother small speckle detail is debris. The objective in this type of inspection is to ﬁnd \\ndamaged or incorrectly manufactured implants automatically, prior to packaging.\\nFIGURE 1.11\\nSatellite image of \\nHurricane Katrina \\ntaken on August \\n29, 2005.  \\n(Courtesy of \\nNOAA.)\\nDIP4E_GLOBAL_Print_Ready.indb   31\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 32}),\n",
       " Document(page_content='32\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nFigure 1.15 illustrates some additional examples of image processing in the vis-\\nible spectrum. Figure 1.15(a) shows a thumb print. Images of ﬁngerprints are rou-\\ntinely processed by computer, either to enhance them or to ﬁnd features that aid \\nin the automated search of a database for potential matches. Figure 1.15(b) shows \\nan image of paper currency. Applications of digital image processing in this area \\nFIGURE 1.12\\nInfrared  \\nsatellite images of \\nthe Americas. The \\nsmall shaded map \\nis provided for  \\nreference.  \\n(Courtesy of \\nNOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   32\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 33}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n33\\ninclude automated counting and, in law enforcement, the reading of the serial num-\\nber for the purpose of tracking and identifying currency bills. The two vehicle images \\nshown in Figs. 1.15(c) and (d) are examples of automated license plate reading. The \\nlight rectangles indicate the area in which the imaging system detected the plate. \\nThe black rectangles show the results of automatically reading the plate content by \\nthe system. License plate and other applications of character recognition are used \\nextensively for trafﬁc monitoring and surveillance.\\nIMAGING IN THE MICROWAVE BAND\\nThe principal application of imaging in the microwave band is radar. The unique \\nfeature of imaging radar is its ability to collect data over virtually any region at any \\ntime, regardless of weather or ambient lighting conditions. Some radar waves can \\npenetrate clouds, and under certain conditions, can also see through vegetation, ice,', metadata={'source': 'imagepro.pdf', 'page': 34}),\n",
       " Document(page_content='and dry sand. In many cases, radar is the only way to explore inaccessible regions of \\nthe Earth’s surface. An imaging radar works like a flash camera in that it provides \\nits own illumination (microwave pulses) to illuminate an area on the ground and \\nFIGURE 1.13\\nInfrared  \\nsatellite images \\nof the remaining \\npopulated parts \\nof the world. The \\nsmall shaded map \\nis provided for \\nreference.  \\n(Courtesy of \\nNOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   33\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 34}),\n",
       " Document(page_content='34\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\ntake a snapshot image. Instead of a camera lens, a radar uses an antenna and digital \\ncomputer processing to record its images. In a radar image, one can see only the \\nmicrowave energy that was reflected back toward the radar antenna.\\nFigure 1.16 shows a spaceborne radar image covering a rugged mountainous area \\nof southeast Tibet, about 90 km east of the city of Lhasa. In the lower right cor-\\nner is a wide valley of the Lhasa River, which is populated by Tibetan farmers and \\nyak herders, and includes the village of Menba. Mountains in this area reach about \\n5800 m (19,000 ft) above sea level, while the valley ﬂoors lie about 4300 m (14,000 ft) \\nabove sea level. Note the clarity and detail of the image, unencumbered by clouds or \\nother atmospheric conditions that normally interfere with images in the visual band.\\nIMAGING IN THE RADIO BAND\\nAs in the case of imaging at the other end of the spectrum (gamma rays), the major', metadata={'source': 'imagepro.pdf', 'page': 35}),\n",
       " Document(page_content='applications of imaging in the radio band are in medicine and astronomy. In medicine, \\nradio waves are used in magnetic resonance imaging (MRI). This technique places a \\nb a\\nc\\ne\\nd\\nf\\nFIGURE 1.14\\n Some examples of manufactured goods checked using digital image processing. (a) Circuit board con-\\ntroller. (b) Packaged pills. (c) Bottles. (d) Air bubbles in a clear plastic product. (e) Cereal. (f) Image of intraocular \\nimplant. (Figure (f) courtesy of Mr. Pete Sites, Perceptics Corporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   34\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 35}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n35\\npatient in a powerful magnet and passes radio waves through the individual’s body \\nin short pulses. Each pulse causes a responding pulse of radio waves to be emitted \\nby the patient’s tissues. The location from which these signals originate and their \\nstrength are determined by a computer, which produces a two-dimensional image \\nof a section of the patient. MRI can produce images in any plane. Figure 1.17 shows \\nMRI images of a human knee and spine.\\nThe rightmost image in Fig. 1.18 is an image of the Crab Pulsar in the radio band. \\nAlso shown for an interesting comparison are images of the same region, but taken \\nin most of the bands discussed earlier. Observe that each image gives a totally dif-\\nferent “view” of the pulsar.\\nOTHER IMAGING MODALITIES\\nAlthough imaging in the electromagnetic spectrum is dominant by far, there are a \\nnumber of other imaging modalities that are also important. Specifically, we discuss \\nb\\na', metadata={'source': 'imagepro.pdf', 'page': 36}),\n",
       " Document(page_content='b\\na\\nd\\nc\\nFIGURE 1.15\\nSome additional \\nexamples of  \\nimaging in the  \\nvisible spectrum. \\n(a) Thumb print. \\n(b) Paper  \\ncurrency.  \\n(c) and (d) Auto-\\nmated license \\nplate reading.  \\n(Figure (a) \\ncourtesy of the \\nNational  \\nInstitute of  \\nStandards and \\nTechnology.  \\nFigures (c) and \\n(d) courtesy of \\nDr. Juan  \\nHerrera,  \\nPerceptics  \\nCorporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   35\\n6/16/2017   2:02:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 36}),\n",
       " Document(page_content='36\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nin this section acoustic imaging, electron microscopy, and synthetic (computer-gen-\\nerated) imaging. \\nImaging using “sound” ﬁnds application in geological exploration, industry, and \\nmedicine. Geological applications use sound in the low end of the sound spectrum \\n(hundreds of Hz) while imaging in other areas use ultrasound (millions of Hz). The \\nmost important commercial applications of image processing in geology are in min-\\neral and oil exploration. For image acquisition over land, one of the main approaches \\nis to use a large truck and a large ﬂat steel plate. The plate is pressed on the \\nground by \\nFIGURE 1.16\\nSpaceborne radar \\nimage of  \\nmountainous \\nregion in  \\nsoutheast Tibet. \\n(Courtesy of \\nNASA.)\\nb a\\nFIGURE 1.17\\n MRI images of a human (a) knee, and (b) spine. (Figure (a) courtesy of Dr. Thom-\\nas R. Gest, Division of Anatomical Sciences, University of Michigan Medical School, and', metadata={'source': 'imagepro.pdf', 'page': 37}),\n",
       " Document(page_content='(b) courtesy of Dr. David R. Pickens, Department of Radiology and Radiological Sciences, \\nVanderbilt University Medical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   36\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 37}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n37\\nthe truck, and the truck is vibrated through a frequency spectrum up to 100 Hz. The \\nstrength and speed of the returning sound waves are determined by the composi-\\ntion of the Earth below the surface. These are analyzed by computer, and images are \\ngenerated from the resulting analysis.\\nFor marine image acquisition, the energy source consists usually of two air guns \\ntowed behind a ship. Returning sound waves are detected by hydrophones placed \\nin cables that are either towed behind the ship, laid on the bottom of the ocean, \\nor hung from buoys (vertical cables). The two air guns are alternately pressurized  \\nto ~2000 psi and then set off. The constant motion of the ship provides a transversal \\ndirection of motion that, together with the returning sound waves, is used to gener-\\nate a 3-D map of the composition of the Earth below the bottom of the ocean.', metadata={'source': 'imagepro.pdf', 'page': 38}),\n",
       " Document(page_content='Figure 1.19 shows a cross-sectional image of a well-known 3-D model against \\nwhich the performance of seismic imaging algorithms is tested. The arrow points to a \\nhydrocarbon (oil and/or gas) trap. This target is brighter than the surrounding layers \\nbecause the change in density in the target region is larger. Seismic interpreters look \\nfor these “bright spots” to ﬁnd oil and gas. The layers above also are bright, but their \\nbrightness does not vary as strongly across the layers. Many seismic reconstruction \\nalgorithms have difﬁculty imaging this target because of the faults above it.\\nAlthough ultrasound imaging is used routinely in manufacturing, the best known \\napplications of this technique are in medicine, especially in obstetrics, where fetuses \\nare imaged to determine the health of their development. A byproduct of this \\nGamma\\nX-ray\\nOptical\\nInfrared\\nRadio\\nFIGURE 1.18\\n Images of the Crab Pulsar (in the center of each image) covering the electromagnetic spectrum. (Cour-', metadata={'source': 'imagepro.pdf', 'page': 38}),\n",
       " Document(page_content='tesy of NASA.)\\nFIGURE 1.19\\nCross-sectional \\nimage of a  \\nseismic model. \\nThe arrow points \\nto a hydrocarbon \\n(oil and/or gas) \\ntrap. (Courtesy of \\nDr. Curtis Ober, \\nSandia National \\nLaboratories.)\\nDIP4E_GLOBAL_Print_Ready.indb   37\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 38}),\n",
       " Document(page_content='38\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nexamination is determining the sex of the baby. Ultrasound images are generated \\nusing the following basic procedure:\\n1. \\nThe ultrasound system (a computer, ultrasound probe consisting of a source, a \\nreceiver\\n, and a display) transmits high-frequency (1 to 5 MHz) sound pulses \\ninto the body.\\n2. \\nThe sound waves travel into the body and hit a boundary between tissues (e.g., \\nbetween ﬂuid and soft tissue\\n, soft tissue and bone). Some of the sound waves \\nare reﬂected back to the probe, while some travel on further until they reach \\nanother boundary and are reﬂected.\\n3. \\nThe reﬂected waves are picked up by the probe and relayed to the computer.\\n4. \\nThe machine calculates the distance from the probe to the tissue or organ bound-\\naries using the speed of sound in tissue (1540 m/s) and the time of each echo’\\ns \\nreturn.\\n5. \\nThe system displays the distances and intensities of the echoes on the screen, \\nforming a two-dimensional image\\n.', metadata={'source': 'imagepro.pdf', 'page': 39}),\n",
       " Document(page_content='.\\nIn a typical ultrasound image, millions of pulses and echoes are sent and received \\neach second. The probe can be moved along the surface of the body and angled to \\nobtain various views. Figure 1.20 shows several examples of medical uses of ultra-\\nsound. \\nWe continue the discussion on imaging modalities with some examples of elec-\\ntron microscopy. Electron microscopes function as their optical counterparts, except \\nb a\\nd c\\nFIGURE 1.20\\nExamples of \\nultrasound  \\nimaging. (a) A \\nfetus. (b) Another \\nview of the fetus.  \\n(c) Thyroids.  \\n(d) Muscle layers \\nshowing lesion. \\n(Courtesy of \\nSiemens  \\nMedical Systems, \\nInc., Ultrasound \\nGroup.)\\nDIP4E_GLOBAL_Print_Ready.indb   38\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 39}),\n",
       " Document(page_content='1.3\\n  \\nExamples of Fields that Use Digital Image Processing\\n    \\n39\\nthat they use a focused beam of electrons instead of light to image a specimen. The \\noperation of electron microscopes involves the following basic steps: A stream \\nof electrons is produced by an electron source and accelerated toward the speci-\\nmen using a positive electrical potential. This \\nstream is conﬁned and focused using \\nmetal apertures and magnetic lenses into a thin, monochromatic beam. \\nThis beam is \\nfocused onto the sample using a magnetic lens. Interactions occur inside the irradi-\\nated sample, affecting the electron beam. These interactions and effects are detected \\nand transformed into an image, much in the same way that light is reﬂected from, \\nor absorbed by, objects in a scene. These basic steps are carried out in all electron \\nmicroscopes.\\nA \\ntransmission electron microscope\\n (TEM) works much like a slide projector. A', metadata={'source': 'imagepro.pdf', 'page': 40}),\n",
       " Document(page_content='projector transmits a beam of light through a slide; as the light passes through the \\nslide, it is modulated by the contents of the slide. This transmitted beam is then \\nprojected onto the viewing screen, forming an enlarged image of the slide. TEMs \\nwork in the same way, except that they shine a beam of electrons through a spec-\\nimen (analogous to the slide). The fraction of the beam transmitted through the \\nspecimen is projected onto a phosphor screen. The interaction of the electrons with \\nthe phosphor produces light and, therefore, a viewable image. A \\nscanning electron \\nmicroscope\\n (SEM), on the other hand, actually scans the electron beam and records \\nthe interaction of beam and sample at each location. This produces one dot on a \\nphosphor screen. A complete image is formed by a raster scan of the beam through \\nthe sample, much like a TV camera. The electrons interact with a phosphor screen \\nand produce light. SEMs are suitable for “bulky” samples, while TEMs require very', metadata={'source': 'imagepro.pdf', 'page': 40}),\n",
       " Document(page_content='thin samples.\\nElectron microscopes are capable of very high magniﬁcation. While light micros-\\ncopy is limited to magniﬁcations on the order of \\n1000\\n×\\n,\\n electron microscopes can \\nachieve magniﬁcation of \\n10 000 ,\\n×\\n or more. Figure 1.21 shows two SEM images of \\nspecimen failures due to thermal overload.\\nW\\ne conclude the discussion of imaging modalities by looking brieﬂy at images \\nthat are not obtained from physical objects. Instead, they are generated by computer. \\nFractals are striking examples of computer-generated images. Basically, a fractal is \\nnothing more than an iterative reproduction of a basic pattern according to some \\nmathematical rules. For instance, tiling is one of the simplest ways to generate a frac-\\ntal image. A square can be subdivided into four square subregions, each of which can \\nbe further subdivided into four smaller square regions, and so on. Depending on the \\ncomplexity of the rules for ﬁlling each subsquare, some beautiful tile images can be', metadata={'source': 'imagepro.pdf', 'page': 40}),\n",
       " Document(page_content='generated using this method. Of course, the geometry can be arbitrary. For instance, \\nthe fractal image could be grown radially out of a center point. Figure 1.22(a) shows \\na fractal grown in this way. Figure 1.22(b) shows another fractal (a “moonscape”) \\nthat provides an interesting analogy to the images of space used as illustrations in \\nsome of the preceding sections.\\nA more structured approach to image generation by computer lies in 3-D model-\\ning. This is an area that provides an important intersection between image process-\\ning and computer graphics, and is the basis for many 3-D visualization systems (e.g., \\nﬂight simulators). Figures 1.22(c) and (d) show examples of computer-generated \\nimages. Because the original \\nobject is created in 3-D, images can be generated in any \\nDIP4E_GLOBAL_Print_Ready.indb   39\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 40}),\n",
       " Document(page_content='40\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nperspective from plane projections of the 3-D volume. Images of this type can be \\nused for medical training and for a host of other applications, such as criminal foren-\\nsics and special effects.\\nb a\\nFIGURE 1.21\\n (a) \\n250\\n×\\n SEM image of a tungsten ﬁlament following thermal failure (note the \\nshattered pieces on the lower left).\\n (b) \\n2500\\n×\\n SEM image of a damaged integrated circuit. \\nThe white ﬁbers are oxides resulting from thermal destruction. (Figure (a) courtesy of Mr. \\nMichael Shaffer, Department of Geological Sciences, University of Oregon, Eugene; (b) cour-\\ntesy of Dr. J. M. Hudak, McMaster University, Hamilton, Ontario, Canada.) \\nb a\\nd c\\nFIGURE 1.22\\n(a) and (b) Fractal \\nimages.  \\n(c) and (d) Images \\ngenerated from \\n3-D computer \\nmodels of the \\nobjects shown. \\n(Figures (a) and \\n(b) courtesy of \\nMs. Melissa D. \\nBinde,  \\nSwarthmore \\nCollege; (c) and \\n(d) courtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   40\\n6/16/2017   2:02:01 PM', metadata={'source': 'imagepro.pdf', 'page': 41}),\n",
       " Document(page_content='www.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 41}),\n",
       " Document(page_content='1.4\\n  \\nFundamental Steps in Digital Image Processing\\n    \\n41\\n1.4 FUNDAMENTAL STEPS IN DIGITAL IMAGE PROCESSING  \\nIt is helpful to divide the material covered in the following chapters into the two \\nbroad categories defined in Section 1.1: methods whose input and output are images, \\nand methods whose inputs may be images, but whose outputs are attributes extract-\\ned from those images. This organization is summarized in Fig. 1.23. The diagram \\ndoes not imply that every process is applied to an image. Rather, the intention is to \\nconvey an idea of all the methodologies that can be applied to images for different \\npurposes, and possibly with different objectives. The discussion in this section may \\nbe viewed as a brief overview of the material in the remainder of the book. \\nImage acquisition\\n is the ﬁrst process in Fig. 1.23. The discussion in Section 1.3 \\ngave some hints regarding the origin of digital images. This topic will be considered', metadata={'source': 'imagepro.pdf', 'page': 42}),\n",
       " Document(page_content='in much more detail in Chapter 2, where we also introduce a number of basic digital \\nimage concepts that are used throughout the book. Acquisition could be as simple as \\nbeing given an image that is already in digital form. Generally, the image acquisition \\nstage involves preprocessing, such as scaling.\\nImage enhancement\\n is the process of manipulating an image so the result is more \\nsuitable than the original for a speciﬁc application. The word \\nspeciﬁc\\n is important \\nhere, because it establishes at the outset that enhancement techniques are problem \\noriented. Thus, for example, a method that is quite useful for enhancing X-ray images \\nmay not be the best approach for enhancing satellite images taken in the infrared \\nband of the electromagnetic spectrum.\\nThere is no general “theory” of image enhancement. When an image is processed \\nfor visual interpretation, the viewer is the ultimate judge of how well a particular \\n1.4\\nKnowledge base\\nCHAPTER 7\\nWavelets and\\nother image\\ntransforms', metadata={'source': 'imagepro.pdf', 'page': 42}),\n",
       " Document(page_content='transforms\\nOutputs of these processes generally are images\\nCHAPTER 5\\nImage\\nrestoration\\nCHAPTERS 3 & 4\\nImage\\nfiltering and \\nenhancement\\nProblem\\ndomain\\nOutputs of these processes generally are image attributes\\nCHAPTER 8\\nCompression and\\nwatermarking\\nCHAPTER 2\\nImage\\nacquisition\\nCHAPTER 9\\nMorphological\\nprocessing\\n    CHAPTERS 10 \\nSegmentation\\n  CHAPTER 11\\nFeature \\nextraction\\nCHAPTER 12\\nImage \\npattern\\nclassification\\nWavelets and\\nmultiresolution\\nprocessing\\nColor Image\\nProcessing\\nCHAPTER 6\\nFIGURE 1.23\\nFundamental \\nsteps in digital \\nimage processing. \\nThe chapter(s) \\nindicated in the \\nboxes is where \\nthe material \\ndescribed in the \\nbox is discussed.\\nDIP4E_GLOBAL_Print_Ready.indb   41\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 42}),\n",
       " Document(page_content='42\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nmethod works. Enhancement techniques are so varied, and use so many different \\nimage processing approaches, that it is difﬁcult to assemble a meaningful body of \\ntechniques suitable for enhancement in one chapter without extensive background \\ndevelopment. For this reason, and also because beginners in the ﬁeld of image pro-\\ncessing generally ﬁnd enhancement applications visually appealing, interesting, and \\nrelatively simple to understand, we will use image enhancement as examples when \\nintroducing new concepts in parts of Chapter 2 and in Chapters 3 and 4. The mate-\\nrial in the latter two chapters span many of the methods used traditionally for image \\nenhancement. Therefore, using examples from image enhancement to introduce new \\nimage processing methods developed in these early chapters not only saves having \\nan extra chapter in the book dealing with image enhancement but, more importantly,', metadata={'source': 'imagepro.pdf', 'page': 43}),\n",
       " Document(page_content='is an effective approach for introducing newcomers to the details of processing tech-\\nniques early in the book. However, as you will see in progressing through the rest \\nof the book, the material developed in Chapters 3 and 4 is applicable to a much \\nbroader class of problems than just image enhancement.\\nImage restoration\\n is an area that also deals with improving the appearance of \\nan image. However, unlike enhancement, which is subjective, image restoration \\nis objective, in the sense that restoration techniques tend to be based on mathe-\\nmatical or probabilistic models of image degradation. Enhancement, on the other \\nhand, is based on human subjective preferences regarding what constitutes a “good” \\nenhancement result.\\nColor image processing\\n is an area that has been gaining in importance because of \\nthe signiﬁcant increase in the use of digital images over the internet. Chapter 6 cov-\\ners a number of fundamental concepts in color models and basic color processing', metadata={'source': 'imagepro.pdf', 'page': 43}),\n",
       " Document(page_content='in a digital domain. Color is used also as the basis for extracting features of interest \\nin an image.\\nWavelets\\n are the foundation for representing images in various degrees of reso-\\nlution. In particular, this material is used in the book for image data compression \\nand for pyramidal representation, in which images are subdivided successively into \\nsmaller regions. The material in Chapters 4 and 5 is based mostly on the Fourier \\ntransform. In addition to wavelets, we will also discuss in Chapter 7 a number of \\nother transforms that are used routinely in image processing.\\nCompression\\n, as the name implies, deals with techniques for reducing the storage \\nrequired to save an image, or the bandwidth required to transmit it. Although stor-\\nage technology has improved signiﬁcantly over the past decade, the same cannot be \\nsaid for transmission capacity. This is true particularly in uses of the internet, which', metadata={'source': 'imagepro.pdf', 'page': 43}),\n",
       " Document(page_content='are characterized by signiﬁcant pictorial content. Image compression is familiar \\n(perhaps inadvertently) to most users of computers in the form of image ﬁle exten-\\nsions, such as the jpg ﬁle extension used in the JPEG (Joint Photographic Experts \\nGroup) image compression standard.\\nMorphological\\n processing deals with tools for extracting image components that \\nare useful in the representation and description of shape. The material in this chap-\\nter begins a transition from processes that output images to processes that output \\nimage attributes, as indicated in Section 1.1.\\nSegmentation\\n partitions an image into its constituent parts or objects. In gen-\\neral, autonomous segmentation is one of the most difﬁcult tasks in digital image \\nDIP4E_GLOBAL_Print_Ready.indb   42\\n6/16/2017   2:02:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 43}),\n",
       " Document(page_content='1.4\\n  \\nFundamental Steps in Digital Image Processing\\n    \\n43\\nprocessing. A rugged segmentation procedure brings the process a long way toward \\nsuccessful solution of imaging problems that require objects to be identiﬁed indi-\\nvidually. On the other hand, weak or erratic segmentation algorithms almost always \\nguarantee eventual failure. In general, the more accurate the segmentation, the \\nmore likely automated object classiﬁcation is to succeed.\\nFeature extraction\\n almost always follows the output of a segmentation stage, which \\nusually is raw pixel data, constituting either the boundary of a region (i.e., the set \\nof pixels separating one image region from another) or all the points in the region \\nitself. Feature extraction consists of feature detection and feature description. \\nFea-\\nture detection\\n refers to ﬁnding the features in an image, region, or boundary. \\nFeature \\ndescription\\n assigns quantitative attributes to the detected features. For example, we', metadata={'source': 'imagepro.pdf', 'page': 44}),\n",
       " Document(page_content='might detect corners in a region, and describe those corners by their orientation \\nand location; both of these descriptors are quantitative attributes. Feature process-\\ning methods discussed in this chapter are subdivided into three principal categories, \\ndepending on whether they are applicable to boundaries, regions, or whole images. \\nSome features are applicable to more than one category. Feature descriptors should \\nbe as insensitive as possible to variations in parameters such as scale, translation, \\nrotation, illumination, and viewpoint. \\nImage pattern classiﬁcation\\n is the process that assigns a label (e.g., “vehicle”) to an \\nobject based on its feature descriptors. In the last chapter of the book, we will discuss  \\nmethods of image pattern classiﬁcation ranging from “classical” approaches such as \\nminimum-distance\\n, \\ncorrelation\\n, and \\nBayes classiﬁers\\n, to more modern approaches \\nimplemented using \\ndeep neural networks\\n. In particular, we will discuss in detail \\ndeep', metadata={'source': 'imagepro.pdf', 'page': 44}),\n",
       " Document(page_content='deep\\n \\nconvolutional neural networks\\n, which are ideally suited for image processing work.\\nSo far, we have said nothing about the need for prior knowledge or about the \\ninteraction between the knowledge base and the processing modules in Fig. 1.23. \\nKnowledge\\n about a problem domain is coded into an image processing system in the \\nform of a knowledge database. This knowledge may be as simple as detailing regions \\nof an image where the information of interest is known to be located, thus limiting \\nthe search that has to be conducted in seeking that information. The knowledge base \\ncan also be quite complex, such as an interrelated list of all major possible defects \\nin a materials inspection problem, or an image database containing high-resolution \\nsatellite images of a region in connection with change-detection applications. In \\naddition to guiding the operation of each processing module, the knowledge base', metadata={'source': 'imagepro.pdf', 'page': 44}),\n",
       " Document(page_content='also controls the interaction between modules. This distinction is made in Fig. 1.23 \\nby the use of double-headed arrows between the processing modules and the knowl-\\nedge base, as opposed to single-headed arrows linking the processing modules.\\nAlthough we do not discuss image display explicitly at this point, it is important to \\nkeep in mind that viewing the results of image processing can take place at the out-\\nput of any stage in Fig. 1.23. We also note that not all image processing applications \\nrequire the complexity of interactions implied by Fig. 1.23. In fact, not even all those \\nmodules are needed in many cases. For example, image enhancement for human \\nvisual interpretation seldom requires use of any of the other stages in Fig. 1.23. In \\ngeneral, however, as the complexity of an image processing task increases, so does \\nthe number of processes required to solve the problem.\\nDIP4E_GLOBAL_Print_Ready.indb   43\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 44}),\n",
       " Document(page_content='44\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\n1.5 COMPONENTS OF AN IMAGE PROCESSING SYSTEM  \\nAs recently as the mid-1980s, numerous models of image processing systems being \\nsold throughout the world were rather substantial peripheral devices that attached \\nto equally substantial host computers. Late in the 1980s and early in the 1990s, the \\nmarket shifted to image processing hardware in the form of single boards designed \\nto be compatible with industry standard buses and to ﬁt into engineering work-\\nstation cabinets and personal computers. In the late 1990s and early 2000s, a new \\nclass of add-on boards, called graphics processing units (GPUs) were introduced for \\nwork on 3-D applications, such as games and other 3-D graphics applications. It was \\nnot long before GPUs found their way into image processing applications involving \\nlarge-scale matrix implementations, such as training deep convolutional networks.', metadata={'source': 'imagepro.pdf', 'page': 45}),\n",
       " Document(page_content='In addition to lowering costs, the market shift from substantial peripheral devices to \\nadd-on processing boards also served as a catalyst for a signiﬁcant number of new \\ncompanies specializing in the development of software written speciﬁcally for image \\nprocessing. \\nThe trend continues toward miniaturizing and blending of general-purpose small \\ncomputers with specialized image processing hardware and software. Figure 1.24 \\nshows the basic components comprising a typical general-purpose system used for \\ndigital image processing. The function of each component will be discussed in the \\nfollowing paragraphs, starting with image sensing.\\nTwo subsystems are required to acquire digital images. The ﬁrst is a physical \\nsen-\\nsor\\n that responds to the energy radiated by the object we wish to image. The second, \\ncalled a \\ndigitizer\\n, is a device for converting the output of the physical sensing device \\ninto digital form. For instance, in a digital video camera, the sensors (CCD chips)', metadata={'source': 'imagepro.pdf', 'page': 45}),\n",
       " Document(page_content='produce an electrical output proportional to light intensity. The digitizer converts \\nthese outputs to digital data. These topics will be covered in Chapter 2.\\nSpecialized image processing hardware usually consists of the digitizer just men-\\ntioned, plus hardware that performs other primitive operations, such as an \\narithme-\\ntic logic unit\\n (ALU), that performs arithmetic and logical operations in parallel on \\nentire images. One example of how an ALU is used is in averaging images as quickly \\nas they are digitized, for the purpose of noise reduction. This type of hardware some-\\ntimes is called a \\nfront-end subsystem\\n, and its most distinguishing characteristic is \\nspeed. In other words, this unit performs functions that require fast data through-\\nputs (e.g., digitizing and averaging video images at 30 frames/s) that the typical main \\ncomputer cannot handle. One or more GPUs (see above) also are common in image \\nprocessing systems that perform intensive matrix operations.\\nThe \\ncomputer', metadata={'source': 'imagepro.pdf', 'page': 45}),\n",
       " Document(page_content='The \\ncomputer\\n in an image processing system is a general-purpose computer and \\ncan range from a PC to a supercomputer. In dedicated applications, sometimes cus-\\ntom computers are used to achieve a required level of performance, but our interest \\nhere is on general-purpose image processing systems. In these systems, almost any \\nwell-equipped PC-type machine is suitable for off-line image processing tasks.\\nSoftware\\n for image processing consists of specialized modules that perform \\nspeciﬁc tasks. A well-designed package also includes the capability for the user to \\nwrite code that, as a minimum, utilizes the specialized modules. More sophisticated \\n1.5\\nDIP4E_GLOBAL_Print_Ready.indb   44\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 45}),\n",
       " Document(page_content='1.5\\n  \\nComponents of an Image Processing System\\n45\\nsoftware packages allow the integration of those modules and general-purpose \\nsoftware commands from at least one computer language. Commercially available \\nimage processing software, such as the well-known MATLAB\\n®\\n Image Processing \\nToolbox, is also common in a well-equipped image processing system. \\nMass storage\\n is a must in image processing applications. An image of size \\n1024 1024\\n×\\npixels, in which the intensity of each pixel is an 8-bit quantity,  requires one megabyte  \\nof storage space if the image is not compressed.\\n When dealing with image databases \\nthat contain thousands, or even millions, of images, providing adequate storage in \\nan image processing system can be a challenge. Digital storage for image processing \\napplications falls into three principal categories: (1) short-term storage for use dur-\\ning processing; (2) on-line storage for relatively fast recall; and (3) archival storage,', metadata={'source': 'imagepro.pdf', 'page': 46}),\n",
       " Document(page_content='characterized by infrequent access. Storage is measured in bytes (eight bits), Kbytes \\n(10\\n3\\n bytes), Mbytes (\\n10\\n6\\n bytes), Gbytes (\\n10\\n9\\n bytes), and Tbytes (\\n10\\n12\\n bytes).\\noud\\nud\\nCloud\\nImage displays\\nComputer\\nMass storage\\nHardcopy\\nSpecialized\\nimage processing\\nhardware\\nImage sensors\\nProblem\\ndomain\\nImage processing\\nsoftware\\nNetwork\\nCloud\\nFIGURE 1.24\\nComponents of a \\ngeneral-purpose \\nimage processing \\nsystem. \\nDIP4E_GLOBAL_Print_Ready.indb   45\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 46}),\n",
       " Document(page_content='46\\n    \\nChapter\\n \\n1\\n  \\nIntroduction\\nOne method of providing short-term storage is computer memory. Another is by \\nspecialized boards, called \\nframe buffers\\n, that store one or more images and can be \\naccessed rapidly, usually at video rates (e.g., at 30 complete images per second). The \\nlatter method allows virtually instantaneous image \\nzoom\\n, as well as \\nscroll\\n (vertical \\nshifts) and \\npan\\n (horizontal shifts). Frame buffers usually are housed in the special-\\nized image processing hardware unit in Fig. 1.24. On-line storage generally takes \\nthe form of magnetic disks or optical-media storage. The key factor characterizing \\non-line storage is frequent access to the stored data. Finally, archival storage is char-\\nacterized by massive storage requirements but infrequent need for access. Magnetic \\ntapes and optical disks housed in “jukeboxes” are the usual media for archival appli-\\ncations.\\nImage displays\\n in use today are mainly color, ﬂat screen monitors. Monitors are', metadata={'source': 'imagepro.pdf', 'page': 47}),\n",
       " Document(page_content='driven by the outputs of image and graphics display cards that are an integral part of \\nthe computer system. Seldom are there requirements for image display applications \\nthat cannot be met by display cards and GPUs available commercially as part of the \\ncomputer system. In some cases, it is necessary to have stereo displays, and these are \\nimplemented in the form of headgear containing two small displays embedded in \\ngoggles worn by the user.\\nHardcopy\\n devices for recording images include laser printers, ﬁlm cameras, heat-\\nsensitive devices, ink-jet units, and digital units, such as optical and CD-ROM disks. \\nFilm provides the highest possible resolution, but paper is the obvious medium of \\nchoice for written material. For presentations, images are displayed on ﬁlm trans-\\nparencies or in a digital medium if image projection equipment is used. The latter \\napproach is gaining acceptance as the standard for image presentations.\\nNetworking\\n and \\ncloud', metadata={'source': 'imagepro.pdf', 'page': 47}),\n",
       " Document(page_content='and \\ncloud\\n communication are almost default functions in any com-\\nputer system in use today. Because of the large amount of data inherent in image \\nprocessing applications, the key consideration in image transmission is \\nbandwidth\\n. In \\ndedicated networks, this typically is not a problem, but communications with remote \\nsites via the internet are not always as efﬁcient. Fortunately, transmission bandwidth \\nis improving quickly as a result of optical ﬁber and other broadband technologies. \\nImage data compression continues to play a major role in the transmission of large \\namounts of image data.\\nSummary, References, and Further Reading\\n  \\nThe main purpose of the material presented in this chapter is to provide a sense of perspective about the origins \\nof digital image processing and, more important, about current and future areas of application of this technology. \\nAlthough the coverage of these topics in this chapter was necessarily incomplete due to space limitations, it should', metadata={'source': 'imagepro.pdf', 'page': 47}),\n",
       " Document(page_content='have left you with a clear impression of the breadth and practical scope of digital image processing. As we proceed \\nin the following chapters with the development of image processing theory and applications, numerous examples \\nare provided to keep a clear focus on the utility and promise of these techniques. Upon concluding the study of the \\nﬁnal chapter, a reader of this book will have arrived at a level of understanding that is the foundation for most of \\nthe work currently underway in this ﬁeld. \\nIn past editions, we have provided a long list of journals and books to give readers an idea of the breadth of the \\nimage processing literature, and where this literature is reported. The list has been updated, and it has become so \\nextensive that it is more practical to include it in the book website: \\nwww.ImageProcessingPlace.com\\n, in the section \\nentitled \\nPublications\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   46\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 47}),\n",
       " Document(page_content='472\\nDigital Image Fundamentals\\nPreview\\nThis chapter is an introduction to a number of basic concepts in digital image processing that are used \\nthroughout the book. Section 2.1 summarizes some important aspects of the human visual system, includ-\\ning image formation in the eye and its capabilities for brightness adaptation and discrimination. Section \\n2.2 discusses light, other components of the electromagnetic spectrum, and their imaging characteristics. \\nSection 2.3 discusses imaging sensors and how they are used to generate digital images. Section 2.4 intro-\\nduces the concepts of uniform image sampling and intensity quantization. Additional topics discussed \\nin that section include digital image representation, the effects of varying the number of samples and \\nintensity levels in an image, the concepts of spatial and intensity resolution, and the principles of image \\ninterpolation. Section 2.5 deals with a variety of basic relationships between pixels. Finally, Section 2.6', metadata={'source': 'imagepro.pdf', 'page': 48}),\n",
       " Document(page_content='is an introduction to the principal mathematical tools we use throughout the book. A second objective \\nof that section is to help you begin developing a “feel” for how these tools are used in a variety of basic \\nimage processing tasks. \\nUpon completion of this chapter, readers should:\\n Have an understanding of some important \\nfunctions and limitations of human vision.\\n Be familiar with the electromagnetic energy \\nspectrum, including basic properties of light.\\n Know how digital images are generated and \\nrepresented.\\n Understand the basics of image sampling and \\nquantization.\\n Be familiar with spatial and intensity resolu-\\ntion and their effects on image appearance.\\n Have an understanding of basic geometric \\nrelationships between image pixels.\\n Be familiar with the principal mathematical \\ntools used in digital image processing.\\n Be able to apply a variety of introductory dig-\\nital image processing techniques.\\nThose who wish to succeed must ask the right preliminary \\nquestions.\\nAristotle', metadata={'source': 'imagepro.pdf', 'page': 48}),\n",
       " Document(page_content='Aristotle\\nDIP4E_GLOBAL_Print_Ready.indb   47\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 48}),\n",
       " Document(page_content='48\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n2.1 ELEMENTS OF VISUAL PERCEPTION  \\nAlthough the field of digital image processing is built on a foundation of mathemat-\\nics, human intuition and analysis often play a role in the choice of one technique \\nversus another, and this choice often is made based on subjective, visual judgments. \\nThus, developing an understanding of basic characteristics of human visual percep-\\ntion as a first step in our journey through this book is appropriate. In particular, our \\ninterest is in the elementary mechanics of how images are formed and perceived \\nby humans. We are interested in learning the physical limitations of human vision \\nin terms of factors that also are used in our work with digital images. Factors such \\nas how human and electronic imaging devices compare in terms of resolution and \\nability to adapt to changes in illumination are not only interesting, they are also \\nimportant from a practical point of view.\\nSTRUCTURE OF THE HUMAN EYE', metadata={'source': 'imagepro.pdf', 'page': 49}),\n",
       " Document(page_content='Figure 2.1 shows a simplified cross section of the human eye. The eye is nearly a \\nsphere (with a diameter of about 20 mm) enclosed by three membranes: the \\ncornea\\n \\nand \\nsclera\\n outer cover; the \\nchoroid\\n; and the \\nretina\\n. The cornea is a tough, transparent \\ntissue that covers the anterior surface of the eye. Continuous with the cornea, the \\nsclera is an opaque membrane that encloses the remainder of the optic globe.\\nThe choroid lies directly below the sclera. This membrane contains a network of \\nblood vessels that serve as the major source of nutrition to the eye. Even superﬁcial \\n2.1\\nRetina\\nBlind spot\\nSclera\\nChoroid\\nNerve & sheath\\nFovea\\nVitreous humor\\nVisual axis\\nCiliary fibers\\nCiliary muscle\\nIris\\nCornea\\nLens\\nAnterior chamber\\nCiliary body\\nFIGURE 2.1\\nSimpliﬁed  \\ndiagram of a  \\ncross section of \\nthe human eye.\\nDIP4E_GLOBAL_Print_Ready.indb   48\\n6/16/2017   2:02:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 49}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n    \\n49\\ninjury to the choroid can lead to severe eye damage as a result of inﬂammation that \\nrestricts blood ﬂow. The choroid coat is heavily pigmented, which helps reduce the \\namount of extraneous light entering the eye and the backscatter within the optic \\nglobe. At its anterior extreme, the choroid is divided into the \\nciliary body\\n and the \\niris\\n. The latter contracts or expands to control the amount of light that enters the eye. \\nThe central opening of the iris (the \\npupil\\n) varies in diameter from approximately 2 \\nto 8 mm. The front of the iris contains the visible pigment of the eye, whereas the \\nback contains a black pigment.\\nThe \\nlens\\n consists of concentric layers of ﬁbrous cells and is suspended by ﬁbers \\nthat attach to the ciliary body. It is composed of 60% to 70% water, about 6% fat, \\nand more protein than any other tissue in the eye. The lens is colored by a slightly', metadata={'source': 'imagepro.pdf', 'page': 50}),\n",
       " Document(page_content='yellow pigmentation that increases with age. In extreme cases, excessive clouding of \\nthe lens, referred to as \\ncataracts\\n, can lead to poor color discrimination and loss of \\nclear vision. The lens absorbs approximately 8% of the visible light spectrum, with \\nhigher absorption at shorter wavelengths. Both infrared and ultraviolet light are \\nabsorbed by proteins within the lens and, in excessive amounts, can damage the eye.\\nThe innermost membrane of the eye is the \\nretina\\n, which lines the inside of the \\nwall’s entire posterior portion. When the eye is focused, light from an object is \\nimaged on the retina. Pattern vision is afforded by discrete light receptors distrib-\\nuted over the surface of the retina. There are two types of receptors: \\ncones\\n and \\nrods\\n. \\nThere are between 6 and 7 million cones in each eye. They are located primarily in \\nthe central portion of the retina, called the \\nfovea\\n, and are highly sensitive to color.', metadata={'source': 'imagepro.pdf', 'page': 50}),\n",
       " Document(page_content='Humans can resolve ﬁne details because each cone is connected to its own nerve end. \\nMuscles rotate the eye until the image of a region of interest falls on the fovea. Cone \\nvision is called \\nphotopic\\n or \\nbright-light\\n vision.\\nThe number of rods is much larger: Some 75 to 150 million are distributed over \\nthe retina. The larger area of distribution, and the fact that several rods are connect-\\ned to a single nerve ending, reduces the amount of detail discernible by these recep-\\ntors. Rods capture an overall image of the ﬁeld of view. They are not involved in \\ncolor vision, and are sensitive to low levels of illumination. For example, objects that \\nappear brightly colored in daylight appear as colorless forms in moonlight because \\nonly the rods are stimulated. This phenomenon is known as \\nscotopic\\n or \\ndim-light\\n \\nvision.\\nFigure 2.2\\n \\nshows the density of rods and cones for a cross section of the right eye, \\npassing through the region where the optic nerve emerges from the eye', metadata={'source': 'imagepro.pdf', 'page': 50}),\n",
       " Document(page_content='. The absence \\nof receptors in this area causes the so-called \\nblind spot\\n (see Fig. 2.1). Except for this \\nregion, the distribution of receptors is radially symmetric about the fovea. Receptor \\ndensity is measured in degrees from the visual axis. Note in Fig. 2.2 that cones are \\nmost dense in the center area of the fovea, and that rods increase in density from \\nthe center out to approximately 20° off axis. Then, their density decreases out to the \\nperiphery of the retina.\\nThe fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, \\nso it has an area of approximately 1.77 \\nmm\\n2\\n.\\n As Fig. 2.2 shows, the density of cones \\nin that area of the retina is on the order of 150,000 elements per \\nmm\\n2\\n. Based on \\nthese ﬁgures, the number of cones in the fovea, which is the region of highest acuity \\nDIP4E_GLOBAL_Print_Ready.indb   49\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 50}),\n",
       " Document(page_content='50\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nin the eye, is about 265,000 elements. Modern electronic imaging chips exceed this \\nnumber by a large factor. While the ability of humans to integrate intelligence and \\nexperience with vision makes purely quantitative comparisons somewhat superﬁcial, \\nkeep in mind for future discussions that electronic imaging sensors can easily exceed \\nthe capability of the eye in resolving image detail.\\nIMAGE FORMATION IN THE EYE\\nIn an ordinary photographic camera, the lens has a fixed focal length. Focusing at \\nvarious distances is achieved by varying the distance between the lens and the imag-\\ning plane, where the film (or imaging chip in the case of a digital camera) is located. \\nIn the human eye, the converse is true; the distance between the center of the lens \\nand the imaging sensor (the retina) is fixed, and the focal length needed to achieve \\nproper focus is obtained by varying the shape of the lens. The fibers in the ciliary', metadata={'source': 'imagepro.pdf', 'page': 51}),\n",
       " Document(page_content='body accomplish this by flattening or thickening the lens for distant or near ob-\\njects, respectively. The distance between the center of the lens and the retina along \\nthe visual axis is approximately 17 mm. The range of focal lengths is approximately \\n14 mm to 17 mm, the latter taking place when the eye is relaxed and focused at dis-\\ntances greater than about 3 m. The geometry in Fig. 2.3 illustrates how to obtain the \\ndimensions of an image formed on the retina. For example, suppose that a person \\nis looking at a tree 15 m high at a distance of 100 m. Letting \\nh\\n denote the height \\nof that object in the retinal image, the geometry of Fig. 2.3 yields \\n15 100 17\\n=\\nh\\n or \\nh\\n=\\n25\\n.\\n mm.\\n As indicated earlier in this section, the retinal image is focused primar-\\nily on the region of the fovea.\\n Perception then takes place by the relative excitation \\nof light receptors, which transform radiant energy into electrical impulses that ulti-\\nmately are decoded by the brain.', metadata={'source': 'imagepro.pdf', 'page': 51}),\n",
       " Document(page_content='BRIGHTNESS ADAPTATION AND DISCRIMINATION\\nBecause digital images are displayed as sets of discrete intensities, the eye’s abil-\\nity to discriminate between different intensity levels is an important consideration \\nFIGURE 2.2\\nDistribution of \\nrods and cones in \\nthe retina.\\nBlind spot\\nCones\\nRods\\nNo. of rods or cones per mm\\n2\\nDegrees from visual axis (center of fovea)\\n180,000\\n135,000\\n90,000\\n45,000\\n80\\n/H11034\\n60\\n/H11034\\n40\\n/H11034\\n20\\n/H11034\\n0\\n/H11034\\n20\\n/H11034\\n40\\n/H11034\\n60\\n/H11034\\n80\\n/H11034\\nDIP4E_GLOBAL_Print_Ready.indb   50\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 51}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n51\\nin presenting image processing results. The range of light intensity levels to which \\nthe human visual system can adapt is enormous—on the order of \\n10\\n10\\n— from the \\nscotopic threshold to the glare limit. Experimental evidence indicates that \\nsubjec-\\ntive brightness\\n (intensity as perceived by the human visual system) is a logarithmic \\nfunction of the light intensity incident on the eye. Figure 2.4, a plot of light inten-\\nsity versus subjective brightness, illustrates this characteristic. The long solid curve \\nrepresents the range of intensities to which the visual system can adapt. In photopic \\nvision alone, the range is about \\n10\\n6\\n. The transition from scotopic to photopic vision \\nis gradual over the approximate range from 0.001 to 0.1 millilambert (\\n−\\n3\\n to \\n−\\n1\\n mL \\nin the log scale),\\n as the double branches of the adaptation curve in this range show.\\nThe key point in interpreting the impressive dynamic range depicted in Fig. 2.4', metadata={'source': 'imagepro.pdf', 'page': 52}),\n",
       " Document(page_content='is that the visual system cannot operate over such a range \\nsimultaneously\\n. Rather, it \\naccomplishes this large variation by changing its overall sensitivity, a phenomenon \\nknown as \\nbrightness adaptation\\n. The total range of distinct intensity levels the eye \\ncan discriminate simultaneously is rather small when compared with the total adap-\\ntation range. For a given set of conditions, the current sensitivity level of the visual \\nsystem is called the \\nbrightness adaptation level\\n, which may correspond, for example, \\nFIGURE 2.3\\nGraphical  \\nrepresentation of \\nthe eye looking at \\na palm tree. Point \\nC \\nis the focal  \\ncenter of the lens.\\n15 m\\nC\\n17 mm\\n100 m\\nFIGURE 2.4\\nRange of subjec-\\ntive brightness \\nsensations  \\nshowing a  \\nparticular  \\nadaptation level, \\nB\\na\\n.\\nGlare limit\\nSubjective brightness\\nAdaptation range\\nScotopic\\nthreshold\\nLog of intensity (mL)\\nScotopic\\nPhotopic\\n/H11002\\n6\\n/H11002\\n4\\n/H11002\\n20 24\\nB\\na\\nB\\nb\\nDIP4E_GLOBAL_Print_Ready.indb   51\\n6/16/2017   2:02:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 52}),\n",
       " Document(page_content='52\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nto brightness \\nB\\na\\n in Fig. 2.4. The short intersecting curve represents the range of sub-\\njective brightness that the eye can perceive when adapted to \\nthis\\n level. This range is \\nrather restricted, having a level \\nB\\nb\\n at, and below which, all stimuli are perceived as \\nindistinguishable blacks. The upper portion of the curve is not actually restricted but, \\nif extended too far, loses its meaning because much higher intensities would simply \\nraise the adaptation level higher than \\nB\\na\\n.\\nThe ability of the eye to discriminate between \\nc\\nhanges\\n in light intensity at any \\nspeciﬁc adaptation level is of considerable interest. A classic experiment used to \\ndetermine the capability of the human visual system for brightness discrimination \\nconsists of having a subject look at a ﬂat, uniformly illuminated area large enough to \\noccupy the entire ﬁeld of view. This area typically is a diffuser, such as opaque glass,', metadata={'source': 'imagepro.pdf', 'page': 53}),\n",
       " Document(page_content='illuminated from behind by a light source, \\nI\\n, with variable intensity. To this ﬁeld is \\nadded an increment of illumination, \\n/H9004\\nI\\n, in the form of a short-duration ﬂash that \\nappears as a circle in the center of the uniformly illuminated ﬁeld,\\n as Fig. 2.5 shows.\\nIf \\n/H9004\\nI\\n is not bright enough, the subject says “no,” indicating no perceivable change. \\nAs \\n/H9004\\nI\\n gets stronger, the subject may give a positive response of “ye\\ns,” \\nindicating a \\nperceived change\\n. Finally, when \\n/H9004\\nI\\n is strong enough, the subject will give a response \\nof \\n“yes” all the time. The quantity \\n/H9004\\nII\\nc\\n, where \\n/H9004\\nI\\nc\\n is the increment of illumination \\ndiscriminable 50% of the time with background illumination \\nI\\n, is called the \\nWeber \\nratio\\n. A small value of \\n/H9004\\nII\\nc\\n means that a small percentage change in intensity is \\ndiscriminable. This represents “good” brightness discrimination. Conversely, a large \\nvalue of \\n/H9004\\nII\\nc', metadata={'source': 'imagepro.pdf', 'page': 53}),\n",
       " Document(page_content='/H9004\\nII\\nc\\n means that a large percentage change in intensity is required for the \\neye to detect the change. This represents “poor” brightness discrimination.\\nA plot of \\n/H9004\\nII\\nc\\n as a function of \\nlog\\nI\\n has the characteristic shape shown in Fig. 2.6. \\nT\\nhis curve shows that brightness discrimination is poor (the Weber ratio is large) at \\nlow levels of illumination, and it improves signiﬁcantly (the Weber ratio decreases) \\nas background illumination increases. The two branches in the curve reﬂect the fact \\nthat at low levels of illumination vision is carried out by the rods, whereas, at high \\nlevels, vision is a function of cones.\\nIf the background illumination is held constant and the intensity of the other \\nsource, instead of ﬂashing, is now allowed to vary incrementally from never being \\nperceived to always being perceived, the typical observer can discern a total of one \\nto two dozen different intensity changes. Roughly, this result is related to the num-', metadata={'source': 'imagepro.pdf', 'page': 53}),\n",
       " Document(page_content='ber of different intensities a person can see at any one\\n point \\nor\\n small area\\n in a mono-\\nchrome image. This does not mean that an image can be represented by \\nsuch a \\nsmall \\nnumber of intensity values because, as the eye roams about the image, the average \\nFIGURE 2.5  \\nBasic\\nexperimental  \\nsetup used to \\ncharacterize \\nbrightness  \\ndiscrimination.\\nI\\nI\\n \\n/H9004\\nI\\n+\\nDIP4E_GLOBAL_Print_Ready.indb   52\\n6/16/2017   2:02:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 53}),\n",
       " Document(page_content='2.1\\n  \\nElements of Visual Perception\\n    \\n53\\nbackground changes, thus allowing a \\ndifferent\\n set of incremental changes to be detect-\\ned at each new adaptation level. The net result is that the eye is capable of a broader \\nrange of \\noverall\\n intensity discrimination. In fact, as we will show in Section 2.4, the eye \\nis capable of detecting objectionable effects in monochrome images whose overall \\nintensity is represented by fewer than approximately two dozen levels.\\nTwo phenomena demonstrate that perceived brightness is not a simple function \\nof intensity. The ﬁrst is based on the fact that the visual system tends to undershoot \\nor overshoot around the boundary of regions of different intensities. Figure 2.7(a) \\nshows a striking example of this phenomenon. Although the intensity of the stripes \\nFIGURE 2.6\\nA typical plot of \\nthe Weber ratio \\nas a function of \\nintensity.\\n/H11002\\n1.5\\n/H11002\\n2.0\\n/H11002\\n4\\n/H11002\\n3\\n/H11002\\n2\\n/H11002\\n10\\nlog \\nI\\nlog \\n/H9004\\nI\\nc\\n/\\nI\\n1234\\n/H11002\\n1.0\\n/H11002', metadata={'source': 'imagepro.pdf', 'page': 54}),\n",
       " Document(page_content='/H11002\\n1.0\\n/H11002\\n0.5\\n0.5\\n1.0\\n0\\nActual intensity\\nPerceived intensity\\nFIGURE 2.7\\nIllustration of the \\nMach band effect. \\nPerceived  \\nintensity is not a \\nsimple function of \\nactual intensity.\\nb\\na\\nc\\nDIP4E_GLOBAL_Print_Ready.indb   53\\n6/16/2017   2:02:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 54}),\n",
       " Document(page_content='54\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nis constant [see Fig. 2.7(b)], we actually perceive a brightness pattern that is strongly \\nscalloped near the boundaries, as Fig. 2.7(c) shows. These perceived scalloped bands \\nare called \\nMach bands\\n after Ernst Mach, who ﬁrst described the phenomenon in 1865.\\nThe second phenomenon, called \\nsimultaneous contrast\\n, is that a region’s per-\\nceived brightness does not depend only on its intensity, as Fig. 2.8 demonstrates. All \\nthe center squares have exactly the same intensity, but each appears to the eye to \\nbecome darker as the background gets lighter. A more familiar example is a piece of \\npaper that looks white when lying on a desk, but can appear totally black when used \\nto shield the eyes while looking directly at a bright sky.\\nOther examples of human perception phenomena are \\noptical illusions\\n, in which \\nthe eye ﬁlls in nonexisting details or wrongly perceives geometrical properties of', metadata={'source': 'imagepro.pdf', 'page': 55}),\n",
       " Document(page_content='objects. Figure 2.9 shows some examples. In Fig. 2.9(a), the outline of a square is \\nseen clearly, despite the fact that no lines deﬁning such a ﬁgure are part of the image. \\nThe same effect, this time with a circle, can be seen in Fig. 2.9(b); note how just a few \\nlines are sufﬁcient to give the illusion of a complete circle. The two horizontal line \\nsegments in Fig. 2.9(c) are of the same length, but one appears shorter than the other. \\nFinally, all long lines in Fig. 2.9(d) are equidistant and parallel. Yet, the crosshatching \\ncreates the illusion that those lines are far from being parallel.\\n2.2 LIGHT AND THE ELECTROMAGNETIC SPECTRUM  \\nThe electromagnetic spectrum was introduced in Section 1.3. We now consider this \\ntopic in more detail. In 1666, Sir Isaac Newton discovered that when a beam of \\nsunlight passes through a glass prism, the emerging beam of light is not white but \\nconsists instead of a continuous spectrum of colors ranging from violet at one end', metadata={'source': 'imagepro.pdf', 'page': 55}),\n",
       " Document(page_content='to red at the other. As Fig. 2.10 shows, the range of colors we perceive in visible light \\nis a small portion of the electromagnetic spectrum. On one end of the spectrum are \\nradio waves with wavelengths billions of times longer than those of visible light. On \\nthe other end of the spectrum are gamma rays with wavelengths millions of times \\nsmaller than those of visible light. We showed examples in Section 1.3 of images in \\nmost of the bands in the EM spectrum.\\n2.2\\nb a\\nc\\nFIGURE 2.8\\n Examples of simultaneous contrast. All the inner squares have the same intensity, \\nbut they appear progressively darker as the background becomes lighter.\\nDIP4E_GLOBAL_Print_Ready.indb   54\\n6/16/2017   2:02:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 55}),\n",
       " Document(page_content='2.2\\n  \\nLight and the Electromagnetic Spectrum\\n    \\n55\\nThe electromagnetic spectrum can be expressed in terms of wavelength, frequency, \\nor energy. Wavelength (\\nl\\n) and frequency (\\nn\\n) are related by the expression \\n \\nl\\nn\\n=\\nc\\n \\n(2-1)\\nwhere \\nc\\n is the speed of light (\\n2\\n998 10\\n8\\n.\\n*\\n \\nm/s). Figure 2.11 shows a schematic repre-\\nsentation of one wavelength.\\n \\nThe energy of the various components of the electromagnetic spectrum is given \\nby the expression\\n \\nEh\\n=\\nn\\n \\n(2-2)\\nwhere \\nh\\n is Planck’\\ns constant. The units of wavelength are meters, with the terms \\nmicrons\\n (denoted \\nm\\nm\\n and equal to \\n10\\n6\\n−\\n m) and \\nnanometers\\n (denoted nm and equal \\nto 10\\n9\\n−\\n m) being used just as frequently. Frequency is measured in \\nHertz\\n (Hz), with \\none Hz being equal to one cycle of a sinusoidal wave per second. A commonly used \\nunit of energy is the \\nelectron-volt\\n.\\nElectromagnetic waves can be visualized as propagating sinusoidal waves with \\nwavelength \\nl', metadata={'source': 'imagepro.pdf', 'page': 56}),\n",
       " Document(page_content='wavelength \\nl\\n (Fig. 2.11), or they can be thought of as a stream of massless particles, \\nb a\\nd c\\nFIGURE 2.9  \\nSome \\nwell-known  \\noptical illusions.\\nDIP4E_GLOBAL_Print_Ready.indb   55\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 56}),\n",
       " Document(page_content='56\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\neach traveling in a wavelike pattern and moving at the speed of light. Each mass-\\nless particle contains a certain amount (or bundle) of energy, called a \\nphoton\\n. We \\nsee from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency \\n(shorter wavelength) electromagnetic phenomena carry more energy per photon. \\nThus, radio waves have photons with low energies, microwaves have more energy \\nthan radio waves, infrared still more, then visible, ultraviolet, X-rays, and ﬁnally \\ngamma rays, the most energetic of all. High-energy electromagnetic radiation, espe-\\ncially in the X-ray and gamma ray bands, is particularly harmful to living organisms. \\nLight is a type of electromagnetic radiation that can be sensed by the eye. The \\nvisible (color) spectrum is shown expanded in Fig. 2.10 for the purpose of discussion \\n(we will discuss color in detail in Chapter 6). The visible band of the electromag-', metadata={'source': 'imagepro.pdf', 'page': 57}),\n",
       " Document(page_content='netic spectrum spans the range from approximately 0.43 \\nm\\nm\\n (violet) to about 0.79 \\nm\\nm\\n (red). For convenience, the color spectrum is divided into six broad regions: \\nviolet,\\n blue, green, yellow, orange, and red. No color (or other component of the \\nRadio waves\\nMicrowaves\\nInfrared\\nVisible spectrum\\nUltraviolet\\nGamma rays X-rays\\n0.4 \\n/H11003\\n 10\\n/H11002\\n6\\n0.5 \\n/H11003\\n 10\\n/H11002\\n6\\n0.6 \\n/H11003\\n 10\\n/H11002\\n6\\n0.7 \\n/H11003\\n 10\\n/H11002\\n6\\nInfrared\\nUltraviolet Violet Blue Green Yellow Red\\nOrange\\n10\\n5\\n10\\n6\\n10\\n7\\n10\\n8\\n10\\n9\\n10\\n10\\n10\\n11\\n10\\n12\\n10\\n13\\n10\\n14\\n10\\n15\\n10\\n16\\n10\\n17\\n10\\n18\\n10\\n19\\n10\\n20\\n10\\n21\\nFrequency (Hz)\\n10\\n/H11002\\n9\\n10\\n/H11002\\n8\\n10\\n/H11002\\n7\\n10\\n/H11002\\n6\\n10\\n/H11002\\n5\\n10\\n/H11002\\n4\\n10\\n/H11002\\n3\\n10\\n/H11002\\n2\\n10\\n/H11002\\n1\\n1\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n5\\n10\\n6\\nEnergy of one photon (electron volts)\\n10\\n3\\n10\\n2\\n10\\n1\\n1\\n10\\n/H11002\\n1\\n10\\n/H11002\\n2\\n10\\n/H11002\\n3\\n10\\n/H11002\\n4\\n10\\n/H11002\\n5\\n10\\n/H11002\\n6\\n10\\n/H11002\\n7\\n10\\n/H11002\\n8\\n10\\n/H11002\\n9\\n10\\n/H11002\\n10\\n10\\n/H11002\\n11\\n10\\n/H11002\\n12\\nWavelength (meters)', metadata={'source': 'imagepro.pdf', 'page': 57}),\n",
       " Document(page_content='Wavelength (meters)\\nFIGURE 2.10\\n  The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanations, but note \\nthat it encompasses a very narrow range of the total EM spectrum.\\nl\\nFIGURE 2.11\\nGraphical  \\nrepresentation of \\none wavelength.\\nDIP4E_GLOBAL_Print_Ready.indb   56\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 57}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n    \\n57\\nelectromagnetic spectrum) ends abruptly; rather, each range blends smoothly into \\nthe next, as Fig. 2.10 shows.\\nThe colors perceived in an object are determined by the nature of the light \\nreﬂect-\\ned\\n by the object. A body that reﬂects light relatively balanced in all visible wave-\\nlengths appears white to the observer. However, a body that favors reﬂectance in \\na limited range of the visible spectrum exhibits some shades of color. For example, \\ngreen objects reﬂect light with wavelengths primarily in the 500 to 570 nm range, \\nwhile absorbing most of the energy at other wavelengths.\\nLight that is void of color is called \\nmonochromatic\\n (or \\nachromatic\\n) light. The \\nonly attribute of monochromatic light is its intensity. Because the intensity of mono-\\nchromatic light is perceived to vary from black to grays and ﬁnally to white, the \\nterm \\ngray level\\n is used commonly to denote monochromatic intensity (we use the \\nterms \\nintensity\\n and', metadata={'source': 'imagepro.pdf', 'page': 58}),\n",
       " Document(page_content='intensity\\n and \\ngray level\\n interchangeably in subsequent discussions). The range \\nof values of monochromatic light from black to white is usually called the \\ngray scale\\n, \\nand monochromatic images are frequently referred to as \\ngrayscale images\\n.\\nChromatic\\n (color) light spans the electromagnetic energy spectrum from approxi-\\nmately 0.43 to 0.79 \\nm\\nm,\\n as noted previously. In addition to frequency, three other \\nquantities are used to describe a chromatic light source:\\n radiance, luminance, and \\nbrightness. \\nRadiance\\n is the total amount of energy that ﬂows from the light source, \\nand it is usually measured in watts (W). \\nLuminance\\n, measured in lumens (lm), gives \\na measure of the amount of energy an observer \\nperceives\\n from a light source. For \\nexample, light emitted from a source operating in the far infrared region of the \\nspectrum could have signiﬁcant energy (radiance), but an observer would hardly', metadata={'source': 'imagepro.pdf', 'page': 58}),\n",
       " Document(page_content='perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, \\nbrightness\\n is a subjective descriptor of light perception that is practically impossible \\nto measure. It embodies the achromatic notion of intensity and is one of the key fac-\\ntors in describing color sensation.\\nIn principle, if a sensor can be developed that is capable of detecting energy \\nradiated in a band of the electromagnetic spectrum, we can image events of inter-\\nest in that band. Note, however, that the wavelength of an electromagnetic wave \\nrequired to “see” an object must be of the same size as, or smaller than, the object. \\nFor example, a water molecule has a diameter on the order of \\n10\\n10\\n−\\n m. Thus, to study \\nthese molecules, we would need a source capable of emitting energy in the far (high-\\nenergy) ultraviolet band or soft (low-energy) X-ray bands. \\nAlthough imaging is based predominantly on energy from electromagnetic wave', metadata={'source': 'imagepro.pdf', 'page': 58}),\n",
       " Document(page_content='radiation, this is not the only method for generating images. For example, we saw in \\nSection 1.3 that sound reﬂected from objects can be used to form ultrasonic images. \\nOther sources of digital images are electron beams for electron microscopy, and \\nsoftware for generating synthetic images used in graphics and visualization.\\n2.3 IMAGE SENSING AND ACQUISITION  \\nMost of the images in which we are interested are generated by the combination of \\nan “illumination” source and the reflection or absorption of energy from that source \\nby the elements of the “scene” being imaged. We enclose \\nillumination\\n and \\nscene\\n \\nin quotes to emphasize the fact that they are considerably more general than the \\n2.3\\nDIP4E_GLOBAL_Print_Ready.indb   57\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 58}),\n",
       " Document(page_content='58\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nfamiliar situation in which a visible light source illuminates a familiar 3-D scene. For \\nexample, the illumination may originate from a source of electromagnetic energy, \\nsuch as a radar, infrared, or X-ray system. But, as noted earlier, it could originate \\nfrom less traditional sources, such as ultrasound or even a computer-generated illu-\\nmination pattern. Similarly, the scene elements could be familiar objects, but they \\ncan just as easily be molecules, buried rock formations, or a human brain. Depend-\\ning on the nature of the source, illumination energy is reflected from, or transmitted \\nthrough, objects. An example in the first category is light reflected from a planar \\nsurface. An example in the second category is when X-rays pass through a patient’s \\nbody for the purpose of generating a diagnostic X-ray image. In some applications, \\nthe reflected or transmitted energy is focused onto a photo converter (e.g., a phos-', metadata={'source': 'imagepro.pdf', 'page': 59}),\n",
       " Document(page_content='phor screen) that converts the energy into visible light. Electron microscopy and \\nsome applications of gamma imaging use this approach. \\nFigure 2.12 shows the three principal sensor arrangements used to transform inci-\\ndent energy into digital images. The idea is simple: Incoming energy is transformed \\ninto a voltage by a combination of the input electrical power and sensor material \\nthat is responsive to the type of energy being detected. The output voltage wave-\\nform is the response of the sensor, and a digital quantity is obtained by digitizing that \\nresponse. In this section, we look at the principal modalities for image sensing and \\ngeneration. We will discuss image digitizing in Section 2.4.\\nIMAGE ACQUISITION USING A SINGLE SENSING ELEMENT\\nFigure 2.12(a) shows the components of a single sensing element. A familiar sensor \\nof this type is the photodiode, which is constructed of silicon materials and whose', metadata={'source': 'imagepro.pdf', 'page': 59}),\n",
       " Document(page_content='output is a voltage proportional to light intensity. Using a filter in front of a sensor \\nimproves its selectivity. For example, an optical green-transmission filter favors light \\nin the green band of the color spectrum. As a consequence, the sensor output would \\nbe stronger for green light than for other visible light components.\\nIn order to generate a 2-D image using a single sensing element, there has to \\nbe relative displacements in both the \\nx\\n- and \\ny\\n-directions between the sensor and \\nthe area to be imaged. Figure 2.13 shows an arrangement used in high-precision \\nscanning, where a ﬁlm negative is mounted onto a drum whose mechanical rotation \\nprovides displacement in one dimension. The sensor is mounted on a lead screw \\nthat provides motion in the perpendicular direction. A light source is contained \\ninside the drum. As the light passes through the ﬁlm, its intensity is modiﬁed by \\nthe ﬁlm density before it is captured by the sensor. This \"modulation\" of the light', metadata={'source': 'imagepro.pdf', 'page': 59}),\n",
       " Document(page_content='intensity causes corresponding variations in the sensor voltage, which are ultimately \\nconverted to image intensity levels by digitization. \\nThis method is an inexpensive way to obtain high-resolution images because \\nmechanical motion can be controlled with high precision. The main disadvantages \\nof this method are that it is slow and not readily portable. Other similar mechanical \\narrangements use a ﬂat imaging bed, with the sensor moving in two linear direc-\\ntions. These types of mechanical digitizers sometimes are referred to as \\ntransmission\\n \\nmicrodensitometers\\n. Systems in which light is reﬂected from the medium, instead \\nof passing through it, are called \\nreﬂection microdensitometers\\n. Another example \\nof imaging with a single sensing element places a laser source coincident with the \\nDIP4E_GLOBAL_Print_Ready.indb   58\\n6/16/2017   2:02:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 59}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n59\\nSensing material\\nVoltage waveform out\\nFilter\\nEnergy\\nPower in\\nHousing\\nb\\na\\nc\\nFIGURE 2.12\\n(a) Single sensing \\nelement. \\n(b) Line sensor.  \\n(c) Array sensor.\\nSensor\\nLinear motion\\nOne image line out\\nper increment of rotation\\nand full linear displacement\\nof sensor from left to right\\nFilm\\nRotation\\nFIGURE 2.13\\nCombining a \\nsingle sensing \\nelement with \\nmechanical  \\nmotion to  \\ngenerate a 2-D \\nimage.\\nDIP4E_GLOBAL_Print_Ready.indb   59\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 60}),\n",
       " Document(page_content='60\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nsensor. Moving mirrors are used to control the outgoing beam in a scanning pattern \\nand to direct the reﬂected laser signal onto the sensor. \\nIMAGE ACQUISITION USING SENSOR STRIPS\\nA geometry used more frequently than single sensors is an in-line sensor strip, as in \\nFig. 2.12(b). The strip provides imaging elements in one direction. Motion perpen-\\ndicular to the strip provides imaging in the other direction, as shown in Fig. 2.14(a). \\nThis arrangement is used in most flat bed scanners. Sensing devices with 4000 or \\nmore in-line sensors are possible. In-line sensors are used routinely in airborne \\nimaging applications, in which the imaging system is mounted on an aircraft that \\nflies at a constant altitude and speed over the geographical area to be imaged. One-\\ndimensional imaging sensor strips that respond to various bands of the electromag-\\nnetic spectrum are mounted perpendicular to the direction of flight. An imaging', metadata={'source': 'imagepro.pdf', 'page': 61}),\n",
       " Document(page_content='strip gives one line of an image at a time, and the motion of the strip relative to \\nthe scene completes the other dimension of a 2-D image. Lenses or other focusing \\nschemes are used to project the area to be scanned onto the sensors.\\nSensor strips in a ring conﬁguration are used in medical and industrial imaging \\nto obtain cross-sectional (“slice”) images of 3-D objects, as Fig. 2.14(b) shows. A \\nrotating X-ray source provides illumination, and X-ray sensitive sensors opposite \\nthe source collect the energy that passes through the object. This is the basis for \\nmedical and industrial computerized axial tomography (CAT) imaging, as indicated \\nin Sections 1.2 and 1.3. The output of the sensors is processed by reconstruction \\nalgorithms whose objective is to transform the sensed data into meaningful cross-\\nsectional images (see Section 5.11). In other words, images are not obtained directly \\nSensor strip\\nLinear \\nmotion\\nImaged area\\nOne image line out per\\nincrement of linear motion\\nImage', metadata={'source': 'imagepro.pdf', 'page': 61}),\n",
       " Document(page_content='Image\\nreconstruction\\n3-D object\\nLinear motion\\nSensor ring\\nX-ray source\\nCross-sectional images\\nof 3-D object\\nSource\\nrotation\\nb a\\nFIGURE 2.14\\n(a) Image  \\nacquisition using \\na linear sensor \\nstrip. (b) Image \\nacquisition using \\na circular sensor \\nstrip.\\nDIP4E_GLOBAL_Print_Ready.indb   60\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 61}),\n",
       " Document(page_content='2.3\\n  \\nImage Sensing and Acquisition\\n    \\n61\\nfrom the sensors by motion alone; they also require extensive computer process-\\ning. A 3-D digital volume consisting of stacked images is generated as the object is \\nmoved in a direction perpendicular to the sensor ring. Other modalities of imaging \\nbased on the CAT principle include magnetic resonance imaging (MRI) and posi-\\ntron emission tomography (PET). The illumination sources, sensors, and types of \\nimages are different, but conceptually their applications are very similar to the basic \\nimaging approach shown in Fig. 2.14(b).\\nIMAGE ACQUISITION USING SENSOR ARRAYS\\nFigure 2.12(c) shows individual sensing elements arranged in the form of a 2-D array. \\nElectromagnetic and ultrasonic sensing devices frequently are arranged in this man-\\nner. This is also the predominant arrangement found in digital cameras. A typical \\nsensor for these cameras is a CCD (charge-coupled device) array, which can be', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='manufactured with a broad range of sensing properties and can be packaged in rug-\\nged arrays of \\n4000 4000\\n*\\n elements or more. CCD sensors are used widely in digital \\ncameras and other light-sensing instruments\\n. The response of each sensor is pro-\\nportional to the integral of the light energy projected onto the surface of the sensor, \\na property that is used in astronomical and other applications requiring low noise \\nimages. Noise reduction is achieved by letting the sensor integrate the input light \\nsignal over minutes or even hours. Because the sensor array in Fig. 2.12(c) is two-\\ndimensional, its key advantage is that a complete image can be obtained by focusing \\nthe energy pattern onto the surface of the array. Motion obviously is not necessary, \\nas is the case with the sensor arrangements discussed in the preceding two sections.\\nFigure 2.15 shows the principal manner in which array sensors are used. This', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='ﬁgure shows the energy from an illumination source being reﬂected from a scene \\n(as mentioned at the beginning of this section, the energy also could be transmit-\\nted through the scene). The ﬁrst function performed by the imaging system in Fig. \\n2.15(c) is to collect the incoming energy and focus it onto an image plane. If the illu-\\nmination is light, the front end of the imaging system is an optical lens that projects \\nthe viewed scene onto the focal plane of the lens, as Fig. 2.15(d) shows. The sensor \\narray, which is coincident with the focal plane, produces outputs proportional to the \\nintegral of the light received at each sensor. Digital and analog circuitry sweep these \\noutputs and convert them to an analog signal, which is then digitized by another sec-\\ntion of the imaging system. The output is a digital image, as shown diagrammatically \\nin Fig. 2.15(e). Converting images into digital form is the topic of Section 2.4.\\nA SIMPLE IMAGE FORMATION MODEL', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='As introduced in Section 1.1, we denote images by two-dimensional functions of the \\nform \\nfx y\\n(,\\n)\\n. The value of \\nf\\n at spatial coordinates \\n(,)\\nxy\\n is a scalar quantity whose \\nphysical meaning is determined by the source of the image\\n, and whose values are \\nproportional to energy radiated by a physical source (e.g., electromagnetic waves). \\nAs a consequence, \\nfx y\\n(,\\n)\\n must be nonnegative\\n†\\n and finite; that is,\\n†\\n  Image intensities can become negative during processing, or as a result of interpretation. For example, in radar \\nimages, objects moving toward the radar often are interpreted as having negative velocities while objects moving \\naway are interpreted as having positive velocities. Thus, a velocity image might be coded as having both positive \\nand negative values. When storing and displaying images, we normally scale the intensities so that the smallest \\nnegative value becomes 0 (see Section 2.6 regarding intensity scaling).\\nIn some cases, the source', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='is imaged directly, as \\nin obtaining images of \\nthe sun.\\nDIP4E_GLOBAL_Print_Ready.indb   61\\n6/16/2017   2:02:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 62}),\n",
       " Document(page_content='62\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n \\n0\\n≤<\\nfx\\ny\\n(,)\\n/H11009\\n \\n(2-3)\\nFunction \\nfx y\\n(,\\n)\\n is characterized by two components: (1) the amount of source illu-\\nmination incident on the scene being viewed,\\n and (2) the amount of illumination \\nreflected by the objects in the scene. Appropriately, these are called the \\nillumination\\n \\nand \\nreflectance\\n components, and are denoted by \\nixy\\n(,\\n)\\n and \\nrxy\\n(,\\n)\\n, respectively. The \\ntwo functions combine as a product to form \\nfx y\\n(,\\n)\\n:\\n \\nfx y i x yrx y\\n(,\\n) (,)(,)\\n=\\n \\n(2-4)\\nwhere\\n \\n0\\n≤<\\nix\\ny\\n(,)\\n/H11009\\n \\n(2-5)\\nand\\n \\n01\\n≤≤\\nrx\\ny\\n(,)\\n \\n(2-6)\\nThus, reflectance is bounded by 0 (total absorption) and 1 (total reflectance). The \\nnature of \\nixy\\n(,\\n)\\n is determined by the illumination source, and \\nrxy\\n(,\\n)\\n is determined \\nby the characteristics of the imaged objects\\n. These expressions are applicable also \\nto images formed via transmission of the illumination through a medium, such as a \\nIllumination (energy)\\nsource\\nImaging system', metadata={'source': 'imagepro.pdf', 'page': 63}),\n",
       " Document(page_content='Imaging system\\n(Internal) image plane\\nOutput (digitized) image\\nScene\\nb\\na\\nd\\nc\\ne\\nFIGURE 2.15\\n  An example of digital image acquisition. (a) Illumination (energy) source. (b) A scene. (c) Imaging \\nsystem. (d) Projection of the scene onto the image plane. (e) Digitized image.\\nDIP4E_GLOBAL_Print_Ready.indb   62\\n6/16/2017   2:02:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 63}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n63\\nchest X-ray. In this case, we would deal with a \\ntransmissivity\\n instead of a \\nreflectivity\\n \\nfunction, but the limits would be the same as in Eq. (2-6), and the image function \\nformed would be modeled as the product in Eq. (2-4).\\nEXAMPLE 2.1 :  Some typical values of illumination and reﬂectance.\\nThe following numerical quantities illustrate some typical values of illumination and reﬂectance for \\nvisible light. On a clear day, the sun may produce in excess of \\n90 000\\n,\\n lm/m\\n2\\n of illumination on the sur-\\nface of the earth. This value decreases to less than \\n10 000\\n,\\n lm/m\\n2\\n on a cloudy day. On a clear evening, a \\nfull moon yields about \\n01\\n.\\n lm/m\\n2\\n of illumination. The typical illumination level in a commercial ofﬁce \\nis about \\n1 000\\n,\\n lm/m\\n2\\n. Similarly, the following are typical values of \\nrxy\\n(,\\n)\\n: 0.01 for black velvet, 0.65 for \\nstainless steel,\\n 0.80 for ﬂat-white wall paint, 0.90 for silver-plated metal, and 0.93 for snow.', metadata={'source': 'imagepro.pdf', 'page': 64}),\n",
       " Document(page_content='Let the intensity (gray level) of a monochrome image at any coordinates \\n(,)\\nxy\\n \\nbe denoted by \\n \\n/\\n=\\nfx\\ny\\n(,)\\n \\n(2-7)\\nFrom Eqs. (2-4) through (2-6) it is evident that \\n/\\n lies in the range\\n \\nLL\\nmin\\nmax\\n≤≤\\n/\\n \\n(2-8)\\nIn theory, the requirement on \\nL\\nmin\\n is that it be nonnegative, and on \\nL\\nmax\\n that it \\nbe finite. In practice, \\nLi r\\nmin min min\\n=\\n and \\nLi r\\nmax max max\\n=\\n. From Example 2.1, using \\naverage office illumination and reflectance values as guidelines\\n, we may expect \\nL\\nmin\\n≈\\n10\\n and \\nL\\nmax\\n≈\\n1000\\n to be typical indoor values in the absence of additional \\nillumination.\\n The units of these quantities are \\nlum/m\\n2\\n.\\n However, actual units sel-\\ndom are of interest,\\n except in cases where photometric measurements are being \\nperformed.\\nThe interval \\n[, ]\\nmin max\\nLL\\n is called the \\nintensity\\n (or \\ngra\\ny\\n) \\nscale\\n. Common practice is \\nto shift this interval numerically to the interval \\n[,] ,\\n01\\n or \\n[, ] ,\\n0\\nC\\n where \\n/\\n=\\n0\\n is consid-\\nered black and \\n/\\n=\\n1 (or \\n)\\nC', metadata={'source': 'imagepro.pdf', 'page': 64}),\n",
       " Document(page_content='/\\n=\\n1 (or \\n)\\nC\\n is considered white on the scale. All intermediate values \\nare shades of gray varying from black to white\\n.\\n2.4  IMAGE SAMPLING AND QUANTIZATION  \\nAs discussed in the previous section, there are numerous ways to acquire images, but \\nour objective in all is the same: to generate digital images from sensed data. The out-\\nput of most sensors is a continuous voltage waveform whose amplitude and spatial \\nbehavior are related to the physical phenomenon being sensed. To create a digital \\nimage, we need to convert the continuous sensed data into a digital format. This \\nrequires two processes: \\nsampling\\n and \\nquantization\\n.\\nBASIC CONCEPTS IN SAMPLING AND QUANTIZATION\\nFigure 2.16(a) shows a continuous image \\nf\\n that we want to convert to digital form. \\nAn image may be continuous with respect to the \\nx\\n- and \\ny\\n-coordinates, and also in \\n2.4\\nThe discussion of sam-\\npling in this section is of \\nan intuitive nature. We \\nwill discuss this topic in \\ndepth in Chapter 4.', metadata={'source': 'imagepro.pdf', 'page': 64}),\n",
       " Document(page_content='depth in Chapter 4.\\nDIP4E_GLOBAL_Print_Ready.indb   63\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 64}),\n",
       " Document(page_content='64\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\namplitude. To digitize it, we have to sample the function in both coordinates and \\nalso in amplitude. Digitizing the coordinate values is called \\nsampling\\n. Digitizing the \\namplitude values is called \\nquantization\\n.\\nThe one-dimensional function in Fig. 2.16(b) is a plot of amplitude (intensity \\nlevel) values of the continuous image along the line segment \\nAB\\n in Fig. 2.16(a). The \\nrandom variations are due to image noise. To sample this function, we take equally \\nspaced samples along line \\nAB\\n, as shown in Fig. 2.16(c). The samples are shown as \\nsmall dark squares superimposed on the function, and their (discrete) spatial loca-\\ntions are indicated by corresponding tick marks in the bottom of the ﬁgure. The set \\nof dark squares constitute the \\nsampled\\n function. However, the \\nvalues\\n of the sam-\\nples still span (vertically) a continuous range of intensity values. In order to form a', metadata={'source': 'imagepro.pdf', 'page': 65}),\n",
       " Document(page_content='digital function, the intensity values also must be converted (\\nquantized\\n) into \\ndiscrete\\n \\nquantities. The vertical gray bar in Fig. 2.16(c) depicts the intensity scale divided \\ninto eight discrete intervals, ranging from black to white. The vertical tick marks \\nindicate the speciﬁc value assigned to each of the eight intensity intervals. The con-\\ntinuous intensity levels are quantized by assigning one of the eight values to each \\nsample, depending on the vertical proximity of a sample to a vertical tick mark. The \\ndigital samples resulting from both sampling and quantization are shown as white \\nsquares in Fig. 2.16(d). Starting at the top of the continuous image and carrying out \\nthis procedure downward, line by line, produces a two-dimensional digital image. \\nIt is implied in Fig. 2.16 that, in addition to the number of discrete levels used, the \\naccuracy achieved in quantization is highly dependent on the noise content of the \\nsampled signal. \\nb a\\nd c\\nFIGURE 2.16\\n(a) Continuous', metadata={'source': 'imagepro.pdf', 'page': 65}),\n",
       " Document(page_content='(a) Continuous \\nimage. (b) A \\nscan line show-\\ning intensity \\nvariations along \\nline \\nAB\\n in the \\ncontinuous image. \\n(c) Sampling and \\nquantization.  \\n(d) Digital scan \\nline. (The black \\nborder in (a) is \\nincluded for  \\nclarity. It is not \\npart of the image).\\nAB\\nAB\\nSampling\\nAB\\nAB\\nQuantization\\nDIP4E_GLOBAL_Print_Ready.indb   64\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 65}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n65\\nIn practice, the method of sampling is determined by the sensor arrangement \\nused to generate the image. When an image is generated by a single sensing element \\ncombined with mechanical motion, as in Fig. 2.13, the output of the sensor is quan-\\ntized in the manner described above. However, spatial sampling is accomplished by \\nselecting the number of individual mechanical increments at which we activate the \\nsensor to collect data. Mechanical motion can be very exact so, in principle, there is \\nalmost no limit on how ﬁne we can sample an image using this approach. In practice, \\nlimits on sampling accuracy are determined by other factors, such as the quality of \\nthe optical components used in the system.\\nWhen a sensing strip is used for image acquisition, the number of sensors in the \\nstrip establishes the samples in the resulting image in one direction, and mechanical', metadata={'source': 'imagepro.pdf', 'page': 66}),\n",
       " Document(page_content='motion establishes the number of samples in the other. Quantization of the sensor \\noutputs completes the process of generating a digital image.\\nWhen a sensing array is used for image acquisition, no motion is required. The \\nnumber of sensors in the array establishes the limits of sampling in both directions. \\nQuantization of the sensor outputs is as explained above. Figure 2.17 illustrates this \\nconcept. Figure 2.17(a) shows a continuous image projected onto the plane of a 2-D \\nsensor. Figure 2.17(b) shows the image after sampling and quantization. The quality \\nof a digital image is determined to a large degree by the number of samples and dis-\\ncrete intensity levels used in sampling and quantization. However, as we will show \\nlater in this section, image content also plays a role in the choice of these parameters.\\nREPRESENTING DIGITAL IMAGES\\nLet \\nfst\\n(,\\n)\\n represent a \\ncontinuous\\n image function of two continuous variables\\n, \\ns\\n and \\nt\\n. We convert this function into a \\ndigital image', metadata={'source': 'imagepro.pdf', 'page': 66}),\n",
       " Document(page_content='digital image\\n by sampling and quantization, as \\nexplained in the previous section. Suppose that we sample the continuous image \\ninto a digital image, \\nfx y\\n(,\\n)\\n, containing \\nM\\n rows and \\nN\\n columns\\n, where \\n(,)\\nxy\\n are \\ndiscrete coordinates\\n. For notational clarity and convenience, we use integer values \\nfor these discrete coordinates: \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, ,\\n…\\n. Thus, \\nfor example\\n, the value of the digital image at the origin is \\nf\\n(,\\n)\\n00\\n, and its value at \\nthe next coordinates along the first row is \\nf\\n(,\\n)\\n01\\n. Here, the notation (0, 1) is used \\nb a\\nFIGURE 2.17\\n(a) Continuous \\nimage projected \\nonto a sensor \\narray. (b) Result \\nof image sampling \\nand quantization.\\nDIP4E_GLOBAL_Print_Ready.indb   65\\n6/16/2017   2:02:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 66}),\n",
       " Document(page_content='66\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nto denote the second sample along the first row. It \\ndoes not\\n mean that these are \\nthe values of the physical coordinates when the image was sampled. In general, the \\nvalue of a digital image at any coordinates \\n(,)\\nxy\\n is denoted \\nfx y\\n(,\\n)\\n, where \\nx\\n and \\ny\\n \\nare integers\\n. When we need to refer to specific coordinates \\n(, )\\nij\\n, we use the notation \\nfij\\n(,\\n) ,\\n where the arguments are integers. The section of the real plane spanned by \\nthe coordinates of an image is called the \\nspatial domain\\n,\\n with \\nx\\n and \\ny\\n being referred \\nto as \\nspatial variables\\n or \\nspatial coordinates\\n.\\nFigure 2.18\\n \\nshows three ways of representing \\nfx y\\n(,\\n)\\n. Figure 2.18(a) is a plot of \\nthe function,\\n with two axes determining spatial location and the third axis being the \\nvalues of \\nf\\n as a function of \\nx \\nand \\ny\\n. This representation is useful when working with \\ngrayscale sets whose elements are expressed as triplets of the form \\n(,,)\\nxy\\nz\\n, where \\nx', metadata={'source': 'imagepro.pdf', 'page': 67}),\n",
       " Document(page_content='xy\\nz\\n, where \\nx\\n and \\ny\\n are spatial coordinates and \\nz\\n is the value of \\nf\\n at coordinates \\n(,) .\\nxy\\n We will \\nwork with this representation brieﬂy in Section 2.6.\\nT\\nhe representation in Fig. 2.18(b) is more common, and it shows \\nfx y\\n(,\\n)\\n as it would \\nappear on a computer display or photograph.\\n Here, the intensity of each point in the \\ndisplay is proportional to the value of \\nf\\n at that point. In this ﬁgure, there are only \\nthree equally spaced intensity values. If the intensity is normalized to the interval \\n[,] ,\\n01\\n then each point in the image has the value 0, 0.5, or 1. A monitor or printer con-\\nverts these three values to black,\\n gray, or white, respectively, as in Fig. 2.18(b). This \\ntype of representation includes color images, and allows us to view results at a glance.\\nAs Fig. 2.18(c) shows, the third representation is an array (matrix) composed of \\nthe numerical values of \\nfx y\\n(,\\n)\\n. This is the representation used for computer process-\\ning', metadata={'source': 'imagepro.pdf', 'page': 67}),\n",
       " Document(page_content='ing\\n. In equation form, we write the representation of an \\nMN\\n*\\n numerical array as\\n \\nfx y\\nff\\nf N\\nff f N\\nfM\\n(,)\\n(,) (,) (, )\\n(, ) (,) (, )\\n(,\\n=\\n−\\n−\\n−\\n00 01\\n0 1\\n10 11\\n1 1\\n1\\n/midhorizellipsis\\n/midhorizellipsis\\n/vertellipsis/vertellipsis /vertellipsis\\n0\\n01 1 1 1\\n)( , ) ( , )\\nfM fM N\\n−− −\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n/midhorizellipsis\\n \\n(2-9)\\nThe right side of this equation is a digital image represented as an array of real \\nnumbers\\n. Each element of this array is called an \\nimage element\\n, \\npicture element\\n, \\npixel\\n, \\nor \\npel\\n. We use the terms \\nimage\\n and \\npixel\\n throughout the book to denote a digital \\nimage and its elements. Figure 2.19 shows a graphical representation of an image \\narray, where the \\nx\\n- and \\ny\\n-axis are used to denote the rows and columns of the array. \\nSpeciﬁc pixels are values of the array at a ﬁxed pair of coordinates. As mentioned \\nearlier, we generally use \\nfij\\n(,\\n)\\n when referring to a pixel with coordinates \\n(, ) .\\nij', metadata={'source': 'imagepro.pdf', 'page': 67}),\n",
       " Document(page_content='(, ) .\\nij\\nWe can also represent a digital image in a traditional matrix form:\\n \\nA\\n=\\n⎡\\n⎣\\n−\\n−\\n−− − −\\naa a\\naa a\\naa a\\nN\\nN\\nMM M N\\n00 01\\n0 1\\n10 11\\n1 1\\n10 11\\n11\\n,, ,\\n,, ,\\n,, ,\\n/midhorizellipsis\\n/midhorizellipsis\\n/vertellipsis/vertellipsis /vertellipsis\\n/midhorizellipsis\\n⎢ ⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n \\n(2-10)\\nClearly, \\naf i j\\nij\\n=\\n(, ) ,\\n so Eqs. (2-9) and (2-10) denote identical arrays.\\nDIP4E_GLOBAL_Print_Ready.indb   66\\n6/16/2017   2:02:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 67}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n67\\nAs Fig. 2.19 shows, we deﬁne the \\norigin\\n of an image at the top left corner. This is \\na convention based on the fact that many image displays (e.g., TV monitors) sweep \\nan image starting at the top left and moving to the right, one row at a time. More \\nimportant is the fact that the ﬁrst element of a matrix is by convention at the top \\nleft of the array. Choosing the origin of \\nfx y\\n(,\\n)\\n at that point makes sense mathemati-\\ncally because digital images in reality are matrices\\n. In fact, as you will see, sometimes \\nwe use \\nx\\n and \\ny\\n interchangeably in equations with the \\nrows\\n (\\nr\\n) and \\ncolumns\\n (\\nc\\n) of a \\nmatrix.\\nIt is important to note that the representation in Fig. 2.19, in which the positive \\nx\\n-axis extends downward and the positive \\ny\\n-axis extends to the right, is precisely the \\nright-handed Cartesian coordinate system with which you are familiar,\\n†\\n but shown \\nrotated by 90\\n°\\n so that the origin appears on the top, left.\\n†', metadata={'source': 'imagepro.pdf', 'page': 68}),\n",
       " Document(page_content='†\\n Recall that a right-handed coordinate system is such that, when the index of the right hand points in the direc-\\ntion of the positive \\nx\\n-axis and the middle ﬁnger points in the (perpendicular) direction of the positive \\ny\\n-axis, the \\nthumb points up. As Figs. 2.18 and 2.19 show, this indeed is the case in our image coordinate system. In practice, \\nyou will also ﬁnd implementations based on a left-handed system, in which the \\nx\\n- and \\ny\\n-axis are interchanged \\nfrom the way we show them in Figs. 2.18 and 2.19. For example, MATLAB uses a left-handed system for image \\nprocessing. Both systems are perfectly valid, provided they are used consistently.\\nx\\ny\\nf\\n(\\nx, y\\n)\\n.5\\ny\\nx\\nOrigin\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1 1\\n1\\n.5\\n.5\\n.5\\n.5\\n.5\\n.5\\nb\\na\\nc\\nFIGURE 2.18\\n(a) Image plotted \\nas a surface.  \\n(b) Image displayed \\nas a visual intensity \\narray. (c) Image \\nshown as a 2-D nu-\\nmerical array. (The \\nnumbers 0, .5, and \\n1 represent black, \\ngray, and white, \\nrespectively.)', metadata={'source': 'imagepro.pdf', 'page': 68}),\n",
       " Document(page_content='respectively.)\\nDIP4E_GLOBAL_Print_Ready.indb   67\\n6/16/2017   2:02:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 68}),\n",
       " Document(page_content='68\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe \\ncenter\\n of an \\nMN\\n×\\n digital image with origin at \\n(,)\\n00\\n and range to \\n(,)\\nMN\\n−−\\n11\\nis obtained by dividing \\nM\\n and \\nN\\n by 2 and rounding \\ndo\\nwn\\n to the nearest integer. \\nThis operation sometimes is denoted using the ﬂoor operator, \\nJK\\ni\\n,\\n as shown in Fig. \\n2.19.\\n This holds true for \\nM\\n and \\nN\\n even \\nor\\n odd. For example, the center of an image \\nof size \\n1023 1024\\n×\\n is at \\n(, ) .\\n511\\n512\\n Some programming languages (e.g., MATLAB) \\nstart indexing at 1 instead of at 0.\\n The center of an image in that case is found at \\n(,) ( ) , ( ) .\\nxy M N\\ncc\\n=+ +\\n()\\nfloor floor\\n21 21\\nTo express sampling and quantization in more formal mathematical terms, let \\nZ\\n and \\nR\\n denote the set of integers and the set of real numbers\\n, respectively. The \\nsampling process may be viewed as partitioning the \\nxy\\n-plane into a grid, with the \\ncoordinates of the center of each cell in the grid being a pair of elements from the \\nCartesian product \\nZ\\n2', metadata={'source': 'imagepro.pdf', 'page': 69}),\n",
       " Document(page_content='Z\\n2\\n (also denoted \\nZZ\\n×\\n)\\n which, as you may recall, is the set of \\nall ordered pairs of elements \\n(, )\\nzz\\nij\\n with \\nz\\ni\\n and \\nz\\nj\\n being integers from set \\nZ\\n. Hence, \\nfx y\\n(,\\n)\\n is a digital image if \\n(,)\\nxy\\n are integers from \\nZ\\n2\\n and \\nf\\n is a function that assigns \\nan intensity value (that is, a real number from the set of real numbers, \\nR\\n) to each \\ndistinct pair of coordinates \\n(,)\\nxy\\n. This functional assignment is the quantization pro-\\ncess described earlier\\n. If the intensity levels also are integers, then\\nRZ\\n=\\n,\\n and a \\ndigital image becomes a 2-D function whose coordinates and amplitude values are \\nintegers\\n. This is the representation we use in the book.\\nImage digitization requires that decisions be made regarding the values for \\nM\\n, \\nN\\n, \\nand for the number, \\nL\\n, of discrete intensity levels. There are no restrictions placed \\non \\nM\\n and \\nN\\n, other than they have to be positive integers. However, digital storage', metadata={'source': 'imagepro.pdf', 'page': 69}),\n",
       " Document(page_content='and quantizing hardware considerations usually lead to the number of intensity lev-\\nels, \\nL\\n, being an integer power of two; that is\\nL\\nk\\n=\\n2\\n(2-11)\\nwhere \\nk\\n is an integer\\n. We assume that the discrete levels are equally spaced and that \\nthey are integers in the range \\n[, ]\\n01\\nL\\n−\\n. \\nThe \\nﬂoor\\n of \\nz\\n, sometimes \\ndenoted \\nJ\\nz\\nK\\n, is the largest \\ninteger that is less than \\nor equal to \\nz\\n. The \\nceiling\\n \\nof \\nz\\n, denoted \\nL\\nz\\nM\\n, is the \\nsmallest integer that is \\ngreater than or equal \\nto \\nz\\n.\\nSee Eq. (2-41) in  \\nSection 2.6 for a formal \\ndeﬁnition of the  \\nCartesian product.\\nFIGURE 2.19\\nCoordinate  \\nconvention used \\nto represent digital \\nimages. Because \\ncoordinate values \\nare integers, there \\nis a one-to-one \\ncorrespondence \\nbetween \\nx\\n and \\ny\\n \\nand the rows (\\nr\\n) \\nand columns (\\nc\\n) of \\na matrix.\\nOrigin\\n0\\nN \\n-\\n1\\n-\\n1\\nM\\n0\\ny\\nx\\ni\\nj \\npixel \\nf\\n(\\ni\\n, \\nj\\n)\\nImage \\nf\\n(\\nx\\n, \\ny\\n)\\n1\\n1\\n2\\nCenter\\nThe coordinates of the \\nimage center are\\nx\\nc\\ny\\nc\\nx\\nc\\n, \\ny\\nc\\n  =  \\nN\\n2\\nQR\\nfloor\\nM\\n2\\nQR\\nfloor ,\\na\\nb\\nB A', metadata={'source': 'imagepro.pdf', 'page': 69}),\n",
       " Document(page_content='QR\\nfloor ,\\na\\nb\\nB A\\nDIP4E_GLOBAL_Print_Ready.indb   68\\n6/16/2017   2:02:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 69}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n69\\nSometimes, the range of values spanned by the gray scale is referred to as the \\ndynamic range\\n, a term used in different ways in different ﬁelds. Here, we deﬁne the \\ndynamic range of an imaging system to be the ratio of the maximum measurable \\nintensity to the minimum detectable intensity level in the system. As a rule, the \\nupper limit is determined by \\nsaturation\\n and the lower limit by \\nnoise\\n, although noise \\ncan be present also in lighter intensities. Figure 2.20 shows examples of saturation \\nand slight visible noise. Because the darker regions are composed primarily of pixels \\nwith the minimum detectable intensity, the background in Fig. 2.20 is the noisiest \\npart of the image; however, dark background noise typically is much harder to see. \\nThe dynamic range establishes the lowest and highest intensity levels that a system \\ncan represent and, consequently, that an image can have. Closely associated with this \\nconcept is', metadata={'source': 'imagepro.pdf', 'page': 70}),\n",
       " Document(page_content='concept is \\nimage contrast\\n, which we deﬁne as the difference in intensity between \\nthe highest and lowest intensity levels in an image. The \\ncontrast ratio\\n is the ratio of \\nthese two quantities. When an appreciable number of pixels in an image have a high \\ndynamic range, we can expect the image to have high contrast. Conversely, an image \\nwith low dynamic range typically has a dull, washed-out gray look. We will discuss \\nthese concepts in more detail in Chapter 3.\\nThe number, \\nb\\n, of bits required to store a digital image is\\n \\nbMNk\\n=\\n**\\n \\n(2-12)\\nWhen \\nMN\\n=\\n, this equation becomes\\n \\nbN k\\n=\\n2\\n \\n(2-13)\\nNoise\\nSaturation\\nFIGURE 2.20\\nAn image exhibit-\\ning saturation and \\nnoise. Saturation \\nis the highest val-\\nue beyond which \\nall intensity values \\nare clipped (note \\nhow the entire \\nsaturated area has \\na high, constant \\nintensity level). \\nVisible noise in \\nthis case appears \\nas a grainy texture \\npattern. The dark \\nbackground is \\nnoisier, but the \\nnoise is difﬁcult \\nto see.', metadata={'source': 'imagepro.pdf', 'page': 70}),\n",
       " Document(page_content='to see.\\nDIP4E_GLOBAL_Print_Ready.indb   69\\n6/16/2017   2:02:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 70}),\n",
       " Document(page_content='70\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nFigure 2.21 shows the number of megabytes required to store square images for \\nvarious values of \\nN\\n and \\nk\\n (as usual, one byte equals 8 bits and a megabyte equals \\n10\\n6\\n bytes). \\nWhen an image can have \\n2\\nk\\n possible intensity levels, it is common practice to \\nrefer to it as a “\\nk\\n-bit image,” (e,g., a 256-level image is called an \\n8-bit image\\n). Note \\nthat storage requirements for large 8-bit images (e.g., \\n10 000 10 000\\n,,\\n*\\n pixels) are \\nnot insigniﬁcant.\\nLINEAR VS. COORDINATE INDEXING\\nThe convention discussed in the previous section, in which the location of a pixel is \\ngiven by its 2-D coordinates, is referred to as \\ncoordinate indexing\\n, or \\nsubscript index-\\ning\\n. Another type of indexing used extensively in programming image processing \\nalgorithms is \\nlinear indexing\\n, which consists of a 1-D string of nonnegative integers \\nbased on computing offsets from coordinates \\n(,)\\n00\\n. There are two principal types of \\nlinear indexing', metadata={'source': 'imagepro.pdf', 'page': 71}),\n",
       " Document(page_content='linear indexing\\n, one is based on a row scan of an image, and the other on a column scan.\\nFigure 2.22 illustrates the principle of linear indexing based on a column scan. \\nThe idea is to scan an image column by column, starting at the origin and proceeding \\ndown and then to the right. The linear index is based on counting pixels as we scan \\nthe image in the manner shown in Fig. 2.22. Thus, a scan of the ﬁrst (leftmost) column \\nyields linear indices 0 through \\nM\\n−\\n1\\n. A scan of the second column yields indices \\nM\\n \\nthrough \\n21\\nM\\n−\\n, and so on, until the last pixel in the last column is assigned the linear \\nindex value \\nMN\\n−\\n1\\n. Thus, a linear index, denoted by \\na\\n, has one of \\nMN\\n possible \\nvalues:\\n \\n012 1\\n,,\\n, ,\\n…\\nMN\\n−\\n, as Fig. 2.22 shows. The important thing to notice here is \\nthat each pixel is assigned a linear index value that identiﬁes it uniquely\\n.\\nThe formula for generating linear indices based on a column scan is straightfor-', metadata={'source': 'imagepro.pdf', 'page': 71}),\n",
       " Document(page_content='ward and can be determined by inspection. For any pair of coordinates \\n(, )\\nxy\\n, the \\ncorresponding linear index value is\\n \\na\\n=+\\nMy\\nx\\n \\n(2-14)\\nN\\n*\\n10\\n3\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n123456789 1 0\\nk\\n = 8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n0\\nMegabytes (\\n    \\n            )\\n*\\nb\\n8\\n10\\n6\\nFIGURE 2.21\\nNumber of  \\nmegabytes \\nrequired to store \\nimages for  \\nvarious values of \\nN\\n and \\nk\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   70\\n6/16/2017   2:02:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 71}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n71\\nConversely, the coordinate indices for a given linear index value \\na\\n are given by the \\nequations\\n†\\nxM\\n=\\na\\nmod\\n(2-15)\\nand\\n \\nyx M\\n=\\n()\\na\\n-\\n(2-16)\\nRecall that \\na\\nmo\\nd\\nM\\n means “the remainder of the division of \\na\\n by \\nM\\n.\\n” This is a \\nformal way of stating that row numbers repeat themselves at the start of every col-\\numn. Thus, when \\na\\n=\\n0, the remainder of the division of 0 by \\nM\\n is 0,\\n so \\nx\\n=\\n0\\n. When \\na\\n=\\n1,\\n the remainder is 1, and so \\nx\\n=\\n1\\n. You can see that \\nx\\n will continue to be equal \\nto \\na\\n until \\na\\n=−\\nM\\n1\\n. When \\na\\n=\\nM\\n (which is at the beginning of the second column), \\nthe remainder is 0,\\n and thus \\nx\\n=\\n0\\n again, and it increases by 1 until the next column \\nis reached,\\n when the pattern repeats itself. Similar comments apply to Eq. (2-16). See \\nProblem 2.11 for a derivation of the preceding two equations.\\nSPATIAL AND INTENSITY RESOLUTION\\nIntuitively, \\nspatial resolution\\n is a measure of the smallest discernible detail in an', metadata={'source': 'imagepro.pdf', 'page': 72}),\n",
       " Document(page_content='image. Quantitatively, spatial resolution can be stated in several ways, with \\nline \\npairs per unit distance\\n, and \\ndots (pixels) per unit distance\\n being common measures. \\nSuppose that we construct a chart with alternating black and white vertical lines, \\neach of width \\nW\\n units (\\nW\\n can be less than 1). The width of a \\nline pair\\n is thus 2\\nW\\n, and \\nthere are \\nW\\n2\\n line pairs per unit distance. For example, if the width of a line is 0.1 mm, \\nthere are 5 line pairs per unit distance (i.e\\n., per mm). A widely used definition of \\nimage resolution is the largest number of \\ndiscernible\\n line pairs per unit distance (e.g., \\n100 line pairs per mm). Dots per unit distance is a measure of image resolution used \\nin the printing and publishing industry. In the U.S., this measure usually is expressed \\nas \\ndots per inch\\n (dpi). To give you an idea of quality, newspapers are printed with a \\n†\\nWhen working with modular number systems, it is more accurate to write \\nxM\\n≡\\na\\nmo\\nd\\n, where the symbol \\n≡', metadata={'source': 'imagepro.pdf', 'page': 72}),\n",
       " Document(page_content='≡\\nmeans \\ncongruence\\n.\\n However, our interest here is just on converting from linear to coordinate indexing, so we \\nuse the more familiar equal sign.\\nx\\ny\\nImage\\n f\\n(\\nx\\n, \\ny\\n)\\n(0, 0)  \\nα\\n = 0\\n(\\nM\\n \\n-\\n 1, 0)  \\nα\\n = \\nM\\n \\n-\\n 1\\n(\\nM\\n \\n-\\n 1, \\nN\\n \\n-\\n 1)  \\nα\\n = \\nMN\\n \\n-\\n 1\\n(0, 1)  \\nα\\n = \\nM\\n(0, 2)  \\nα\\n = 2\\nM\\n(\\nM\\n \\n-\\n 1, 1)  \\nα\\n = 2\\nM\\n \\n-\\n 1\\nI\\nma\\ng\\ne\\n \\nf\\n(\\nf\\nf\\nx\\n,\\n \\ny\\n)\\nFIGURE 2.22\\nIllustration of  \\ncolumn scanning \\nfor generating  \\nlinear indices. \\nShown are several \\n2-D coordinates (in \\nparentheses) and \\ntheir corresponding \\nlinear indices.\\nDIP4E_GLOBAL_Print_Ready.indb   71\\n6/16/2017   2:02:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 72}),\n",
       " Document(page_content='72\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nresolution of 75 dpi, magazines at 133 dpi, glossy brochures at 175 dpi, and the book \\npage at which you are presently looking was printed at 2400 dpi. \\nTo be meaningful, measures of spatial resolution must be stated with respect to \\nspatial units. Image size by itself does not tell the complete story. For example, to say \\nthat an image has a resolution of \\n1024 1024\\n*\\n pixels is not a meaningful statement \\nwithout stating the spatial dimensions encompassed by the image\\n. Size by itself is \\nhelpful only in making comparisons between imaging capabilities. For instance, a \\ndigital camera with a 20-megapixel CCD imaging chip can be expected to have a \\nhigher capability to resolve detail than an 8-megapixel camera, assuming that both \\ncameras are equipped with comparable lenses and the comparison images are taken \\nat the same distance.\\nIntensity resolution\\n similarly refers to the smallest \\ndiscernible\\n change in inten-', metadata={'source': 'imagepro.pdf', 'page': 73}),\n",
       " Document(page_content='change in inten-\\nsity level. We have considerable discretion regarding the number of spatial samples \\n(pixels) used to generate a digital image, but this is not true regarding the number \\nof intensity levels. Based on hardware considerations, the number of intensity levels \\nusually is an integer power of two, as we mentioned when discussing Eq. (2-11). The \\nmost common number is 8 bits, with 16 bits being used in some applications in which \\nenhancement of speciﬁc intensity ranges is necessary. Intensity quantization using \\n32 bits is rare. Sometimes one ﬁnds systems that can digitize the intensity levels of \\nan image using 10 or 12 bits, but these are not as common. \\nUnlike spatial resolution, which must be based on a per-unit-of-distance basis to \\nbe meaningful, it is common practice to refer to the number of bits used to quan-\\ntize intensity as the “\\nintensity resolution\\n.” For example, it is common to say that an', metadata={'source': 'imagepro.pdf', 'page': 73}),\n",
       " Document(page_content='image whose intensity is quantized into 256 levels has 8 bits of intensity resolution. \\nHowever, keep in mind that \\ndiscernible\\n changes in intensity are inﬂuenced also by \\nnoise and saturation values, and by the capabilities of human perception to analyze \\nand interpret details in the context of an entire scene (see Section 2.1). The following \\ntwo examples illustrate the effects of spatial and intensity resolution on discernible \\ndetail. Later in this section, we will discuss how these two parameters interact in \\ndetermining perceived image quality.\\nEXAMPLE 2.2 : Effects of reducing the spatial resolution of a digital image.\\nFigure 2.23 shows the effects of reducing the spatial resolution of an image. The images in Figs. 2.23(a) \\nthrough (d) have resolutions of 930, 300, 150, and 72 dpi, respectively. Naturally, the lower resolution \\nimages are smaller than the original image in (a). For example, the original image is of size \\n2136 2140\\n*\\n \\npixels', metadata={'source': 'imagepro.pdf', 'page': 73}),\n",
       " Document(page_content='*\\n \\npixels\\n, but the 72 dpi image is an array of only \\n165 166\\n*\\n pixels. In order to facilitate comparisons, all the \\nsmaller images were zoomed back to the original size (the method used for zooming will be discussed \\nlater in this section).\\n This is somewhat equivalent to “getting closer” to the smaller images so that we can \\nmake comparable statements about visible details. \\nThere are some small visual differences between Figs. 2.23(a) and (b), the most notable being a slight \\ndistortion in the seconds marker pointing to 60 on the right side of the chronometer. For the most part, \\nhowever, Fig. 2.23(b) is quite acceptable. In fact, 300 dpi is the typical minimum image spatial resolution \\nused for book publishing, so one would not expect to see much difference between these two images. \\nFigure 2.23(c) begins to show visible degradation (see, for example, the outer edges of the chronometer \\nDIP4E_GLOBAL_Print_Ready.indb   72\\n6/16/2017   2:02:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 73}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n73\\ncase and compare the seconds marker with the previous two images). The numbers also show visible \\ndegradation. Figure 2.23(d) shows degradation that is visible in most features of the image. When print-\\ning at such low resolutions, the printing and publishing industry uses a number of techniques (such as \\nlocally varying the pixel size) to produce much better results than those in Fig. 2.23(d). Also, as we will \\nshow later in this section, it is possible to improve on the results of Fig. 2.23 by the choice of interpola-\\ntion method used.\\nEXAMPLE 2.3 :  Effects of varying the number of intensity levels in a digital image.\\nFigure 2.24(a) is a \\n774 640\\n×\\n CT projection image, displayed using 256 intensity levels (see Chapter 1 \\nregarding CT images).\\n The objective of this example is to reduce the number of intensities of the image \\nfrom 256 to 2 in integer powers of 2, while keeping the spatial resolution constant. Figures 2.24(b)', metadata={'source': 'imagepro.pdf', 'page': 74}),\n",
       " Document(page_content='through (d) were obtained by reducing the number of intensity levels to 128, 64, and 32, respectively (we \\nwill discuss in Chapter 3 how to reduce the number of levels). \\nb a\\nd c\\nFIGURE 2.23\\nEffects of  \\nreducing spatial \\nresolution. The \\nimages shown \\nare at:  \\n(a) 930 dpi,  \\n(b) 300 dpi,  \\n(c) 150 dpi, and \\n(d) 72 dpi.\\nDIP4E_GLOBAL_Print_Ready.indb   73\\n6/16/2017   2:02:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 74}),\n",
       " Document(page_content='74\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe 128- and 64-level images are visually identical for all practical purposes. However, the 32-level image \\nin Fig. 2.24(d) has a set of almost imperceptible, very ﬁne ridge-like structures in areas of constant inten-\\nsity. These structures are clearly visible in the 16-level image in Fig. 2.24(e). This effect, caused by using \\nan insufﬁcient number of intensity levels in smooth areas of a digital image, is called \\nfalse contouring\\n, so \\nnamed because the ridges resemble topographic contours in a map. False contouring generally is quite \\nobjectionable in images displayed using 16 or fewer uniformly spaced intensity levels, as the images in \\nFigs. 2.24(e)-(h) show. \\nAs a very rough guideline, and assuming integer powers of 2 for convenience, images of size \\n256 256\\n*\\n \\npixels with 64 intensity levels\\n, and printed on a size format on the order of \\n55\\n*\\n cm, are about the lowest', metadata={'source': 'imagepro.pdf', 'page': 75}),\n",
       " Document(page_content='spatial and intensity resolution images that can be expected to be reasonably free of objectionable sam-\\npling distortions and false contouring\\n.\\nb a\\nd c\\nFIGURE 2.24\\n(a) 774 \\n×\\n 640, \\n256-level image. \\n(b)-(d) Image  \\ndisplayed in 128, \\n64, and 32 inten-\\nsity levels, while  \\nkeeping the  \\nspatial resolution  \\nconstant.  \\n(Original image \\ncourtesy of the \\nDr. David R.  \\nPickens,  \\nDepartment of \\nRadiology & \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   74\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 75}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n75\\nThe results in Examples 2.2 and 2.3 illustrate the effects produced on image qual-\\nity by varying spatial and intensity resolution independently. However, these results \\ndid not consider any relationships that might exist between these two parameters. \\nAn early study by Huang [1965] attempted to quantify experimentally the effects on \\nimage quality produced by the interaction of these two variables. The experiment \\nconsisted of a set of subjective tests. Images similar to those shown in Fig. 2.25 were \\nused. The woman’s face represents an image with relatively little detail; the picture \\nof the cameraman contains an intermediate amount of detail; and the crowd picture \\ncontains, by comparison, a large amount of detail. \\nSets of these three types of images of various sizes and intensity resolution were \\ngenerated by varying \\nN\\n and \\nk\\n [see Eq. (2-13)]. Observers were then asked to rank \\nf e\\nh\\ng\\nFIGURE 2.24\\n(\\nContinued\\n) \\n(e)-(h) Image', metadata={'source': 'imagepro.pdf', 'page': 76}),\n",
       " Document(page_content=') \\n(e)-(h) Image \\ndisplayed in 16, 8, \\n4, and 2 intensity \\nlevels. \\nDIP4E_GLOBAL_Print_Ready.indb   75\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 76}),\n",
       " Document(page_content='76\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nthem according to their subjective quality. Results were summarized in the form of \\nso-called \\nisopreference curves\\n in the \\nNk\\n-plane. (Figure 2.26 shows average isopref-\\nerence curves representative of the types of images in Fig. 2.25.) Each point in the \\nNk\\n-plane represents an image having values of \\nN\\n and \\nk\\n equal to the coordinates \\nof that point. Points lying on an isopreference curve correspond to images of equal \\nsubjective quality. It was found in the course of the experiments that the isoprefer-\\nence curves tended to shift right and upward, but their shapes in each of the three \\nimage categories were similar to those in Fig. 2.26. These results were not unexpect-\\ned, because a shift up and right in the curves simply means larger values for \\nN\\n and \\nk\\n, \\nwhich implies better picture quality.\\nb a\\nc\\nFIGURE 2.25\\n (a) Image with a low level of detail. (b) Image with a medium level of detail. (c) Image with a relatively', metadata={'source': 'imagepro.pdf', 'page': 77}),\n",
       " Document(page_content='large amount of detail. (Image (b) courtesy of the Massachusetts Institute of Technology.)\\nFace\\n256\\n128\\n64\\n32\\n4\\n5\\nk\\nN\\nCrowd\\nCameraman\\nFIGURE 2.26\\nRepresentative  \\nisopreference \\ncurves for the \\nthree types of  \\nimages in  \\nFig. 2.25.\\nDIP4E_GLOBAL_Print_Ready.indb   76\\n6/16/2017   2:02:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 77}),\n",
       " Document(page_content='2.4\\n  \\nImage Sampling and Quantization\\n    \\n77\\nObserve that isopreference curves tend to become more vertical as the detail in \\nthe image increases. This result suggests that for images with a large amount of detail \\nonly a few intensity levels may be needed. For example, the isopreference curve in \\nFig. 2.26 corresponding to the crowd is nearly vertical. This indicates that, for a ﬁxed \\nvalue of \\nN\\n, the perceived quality for this type of image is nearly independent of the \\nnumber of intensity levels used (for the range of intensity levels shown in Fig. 2.26). \\nThe perceived quality in the other two image categories remained the same in some \\nintervals in which the number of samples was increased, but the number of intensity \\nlevels actually decreased. The most likely reason for this result is that a decrease in \\nk\\n \\ntends to increase the apparent contrast, a visual effect often perceived as improved \\nimage quality.\\nIMAGE INTERPOLATION', metadata={'source': 'imagepro.pdf', 'page': 78}),\n",
       " Document(page_content='IMAGE INTERPOLATION\\nInterpolation is used in tasks such as zooming, shrinking, rotating, and geometrically \\ncorrecting digital images. Our principal objective in this section is to introduce inter-\\npolation and apply it to image resizing (shrinking and zooming), which are basically \\nimage resampling methods. Uses of interpolation in applications such as rotation \\nand geometric corrections will be discussed in Section 2.6.\\nInterpolation\\n is the process of using known data to estimate values at unknown \\nlocations. We begin the discussion of this topic with a short example. Suppose that \\nan image of size \\n500 500\\n*\\n pixels has to be enlarged 1.5 times to \\n750 750\\n*\\n pixels. A \\nsimple way to visualize zooming is to create an imaginary \\n750 750\\n*\\n grid with the \\nsame pixel spacing as the original image\\n, then shrink it so that it exactly overlays the \\noriginal image. Obviously, the pixel spacing in the shrunken \\n750 750\\n*\\n grid will be \\nless than the pixel spacing in the original image', metadata={'source': 'imagepro.pdf', 'page': 78}),\n",
       " Document(page_content='. To assign an intensity value to any \\npoint in the overlay, we look for its closest pixel in the underlying original image and \\nassign the intensity of that pixel to the new pixel in the \\n750 750\\n*\\n grid. When intensi-\\nties have been assigned to all the points in the overlay grid,\\n we expand it back to the \\nspeciﬁed size to obtain the resized image.\\nThe method just discussed is called \\nnearest neighbor interpolation\\n because it \\nassigns to each new location the intensity of its nearest neighbor in the original \\nimage (see Section 2.5 regarding neighborhoods). This approach is simple but, it has \\nthe tendency to produce undesirable artifacts, such as severe distortion of straight \\nedges. A more suitable approach is \\nbilinear interpolation\\n, in which we use the four \\nnearest neighbors to estimate the intensity at a given location. Let \\n(,)\\nxy\\n denote the \\ncoordinates of the location to which we want to assign an intensity value (think of', metadata={'source': 'imagepro.pdf', 'page': 78}),\n",
       " Document(page_content='it as a point of the grid described previously),\\n and let \\nv\\n(,\\n)\\nxy\\n denote that intensity \\nvalue\\n. For bilinear interpolation, the assigned value is obtained using the equation\\n \\nv\\n(,\\n)\\nxy a x b y c x y d\\n=\\n++ +\\n \\n(2-17)\\nwhere the four coefficients are determined from the four equations in four \\nunknowns that can be written using the \\nfour\\n nearest neighbors of point \\n(,)\\nxy\\n. \\nBilinear interpolation gives much better results than nearest neighbor interpolation,\\n \\nwith a modest increase in computational burden.\\nContrary to what the \\nname suggests, bilinear \\ninterpolation is \\nnot\\n a \\nlinear operation because \\nit involves multiplication \\nof coordinates (which is \\nnot a linear operation). \\nSee Eq. (2-17).\\nDIP4E_GLOBAL_Print_Ready.indb   77\\n6/16/2017   2:02:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 78}),\n",
       " Document(page_content='78\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThe next level of complexity is \\nbicubic interpolation\\n, which involves the sixteen \\nnearest neighbors of a point. The intensity value assigned to point \\n(,)\\nxy\\n is obtained \\nusing the equation\\n \\nv\\n(,\\n)\\nxy axy\\nij\\nij\\nj i\\n=\\n= =\\n∑ ∑\\n0\\n3\\n0\\n3\\n  \\n(2-18)\\nThe sixteen coefficients are determined from the sixteen equations with six-\\nteen unknowns that can be written using the sixteen nearest neighbors of point \\n(,)\\nxy\\n . Observe that Eq. (2-18) reduces in form to Eq. (2-17) if the limits of both \\nsummations in the former equation are 0 to 1.\\n Generally, bicubic interpolation does \\na better job of preserving fine detail than its bilinear counterpart. Bicubic interpola-\\ntion is the standard used in commercial image editing applications, such as Adobe \\nPhotoshop and Corel Photopaint.\\nAlthough images are displayed with integer coordinates, it is possible during pro-\\ncessing to work with \\nsubpixel accuracy\\n by increasing the size of the image using', metadata={'source': 'imagepro.pdf', 'page': 79}),\n",
       " Document(page_content='interpolation to “ﬁll the gaps” between pixels in the original image.\\nEXAMPLE 2.4 :  Comparison of interpolation approaches for image shrinking and zooming.\\nFigure 2.27(a) is the same as Fig. 2.23(d), which was obtained by reducing the resolution of the 930 dpi \\nimage in Fig. 2.23(a) to 72 dpi (the size shrank from \\n2136 2140\\n*\\n to \\n165 166\\n*\\n pixels) and then zooming \\nthe reduced image back to its original size\\n. To generate Fig. 2.23(d) we used nearest neighbor interpola-\\ntion both to shrink and zoom the image. As noted earlier, the result in Fig. 2.27(a) is rather poor. Figures \\n2.27(b) and (c) are the results of repeating the same procedure but using, respectively, bilinear and bicu-\\nbic interpolation for both shrinking and zooming. The result obtained by using bilinear interpolation is a \\nsigniﬁcant improvement over nearest neighbor interpolation, but the resulting image is blurred slightly.', metadata={'source': 'imagepro.pdf', 'page': 79}),\n",
       " Document(page_content='Much sharper results can be obtained using bicubic interpolation, as Fig. 2.27(c) shows. \\n \\nFIGURE 2.27\\n (a) Image reduced to 72 dpi and zoomed back to its original 930 dpi using nearest neighbor interpolation. \\nThis ﬁgure is the same as Fig. 2.23(d). (b) Image reduced to 72 dpi and zoomed using bilinear interpolation. (c) Same \\nas (b) but using bicubic interpolation.\\nb a\\nc\\nDIP4E_GLOBAL_Print_Ready.indb   78\\n6/16/2017   2:02:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 79}),\n",
       " Document(page_content='2.5\\n  \\nSome Basic Relationships Between Pixels\\n    \\n79\\nIt is possible to use more neighbors in interpolation, and there are more complex \\ntechniques, such as using \\nsplines\\n or \\nwavelets\\n, that in some instances can yield better \\nresults than the methods just discussed. While preserving ﬁne detail is an exception-\\nally important consideration in image generation for 3-D graphics (for example, see \\nHughes and Andries [2013]), the extra computational burden seldom is justiﬁable \\nfor general-purpose digital image processing, where bilinear or bicubic interpola-\\ntion typically are the methods of choice.\\n2.5 SOME BASIC RELATIONSHIPS BETWEEN PIXELS  \\nIn this section, we discuss several important relationships between pixels in a digital \\nimage. When referring in the following discussion to particular pixels, we use lower-\\ncase letters, such as \\np\\n and \\nq\\n.\\nNEIGHBORS OF A PIXEL\\nA pixel \\np\\n at coordinates \\n(,)\\nxy\\n has two horizontal and two vertical neighbors with \\ncoordinates', metadata={'source': 'imagepro.pdf', 'page': 80}),\n",
       " Document(page_content='coordinates\\n \\n( , ), ( , ), ( , ), ( , )\\nxy\\nxy x y x y\\n+− +−\\n11 11\\nThis set of pixels, called the 4\\n-neighbors\\n of \\np\\n,\\n is denoted \\nNp\\n4\\n()\\n.\\nT\\nhe four \\ndiagonal\\n neighbors of \\np\\n have coordinates\\n \\n(, ) , (, ) , (, ) , (, )\\nxy\\nxy xy xy\\n++ +− −+ −−\\n11 11 11 11\\nand are denoted \\nNp\\nD\\n()\\n. These neighbors, together with the 4-neighbors, are called \\nthe 8-\\nneighbors\\n of \\np\\n,\\n denoted by \\nNp\\n8\\n()\\n. The set of image locations of the neighbors \\nof a point \\np\\n is called the \\nneighborhood\\n of \\np\\n.\\n The neighborhood is said to be \\nclosed\\n if \\nit contains \\np\\n. Otherwise, the neighborhood is said to be \\nopen\\n.\\nADJACENCY, CONNECTIVITY, REGIONS, AND BOUNDARIES\\nLet \\nV\\n be the set of intensity values used to define adjacency. In a binary image, \\nV\\n=\\n{}\\n1\\n if we are referring to adjacency of pixels with value 1. In a grayscale image, \\nthe idea is the same\\n, but set \\nV\\n typically contains more elements. For example, if we', metadata={'source': 'imagepro.pdf', 'page': 80}),\n",
       " Document(page_content='are dealing with the adjacency of pixels whose values are in the range 0 to 255, set \\nV\\n \\ncould be any subset of these 256 values. We consider three types of adjacency:\\n1. \\n4-\\nadjacency\\n.\\n Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are 4-adjacent if \\nq\\n is in the \\nset \\nNp\\n4\\n() .\\n2. \\n8-\\nadjacency\\n.\\n Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are 8-adjacent if \\nq\\n is in the \\nset \\nNp\\n8\\n()\\n.\\n3. \\nm-adjacency \\n(also called \\nmix\\ned adjacency\\n). Two pixels \\np\\n and \\nq\\n with values from \\nV\\n are \\nm\\n-adjacent if\\n2.5\\nDIP4E_GLOBAL_Print_Ready.indb   79\\n6/16/2017   2:02:24 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 80}),\n",
       " Document(page_content='80\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\n(a) \\nq\\n is in \\nNp\\n4\\n()\\n, \\nor\\n(b) \\nq\\n is in \\nNp\\nD\\n()\\n \\nand\\n the set \\nNp Nq\\n44\\n() ()\\n¨\\n has no pixels whose values are \\nfrom \\nV\\n.\\nMixed adjacenc\\ny is a modification of 8-adjacency, and is introduced to eliminate the \\nambiguities that may result from using 8-adjacency. For example, consider the pixel \\narrangement in Fig. 2.28(a) and let \\nV\\n=\\n{}\\n1\\n. The three pixels at the top of Fig. 2.28(b) \\nshow multiple (ambiguous) 8-adjacenc\\ny, as indicated by the dashed lines. This ambi-\\nguity is removed by using \\nm\\n-adjacency, as in Fig. 2.28(c). In other words, the center \\nand upper-right diagonal pixels are not \\nm\\n-adjacent because they do not satisfy con-\\ndition (b).\\nA \\ndigital\\n \\npath\\n (or \\ncurve\\n) from pixel \\np\\n with coordinates \\n(, )\\nxy\\n00\\n to pixel \\nq\\n with \\ncoordinates \\n(, )\\nxy\\nnn\\n is a sequence of distinct pixels with coordinates\\n \\n( , ), ( , ), , ( , )\\nxy\\nxy xy\\nnn\\n00 11\\n…\\nwhere points \\n(,)\\nxy\\nii\\n and \\n(,)\\nxy\\nii\\n−−\\n11\\n are adjacent for \\n1\\n≤≤', metadata={'source': 'imagepro.pdf', 'page': 81}),\n",
       " Document(page_content='1\\n≤≤\\nin\\n. In this case, \\nn\\n is the \\nlength\\n of the path.\\n If \\n(,)(, )\\nxy\\nxy\\nnn\\n00\\n=\\n the path is a \\nclosed\\n path.\\n We can define 4-, 8-, \\nor \\nm\\n-paths, depending on the type of adjacency specified. For example, the paths in \\nFig. 2.28(b) between the top right and bottom right points are 8-paths, and the path \\nin Fig. 2.28(c) is an \\nm\\n-path.\\nLet \\nS\\n represent a subset of pixels in an image. Two pixels \\np\\n and \\nq\\n are said to be \\nconnected in\\n \\nS\\n if there exists a path between them consisting entirely of pixels in \\nS\\n. \\nFor any pixel \\np\\n in \\nS\\n, the set of pixels that are connected to it in \\nS\\n is called a \\nconnected \\ncomponent\\n of \\nS\\n. If it only has one component, and that component is connected, \\nthen \\nS\\n is called a \\nconnected set\\n.\\nLet \\nR\\n represent a subset of pixels in an image. We call \\nR\\n a \\nregion\\n of the image if \\nR\\n \\nis a connected set. Two regions, \\nR\\ni\\n and \\nR\\nj\\n are said to be \\nadjacent\\n if their union forms \\na connected set. Regions that are not adjacent are said to be', metadata={'source': 'imagepro.pdf', 'page': 81}),\n",
       " Document(page_content='disjoint\\n. We consider 4- \\nand 8-adjacency when referring to regions. For our deﬁnition to make sense, the type \\nof adjacency used must be speciﬁed. For example, the two regions of 1’s in Fig. 2.28(d) \\nare adjacent only if 8-adjacency is used (according to the deﬁnition in the previous \\nWe use the symbols \\n¨ \\nand \\n´\\n to denote set \\nintersection and union, \\nrespectively. Given sets \\nA\\n and \\nB\\n, recall that \\ntheir intersection is the \\nset of elements that \\nare members of both \\nA\\n and \\nB\\n. The union of \\nthese two sets is the set \\nof elements that are \\nmembers of \\nA\\n, of \\nB\\n, or \\nof both. We will discuss \\nsets in more detail in \\nSection 2.6.\\n0\\n11\\n0\\n1\\n0\\n00\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n111\\n1\\n0\\n1\\n0\\n1\\n0\\nR\\ni\\nR\\nj\\n00\\n1\\n11\\n1\\n111\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n11\\n0\\n1\\n0\\n00\\n1\\n0\\n0\\n0\\n11\\n1\\n0\\n0\\n1\\nb a\\nc\\ne d\\nf\\nFIGURE 2.28\\n (a) An arrangement of pixels. (b) Pixels that are 8-adjacent (adjacency is shown by dashed lines). \\n \\n(c) \\nm', metadata={'source': 'imagepro.pdf', 'page': 81}),\n",
       " Document(page_content='(c) \\nm\\n-adjacency. (d) Two regions (of 1’s) that are 8-adjacent. (e) The circled point is on the boundary of the 1-valued \\npixels only if 8-adjacency between the region and background is used. (f) The inner boundary of the 1-valued region \\ndoes not form a closed path, but its outer boundary does.\\nDIP4E_GLOBAL_Print_Ready.indb   80\\n6/16/2017   2:02:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 81}),\n",
       " Document(page_content='2.5\\n  \\nSome Basic Relationships Between Pixels\\n    \\n81\\nparagraph, a 4-path between the two regions does not exist, so their union is not a \\nconnected set).\\nSuppose an image contains \\nK\\n disjoint regions, \\nRk K\\nk\\n,, , , ,\\n=\\n12\\n…\\n none of which \\ntouches the image border\\n.\\n†\\n Let \\nR\\nu\\n denote the union of all the \\nK\\n regions, and let \\nR\\nu\\nc\\n()\\n denote its complement (recall that the \\ncomplement\\n of a set \\nA\\n is the set of \\npoints that are not in \\nA\\n). We call all the points in \\nR\\nu\\n the \\nforeground\\n, and all the \\npoints in \\nR\\nu\\nc\\n()\\n the \\nbackground\\n of the image.\\nThe \\nboundary\\n (also called the \\nborder\\n or \\ncontour\\n) of a region \\nR\\n is the set of pixels in \\nR\\n that are adjacent to pixels in the complement of \\nR\\n. Stated another way, the border \\nof a region is the set of pixels in the region that have at least one background neigh-\\nbor. Here again, we must specify the connectivity being used to deﬁne adjacency. For', metadata={'source': 'imagepro.pdf', 'page': 82}),\n",
       " Document(page_content='example, the point circled in Fig. 2.28(e) is not a member of the border of the 1-val-\\nued region if 4-connectivity is used between the region and its background, because \\nthe only possible connection between that point and the background is diagonal. \\nAs a rule, adjacency between points in a region and its background is deﬁned using \\n8-connectivity to handle situations such as this.\\nThe preceding deﬁnition sometimes is referred to as the \\ninner border\\n of the \\nregion to distinguish it from its \\nouter border\\n, which is the corresponding border in \\nthe background. This distinction is important in the development of border-follow-\\ning algorithms. Such algorithms usually are formulated to follow the outer boundary \\nin order to guarantee that the result will form a closed path. For instance, the inner \\nborder of the 1-valued region in Fig. 2.28(f) is the region itself. This border does not \\nsatisfy the deﬁnition of a closed path. On the other hand, the outer border of the', metadata={'source': 'imagepro.pdf', 'page': 82}),\n",
       " Document(page_content='region does form a closed path around the region.\\nIf \\nR\\n happens to be an entire image, then its \\nboundary\\n (or \\nborder\\n) is deﬁned as the \\nset of pixels in the ﬁrst and last rows and columns of the image. This extra deﬁnition \\nis required because an image has no neighbors beyond its border. Normally, when \\nwe refer to a region, we are referring to a subset of an image, and any pixels in the \\nboundary of the region that happen to coincide with the border of the image are \\nincluded implicitly as part of the region boundary.\\nThe concept of an \\nedge\\n is found frequently in discussions dealing with regions \\nand boundaries. However, there is a key difference between these two concepts. The \\nboundary of a ﬁnite region forms a closed path and is thus a “global” concept. As we \\nwill discuss in detail in Chapter 10, edges are formed from pixels with derivative val-\\nues that exceed a preset threshold. Thus, an edge is a “local” concept that is based on', metadata={'source': 'imagepro.pdf', 'page': 82}),\n",
       " Document(page_content='a measure of intensity-level discontinuity at a point. It is possible to link edge points \\ninto edge segments, and sometimes these segments are linked in such a way that \\nthey correspond to boundaries, but this is not always the case. The one exception in \\nwhich edges and boundaries correspond is in binary images. Depending on the type \\nof connectivity and edge operators used (we will discuss these in Chapter 10), the \\nedge extracted from a binary region will be the same as the region boundary. This is \\n†\\n  We make this assumption to avoid having to deal with special cases. This can be done without loss of generality \\nbecause if one or more regions touch the border of an image, we can simply pad the image with a 1-pixel-wide \\nborder of background values.\\nDIP4E_GLOBAL_Print_Ready.indb   81\\n6/16/2017   2:02:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 82}),\n",
       " Document(page_content='82\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nintuitive. Conceptually, until we arrive at Chapter 10, it is helpful to think of edges \\nas intensity discontinuities, and of boundaries as closed paths.\\nDISTANCE MEASURES\\nFor pixels \\np\\n, \\nq\\n, and \\ns\\n, with coordinates \\n(,)\\nxy\\n, \\n(,)\\nu\\nv\\n, and \\n(, ) ,\\nw\\nz\\n respectively, \\nD\\nis a \\ndistance function\\n or \\nmetric\\n if\\n(a) \\nD\\npq D pq p q\\n(,) ( (,) )\\n≥\\n00\\n==\\niff\\n,\\n(b) \\nDp\\nq Dqp\\n(,) (,)\\n=\\n, and\\n(c) \\nDps Dpq Dqs\\n(,) (,) (,)\\n.\\n≤+\\nThe \\nEuclidean distance\\n between \\np\\n and \\nq\\n is defined as\\n \\nDp q x y\\ne\\n(,) ( ) ( )\\n=−+ −\\n⎡\\n⎣\\n⎤\\n⎦\\nuv\\n22\\n1\\n2\\n \\n(2-19)\\nFor this distance measure, the pixels having a distance less than or equal to some \\nvalue \\nr\\n from \\n(,)\\nxy\\n are the points contained in a disk of radius \\nr\\n centered at \\n(,)\\nxy\\n.\\nT\\nhe \\nD\\n4\\n \\ndistance\\n, (called the \\ncity-block distance\\n) between \\np\\n and \\nq\\n is deﬁned as\\n \\nDp q x y\\n4\\n(,)\\n=− −\\nuv\\n+\\n \\n(2-20)\\nIn this case, pixels having a \\nD\\n4\\n distance from \\n(,)\\nxy\\n that is less than or equal to some \\nvalue \\nd', metadata={'source': 'imagepro.pdf', 'page': 83}),\n",
       " Document(page_content='value \\nd\\n form a diamond centered at \\n(,)\\nxy\\n. For example, the pixels with \\nD\\n4\\n distance \\n≤2\\n \\nfrom \\n(,)\\nxy\\n (the center point) form the following contours of constant distance:\\n \\n2\\n212\\n2101\\n2\\n212\\n2\\nThe pixels with \\nD\\n4\\n1\\n=\\n are the 4-neighbors of \\n(,)\\nxy\\n.\\nT\\nhe \\nD\\n8\\n \\ndistance\\n (called the \\nchessboard distance\\n) between \\np\\n and \\nq\\n is deﬁned as\\n \\nDp q x y\\n8\\n( , ) max( , )\\n=− −\\nuv\\n \\n(2-21)\\nIn this case, the pixels with \\nD\\n8\\n distance from \\n(,)\\nxy\\n less than or equal to some value \\nd\\n \\nform a square centered at \\n(,)\\nxy\\n. For example, the pixels with \\nD\\n8\\n distance \\n≤2\\n form \\nthe following contours of constant distance:\\n \\n22222\\n21112\\n2101\\n2\\n21112\\n22222\\nThe pixels with \\nD\\n8\\n1\\n=\\n are the 8-neighbors of the pixel at \\n(,)\\nxy\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   82\\n6/16/2017   2:02:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 83}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n83\\nNote that the \\nD\\n4\\n and \\nD\\n8\\n distances between \\np\\n and \\nq\\n are independent of any paths \\nthat might exist between these points because these distances involve only the coor-\\ndinates of the points. In the case of \\nm\\n-adjacency, however, the \\nD\\nm\\n distance between \\ntwo points is deﬁned as the shortest \\nm\\n-path between the points. In this case, the \\ndistance between two pixels will depend on the values of the pixels along the path, \\nas well as the values of their neighbors. For instance, consider the following arrange-\\nment of pixels and assume that \\np\\n, \\np\\n2\\n, and \\np\\n4\\n have a value of 1, and that \\np\\n1\\n and \\np\\n3\\n \\ncan be 0 or 1:\\n \\npp\\npp\\np\\n34\\n12\\nSuppose that we consider adjacency of pixels valued 1 (i.e.,\\nV\\n=\\n{}\\n1\\n). If \\np\\n1\\n and \\np\\n3\\n are 0, \\nthe length of the shortest \\nm\\n-path (the \\nD\\nm\\n distance) between \\np\\n and \\np\\n4\\n is 2. If \\np\\n1\\n is 1, \\nthen \\np\\n2\\n and \\np\\n will no longer be \\nm', metadata={'source': 'imagepro.pdf', 'page': 84}),\n",
       " Document(page_content='m\\n-adjacent (see the definition of \\nm\\n-adjacency given \\nearlier) and the length of the shortest \\nm\\n-path becomes 3 (the path goes through the \\npoints \\npp p p\\n124\\n). Similar comments apply if \\np\\n3\\n is 1 (and \\np\\n1\\n is 0); in this case, the \\nlength of the shortest \\nm\\n-path also is 3. Finally, if both \\np\\n1\\n and \\np\\n3\\n are 1, the length of \\nthe shortest \\nm\\n-path between \\np\\n and \\np\\n4\\n is 4. In this case, the path goes through the \\nsequence of points \\npp p p p\\n1234\\n.\\n2.6 INTRODUCTION TO THE BASIC MATHEMATICAL TOOLS USED IN \\nDIGITAL IMAGE PROCESSING \\nThis section has two principal objectives: (1) to introduce various mathematical \\ntools we use throughout the book; and (2) to help you begin developing a “feel” for \\nhow these tools are used by applying them to a variety of basic image-processing \\ntasks, some of which will be used numerous times in subsequent discussions. \\nELEMENTWISE VERSUS MATRIX OPERATIONS\\nAn \\nelementwise operation\\n involving one or more images is carried out on a \\npixel-by-', metadata={'source': 'imagepro.pdf', 'page': 84}),\n",
       " Document(page_content='pixel-by-\\npixel \\nbasis. We mentioned earlier in this chapter that images can be viewed equiva-\\nlently as matrices. In fact, as you will see later in this section, there are many situ-\\nations in which operations between images are carried out using matrix theory. It \\nis for this reason that a clear distinction must be made between elementwise and \\nmatrix operations. For example, consider the following \\n22\\n*\\n images (matrices):\\n \\naa\\naa\\nbb\\nbb\\n11 12\\n21 22\\n11 12\\n21 22\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nand\\nThe \\nelementwise product\\n (often denoted using the symbol \\n}\\n or \\nz\\n)\\n of these two \\nimages is\\n \\naa\\naa\\nbb\\nbb\\nab\\nab\\nab a\\n11 12\\n21 22\\n11 12\\n21 22\\n11 11 12 12\\n21 21 2\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n}\\n2\\n22 2\\nb\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n2.6\\nYou may ﬁnd it helpful \\nto download and study \\nthe review material \\ndealing with probability, \\nvectors, linear algebra, \\nand linear systems. The \\nreview is available in the \\nTutorials section of the \\nbook website. \\nThe elementwise product \\nof two matrices is also \\ncalled the \\nHadamar', metadata={'source': 'imagepro.pdf', 'page': 84}),\n",
       " Document(page_content='called the \\nHadamar\\nd \\nproduct\\n of the matrices.\\nThe symbol \\n| \\nis often \\nused to denote \\nelement-\\nwise division\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   83\\n6/16/2017   2:02:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 84}),\n",
       " Document(page_content='84\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nThat is, the elementwise product is obtained by multiplying pairs of \\ncorresponding\\n \\npixels. On the other hand, the \\nmatrix product\\n of the images is formed using the rules \\nof matrix multiplication:\\n \\naa\\naa\\nbb\\nbb\\nab\\nab ab a\\n11 12\\n21 22\\n11 12\\n21 22\\n11 11 12 21 11 12\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n++\\n1 12 22\\n21 11 22 21 21 12 22 22\\nb\\nab ab ab ab\\n++\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nWe assume elementwise operations throughout the book, unless stated otherwise. \\nF\\nor example, when we refer to raising an image to a power, we mean that each indi-\\nvidual pixel is raised to that power; when we refer to dividing an image by another, \\nwe mean that the division is between corresponding pixel pairs, and so on. The terms \\nelementwise addition\\n and \\nsubtraction\\n of two images are redundant because these are \\nelementwise operations by deﬁnition. However, you may see them used sometimes \\nto clarify notational ambiguities. \\nLINEAR VERSUS NONLINEAR OPERATIONS', metadata={'source': 'imagepro.pdf', 'page': 85}),\n",
       " Document(page_content='One of the most important classifications of an image processing method is whether \\nit is linear or nonlinear. Consider a general operator, \\n/H5108\\n,\\n that produces an output \\nimage\\n, \\ngxy\\n(,\\n)\\n, from a given input image, \\nfx y\\n(,\\n)\\n:\\n \\n/H5108\\nfx\\ny g x y\\n(,) (,)\\n[]\\n=\\n \\n(2-22)\\nGiven two arbitrary constants, \\na\\n and \\nb\\n,\\n and two arbitrary images \\nfx y\\n1\\n(,)\\n and \\nfx y\\n2\\n(,) ,\\n/H5108\\n is said to be a \\nlinear operator\\n if\\n \\n/H5108/H5108\\n/H5108\\na\\nf xy b f xy a f xy b f xy\\nag x y bg\\n12 1 2\\n12\\n(,) (,) (,) (,)\\n(,) (\\n+\\n[]\\n=\\n[]\\n+\\n[]\\n=+\\nx\\nxy\\n,)\\n \\n(2-23)\\nThis equation indicates that the output of a linear operation applied to the sum of \\ntwo inputs is the same as performing the operation individually on the inputs and \\nthen summing the results\\n. In addition, the output of a linear operation on a con-\\nstant multiplied by an input is the same as the output of the operation due to the \\noriginal input multiplied by that constant. The first property is called the property \\nof \\nadditivity,', metadata={'source': 'imagepro.pdf', 'page': 85}),\n",
       " Document(page_content='of \\nadditivity,\\n and the second is called the property of \\nhomogeneity\\n. By definition, an \\noperator that fails to satisfy Eq. (2-23) is said to be \\nnonlinear\\n.\\nAs an example, suppose that \\n/H5108\\n is the sum operator, \\nΣ\\n.\\n The function performed \\nby this operator is simply to sum its inputs. To test for linearity, we start with the left \\nside of Eq. (2-23) and attempt to prove that it is equal to the right side:\\n \\na f xy b f xy a f xy b f xy\\na\\nf xy b f xy\\n12 1 2\\n12\\n(,) (,) (,) (,)\\n(,) (,)\\n+\\n[]\\n=+\\n=+\\n∑∑\\n∑\\n∑ ∑ ∑\\n=+\\na g xy b g xy\\n12\\n(,) (,)\\n \\nwhere the first step follows from the fact that summation is distributive\\n. So, an \\nexpansion of the left side is equal to the right side of Eq. (2-23), and we conclude \\nthat the sum operator is linear.\\nThese are image  \\nsummations, not the \\nsums of all the elements \\nof an image. \\nDIP4E_GLOBAL_Print_Ready.indb   84\\n6/16/2017   2:02:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 85}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n85\\nOn the other hand, suppose that we are working with the max operation, whose \\nfunction is to ﬁnd the maximum value of the pixels in an image. For our purposes \\nhere, the simplest way to prove that this operator is nonlinear is to ﬁnd an example \\nthat fails the test in Eq. (2\\n-23). Consider the following two images\\n \\nff\\n12\\n02\\n23\\n65\\n47\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nand\\nand suppose that we let \\na\\n=\\n1\\n and \\nb\\n=−\\n1\\n. To test for linearity, we again start with the \\nleft side of Eq.\\n (2-23):\\n \\nmax ( ) ( ) max\\n1\\n02\\n23\\n1\\n65\\n47\\n63\\n24\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n+−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n=\\n−−\\n−−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫ ⎫\\n⎬\\n⎭\\n=−\\n2\\nWorking next with the right side, we obtain\\n \\n() m a x ( ) m a x\\n( )\\n1\\n02\\n23\\n1\\n65\\n47\\n31\\n7 4\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n+−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎧\\n⎨\\n⎩\\n⎫\\n⎬\\n⎭\\n=+\\n−= −\\nThe left and right sides of Eq. (2-23) are not equal in this case, so we have proved \\nthat the max operator is nonlinear\\n.', metadata={'source': 'imagepro.pdf', 'page': 86}),\n",
       " Document(page_content='.\\nAs you will see in the next three chapters, linear operations are exceptionally impor-\\ntant because they encompass a large body of theoretical and practical results that are \\napplicable to image processing. \\nThe scope of nonlinear operations is considerably \\nmore limited. However, you will encounter in the following chapters several nonlin-\\near image processing operations whose performance far exceeds what is achievable \\nby their linear counterparts.\\nARITHMETIC OPERATIONS\\nArithmetic operations between two images \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n are denoted as\\n \\nsxy f xy gxy\\ndx\\ny f xy gxy\\npxy f xy gx\\n(,) (,) (,)\\n(,) (,) (,)\\n(,) (,) (,\\n=+\\n=−\\n=×\\ny y\\nxy f xy gxy\\n)\\n(,) (,) (,)\\nv\\n=\\n÷\\n \\n(2-24)\\nThese are elementwise operations which, as noted earlier in this section, means \\nthat they are performed between corresponding pixel pairs in \\nf\\n and \\ng\\n for \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As usual, \\nM\\n and \\nN\\n are the row and \\ncolumn sizes of the images\\n. Clearly, \\ns\\n, \\nd\\n, \\np\\n, and \\nv', metadata={'source': 'imagepro.pdf', 'page': 86}),\n",
       " Document(page_content=', \\nd\\n, \\np\\n, and \\nv\\n are images of size \\nMN\\n×\\n also. \\nNote that image arithmetic in the manner just defined involves images of the same \\nsize\\n. The following examples illustrate the important role of arithmetic operations \\nin digital image processing.\\nDIP4E_GLOBAL_Print_Ready.indb   85\\n6/16/2017   2:02:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 86}),\n",
       " Document(page_content='86\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nEXAMPLE 2.5 :  Using image addition (averaging) for noise reduction.\\nSuppose that \\ngxy\\n(,\\n)\\n is a corrupted image formed by the addition of noise, \\nh\\n(, )\\nxy\\n, to a \\nnoiseless\\n image \\nfx y\\n(,\\n)\\n \\n; that is,\\n \\ngxy fxy xy\\n(,\\n) (, ) (, )\\n=+\\nh\\n \\n(2-25)\\nwhere the assumption is that at every pair of coordinates \\n(,)\\nxy\\n the noise is uncorrelated\\n†\\n and has \\nzero average value. We assume also that the noise and image values are uncorrelated (this is a typical \\nassumption for additive noise). The objective of the following procedure is to reduce the noise content \\nof the output image by adding a set of noisy input images, \\ngx y\\ni\\n(,).\\n{}\\n This is a technique used frequently \\nfor image enhancement.\\nIf the noise satisﬁes the constraints just stated, it can be shown (Problem 2.26) that if an image \\ngxy\\n(,)\\n \\nis formed by averaging \\nK\\n different noisy images\\n,\\n \\ngxy\\nK\\ngx y\\ni\\ni\\nK\\n(,) (,)\\n=\\n=\\n∑\\n1\\n1\\n \\n(2-26)\\nthen it follows that \\n \\nEg x y fx y\\n(,) (,)', metadata={'source': 'imagepro.pdf', 'page': 87}),\n",
       " Document(page_content='Eg x y fx y\\n(,) (,)\\n{}\\n=\\n \\n(2-27)\\nand\\n \\nss\\nh\\ngx y\\nx y\\nK\\n(,) (,)\\n22\\n1\\n=\\n \\n(2-28)\\nwhere \\nEg x y\\n(,)\\n{}\\n is the expected value of \\ngxy\\n(,)\\n, and \\ns\\ngx y\\n(,)\\n2\\n and \\ns\\nh\\n(,)\\nxy\\n2\\n are the variances of \\ngxy\\n(,)\\n and \\nh\\n(,\\n)\\nxy\\n, respectively, all at coordinates \\n(,)\\nxy\\n. These variances are arrays of the same size as the input \\nimage\\n, and there is a scalar variance value for each pixel location. \\nThe standard deviation (square root of the variance) at any point \\n(,)\\nxy\\n in the average image is\\n \\nss\\nh\\ngx y\\nx y\\nK\\n(,)\\n(,)\\n=\\n1\\n \\n(2-29)\\nAs \\nK\\n increases\\n, Eqs. (2-28) and (2-29) indicate that the variability (as measured by the variance or the \\nstandard deviation) of the pixel values at each location \\n(,)\\nxy\\n decreases. Because \\nEg x y fx y\\n(,) (,) ,\\n{}\\n=\\n \\nthis means that \\ngxy\\n(,)\\n approaches the noiseless image \\nfx y\\n(,\\n)\\n as the number of noisy images used in the \\naveraging process increases\\n. In order to avoid blurring and other artifacts in the output (average) image,', metadata={'source': 'imagepro.pdf', 'page': 87}),\n",
       " Document(page_content='it is necessary that the images \\ngx y\\ni\\n(,)\\n be \\nregistered\\n (i.e\\n., spatially aligned).\\nAn important application of image averaging is in the ﬁeld of astronomy, where imaging under \\nvery low light levels often cause sensor noise to render individual images virtually useless for analysis \\n(lowering the temperature of the sensor helps reduce noise). Figure 2.29(a) shows an 8-bit image of the \\nGalaxy Pair NGC 3314, in which noise corruption was simulated by adding to it Gaussian noise with \\nzero mean and a standard deviation of 64 intensity levels. This image, which is representative of noisy \\nastronomical images taken under low light conditions, is useless for all practical purposes. Figures \\n2.29(b) through (f) show the results of averaging 5, 10, 20, 50, and 100 images, respectively. We see from \\nFig. 2.29(b) that an average of only 10 images resulted in some visible improvement.\\n \\nAccording to Eq. \\n†\\n The variance of a random variable \\nz\\n with mean \\nz  \\nis deﬁned as \\nEz z\\n{( ) }\\n−\\n2', metadata={'source': 'imagepro.pdf', 'page': 87}),\n",
       " Document(page_content='Ez z\\n{( ) }\\n−\\n2\\n, where \\nE\\n{}\\n/H17033\\n is the expected value of the argument. The covari-\\nance of two random variables \\nz\\ni\\n and \\nz\\nj\\n is deﬁned as \\nEz z z z\\nii jj\\n{( )( )}.\\n−−\\n If the variables are uncorrelated, their covariance is 0, and vice \\nversa.\\n (Do not confuse correlation and statistical independence. If two random variables are statistically independent, their correlation is \\nzero. However, the converse is not true in general.)\\nDIP4E_GLOBAL_Print_Ready.indb   86\\n6/16/2017   2:02:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 87}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n87\\n(2-29), the standard deviation of the noise in Fig. 2.29(b) is less than half \\n(. )\\n150 4 5\\n=\\n the standard \\ndeviation of the noise in F\\nig. 2.29(a), or \\n(. ) ( )\\n04\\n5 6 4 2 9\\n≈\\n intensity levels. Similarly, the standard devia-\\ntions of the noise in F\\nigs. 2.29(c) through (f) are 0.32, 0.22, 0.14, and 0.10 of the original, which translates \\napproximately into 20, 14, 9, and 6 intensity levels, respectively. We see in these images a progression \\nof more visible detail as the standard deviation of the noise decreases. The last two images are visually \\nidentical for all practical purposes. This is not unexpected, as the difference between the standard devia-\\ntions of their noise level is only about 3 intensity levels According to the discussion in connection with \\nFig. 2.5, this difference is below what a human generally is able to detect.\\nEXAMPLE 2.6 :  Comparing images using subtraction.', metadata={'source': 'imagepro.pdf', 'page': 88}),\n",
       " Document(page_content='Image subtraction is used routinely for enhancing differences between images. For example, the image \\nin Fig. 2.30(b) was obtained by setting to zero the least-signiﬁcant bit of every pixel in Fig. 2.30(a). \\nVisually, these images are indistinguishable. However, as Fig. 2.30(c) shows, subtracting one image from \\nb a\\nc\\ne d\\nf\\nFIGURE 2.29\\n (a) Image of Galaxy Pair NGC 3314 corrupted by additive Gaussian noise. (b)-(f) Result of averaging \\n5, 10, 20, 50, and 1,00 noisy images, respectively. All images are of size \\n566 598\\n×\\n pixels, and all were scaled so that \\ntheir intensities would span the full [0, 255] intensity scale. (Original image courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   87\\n6/16/2017   2:02:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 88}),\n",
       " Document(page_content='88\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nthe other clearly shows their differences. Black (0) values in the difference image indicate locations \\nwhere there is no difference between the images in Figs. 2.30(a) and (b). \\nWe saw in Fig. 2.23 that detail was lost as the resolution was reduced in the chronometer image \\nshown in Fig. 2.23(a). A vivid indication of image change as a function of resolution can be obtained \\nby displaying the differences between the original image and its various lower-resolution counterparts. \\nFigure 2.31(a) shows the difference between the 930 dpi and 72 dpi images. As you can see, the dif-\\nferences are quite noticeable. The intensity at any point in the difference image is proportional to the \\nmagnitude of the numerical difference between the two images at that point. Therefore, we can analyze \\nwhich areas of the original image are affected the most when \\nresolution is reduced. The \\nnext two images', metadata={'source': 'imagepro.pdf', 'page': 89}),\n",
       " Document(page_content='next two images \\nin Fig. 2.31 show proportionally less overall intensities, indicating smaller differences between the 930 dpi \\nimage and 150 dpi and 300 dpi images, as expected. \\nb a\\nc\\nFIGURE 2.30\\n (a) Infrared image of the Washington, D.C. area. (b) Image resulting from setting to zero the least \\nsigniﬁcant bit of every pixel in (a). (c) Difference of the two images, scaled to the range [0, 255] for clarity. (Original \\nimage courtesy of NASA.)\\nb a\\nc\\nFIGURE 2.31\\n (a) Difference between the 930 dpi and 72 dpi images in Fig. 2.23. (b) Difference between the 930 dpi and \\n150 dpi images. (c) Difference between the 930 dpi and 300 dpi images.\\nDIP4E_GLOBAL_Print_Ready.indb   88\\n6/16/2017   2:02:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 89}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n89\\nAs a ﬁnal illustration, we discuss brieﬂy an area of medical imaging called \\nmask mode radiography\\n, a \\ncommercially successful and highly beneﬁcial use of image subtraction. Consider image differences of \\nthe form\\n \\ngxy f xy hxy\\n(,\\n) (,) (,)\\n=−\\n \\n(2-30)\\nIn this case \\nhxy\\n(,\\n)\\n, the \\nmask\\n,\\n is an X-ray image of a region of a patient’s body captured by an intensiﬁed \\nTV camera (instead of traditional X-ray ﬁlm) located opposite an X-ray source. The procedure consists \\nof injecting an X-ray contrast medium into the patient’s bloodstream, taking a series of images called \\nlive images\\n [samples of which are denoted as \\nfx y\\n(,\\n)\\n] of the same anatomical region as \\nhxy\\n(,\\n)\\n, and sub-\\ntracting the mask from the series of incoming live images after injection of the contrast medium.\\n The net \\neffect of subtracting the mask from each sample live image is that the areas that are different between \\nfx y', metadata={'source': 'imagepro.pdf', 'page': 90}),\n",
       " Document(page_content='fx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n appear in the output image, \\ngxy\\n(,\\n)\\n, as enhanced detail. Because images can be cap-\\ntured at \\nTV rates, this procedure outputs a video showing how the contrast medium propagates through \\nthe various arteries in the area being observed.\\nFigure 2.32(a) shows a mask X-ray image of the top of a patient’s head prior to injection of an iodine \\nmedium into the bloodstream, and Fig. 2.32(b) is a sample of a live image taken after the medium was \\nb a\\nd c\\nFIGURE 2.32\\n  \\nDigital  \\nsubtraction  \\nangiography.  \\n(a) Mask image. \\n(b) A live image. \\n(c) Difference \\nbetween (a) and \\n(b). (d) Enhanced \\ndifference image. \\n(Figures (a) and \\n(b) courtesy of \\nthe Image  \\nSciences  \\nInstitute,  \\nUniversity \\nMedical Center, \\nUtrecht, The \\nNetherlands.)\\nDIP4E_GLOBAL_Print_Ready.indb   89\\n6/16/2017   2:02:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 90}),\n",
       " Document(page_content='90\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\ninjected. Figure 2.32(c) is the difference between (a) and (b). Some ﬁne blood vessel structures are vis-\\nible in this image. The difference is clear in Fig. 2.32(d), which was obtained by sharpening the image and \\nenhancing its contrast (we will discuss these techniques in the next chapter). Figure 2.32(d) is a “snap-\\nshot” of how the medium is propagating through the blood vessels in the subject’s brain.\\nEXAMPLE 2.7 :  Using image multiplication and division for shading correction and for masking.\\nAn important application of image multiplication (and division) is \\nshading correction\\n. Suppose that an \\nimaging sensor produces images that can be modeled as the product of a “perfect image,” denoted by \\nfx y\\n(,\\n)\\n, times a shading function, \\nhxy\\n(,\\n)\\n; that is, \\ngxy f xyhxy\\n(,\\n) (,)(,)\\n=\\n. If \\nhxy\\n(,\\n)\\n is known or can be \\nestimated,\\n we can obtain \\nfx y\\n(,\\n)\\n (or an estimate of it) by multiplying the sensed image by the inverse of \\nhxy', metadata={'source': 'imagepro.pdf', 'page': 91}),\n",
       " Document(page_content='hxy\\n(,\\n)\\n (i.e., dividing \\ng\\n by \\nh \\nusing\\n \\nelementwise division).\\n If access to the imaging system is possible, we \\ncan obtain a good approximation to the shading function by imaging a target of constant intensity. When \\nthe sensor is not available, we often can estimate the shading pattern directly from a shaded image using \\nthe approaches discussed in Sections 3.5 and 9.8. Figure 2.33 shows an example of shading correction \\nusing an estimate of the shading pattern. The corrected image is not perfect because of errors in the \\nshading pattern (this is typical), but the result deﬁnitely is an improvement over the shaded image in Fig. \\n2.33 (a). See Section 3.5\\n \\nfor a discussion of how we estimated Fig. 2.33 (b). Another use of image mul-\\ntiplication is in \\nmasking\\n,\\n also called \\nregion of interest\\n (ROI), operations. As Fig. 2.34 shows, the process \\nconsists of multiplying a given image by a mask image that has 1’s in the ROI and 0’s elsewhere. There', metadata={'source': 'imagepro.pdf', 'page': 91}),\n",
       " Document(page_content='can be more than one ROI in the mask image, and the shape of the ROI can be arbitrary.\\nA few comments about implementing image arithmetic operations are in order \\nbefore we leave this section. In practice, most images are displayed using 8 bits (even \\n24-bit color images consist of three separate 8-bit channels). Thus, we expect image \\nvalues to be in the range from 0 to 255. When images are saved in a standard image \\nformat, such as TIFF or JPEG, conversion to this range is automatic. When image \\nvalues exceed the allowed range, clipping or scaling becomes necessary. For example, \\nthe values in the difference of two 8-bit images can range from a minimum of \\n−\\n255 \\nb a\\nc\\nFIGURE 2.33\\n Shading correction. (a) Shaded test pattern. (b) Estimated shading pattern. (c) Product of (a) by the \\nreciprocal of (b). (See Section 3.5 for a discussion of how (b) was estimated.)\\nDIP4E_GLOBAL_Print_Ready.indb   90\\n6/16/2017   2:02:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 91}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n91\\nto a maximum of 255, and the values of the sum of two such images can range from 0 \\nto 510. When converting images to eight bits, many software applications simply set \\nall negative values to 0 and set to 255 all values that exceed this limit. Given a digital \\nimage \\ng\\n resulting from one or more arithmetic (or other) operations, an approach \\nguaranteeing that the full range of a values is “captured” into a ﬁxed number of bits \\nis as follows. First, we perform the operation\\n \\ngg g\\nm\\n=−\\nmin( )\\n \\n(2-31)\\nwhich creates an image whose minimum value is 0. Then, we perform the operation\\n \\ngK g g\\nsm m\\n=\\n[]\\nmax( )\\n \\n(2-32)\\nwhich creates a scaled image, \\ng\\ns\\n,\\n whose values are in the range [0, \\nK\\n].\\n When working \\nwith 8-bit images, setting \\nK\\n=\\n255\\n gives us a scaled image whose intensities span the \\nfull 8-bit scale from 0 to 255.\\n  Similar comments apply to 16-bit images or higher. This', metadata={'source': 'imagepro.pdf', 'page': 92}),\n",
       " Document(page_content='approach can be used for all arithmetic operations. When performing division, we \\nhave the extra requirement that a small number should be added to the pixels of the \\ndivisor image to avoid division by 0.\\nSET AND LOGICAL OPERATIONS\\nIn this section, we discuss the basics of set theory. We also introduce and illustrate \\nsome important set and logical operations.\\nBasic Set Operations\\nA \\nset\\n is a collection of distinct objects. If \\na\\n is an \\nelement\\n of set \\nA\\n, then we write\\n \\naA\\n∈\\n  \\n(2-33)\\nSimilarly, if \\na\\n is not an element of \\nA\\n we write\\n \\naA\\nx\\n \\n(2-34)\\nThe set with no elements is called the \\nnull\\n or \\nempty set\\n,\\n and is denoted by \\n∅\\n.\\nThese are elementwise \\nsubtraction and division.\\nb a\\nc\\nFIGURE 2.34\\n (a) Digital dental X-ray image. (b) ROI mask for isolating teeth with ﬁllings (white corresponds to 1 and \\nblack corresponds to 0). (c) Product of (a) and (b).\\nDIP4E_GLOBAL_Print_Ready.indb   91\\n6/16/2017   2:02:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 92}),\n",
       " Document(page_content='92\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nA set is denoted by the contents of two braces: \\n{} .\\ni\\n For example, the expression\\n \\nCc c d d D\\n==\\n{}\\n-H\\n,\\nmeans that \\nC\\n is the set of elements\\n, \\nc\\n, such that \\nc\\n is formed by multiplying each of \\nthe elements of set \\nD\\n by \\n−\\n1\\n. \\nIf every element of a set \\nA\\n is also an element of a set \\nB\\n,\\n then \\nA\\n is said to be a \\nsubset\\n of \\nB\\n, denoted as\\n \\nAB\\n8\\n \\n(2-35)\\nThe \\nunion\\n of two sets \\nA\\n and \\nB\\n,\\n denoted as\\n \\nCA B\\n=\\n´\\n \\n(2-36)\\nis a set \\nC\\n consisting of elements belonging \\neither\\n to \\nA\\n,\\n to \\nB\\n, \\nor\\n to \\nboth\\n. Similarly, the \\nintersection\\n of two sets \\nA\\n and \\nB\\n, denoted by\\n \\nDA B\\n=\\n¨\\n \\n(2-37)\\nis a set \\nD\\n consisting of elements belonging to \\nboth\\n \\nA\\n and \\nB\\n.\\n Sets \\nA\\n and \\nB\\n are said to \\nbe \\ndisjoint\\n or \\nmutually\\n \\nexclusive\\n if they have no elements in common, in which case,\\n \\nAB\\n¨\\n=∅\\n \\n(2-38)\\nThe \\nsample space\\n, \\nÆ\\n,\\n (also called the \\nset\\n \\nuniverse\\n) is the set of all possible set \\nelements in a given application.', metadata={'source': 'imagepro.pdf', 'page': 93}),\n",
       " Document(page_content='By deﬁnition, these set elements are members of \\nthe sample space for that application. For example, if you are working with the set \\nof real numbers, then the sample space is the real line, which contains all the real \\nnumbers. In image processing, we typically deﬁne \\nÆ\\n to be the rectangle containing \\nall the pixels in an image\\n.\\nThe \\ncomplement\\n of a set \\nA\\n is the set of elements that are not in \\nA\\n:\\n \\nAA\\nc\\n=\\n{}\\nww\\nx\\n \\n(2-39)\\nThe \\ndifference\\n of two sets \\nA\\n and \\nB\\n,\\n denoted \\nAB\\n−\\n, is defined as\\n \\nAB A B A B\\nc\\n−=\\n{}\\n=\\nww w\\nHx ¨\\n,\\n \\n(2-40)\\nThis is the set of elements that belong to \\nA\\n,\\n but not to \\nB\\n. We can define \\nA\\nc\\n in terms \\nof \\nÆ\\n and the set difference operation; that is, \\nAA\\nc\\n=−\\nÆ\\n. Table 2.1 shows several \\nimportant set properties and relationships\\n.\\nFigure 2.35\\n \\nshows diagrammatically (in so-called \\nV\\nenn diagrams\\n) some of the set \\nrelationships in Table 2.1. The shaded areas in the various figures correspond to the', metadata={'source': 'imagepro.pdf', 'page': 93}),\n",
       " Document(page_content='set operation indicated above or below the figure. Figure 2.35(a) shows the sample \\nset, \\nÆ\\n.\\n As no earlier, this is the set of all possible elements in a given application. Fig-\\nure 2.35(b) shows that the complement of a set \\nA\\n is the set of all elements in \\nÆ\\n \\nthat \\nare not in \\nA\\n,\\n which agrees with our earlier deﬁnition. Observe that Figs. 2.35(e) and \\n(g) are identical, which proves the validity of Eq. (2-40) using Venn diagrams. This \\nDIP4E_GLOBAL_Print_Ready.indb   92\\n6/16/2017   2:02:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 93}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n93\\nis an example of the usefulness of Venn diagrams for proving equivalences between \\nset relationships. \\nWhen applying the concepts just discussed to image processing, we let sets repre-\\nsent objects (regions) in a binary image, and the elements of the sets are the \\n(,)\\nxy\\n \\ncoordinates of those objects\\n. For example, if we want to know whether two objects, \\nA\\n and \\nB,\\n of a binary image overlap, all we have to do is compute \\nAB\\n¨\\n. If the result \\nis not the empty set,\\n we know that some of the elements of the two objects overlap. \\nKeep in mind that the only way that the operations illustrated in Fig. 2.35 can make \\nsense in the context of image processing is if the images containing the sets are \\nbinary, in which case we can talk about set membership based on coordinates, the \\nassumption being that all members of the sets have the same intensity value (typi-', metadata={'source': 'imagepro.pdf', 'page': 94}),\n",
       " Document(page_content='cally denoted by 1). We will discuss set operations involving binary images in more \\ndetail in the following section and in Chapter 9.\\nThe preceding concepts are not applicable when dealing with grayscale images, \\nbecause we have not deﬁned yet a mechanism for assigning intensity values to the \\npixels resulting from a set operation. In Sections 3.8 and 9.6 we will deﬁne the union \\nand intersection operations for grayscale values as the maximum and minimum of \\ncorresponding pixel pairs, respectively. We deﬁne the \\ncomplement\\n of a grayscale \\nimage as the pairwise differences between a constant and the intensity of every pixel \\nin the image. The fact that we deal with corresponding pixel pairs tells us that gray-\\nscale set operations are elementwise operations, as deﬁned earlier. The following \\nexample is a brief illustration of set operations involving grayscale images. We will \\ndiscuss these concepts further in the two sections just mentioned.\\nDescription\\nExpressions', metadata={'source': 'imagepro.pdf', 'page': 94}),\n",
       " Document(page_content='Expressions\\nOperations between the \\nsample space and null sets\\nÆÆ Æ\\n´ Æ Æ\\n¨\\ncc\\n=∅ ∅ = ∅= ∅=∅\\n;; ;\\nUnion and intersection with \\nthe null and sample space sets\\nAA A A A A\\n´¨\\n´ Æ Æ ¨ Æ\\n∅= ∅=∅ = =\\n;;;\\nUnion and intersection of a \\nset with itself\\nAAA AAA\\n´¨\\n==\\n;\\nUnion and intersection of a \\nset with its complement\\nAA AA\\ncc\\n´Æ ¨\\n== ∅\\n;\\nCommutative laws\\nABBA\\nAB\\nBA\\n´´\\n¨¨\\n=\\n=\\nAssociative laws\\n() ()\\n()\\n()\\nAB C A BC\\nAB CA BC\\n´´ ´´\\n¨¨ ¨¨\\n=\\n=\\nDistributive laws\\n() () ()\\n()\\n() ()\\nAB C AC BC\\nAB C AC BC\\n´¨ ¨´¨\\n¨´ ´¨´\\n=\\n=\\nDeMorgan’s laws\\n \\n()\\n()\\nAB\\nA B\\nAB A B\\ncc c\\ncc c\\n´¨\\n¨´\\n=\\n=\\nTABLE \\n2.1\\nSome important \\nset operations \\nand relationships.\\nDIP4E_GLOBAL_Print_Ready.indb   93\\n6/16/2017   2:02:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 94}),\n",
       " Document(page_content='94\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nEXAMPLE 2.8 :  Illustration of set operations involving grayscale images.\\nLet the elements of a grayscale image be represented by a set \\nA\\n whose elements are triplets of the form \\n(, , )\\nxy\\nz\\n, where \\nx\\n and \\ny\\n are spatial coordinates\\n, and \\nz\\n denotes intensity values. We deﬁne the \\ncomplement \\nof \\nA\\n as the set \\n \\nAx y K z x y z A\\nc\\n=−\\n{}\\n(, , ) (, , )\\nH\\n \\nwhich is the set of pixels of \\nA\\n whose intensities have been subtracted from a constant \\nK\\n.\\n This constant \\nis equal to the maximum intensity value in the image, \\n21\\nk\\n−\\n, where \\nk\\n is the number of bits used to \\nrepresent \\nz\\n.\\n Let \\nA\\n denote the 8-bit grayscale image in Fig. 2.36(a), and suppose that we want to form \\nthe negative of \\nA\\n using grayscale set operations. The negative is the set complement, and this is an 8-bit \\nimage, so all we have to do is let \\nK\\n=\\n255 in the set deﬁned above:\\n \\nAx y z x y z A\\nc\\n=−\\n{}\\n(, , ) (, , )\\n255\\nH', metadata={'source': 'imagepro.pdf', 'page': 95}),\n",
       " Document(page_content='(, , ) (, , )\\n255\\nH\\nFigure 2.36(b) shows the result. We show this only for illustrative purposes. Image negatives generally \\nare computed using an intensity transformation function,\\n as discussed later in this section.\\nA\\nc\\nAB\\n¨\\nA\\nA\\nB\\nAB\\n−\\nB\\nc\\nB\\nC\\nA\\nAB\\nc\\n¨\\nAB C\\n¨´\\n()\\n´\\nAB\\nΩ\\nB\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 2.35\\n Venn diagrams corresponding to some of the set operations in Table 2.1. The results of the operations, \\nsuch as \\nA\\nc\\n,\\n are shown shaded. Figures (e) and (g) are the same, proving via Venn diagrams that \\nAB AB\\nc\\n−=\\n¨\\n[see Eq. (2-40)].\\nDIP4E_GLOBAL_Print_Ready.indb   94\\n6/16/2017   2:02:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 95}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n95\\nThe \\nunion\\n of two grayscale sets \\nA\\n and \\nB\\n with the same number of elements is deﬁned as the set\\n \\nAB a baA b B\\nz\\n´H\\nH\\n=\\n{}\\nmax( , ) ,\\nwhere it is understood that the max operation is applied to pairs of corresponding elements. If \\nA\\n and \\nB\\n \\nare grayscale images of the same size\\n, we see that their the union is an array formed from the maximum \\nintensity between pairs of spatially corresponding elements. As an illustration, suppose that \\nA\\n again \\nrepresents the image in Fig. 2.36(a), and let \\nB\\n denote a rectangular array of the same size as \\nA\\n, but in \\nwhich all values of \\nz\\n are equal to 3 times the mean intensity, \\nz\\n,\\n of the elements of \\nA\\n.\\n Figure 2.36(c) shows \\nthe result of performing the set union, in which all values exceeding \\n3\\nz\\n appear as values from \\nA\\n and all \\nother pixels have value \\n3\\nz\\n, which is a mid-gray value.\\nBefore leaving the discussion of sets', metadata={'source': 'imagepro.pdf', 'page': 96}),\n",
       " Document(page_content=', we introduce some additional concepts that \\nare used later in the book. The \\nCartesian product\\n of two sets \\nX\\n and \\nY\\n, denoted \\nXY\\n×\\n,\\n is the set of \\nall\\n possible ordered pairs whose ﬁrst component is a member of \\nX\\n and whose second component is a member of \\nY\\n.\\n In other words,\\n \\nXY x y x X y Y\\n*= H H\\n(, ) a n d\\n{}\\n \\n(2-41)\\nFor example, if \\nX\\n is a set of \\nM\\n equally spaced values on the \\nx\\n-axis and \\nY\\n is a set of \\nN\\n \\nequally spaced values on the \\ny\\n-axis\\n, we see that the Cartesian product of these two \\nsets define the coordinates of an \\nM\\n-by-\\nN\\n rectangular array (i.e., the coordinates of \\nan image). As another example, if \\nX\\n and \\nY\\n denote the specific \\nx\\n- and \\ny\\n-coordinates \\nof a group of 8-connected, 1-valued pixels in a binary image, then set \\nXY\\n×\\n repre-\\nsents the region (object) comprised of those pixels\\n.\\nWe follow convention \\nin using the symbol \\n×\\n \\nto denote the Cartesian \\nproduct. This is not to \\nbe confused with our \\nuse of the same symbol', metadata={'source': 'imagepro.pdf', 'page': 96}),\n",
       " Document(page_content='throughout the book \\nto denote the size of \\nan \\nM\\n-by-\\nN\\n image (i.e., \\nM\\n \\n×\\n \\nN\\n).\\nb a\\nc\\nFIGURE 2.36\\nSet operations  \\ninvolving grayscale \\nimages. (a) Original  \\nimage. (b) Image \\nnegative obtained \\nusing grayscale set  \\ncomplementation. \\n(c) The union of \\nimage (a) and a \\nconstant image. \\n(Original image \\ncourtesy of G.E. \\nMedical Systems.)\\nDIP4E_GLOBAL_Print_Ready.indb   95\\n6/16/2017   2:02:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 96}),\n",
       " Document(page_content='96\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nA \\nrelation\\n (or, more precisely, a \\nbinary relation\\n) on a set \\nA\\n is a collection of \\nordered pairs of elements from \\nA\\n. That is, a binary relation is a subset of the Carte-\\nsian product \\nAA\\n×\\n. A binary relation between \\ntwo\\n sets\\n, \\nA\\n and \\nB\\n, is a subset of \\nAB\\n×\\n.\\nA \\npartial or\\nder\\n on a set \\nS\\n is a relation \\n*\\n on \\nS\\n such that \\n*\\n is:\\n(a) \\nreﬂexive:\\n for any \\naS\\nH\\n, \\naa\\n*\\n;\\n(b) \\ntransitive:\\n for any \\nabc S\\n,,\\nH\\n, \\nab\\n*\\n and \\nbc\\n*\\n implies that \\nac\\n*\\n;\\n(c) \\nantisymmetric:\\n for any \\nab S\\n,,\\nH\\n \\nab\\n*\\n and \\nba\\n*\\n implies that \\nab\\n=\\n.\\nwhere, for example, \\nab\\n*\\n reads “\\na\\n is related to \\nb\\n.\\n” This means that \\na\\n and \\nb\\n are in set \\n*\\n, which itself is a subset of \\nSS\\n×\\n according to the preceding definition of a relation. \\nA set with a partial order is called a \\npartially or\\ndered set\\n.\\nLet the symbol \\nU\\n denote an ordering relation. An expression of the form\\n \\naaa a\\nn\\n123\\nUUUU\\n/midhorizellipsis\\nreads: \\na\\n1\\n precedes \\na\\n2', metadata={'source': 'imagepro.pdf', 'page': 97}),\n",
       " Document(page_content='a\\n1\\n precedes \\na\\n2\\n or is the same as \\na\\n2\\n, \\na\\n2\\n precedes \\na\\n3\\n or is the same as \\na\\n3\\n, and so on. \\nWhen working with numbers, the symbol \\nU\\n typically is replaced by more traditional \\nsymbols\\n. For example, the set of real numbers ordered by the relation “less than or \\nequal to” (denoted by \\n≤\\n)\\n is a partially ordered set (see Problem 2.33). Similarly, the \\nset of natural numbers, paired with the relation “divisible by” (denoted by \\n÷\\n),\\n is a \\npartially ordered set.\\nOf more interest to us later in the book are strict orderings\\n. A \\nstrict ordering\\n on a \\nset \\nS\\n is a relation \\n*\\n on \\nS\\n,\\n such that \\n*\\n is:\\n(a) \\nantireﬂexive:\\n for any \\naS aa\\nH\\n,;\\n¬\\n*\\n(b) \\ntransitive:\\n for any \\nabc S\\n,,\\n,\\nH\\n \\nab\\n*\\n and \\nbc\\n*\\n implies that \\nac\\n*\\n.\\nwhere \\n¬\\naa\\n*\\n means that \\na\\n is \\nnot related\\n to \\na\\n.\\n Let the symbol \\nE\\n denote a strict \\nordering relation.\\n An expression of the form\\n \\naaa a\\nn\\n123\\nEEEE\\n/midhorizellipsis\\nreads \\na\\n1\\n precedes \\na\\n2\\n, \\na\\n2\\n precedes \\na\\n3\\n,', metadata={'source': 'imagepro.pdf', 'page': 97}),\n",
       " Document(page_content='2\\n precedes \\na\\n3\\n,\\n and so on. A set with a strict ordering is called \\na \\nstrict-or\\ndered set\\n. \\nAs an example, consider the set composed of the English alphabet of lowercase \\nletters, \\nSa b c z\\n=\\n{}\\n,,, ,\\n/midhorizellipsis\\n. Based on the preceding deﬁnition, the ordering\\n \\nabc z\\nEE\\nE E\\n/midhorizellipsis\\nis strict because no member of the set can precede itself (antireflexivity) and, for any \\nthree letters in \\nS\\n,\\n if the first precedes the second, and the second precedes the third, \\nthen the first precedes the third (transitivity). Similarly, the set of integers paired \\nwith the relation “less than (<)” is a strict-ordered set. \\nLogical Operations\\nLogical operations deal with TRUE (typically denoted by 1) and FALSE (typically \\ndenoted by 0) variables and expressions. For our purposes, this means binary images \\nDIP4E_GLOBAL_Print_Ready.indb   96\\n6/16/2017   2:02:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 97}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n97\\ncomposed of \\nforeground\\n (1-valued) pixels, and a \\nbackground\\n composed of 0-valued \\npixels. \\nWe work with set and logical operators on binary images using one of two basic \\napproaches: (1) we can use the \\ncoordinates\\n of individual regions of foreground pix-\\nels in a single image as sets, or (2) we can work with one or more images of the same \\nsize and perform logical operations between corresponding pixels in those arrays.\\nIn the ﬁrst category, a binary image can be viewed as a Venn diagram in which \\nthe coordinates of individual regions of 1-valued pixels are treated as sets. The \\nunion of these sets with the set composed of 0-valued pixels comprises the set uni-\\nverse, \\nÆ\\n.\\n In this representation, we work with single images using all the set opera-\\ntions deﬁned in the previous section.\\n For example, given a binary image with two \\n1-valued regions, \\nR\\n1\\n and \\nR\\n2\\n,', metadata={'source': 'imagepro.pdf', 'page': 98}),\n",
       " Document(page_content='R\\n1\\n and \\nR\\n2\\n,\\n we can determine if the regions overlap (i.e., if they \\nhave at least one pair of coordinates in common) by performing the set intersec-\\ntion operation \\nRR\\n12\\n¨\\n (see Fig. 2.35). In the second approach, we perform logical \\noperations on the pixels of one binary image\\n, or on the corresponding pixels of two \\nor more binary images of the same size. \\nLogical operators can be deﬁned in terms of truth tables, as Table 2.2 shows for \\ntwo logical variables \\na\\n and \\nb\\n. The logical AND operation (also denoted \\n¿\\n)\\n yields a 1 \\n(TR\\nUE) only when both \\na\\n \\nand\\n \\nb\\n are 1. Otherwise, it yields 0 (FALSE). Similarly, \\nthe logical OR \\n(\\n¡\\n)\\n yields 1 when both \\na\\n \\nor\\n \\nb\\n or \\nboth\\n are 1,\\n and 0 otherwise. The \\nNOT \\n()\\n/H11011\\n operator is self explanatory. When applied to two binary images, AND \\nand OR operate on pairs of corresponding pixels between the images\\n. That is, they \\nare elementwise operators (see the deﬁnition of elementwise operators given earlier', metadata={'source': 'imagepro.pdf', 'page': 98}),\n",
       " Document(page_content='in this chapter) in this context. The operators AND, OR, and NOT are \\nfunctionally \\ncomplete\\n, in the sense that they can be used as the basis for constructing any other \\nlogical operator. \\nFigure 2.37 illustrates the logical operations deﬁned in Table 2.2 using the second \\napproach discussed above. The NOT of binary image \\nB\\n1\\n is an array obtained by \\nchanging all 1-valued pixels to 0, and vice versa. The AND of \\nB\\n1\\n and \\nB\\n2\\n contains a \\n1 at all spatial locations where the corresponding elements of \\nB\\n1\\n and \\nB\\n2\\n are 1; the \\noperation yields 0’s elsewhere. Similarly, the OR of these two images is an array \\nthat contains a 1 in locations where the corresponding elements of \\nB\\n1\\n, \\nor B\\n2\\n, \\nor \\nboth\\n,\\n are 1. The array contains 0’s elsewhere. The result in the fourth row of Fig. 2.37 \\ncorresponds to the set of 1-valued pixels in \\nB\\n1\\n \\nbut not in \\nB\\n2\\n.\\n \\nT\\nhe last row in the \\nﬁgure is the XOR (exclusive OR) operation, which yields 1 in the locations where', metadata={'source': 'imagepro.pdf', 'page': 98}),\n",
       " Document(page_content='the corresponding elements of \\nB\\n1\\n \\nor\\n \\nB\\n2\\n,\\n \\n(but \\nnot\\n \\nboth\\n) are 1.\\n Note that the logical \\nab\\nab\\nAND\\nab\\nO\\nR NOT(\\na\\n)\\n0 0 001\\n0\\n1\\n011\\n1 0 010\\n1 1 110\\nTABLE \\n2.2\\nTruth table  \\ndeﬁning the \\nlogical operators \\nAND\\n() ,\\n¿\\n  \\nOR\\n() ,\\n¡\\n and  \\nNO\\nT\\n() .\\n/H11011\\nDIP4E_GLOBAL_Print_Ready.indb   97\\n6/16/2017   2:02:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 98}),\n",
       " Document(page_content='98\\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nexpressions in the last two rows of Fig. 2.37 were constructed using operators from \\nTable 2.2; these are examples of the functionally complete nature of these operators.\\nWe can arrive at the same results in Fig. 2.37 using the ﬁrst approach discussed \\nabove. To do this, we begin by labeling the individual 1-valued regions in each of \\nthe two images (in this case there is only one such region in each image). Let \\nA\\n \\nand \\nB \\ndenote the \\nset of coordinates\\n of all the 1-valued pixels in images \\nB\\n1\\n and \\nB\\n2\\n,\\nrespectively. Then we form a \\nsingle\\n array by ORing the two images\\n, while keeping \\nthe labels \\nA\\n and \\nB\\n. The result would look like the array \\nBB\\n12\\nOR\\n in Fig. 2.37, but \\nwith the two white regions labeled \\nA\\n and \\nB\\n.\\n In other words, the resulting array \\nwould look like a Venn diagram. With reference to the Venn diagrams and set opera-\\ntions deﬁned in the previous section, we obtain the results in the rightmost column', metadata={'source': 'imagepro.pdf', 'page': 99}),\n",
       " Document(page_content='of Fig. 2.37 using set operations as follows: \\nAB\\nc\\n=\\nNOT( ),\\n1\\n \\nAB B B\\n¨\\n=\\n12\\nAND ,\\nAB B B\\n´\\n=\\n12\\nOR ,\\n and similarly for the other results in Fig. 2.37. We will make \\nextensive use in Chapter 9 of the concepts developed in this section.\\nSPATIAL OPERATIONS\\nSpatial operations are performed directly on the pixels of an image. We classify \\nspatial operations into three broad categories: (1) single-pixel operations, (2) neigh-\\nborhood operations, and (3) geometric spatial transformations.\\nFIGURE 2.37\\nIllustration of \\nlogical operations \\ninvolving  \\nforeground \\n(white) pixels. \\nBlack represents \\nbinary 0’s and \\nwhite binary 1’s. \\nThe dashed lines \\nare shown for  \\nreference only. \\nThey are not part \\nof the result. \\nNOT\\nNOT(\\nB\\n1\\n)\\nB\\n1\\n AND \\nB\\n2\\nB\\n1\\n OR \\nB\\n2\\nB\\n1\\n AND [NOT (\\nB\\n2\\n)]\\nB\\n1\\n XOR \\nB\\n2\\nAND\\nB\\n1\\nB\\n1\\nB\\n2\\nOR\\nXOR\\nAND-\\nNOT\\nDIP4E_GLOBAL_Print_Ready.indb   98\\n6/16/2017   2:02:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 99}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n99\\nSingle-Pixel Operations\\nThe simplest operation we perform on a digital image is to alter the intensity of its \\npixels individually using a transformation function, \\nT\\n, of the form:\\n \\nsT z\\n=\\n()\\n \\n(2-42)\\nwhere \\nz\\n is the intensity of a pixel in the original image and \\ns\\n is the (mapped) inten-\\nsity of the corresponding pixel in the processed image\\n. For example, Fig. 2.38 shows \\nthe transformation used to obtain the \\nnegative\\n (sometimes called the \\ncomplement\\n) \\nof an 8-bit image. This transformation could be used, for example, to obtain the \\nnegative image in Fig. 2.36, instead of using sets. \\nNeighborhood Operations\\nLet \\nS\\nxy\\n denote the set of coordinates of a neighborhood (see Section 2.5 regarding \\nneighborhoods) centered on an arbitrary point \\n(,)\\nxy\\n in an image, \\nf\\n.\\n Neighborhood \\nprocessing generates a corresponding pixel at the same coordinates in an output \\n(processed) image, \\ng', metadata={'source': 'imagepro.pdf', 'page': 100}),\n",
       " Document(page_content='g\\n, such that the value of that pixel is determined by a specified \\noperation on the neighborhood of pixels in the input image with coordinates in the \\nset \\nS\\nxy\\n. For example, suppose that the specified operation is to compute the average \\nvalue of the pixels in a rectangular neighborhood of size \\nmn\\n×\\n centered on\\n \\n(,\\n)\\nxy\\n \\n.\\n \\nThe coordinates of pixels in this region are the elements of set \\nS\\nxy\\n. Figures 2.39(a) \\nand (b) illustrate the process. We can express this averaging operation as\\n \\ngxy\\nmn\\nfrc\\nrc S\\nxy\\n(,)\\n( ,)\\n(, )\\n=\\n∑\\n1\\nH\\n \\n(2-43)\\nwhere \\nr\\n and \\nc\\n are the row and column coordinates of the pixels whose coordinates \\nare in the set \\nS\\nxy\\n. Image \\ng\\n is created by varying the coordinates \\n(,)\\nxy\\n so that the \\ncenter of the neighborhood moves from pixel to pixel in image \\nf\\n,\\n and then repeat-\\ning the neighborhood operation at each new location. For instance, the image in \\nFig. 2.39(d) was created in this manner using a neighborhood of size \\n41 41\\n×\\n. The', metadata={'source': 'imagepro.pdf', 'page': 100}),\n",
       " Document(page_content='41 41\\n×\\n. The \\nOur use of the word \\n“negative” in this context \\nrefers to the digital \\nequivalent of a  \\nphotographic negative, \\nnot to the numerical \\nnegative of the pixels in \\nthe image.\\ns\\n \\n/H11005\\n \\nT\\n(\\nz\\n)\\nz\\ns\\n0\\n0\\n255\\nz\\n0\\n255\\nFIGURE 2.38\\nIntensity  \\ntransformation \\nfunction used to \\nobtain the digital \\nequivalent of \\nphotographic \\nnegative of an \\n8-bit image..\\nDIP4E_GLOBAL_Print_Ready.indb   99\\n6/16/2017   2:02:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 100}),\n",
       " Document(page_content='100\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nnet effect is to perform local blurring in the original image. This type of process is \\nused, for example, to eliminate small details and thus render “blobs” correspond-\\ning to the largest regions of an image. We will discuss neighborhood processing in \\nChapters 3 and 5, and in several other places in the book. \\nGeometric Transformations\\nWe use geometric transformations modify the spatial arrangement of pixels in an \\nimage. These transformations are called \\nrubber-sheet transformations\\n because they \\nmay be viewed as analogous to “printing” an image on a rubber sheet, then stretch-\\ning or shrinking the sheet according to a predefined set of rules. Geometric transfor-\\nmations of digital images consist of two basic operations: \\nThe value of this pixel\\nis the average value of the\\npixels in \\nS\\nxy\\nImage \\nf\\nImage \\ng\\n(\\nx, y\\n)\\n(\\nx, y\\n)\\nS\\nxy\\nm\\nn\\nb a\\nd c\\nFIGURE 2.39\\nLocal averaging  \\nusing neighbor-\\nhood processing. \\nThe procedure is', metadata={'source': 'imagepro.pdf', 'page': 101}),\n",
       " Document(page_content='The procedure is  \\nillustrated in (a) \\nand (b) for a  \\nrectangular  \\nneighborhood.  \\n(c) An aortic  \\nangiogram (see  \\nSection 1.3).  \\n(d) The result of  \\nusing Eq. (2-43) \\nwith \\nmn\\n==\\n41. \\nT\\nhe images are \\nof size \\n790 686\\n×\\n \\npixels\\n. (Original  \\nimage courtesy \\nof Dr. Thomas R. \\nGest, Division of  \\nAnatomical  \\nSciences,  \\nUniversity of \\nMichigan Medical \\nSchool.)\\nDIP4E_GLOBAL_Print_Ready.indb   100\\n6/16/2017   2:02:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 101}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n101\\n1. \\nSpatial transformation of coordinates. \\n2. \\nIntensity interpolation that assigns intensity values to the spatially transformed \\npixels\\n. \\nThe transformation of coordinates may be expressed as\\n \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nx\\ny\\nx\\ny\\ntt\\ntt\\nx\\ny\\nT\\n11 12\\n21 22\\n \\n(2-44)\\nwhere \\n(,)\\nxy\\n are pixel coordinates in the original image and \\n(,)\\n′′\\nxy\\n are the \\ncorresponding pixel coordinates of the transformed image\\n. For example, the \\ntransformation \\n(,) ( , )\\n′′\\n=\\nxy\\nxy\\n22\\n shrinks the original image to half its size in both \\nspatial directions\\n. \\nOur interest is in so-called \\nafﬁne transformations\\n, which include scaling, translation, \\nrotation, and shearing. The key characteristic of an afﬁne transformation in 2-D is \\nthat it preserves points, straight lines, and planes. Equation (2-44) can be used to', metadata={'source': 'imagepro.pdf', 'page': 102}),\n",
       " Document(page_content='express the transformations just mentioned, except translation, which would require \\nthat a constant 2-D vector be added to the right side of the equation. However, it is \\npossible to use homogeneous coordinates to express all four afﬁne transformations \\nusing a single \\n33\\n×\\n matrix in the following general form: \\n \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\nx\\ny\\nx\\ny\\naa\\na\\naaa\\n11 0 0 1\\n11 12 13\\n21 22 23\\nA\\n⎦ ⎦\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\ny\\n1\\n \\n(2-45)\\nThis transformation can \\nscale\\n, \\nrotate\\n, \\ntranslate\\n,\\n or \\nsheer\\n an image, depending on the \\nvalues chosen for the elements of matrix \\nA\\n. Table 2.3 shows the matrix values used \\nto implement these transformations. A significant advantage of being able to per-\\nform all transformations using the unified representation in Eq. (2-45) is that it pro-\\nvides the framework for concatenating a sequence of operations. For example, if we \\nwant to resize an image, rotate it, and move the result to some location, we simply \\nform a', metadata={'source': 'imagepro.pdf', 'page': 102}),\n",
       " Document(page_content='form a \\n33\\n×\\n matrix equal to the product of the scaling, rotation, and translation \\nmatrices from \\nTable 2.3 (see Problems 2.36 and 2.37).\\nThe preceding transformation moves the coordinates of pixels in an image to new \\nlocations. To complete the process, we have to assign intensity values to those loca-\\ntions. This task is accomplished using \\nintensity interpolation\\n. We already discussed \\nthis topic in Section 2.4. We began that discussion with an example of zooming an \\nimage and discussed the issue of intensity assignment to new pixel locations. Zoom-\\ning is simply scaling, as detailed in the second row of Table 2.3, and an analysis simi-\\nlar to the one we developed for zooming is applicable to the problem of assigning \\nintensity values to the relocated pixels resulting from the other transformations in \\nTable 2.3. As in Section 2.4, we consider nearest neighbor, bilinear, and bicubic inter-\\npolation techniques when working with these transformations.', metadata={'source': 'imagepro.pdf', 'page': 102}),\n",
       " Document(page_content='We can use Eq. (2-45) in two basic ways. The ﬁrst, is a \\nforward mapping\\n, which \\nconsists of scanning the pixels of the input image and, at each location \\n(,) ,\\nxy\\n com-\\nDIP4E_GLOBAL_Print_Ready.indb   101\\n6/16/2017   2:02:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 102}),\n",
       " Document(page_content='102\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nputing the spatial location \\n(,)\\n′′\\nxy\\n of the corresponding pixel in the output image \\nusing Eq.\\n (2-45) directly. A problem with the forward mapping approach is that two \\nor more pixels in the input image can be transformed to the same location in the \\noutput image, raising the question of how to combine multiple output values into a \\nsingle output pixel value. In addition, it is possible that some output locations may \\nnot be assigned a pixel at all. The second approach, called \\ninverse mapping\\n, scans \\nthe output pixel locations and, at each location \\n(,) ,\\n′′\\nxy\\n computes the corresponding \\nlocation in the input image using \\n(,) ( , ) .\\nxy\\n=\\n′′\\n−\\nAx y\\n1\\n It then interpolates (using one \\nof the techniques discussed in Section 2.4) among the nearest input pixels to deter-\\nmine the intensity of the output pixel value. Inverse mappings are more efﬁcient to \\nimplement than forward mappings, and are used in numerous commercial imple-', metadata={'source': 'imagepro.pdf', 'page': 103}),\n",
       " Document(page_content='mentations of spatial transformations (for example, MATLAB uses this approach).\\nTransformation\\nName\\nAffine Matrix, A\\nCoordinate\\nEquations\\nExample\\nIdentity\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nx\\n′\\ny\\n′\\nxx\\n=\\n′\\nyy\\n=\\n′\\nTranslation\\ny\\nyy t\\n=+\\n′\\nx\\nxx t\\n=+\\n′\\n10\\n01\\n001\\nx\\ny\\nt\\nt\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nShear (vertical)\\n10\\n010\\n001\\ns\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nv\\nyy\\n=\\n′\\nxx s y\\n=+\\n′\\nv\\nShear (horizontal)\\n10 0\\n10\\n00 1\\nh\\ns\\n⎡⎤\\n⎢⎥\\n⎢⎥\\n⎢⎥\\n⎣⎦\\nxx\\n=\\n′\\nh\\nys x y\\n=+\\n′\\nScaling/Reflection\\n(For reflection, set one \\nscaling factor to \\n−\\n1\\nand the other to 0)\\nc\\nx\\n0\\n0\\n0\\nc\\ny\\n0\\n0\\n0\\n1\\nx\\nxc x\\n=\\n′\\ny\\nyc y\\n=\\n′\\nRotation (about the\\norigin)\\n0\\ncos \\nu\\n/H11002\\nsin \\nu\\nsin \\nu\\ncos \\nu\\n0\\n00 1\\ncos sin\\nxx y\\n=−\\n′\\nuu\\nsin cos\\nyx y\\n=+\\n′\\nuu\\nx\\n′\\ny\\n′\\nx\\n′\\nx\\n′\\nx\\n′\\nx\\n′\\ny\\n′\\ny\\n′\\ny\\n′\\ny\\n′\\nTABLE \\n2.3\\nAfﬁne  \\ntransformations \\nbased on  \\nEq. (2-45).\\nDIP4E_GLOBAL_Print_Ready.indb   102\\n6/16/2017   2:02:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 103}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n103\\nEXAMPLE 2.9 :  Image rotation and intensity interpolation.\\nThe objective of this example is to illustrate image rotation using an afﬁne transform. Figure 2.40(a) \\nshows a simple image and Figs. 2.40(b)–(d) are the results (using inverse mapping) of rotating the \\noriginal image by \\n−\\n21\\n°\\n (in Table 2.3, clockwise angles of rotation are negative). Intensity assignments \\nwere computed using nearest neighbor\\n, bilinear, and bicubic interpolation, respectively. A key issue in \\nimage rotation is the preservation of straight-line features. As you can see in the enlarged edge sections \\nin Figs. 2.40(f) through (h), nearest neighbor interpolation produced the most jagged edges and, as in \\nSection 2.4, bilinear interpolation yielded signiﬁcantly improved results. As before, using bicubic inter-\\npolation produced slightly better results. In fact, if you compare the progression of enlarged detail in', metadata={'source': 'imagepro.pdf', 'page': 104}),\n",
       " Document(page_content='Figs. 2.40(f) to (h), you can see that the transition from white (255) to black (0) is smoother in the last \\nﬁgure because the edge region has more values, and the distribution of those values is better balanced. \\nAlthough the small intensity differences resulting from bilinear and bicubic interpolation are not always \\nnoticeable in human visual analysis, they can be important in processing image data, such as in auto-\\nmated edge following in rotated images.\\nThe size of the spatial rectangle needed to contain a rotated image is larger than the rectangle of the \\noriginal image, as Figs. 2.41(a) and (b) illustrate. We have two options for dealing with this: (1) we can \\ncrop the rotated image so that its size is equal to the size of the original image, as in Fig. 2.41(c), or we \\ncan keep the larger image containing the full rotated original, an Fig. 2.41(d). We used the ﬁrst option in', metadata={'source': 'imagepro.pdf', 'page': 104}),\n",
       " Document(page_content='Fig. 2.40 because the rotation did not cause the object of interest to lie outside the bounds of the original \\nrectangle. The areas in the rotated image that do not contain image data must be ﬁlled with some value, 0 \\n(black) being \\nthe most common. Note that counterclockwise angles of rotation are considered positive. \\nThis is a result of the way in which our image coordinate system is set up (see Fig. 2.19), and the way in \\nwhich rotation is deﬁned in Table 2.3.\\nImage Registration\\nImage registration is an important application of digital image processing used to \\nalign two or more images of the same scene. In image registration, we have avail-\\nable an \\ninput\\n image and a \\nreference\\n image. The objective is to transform the input \\nimage geometrically to produce an output image that is aligned (registered) with the \\nreference image. Unlike the discussion in the previous section where transformation \\nfunctions are known, the geometric transformation needed to produce the output,', metadata={'source': 'imagepro.pdf', 'page': 104}),\n",
       " Document(page_content='registered image generally is not known, and must be estimated.\\nExamples of image registration include aligning two or more images taken at \\napproximately the same time, but using different imaging systems, such as an MRI \\n(magnetic resonance imaging) scanner and a PET (positron emission tomography) \\nscanner. Or, perhaps the images were taken at different times using the same instru-\\nments, such as satellite images of a given location taken several days, months, or even \\nyears apart. In either case, combining the images or performing quantitative analysis \\nand comparisons between them requires compensating for geometric distortions \\ncaused by differences in viewing angle, distance, orientation, sensor resolution, shifts \\nin object location, and other factors. \\nDIP4E_GLOBAL_Print_Ready.indb   103\\n6/16/2017   2:02:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 104}),\n",
       " Document(page_content='104\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nOne of the principal approaches for solving the problem just discussed is to use \\ntie \\npoints\\n (also called \\ncontrol points\\n). These are corresponding points whose locations \\nare known precisely in the input and reference images. Approaches for selecting tie \\npoints range from selecting them interactively to using algorithms that detect these \\npoints automatically. Some imaging systems have physical artifacts (such as small \\nmetallic objects) embedded in the imaging sensors. These produce a set of known \\npoints (called \\nreseau marks \\nor\\n ﬁducial marks\\n) directly on all images captured by the \\nsystem. These known points can then be used as guides for establishing tie points.\\nThe problem of estimating the transformation function is one of modeling. For \\nexample, suppose that we have a set of four tie points each in an input and a refer-\\nence image. A simple model based on a bilinear approximation is given by\\n \\nxc c c c\\n=+\\n+ +\\n12 3 4', metadata={'source': 'imagepro.pdf', 'page': 105}),\n",
       " Document(page_content='=+\\n+ +\\n12 3 4\\nvwv w\\n \\n(2-46)\\nand\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 2.40\\n (a) A \\n541 421\\n×\\n image of the letter T. (b) Image rotated \\n−\\n21\\n°\\n using nearest-neighbor interpolation for \\nintensity assignments\\n. (c) Image rotated \\n−\\n21\\n°\\n using bilinear interpolation. (d) Image rotated \\n−\\n21\\n°\\n using bicubic \\ninterpolation.\\n (e)-(h) Zoomed sections (each square is one pixel, and the numbers shown are intensity values).\\n45\\n154\\n247\\n0\\n0\\n255\\n255\\n255\\n255\\n0\\n00\\n77\\n168\\n255\\n00\\n255\\nDIP4E_GLOBAL_Print_Ready.indb   104\\n6/16/2017   2:02:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 105}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n105\\nx\\n \\n¿\\ny\\n \\n¿\\ny\\n \\n¿\\nx\\n \\n¿\\ny\\n \\n¿\\nx\\n \\n¿\\ny\\nx\\nOrigin  \\nImage \\nf\\n(\\nx\\n, \\ny\\n)\\nPositive\\nangle of\\nrotation\\nPos\\ni\\nt\\ni\\nve\\nan\\ngl\\ne of\\nr\\notatio\\nn\\nb a\\nd c\\nFIGURE 2.41\\n(a) A digital  \\nimage.  \\n(b) Rotated image \\n(note the  \\ncounterclockwise \\ndirection for a \\npositive angle of \\nrotation).  \\n(c) Rotated image \\ncropped to ﬁt the \\nsame area as the \\noriginal image.  \\n(d) Image  \\nenlarged to  \\naccommodate \\nthe entire rotated \\nimage.\\nyc c c c\\n=+ + +\\n56 7 8\\nvwv w\\n \\n(2-47)\\nDuring the estimation phase, \\n(, )\\nvw\\n and \\n(,)\\nxy\\n are the coordinates of tie points in the \\ninput and reference images\\n, respectively. If we have four pairs of corresponding tie \\npoints in both images, we can write eight equations using Eqs. (2-46) and (2-47) and \\nuse them to solve for the eight unknown coefficients, \\nc\\n1\\n through \\nc\\n8\\n. \\nOnce we have the coefﬁcients, Eqs. (2-46) and (2-47) become our vehicle for trans-', metadata={'source': 'imagepro.pdf', 'page': 106}),\n",
       " Document(page_content='forming all the pixels in the input image. The result is the desired registered image. \\nAfter the coefﬁcients have been computed, we let \\n(\\nv,\\nw)\\n denote the coordinates of \\neach pixel in the input image\\n, and \\n(,)\\nxy\\n become the corresponding coordinates of the \\noutput image\\n. The \\nsame set of coefﬁcients, \\nc\\n1\\n through \\nc\\n8\\n,\\n are used in computing all \\ncoordinates \\n(,)\\nxy\\n; we just step through all \\n(, )\\nvw\\n in the input image to generate the \\ncorresponding \\n(,)\\nxy\\n in the output, registered image. If the tie points were selected \\ncorrectly\\n, this new image should be registered with the reference image, within the \\naccuracy of the bilinear approximation model.\\nIn situations where four tie points are insufﬁcient to obtain satisfactory regis-\\ntration, an approach used frequently is to select a larger number of tie points and \\nthen treat the quadrilaterals formed by groups of four tie points as subimages. The', metadata={'source': 'imagepro.pdf', 'page': 106}),\n",
       " Document(page_content='subimages are processed as above, with all the pixels within a quadrilateral being \\ntransformed using the coefﬁcients determined from the tie points corresponding \\nto that quadrilateral. Then we move to another set of four tie points and repeat the \\nDIP4E_GLOBAL_Print_Ready.indb   105\\n6/16/2017   2:02:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 106}),\n",
       " Document(page_content='106\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nprocedure until all quadrilateral regions have been processed. It is possible to use \\nmore complex regions than quadrilaterals, and to employ more complex models, \\nsuch as polynomials ﬁtted by least squares algorithms. The number of control points \\nand sophistication of the model required to solve a problem is dependent on the \\nseverity of the geometric distortion. Finally, keep in mind that the transformations \\ndeﬁned by Eqs. (2-46) and (2-47), or any other model for that matter, only map the \\nspatial coordinates of the pixels in the input image. We still need to perform inten-\\nsity interpolation using any of the methods discussed previously to assign intensity \\nvalues to the transformed pixels.\\nEXAMPLE 2.10 :  Image registration.\\nFigure 2.42(a) shows a reference image and Fig. 2.42(b) shows the same image, but distorted geometri-\\ncally by vertical and horizontal shear. Our objective is to use the reference image to obtain tie points', metadata={'source': 'imagepro.pdf', 'page': 107}),\n",
       " Document(page_content='and then use them to register the images. The tie points we selected (manually) are shown as small white \\nsquares near the corners of the images (we needed only four tie points because the distortion is linear \\nshear in both directions). Figure 2.42(c) shows the registration result obtained using these tie points in \\nthe procedure discussed in the preceding paragraphs. Observe that registration was not perfect, as is \\nevident by the black edges in Fig. 2.42(c). The difference image in Fig. 2.42(d) shows more clearly the \\nslight lack of registration between the reference and corrected images. The reason for the discrepancies \\nis error in the manual selection of the tie points. It is difﬁcult to achieve perfect matches for tie points \\nwhen distortion is so severe.\\nVECTOR AND MATRIX OPERATIONS\\nMultispectral image processing is a typical area in which vector and matrix opera-\\ntions are used routinely. For example, you will learn in Chapter 6 that color images', metadata={'source': 'imagepro.pdf', 'page': 107}),\n",
       " Document(page_content='are formed in RGB color space by using red, green, and blue component images, as \\nFig. 2.43 illustrates. Here we see that \\neach\\n pixel of an RGB image has three compo-\\nnents, which can be organized in the form of a column vector\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nz\\nz\\nz\\n1\\n2\\n3\\n \\n(2-48)\\nwhere \\nz\\n1\\n is the intensity of the pixel in the red image, and \\nz\\n2\\n and \\nz\\n3\\n are the corre-\\nsponding pixel intensities in the green and blue images, respectively. Thus, an RGB \\ncolor image of size \\nMN\\n×\\n can be represented by three component images of this \\nsize\\n, or by a total of \\nMN\\n vectors of size \\n31\\n×\\n.\\n A general multispectral case involving \\nn\\n component images (e\\n.g., see Fig. 1.10) will result in \\nn\\n-dimensional vectors:\\n \\nz\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\nz\\nz\\nz\\nn\\n1\\n2\\n/vertellipsis\\n \\n(2-49)\\nRecall that an \\nn\\n-dimensional vector \\ncan be thought of as a \\npoint in \\nn\\n-dimensional \\nEuclidean space.\\nDIP4E_GLOBAL_Print_Ready.indb   106\\n6/16/2017   2:02:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 107}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n107\\n We will use this type of vector representation throughout the book.\\nThe \\ninner product\\n (also called the \\ndot product\\n) of two \\nn\\n-dimensional column vec-\\ntors \\na\\n and \\nb\\n is deﬁned as\\n \\nab a b\\ni≜\\n/midhorizellipsis\\nT\\nnn\\nii\\ni\\nn\\nab ab ab\\nab\\n=++ +\\n=\\n=\\n∑\\n11 22\\n1\\n \\n(2-50)\\nwhere \\nT\\n indicates the transpose\\n. The \\nEuclidean vector norm\\n, denoted by \\nz\\n,\\n is \\ndefined as the square root of the inner product:\\n \\nzz z\\n=\\n(\\n)\\nT\\n1\\n2\\n \\n(2-51)\\nThe product \\nab\\nT\\n is called \\nthe \\nouter product\\n \\nof \\na\\n \\nand \\nb\\n. It is a matrix of \\nsize \\nn \\n× \\nn\\n. \\nb a\\nd c\\nFIGURE 2.42\\nImage  \\nregistration. \\n(a) Reference \\nimage. (b) Input \\n(geometrically \\ndistorted image). \\nCorresponding tie \\npoints are shown \\nas small white \\nsquares near the \\ncorners.  \\n(c) Registered \\n(output) image \\n(note the errors \\nin the border). \\n(d) Difference \\nbetween (a) and \\n(c), showing more \\nregistration errors.\\nDIP4E_GLOBAL_Print_Ready.indb   107', metadata={'source': 'imagepro.pdf', 'page': 108}),\n",
       " Document(page_content='6/16/2017   2:02:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 108}),\n",
       " Document(page_content='108\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nComponent image 3 (Blue)\\nComponent image 2 (Green)\\nComponent image 1 (Red)\\nz\\n \\n/H11005\\nz\\n1\\nz\\n2\\nz\\n3\\nFIGURE 2.43\\nForming a vector \\nfrom  \\ncorresponding \\npixel values in \\nthree RGB  \\ncomponent  \\nimages.\\nWe recognize this expression as the length of vector \\nz\\n.\\nWe can use vector notation to express several of the concepts discussed earlier. \\nFor example, the Euclidean distance, \\nD\\n(,\\n)\\nza\\n, between points (vectors) \\nz\\n and \\na\\n in \\nn\\n-dimensional space is deﬁned as the Euclidean vector norm:\\n \\n \\nD\\nza za za\\nT\\nnn\\n(, )\\n() ( )\\n()\\nz a z a zaza\\n=−= −\\n()\\n−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n−−\\n⎡\\n1\\n2\\n11\\n2\\n22\\n22\\n++ +\\n/midhorizellipsis\\n⎣ ⎣\\n⎤\\n⎦\\n1\\n2\\n \\n(2-52)\\nThis is a generalization of the 2-D Euclidean distance defined in Eq. (2-19). \\nAnother advantage of pixel vectors is in linear transformations\\n, represented as\\n \\nwA z a\\n=−\\n()\\n(2-53)\\nwhere \\nA\\n is a matrix of size \\nmn\\n×\\n,\\n and \\nz\\n and \\na\\n are column vectors of size \\nn\\n×\\n1.', metadata={'source': 'imagepro.pdf', 'page': 109}),\n",
       " Document(page_content='n\\n×\\n1.\\nAs noted in Eq. (2-10), entire images can be treated as matrices (or, equivalently, \\nas vectors),\\n a fact that has important implication in the solution of numerous image \\nprocessing problems. For example, we can express an image of size \\nMN\\n×\\n as a col-\\numn vector of dimension \\nMN\\n×\\n1\\n by letting the ﬁrst \\nM\\n elements of the vector equal \\nthe ﬁrst column of the image\\n, the next \\nM\\n elements equal the second column, and \\nso on. With images formed in this manner, we can express a broad range of linear \\nprocesses applied to an image by using the notation\\n \\ngH f n\\n=+\\n \\n(2-54)\\nwhere \\nf\\n is an \\nMN\\n×\\n1\\n vector representing an input image, \\nn\\n is an \\nMN\\n×\\n1\\n vector rep-\\nresenting an \\nMN\\n×\\n noise pattern, \\ng\\n is an \\nMN\\n×\\n1\\n vector representing a processed \\nimage\\n, and \\nH\\n is an \\nMN MN\\n×\\n matrix representing a linear process applied to the \\ninput image (see the discussion earlier in this chapter regarding linear processes).', metadata={'source': 'imagepro.pdf', 'page': 109}),\n",
       " Document(page_content='It is possible, for example, to develop an entire body of generalized techniques for \\nimage restoration starting with Eq. (2-54), as we discuss in Section 5.9. We will men-\\ntion the use of matrices again in the following section, and show other uses of matri-\\nces for image processing in numerous chapters in the book.\\nDIP4E_GLOBAL_Print_Ready.indb   108\\n6/16/2017   2:02:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 109}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n109\\nIMAGE TRANSFORMS\\nAll the image processing approaches discussed thus far operate directly on the pixels \\nof an input image; that is, they work directly in the spatial domain. In some cases, \\nimage processing tasks are best formulated by transforming the input images, carry-\\ning the specified task in a \\ntransform domain\\n, and applying the inverse transform to \\nreturn to the spatial domain. You will encounter a number of different transforms \\nas you proceed through the book. A particularly important class of 2-D \\nlinear trans-\\nforms\\n, denoted \\nT\\n(,\\n)\\nuv\\n, can be expressed in the general form\\n \\nTf\\nx\\ny\\nr\\nx\\ny\\ny\\nN\\nx\\nM\\n( ,) ( ,) ( ,,,)\\nuv uv\\n=\\n= =\\n∑ ∑\\n0\\n1\\n0\\n1\\n- -\\n \\n(2-55)\\nwhere \\nfx y\\n(,\\n)\\n is an input image, \\nrxy\\n(\\n,,,)\\nuv\\n is called a \\nforwar\\nd transformation ker-\\nnel\\n, and Eq. (2-55) is evaluated for \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n and \\nv\\n=−\\n012\\n1\\n,,, ,\\n…\\nN\\n \\n. As \\nbefore, \\nx\\n and \\ny', metadata={'source': 'imagepro.pdf', 'page': 110}),\n",
       " Document(page_content='before, \\nx\\n and \\ny\\n are spatial variables, while \\nM\\n and \\nN\\n are the row and column dimen-\\nsions of \\nf\\n. Variables \\nu\\n \\nand \\nv\\n are called the \\ntransform variables\\n. \\nT\\n(,\\n)\\nuv\\n is called the \\nforwar\\nd transform\\n of \\nfx y\\n(,\\n)\\n. Given \\nT\\n(,\\n) ,\\nuv\\n we can recover \\nfx y\\n(,\\n)\\n using the \\ninverse \\ntransform \\nof \\nT\\n(,\\n) :\\nuv\\n \\nfx y T s xy\\nN M\\n( ,) ( ,) ( ,,,)\\n=\\n= =\\n∑ ∑\\nuv uv\\nv u\\n0\\n1\\n0\\n1\\n- -\\n \\n(2-56)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, ,\\n…\\n, where \\nsxy\\n(\\n,,,)\\nuv\\n is called an \\ninverse \\ntransformation kernel\\n.\\n Together, Eqs. (2-55) and (2-56) are called a \\ntransform pair\\n.\\nFigure 2.44 shows the basic steps for performing image processing in the linear \\ntransform domain. First, the input image is transformed, the transform is then modi-\\nﬁed by a predeﬁned operation and, ﬁnally, the output image is obtained by computing \\nthe inverse of the modiﬁed transform. Thus, we see that the process goes from the \\nspatial domain to the transform domain, and then back to the spatial domain.', metadata={'source': 'imagepro.pdf', 'page': 110}),\n",
       " Document(page_content='The forward transformation kernel is said to be \\nseparable\\n if\\n \\nrxy\\nr y\\n(\\n,,, )( ,)\\nuv ) = r ( x , u v\\n1\\n2\\n \\n(2-57)\\nIn addition, the kernel is said to be \\nsymmetric\\n if \\nrx\\n1\\n(,)\\nu\\n is functionally equal to \\nry\\n2\\n(,)\\nv\\n \\n, so that\\n \\nrxy r x r y\\n(\\n,,,) ( ,)( ,)\\nuv u v\\n=\\n11\\n \\n(2-58)\\nIdentical comments apply to the inverse kernel.\\nT\\nhe nature of a transform is determined by its kernel. A transform of particular \\nimportance in digital image processing is the \\nFourier transform\\n, which has the fol-\\nlowing forward and inverse kernels:\\n \\nrxy e\\nju x M y N\\n( ,,,)\\n()\\nuv\\nv\\n=\\n−+\\n2\\np\\n \\n(2-59)\\nand\\n \\nsxy\\nMN\\ne\\nju x M y N\\n( ,,,)\\n()\\nuv\\nv\\n=\\n+\\n1\\n2\\np\\n \\n(2-60)\\nDIP4E_GLOBAL_Print_Ready.indb   109\\n6/16/2017   2:02:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 110}),\n",
       " Document(page_content='110\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nrespectively, where \\nj\\n=−\\n1\\n, so these kernels are complex functions. Substituting the \\npreceding kernels into the general transform formulations in Eqs\\n. (2-55) and (2-56) \\ngives us the \\ndiscrete Fourier transform pair\\n:\\n \\nTf\\nx\\ny\\ne\\nju x M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nv\\n=\\n−+\\n= =\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n- -\\n(2-61)\\nand\\n \\nfx y\\nMN\\nTe\\nju x M y N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n= =\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nuv\\nv\\nv u\\np\\n- -\\n \\n(2-62)\\nIt can be shown that the Fourier kernels are separable and symmetric (Problem 2.39), \\nand that separable and symmetric kernels allow 2-D transforms to be computed using \\n1-D transforms (see Problem 2.40).\\n The preceding two equations are of fundamental \\nimportance in digital image processing, as you will see in Chapters 4 and 5.\\nEXAMPLE 2.11 :  Image processing in the transform domain.\\nFigure 2.45(a) shows an image corrupted by periodic (sinusoidal) interference. This type of interference', metadata={'source': 'imagepro.pdf', 'page': 111}),\n",
       " Document(page_content='can be caused, for example, by a malfunctioning imaging system; we will discuss it in Chapter 5. In the \\nspatial domain, the interference appears as waves of intensity. In the frequency domain, the interference \\nmanifests itself as bright bursts of intensity, whose location is determined by the frequency of the sinu-\\nsoidal interference (we will discuss these concepts in much more detail in Chapters 4 and 5). Typi-\\ncally, the bursts are easily observable in an image of the magnitude of the Fourier transform, \\nT\\n(,) .\\nuv\\n \\nW\\nith reference to the diagram in Fig. 2.44, the corrupted image is \\nfx y\\n(,\\n) ,\\n the transform in the leftmost \\nbox is the F\\nourier transform, and Fig. 2.45(b) is \\nT\\n(,)\\nuv\\n displayed as an image. The bright dots shown \\nare the bursts of intensity mentioned above\\n. Figure 2.45(c) shows a mask image (called a \\nﬁlter\\n) with \\nwhite and black representing 1 and 0, respectively. For this example, the operation in the second box of', metadata={'source': 'imagepro.pdf', 'page': 111}),\n",
       " Document(page_content='Fig. 2.44 is to multiply the ﬁlter by the transform to remove the bursts associated with the interference. \\nFigure 2.45(d) shows the ﬁnal result, obtained by computing the inverse of the modiﬁed transform. The \\ninterference is no longer visible, and previously unseen image detail is now made quite clear. Observe, \\nfor example, the ﬁducial marks (faint crosses) that are used for image registration, as discussed earlier.\\nWhen the forward and inverse kernels of a transform are separable and sym-\\nmetric, and \\nfx y\\n(,\\n)\\n is a square image of size \\nMM\\n×\\n, Eqs. (2-55) and (2-56) can be \\nexpressed in matrix form:\\nThe exponential terms \\nin the Fourier transform \\nkernels can be expanded \\nas sines and cosines of \\nvarious frequencies. As \\na result, the domain of \\nthe Fourier transform \\nis called the \\nfrequency \\ndomain\\n.\\nT\\n(\\nu\\n, \\nv\\n)\\nTransform\\nOperation\\nR\\nInverse\\ntransform\\nTransform domain\\nR\\n[\\nT\\n(\\nu\\n, \\nv\\n)]\\nf\\n(\\nx\\n, \\ny\\n)\\ng\\n(\\nx\\n, \\ny\\n)\\nSpatial\\ndomain\\nSpatial\\ndomain\\nFIGURE 2.44\\nGeneral approach', metadata={'source': 'imagepro.pdf', 'page': 111}),\n",
       " Document(page_content='General approach \\nfor working in the \\nlinear transform \\ndomain.\\nDIP4E_GLOBAL_Print_Ready.indb   110\\n6/16/2017   2:02:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 111}),\n",
       " Document(page_content='2.6\\n  \\nIntroduction to the Basic Mathematical Tools Used in Digital Image Processing\\n    \\n111\\n \\nTA F A\\n=\\n \\n(2-63)\\n \\nwhere \\nF\\n is an \\nMM\\n×\\n matrix containing the elements of \\nfx y\\n(,\\n)\\n [see Eq. (2-9)], \\nA\\n is \\nan \\nMM\\n×\\n matrix with elements \\nar i j\\nij\\n=\\n1\\n(, ) ,\\n and \\nT\\n is an \\nMM\\n×\\n transform matrix \\nwith elements \\nT\\n(,\\n) ,\\nuv\\n for \\nu,v\\n=−\\n012\\n1\\n,,, , .\\n…\\nM\\nTo obtain the inverse transform, we pre- and post-multiply Eq. (2-63) by an \\ninverse transformation matrix \\nB\\n:\\n \\nBTB BAFAB\\n=\\n \\n(2-64)\\nIf \\nBA\\n=\\n−\\n1\\n,\\n \\nFB T B\\n=\\n \\n(2-65)\\nindicating that \\nF\\n or\\n, equivalently, \\nfx y\\n(,\\n)\\n, can be recovered completely from its \\nforward transform.\\n If \\nB\\n is not equal to \\nA\\n−\\n1\\n, Eq. (2-65) yields an approximation:\\n \\nˆ\\nF\\nBAFAB\\n=\\n \\n(2-66)\\nIn addition to the Fourier transform, a number of important transforms, including \\nthe \\nW\\nalsh\\n, \\nHadamard\\n, \\ndiscrete\\n \\ncosine\\n, \\nHaar\\n, and \\nslant\\n transforms, can be expressed \\nin the form of Eqs. (2-55) and (2-56), or, equivalently, in the form of Eqs. (2-63) and', metadata={'source': 'imagepro.pdf', 'page': 112}),\n",
       " Document(page_content='(2-65). We will discuss these and other types of image transforms in later chapters. \\nb a\\nd c\\nFIGURE 2.45\\n(a) Image  \\ncorrupted by  \\nsinusoidal  \\ninterference.  \\n(b) Magnitude of \\nthe Fourier  \\ntransform  \\nshowing the \\nbursts of energy \\ncaused by the \\ninterference \\n(the bursts were \\nenlarged for \\ndisplay purposes). \\n(c) Mask used \\nto eliminate the \\nenergy bursts.  \\n(d) Result of  \\ncomputing the \\ninverse of the \\nmodiﬁed Fourier \\ntransform.  \\n(Original  \\nimage courtesy of \\nNASA.) \\nDIP4E_GLOBAL_Print_Ready.indb   111\\n6/16/2017   2:02:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 112}),\n",
       " Document(page_content='112\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nIMAGE INTENSITIES AS RANDOM VARIABLES\\nWe treat image intensities as random quantities in numerous places in the book. For \\nexample, let \\nzi L\\ni\\n,, , , ,,\\n=−\\n012 1\\n…\\n denote the values of all possible intensities in an \\nMN\\n×\\n digital image. The probability, \\npz\\nk\\n() ,\\n of intensity level \\nz\\nk\\n occurring in the im-\\nage is estimated as\\n \\npz\\nn\\nMN\\nk\\nk\\n()\\n=\\n \\n(2-67)\\nwhere \\nn\\nk\\n is the number of times that intensity \\nz\\nk\\n occurs in the image and \\nMN\\n is the \\ntotal number of pixels. Clearly,\\n \\npz\\nk\\nk\\nL\\n()\\n=\\n−\\n∑\\n=\\n0\\n1\\n1  \\n(2-68)\\nOnce we have \\npz\\nk\\n() ,\\n we can determine a number of important image characteristics. \\nF\\nor example, the mean (average) intensity is given by\\n \\nmz p z\\nkk\\nk\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(2-69)\\nSimilarly, the variance of the intensities is\\n \\ns\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\nzm p z\\nkk\\nk\\nL\\n \\n(2-70)\\nThe variance is a measure of the spread of the values of \\nz\\n about the mean,\\n so it is \\na useful measure of image contrast. In general, the \\nn', metadata={'source': 'imagepro.pdf', 'page': 113}),\n",
       " Document(page_content='n\\nth central moment of random \\nvariable \\nz\\n about the mean is defined as\\n \\nm\\nnk\\nn\\nk\\nk\\nL\\nzz m p z\\n() ( ) ( )\\n=−\\n=\\n−\\n∑\\n0\\n1\\n \\n(2-71)\\nWe see that \\nm\\n0\\n1\\n() ,\\nz\\n=\\n \\nm\\n1\\n0\\n() ,\\nz\\n=\\n and \\nms\\n2\\n2\\n() .\\nz\\n=\\n Whereas the mean and variance \\nhave an immediately obvious relationship to visual properties of an image\\n, higher-\\norder moments are more subtle. For example, a positive third moment indicates \\nthat the intensities are biased to values higher than the mean, a negative third mo-\\nment would indicate the opposite condition, and a zero third moment would tell us \\nthat the intensities are distributed approximately equally on both sides of the mean. \\nThese features are useful for computational purposes, but they do not tell us much \\nabout the appearance of an image in general.\\nAs you will see in subsequent chapters, concepts from probability play a central \\nrole in a broad range of image processing applications. For example, Eq. (2-67) is', metadata={'source': 'imagepro.pdf', 'page': 113}),\n",
       " Document(page_content='utilized in Chapter 3 as the basis for image enhancement techniques based on his-\\ntograms. In Chapter 5, we use probability to develop image restoration algorithms, \\nin Chapter 10 we use probability for image segmentation, in Chapter 11 we use it \\nto describe texture, and in Chapter 12 we use probability as the basis for deriving \\noptimum pattern recognition algorithms.\\nYou may ﬁnd it useful \\nto  consult the tutorials \\nsection in the book \\nwebsite for a brief review \\nof probability.\\nDIP4E_GLOBAL_Print_Ready.indb   112\\n6/16/2017   2:02:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 113}),\n",
       " Document(page_content='Summary, References, and Further Reading\\n    \\n113\\nProblems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com).\\n2.1 \\nIf you use a sheet of white paper to shield your \\neyes when looking directly at the sun,\\n the side of \\nthe sheet facing you appears black. Which of the \\nvisual processes discussed in Section 2.1 is respon-\\nsible for this?\\n2.2 * \\nUsing the background information provided in \\nSection 2.1,\\n and thinking purely in geometrical \\nterms, estimate the diameter of the smallest \\nprinted dot that the eye can discern if the page \\non which the dot is printed is 0.2 m away from the \\neyes. Assume for simplicity that the visual system \\nceases to detect the dot when the image of the dot \\non the fovea becomes smaller than the diameter \\nof one receptor (cone) in that area of the retina. \\nAssume further that the fovea can be modeled as \\na square array of dimension \\n15\\n.m\\nm', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='15\\n.m\\nm\\n on the side, \\nand that the cones and spaces between the cones \\nare distributed uniformly throughout this array\\n.\\n2.3 \\nAlthough it is not shown in Fig. 2.10, alternating \\ncurrent is part of the electromagnetic spectrum.\\n \\nCommercial alternating current in the United \\nStates has a frequency of 60 Hz. What is the wave-\\nlength in kilometers of this component of the \\nspectrum?\\n2.4 \\nYou are hired to design the front end of an imag-\\ning system for studying the shapes of cells\\n, bacteria, \\nviruses, and proteins. The front end consists in \\nthis case of the illumination source(s) and cor-\\nresponding imaging camera(s).The diameters of \\ncircles required to fully enclose individual speci-\\nmens in each of these categories are 50, 1, 0.1, and \\n00 1\\n.\\nm\\nm,\\n respectively. In order to perform auto-\\nmated analysis\\n, the smallest detail discernible on a \\nspecimen must be \\n0 001 .\\nm\\nm.\\n \\n(a) * \\nCan you solve the imaging aspects of this \\nproblem with a single sensor and camera?', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='If your answer is yes\\n, specify the illumina-\\ntion wavelength band and the type of camera \\nneeded. By “type,” we mean the band of the \\nelectromagnetic spectrum to which the cam-\\nera is most sensitive (e.g., infrared).\\n(b) \\nIf your answer in (a) is no, what type of illu-\\nmination sources and corresponding imaging \\nsensors would you recommend? Specify the \\nlight sources and cameras as requested in \\npart (a).\\n Use the minimum number of illumi-\\nnation sources and cameras needed to solve \\nthe problem. (\\nHint:\\n From the discussion in \\nSummary, References, and Further Reading\\n \\nThe material in this chapter is the foundation for the remainder of the book. For additional reading on visual per-\\nception, see Snowden et al. [2012], and the classic book by Cornsweet [1970]. Born and Wolf [1999] discuss light in \\nterms of electromagnetic theory. A basic source for further reading on image sensing is Trussell and Vrhel [2008].', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='The image formation model discussed in Section 2.3 is from Oppenheim et al. [1968]. The IES Lighting Handbook \\n[2011] is a reference for the illumination and reﬂectance values used in that section. The concepts of image sampling \\nintroduced in Section 2.4 will be covered in detail in Chapter 4. The discussion on experiments dealing with the \\nrelationship between image quality and sampling is based on results from Huang [1965]. For further reading on the \\ntopics discussed in Section 2.5, see Rosenfeld and Kak [1982], and Klette and Rosenfeld [2004].\\nSee Castleman [1996] for additional reading on linear systems in the context of image processing. The method of \\nnoise reduction by image averaging was ﬁrst proposed by Kohler and Howell [1963]. See Ross [2014] regarding the \\nexpected value of the mean and variance of the sum of random variables. See Schröder [2010] for additional read-', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='ing on logic and sets. For additional reading on geometric spatial transformations see Wolberg [1990] and Hughes \\nand Andries [2013]. For further reading on image registration see Goshtasby [2012]. Bronson and Costa [2009] is a \\ngood reference for additional reading on vectors and matrices. See Chapter 4 for a detailed treatment of the Fourier \\ntransform, and Chapters 7, 8, and 11 for details on other image transforms. For details on the software aspects of \\nmany of the examples in this chapter, see Gonzalez, Woods, and Eddins [2009]. \\nDIP4E_GLOBAL_Print_Ready.indb   113\\n6/16/2017   2:02:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 114}),\n",
       " Document(page_content='114\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nSection 2.2, the illumination required to “see” \\nan object must have a wavelength the same \\nsize or smaller than the object.)\\n2.5 \\nYou are preparing a report and have to insert in it \\nan image of size \\n2048 2048\\n×\\n pixels.\\n(a) * \\nAssuming no limitations on the printer, what \\nwould the resolution in line pairs per mm \\nhave to be for the image to ﬁt in a space of \\nsize \\n55\\n×\\n cm?\\n(b) \\nWhat would the resolution have to be in dpi \\nfor the image to ﬁt in \\n22\\n×\\n inches?\\n2.6 * \\nA CCD camera chip of dimensions \\n77\\n×\\n mm and \\n1024 1024\\n×\\n sensing elements, is focused on a \\nsquare\\n, ﬂat area, located 0.5 m away. The camera \\nis equipped with a 35-mm lens. How many line \\npairs per mm will this camera be able to resolve? \\n(\\nHint:\\n Model the imaging process as in Fig. 2.3, \\nwith the focal length of the camera lens substitut-\\ning for the focal length of the eye.)\\n2.7 \\nAn automobile manufacturer is automating the', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content='placement of certain components on the bumpers \\nof a limited-edition line of sports cars\\n. The com-\\nponents are color-coordinated, so the assembly  \\nrobots need to know the color of each car in order \\nto select the appropriate bumper component. \\nModels come in only four colors: blue, green, red, \\nand white. You are hired to propose a solution \\nbased on imaging. How would you solve the prob-\\nlem of determining the color of each car, keeping \\nin mind that cost is the most important consider-\\nation in your choice of components?\\n2.8 * \\nSuppose that a given automated imaging applica-\\ntion requires a minimum resolution of 5 line pairs \\nper mm to be able to detect features of interest \\nin objects viewed by the camera.\\n The distance \\nbetween the focal center of the camera lens and \\nthe area to be imaged is 1 m. The area being \\nimaged is \\n05 5\\n.\\n×0\\n.\\n m. You have available a 200 \\nmm lens\\n, and your job is to pick an appropriate \\nCCD imaging chip. What is the minimum number', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content='of sensing elements and square size, \\ndd\\n×\\n,\\n of the \\nCCD chip that will meet the requirements of this \\napplication? (\\nHint:\\n Model the imaging process \\nas in Fig. 2.3, and assume for simplicity that the \\nimaged area is square.)\\n2.9 \\nA common measure of transmission for digital \\ndata is the \\nbaud rate\\n,\\n deﬁned as symbols (bits in \\nour case) per second. As a minimum, transmission \\nis accomplished in packets consisting of a start \\nbit, a byte (8 bits) of information, and a stop bit. \\nUsing these facts, answer the following:\\n(a) * \\nHow many seconds would it take to transmit \\na sequence of 500 images of size \\n1024 1024\\n×\\n \\npixels with 256 intensity levels using a 3 \\nM-baud \\n(10\\n6\\nbits/sec)\\n baud modem? (This \\nis a representative medium speed for a DSL \\n(Digital Subscriber Line) residential line\\n.\\n(b) \\nWhat would the time be using a 30 G-baud \\n(10\\n9\\nbits/sec)\\n modem? (This is a represen-\\ntative medium speed for a commercial line.)\\n2.10 * \\nHigh-deﬁnition television (HDTV) generates', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content='images with 1125 horizontal \\nTV lines interlaced \\n(i.e., where every other line is “painted” on the \\nscreen in each of two ﬁelds, each ﬁeld being \\n16 0 t h\\n of a second in duration). The width-to-\\nheight aspect ratio of the images is 16:9.\\n The \\nfact that the number of horizontal lines is ﬁxed \\ndetermines the vertical resolution of the images. \\nA company has designed a system that extracts \\ndigital images from HDTV video. The resolution \\nof each horizontal line in their system is propor-\\ntional to vertical resolution of HDTV , with the \\nproportion being the width-to-height ratio of the \\nimages. Each pixel in the color image has 24 bits \\nof intensity, 8 bits each for a red, a green, and a \\nblue component image. These three “primary” \\nimages form a color image. How many bits would \\nit take to store the images extracted from a two-\\nhour HDTV movie?\\n2.11 \\nWhen discussing linear indexing in Section 2.4, \\nwe arrived at the linear index in Eq.\\n (2-14) by', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content='(2-14) by \\ninspection. The same argument used there can be \\nextended to a 3-D array with coordinates \\nx\\n, \\ny\\n, and \\nz\\n, and corresponding dimensions \\nM\\n, \\nN\\n, and \\nP\\n. The \\nlinear index for any \\n(,,)\\nxy\\nz\\n is\\nsx M y N z\\n=+ +\\n()\\nStart with this expression and\\n(a) * \\nDerive Eq. (2-15).\\n(b) \\nDerive Eq. (2-16).\\n2.12 * \\nSuppose that a ﬂat area with center at \\n(,)\\nxy\\n00\\n is \\nDIP4E_GLOBAL_Print_Ready.indb   114\\n6/16/2017   2:02:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 115}),\n",
       " Document(page_content='Problems\\n    \\n115\\nilluminated by a light source with intensity distri-\\nbution\\n \\nixy K e\\nxx yy\\n(,)\\n[( ) ( ) ]\\n=\\n−− +−\\n0\\n2\\n0\\n2\\nAssume for simplicity that the reﬂectance of \\nthe area is constant and equal to 1.0, and let \\nK\\n=\\n255.\\n If the intensity of the resulting image is \\nquantized using \\nk\\n bits, and the eye can detect an \\nabrupt change of eight intensity levels between \\nadjacent pixels, what is the highest value of \\nk\\n that \\nwill cause visible false contouring?\\n2.13 \\nSketch the image in Problem 2.12 for \\nk\\n=\\n2.\\n2.14 \\nConsider the two image subsets, \\nS\\n1\\n and \\nS\\n2\\n in the \\nfollowing ﬁgure. With reference to Section 2.5, \\nand assuming that \\nV\\n=\\n{}\\n1,\\n determine whether \\nthese two subsets are:\\n(a) * \\n 4-adjacent.\\n(b) \\n 8-adjacent. \\n(c) \\nm\\n-adjacent.\\n \\n1\\nS\\n2\\nS\\n00\\n0 0 0 0 0\\n01\\n1\\n11\\n1 0 0 1 0\\n00\\n0\\n10\\n1 1 0 1 0\\n00\\n0\\n00\\n0 0 1 1 1\\n00\\n0\\n01\\n0 0 1 1 1\\n01\\n1\\n2.15 * \\nDevelop an algorithm for converting a one-pixel-\\nthick 8-path to a 4-path.\\n2.16', metadata={'source': 'imagepro.pdf', 'page': 116}),\n",
       " Document(page_content='2.16 \\nDevelop an algorithm for converting a one-pixel-\\nthick \\nm\\n-path to a 4-path.\\n2.17 \\nRefer to the discussion toward the end of Sec-\\ntion 2.5,\\n where we deﬁned the background of an \\nimage as \\n() ,\\nR\\nu\\nc\\n the complement of the union of \\nall the regions in the image. In some applications, \\nit is advantageous to deﬁne the background as the \\nsubset of pixels of \\n()\\nR\\nu\\nc\\n that are not \\nhole\\n pixels \\n(informally, think of holes as sets of background \\npixels surrounded by foreground pixels). How \\nwould you modify the deﬁnition to exclude hole \\npixels from \\n()\\nR\\nu\\nc\\n?\\n An answer such as “the back-\\nground is the subset of pixels of \\n()\\nR\\nu\\nc\\n that are not \\nhole pixels” is not acceptable. (\\nHint:\\n Use the con-\\ncept of connectivity.)\\n2.18 \\nConsider the image segment shown in the ﬁgure \\nthat follows\\n.\\n(a) * \\nAs in Section 2.5, let \\nV\\n=\\n{,\\n}\\n01\\n be the set \\nof intensity values used to deﬁne adjacenc\\ny. \\nCompute the lengths of the shortest 4-, 8-, \\nand \\nm\\n-path between \\np \\nand \\nq', metadata={'source': 'imagepro.pdf', 'page': 116}),\n",
       " Document(page_content='p \\nand \\nq\\n in the follow-\\ning image. If a particular path does not exist \\nbetween these two points, explain why.\\n3121\\n2202\\n1211\\n1012\\n(\\np\\n)\\n(\\nq\\n)\\n(b) \\nRepeat (a) but using \\nV\\n=\\n{,\\n} .\\n12\\n2.19 \\nConsider two points \\np\\n and \\nq\\n.\\n(a) * \\nState the condition(s) under which the \\nD\\n4\\n \\ndistance between \\np\\n and \\nq\\n is equal to the \\nshortest 4-path between these points.\\n(b) \\nIs this path unique?\\n2.20 \\nRepeat problem 2.19 for the \\nD\\n8\\n distance.\\n2.21 \\nConsider two \\none-dimensional\\n images \\nf\\n and \\ng\\n of \\nthe same size\\n. What has to be true about the ori-\\nentation of these images for the elementwise and \\nmatrix products discussed in Section 2.6 to make \\nsense? Either of the two images can be ﬁrst in \\nforming the product.\\n2.22 * \\nIn the next chapter, we will deal with operators \\nwhose function is to compute the sum of pixel val-\\nues in a small subimage area,\\n \\nS\\nxy\\n,\\n \\nas in Eq.\\n (2-43). \\nShow that these are linear operators.\\n2.23 \\nRefer to Eq. (2-24) in answering the following: \\n(a) *', metadata={'source': 'imagepro.pdf', 'page': 116}),\n",
       " Document(page_content='(a) * \\nShow that image summation is a linear opera-\\ntion.\\n(b) \\nShow that image subtraction is a linear oper-\\nation.\\n(c) * \\nShow that image multiplication in a nonlinear \\noperation.\\n(d) \\nShow that image division is a nonlinear opera-\\ntion.\\n2.24 \\nThe median, \\nz\\n,\\n of a set of numbers is such that \\nDIP4E_GLOBAL_Print_Ready.indb   115\\n6/16/2017   2:03:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 116}),\n",
       " Document(page_content='116\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nhalf the values in the set are below \\nz\\n and the oth-\\ner half are above it.\\n For example, the median of \\nthe set of values \\n{,,, , , , }\\n23\\n82 02 12 53 1\\n is 20. Show \\nthat an operator that computes the median of a \\nsubimage area,\\n \\nS\\n, is nonlinear. (\\nHint:\\n It is sufﬁ-\\ncient to show that \\nz\\n fails the linearity test for a \\nsimple numerical example\\n.)\\n2.25 * \\nShow that image averaging can be done recur-\\nsively\\n. That is, show that if \\nak\\n()\\nis the average of \\nk\\n images\\n, then the average of \\nk\\n+\\n1\\n images can \\nbe obtained from the already-computed average\\n, \\nak\\n()\\n,\\n and the new image, \\nf\\nk\\n+\\n1\\n. \\n2.26\\nWith reference to Example 2.5:\\n(a) * \\nProve the validity of Eq. (2-27).\\n(b) \\nProve the validity of Eq. (2-28).\\nF\\nor part (b) you will need the following facts from \\nprobability: (1) the variance of a constant times a \\nrandom variable is equal to the constant squared \\ntimes the variance of the random variable. (2) The', metadata={'source': 'imagepro.pdf', 'page': 117}),\n",
       " Document(page_content='variance of the sum of uncorrelated random vari-\\nables is equal to the sum of the variances of the \\nindividual random variables.\\n2.27 \\nConsider two 8-bit images whose intensity levels \\nspan the full range from 0 to 255.\\n(a) * \\nDiscuss the limiting effect of repeatedly sub-\\ntracting image (2) from image (1).\\n Assume \\nthat the results have to be represented also \\nin eight bits.\\n(b) \\nWould reversing the order of the images \\nyield a different result?\\n2.28 * \\nImage subtraction is used often in industrial appli-\\ncations for detecting missing components in prod-\\nuct assembly\\n. The approach is to store a “golden” \\nimage that corresponds to a correct assembly; this \\nimage is then subtracted from incoming images of \\nthe same product. \\nIdeally\\n, the differences would \\nbe zero if the new products are assembled cor-\\nrectly. Difference images for products with miss-\\ning components would be nonzero in the area \\nwhere they differ from the golden image. What', metadata={'source': 'imagepro.pdf', 'page': 117}),\n",
       " Document(page_content='conditions do you think have to be met in prac-\\ntice for this method to work?\\n2.29 \\nWith reference to Eq. (2-32),\\n(a) * \\nGive a general formula for the value of \\nK\\n \\nas a function of the number of bits\\n, \\nk\\n, in an \\nimage, such that \\nK\\n results in a scaled image \\nwhose intensities span the full \\nk\\n-bit range.\\n(b) \\nFind \\nK \\nfor 16- and 32-bit images\\n. \\n2.30 \\nGive Venn diagrams for the following expres-\\nsions:\\n(a) * \\n()\\n( ) .\\nAC ABC\\n¨¨ ¨\\n−\\n(b) \\n() () .\\nAC BC\\n¨´¨\\n(c) \\nBA BA B C\\n−−\\n[]\\n() ( )\\n¨¨ ¨\\n(d) \\nBB A C A C\\n−=\\n∅\\n¨´\\n¨\\n() ;\\n.\\nGiven that \\n2.31 \\nUse Venn diagrams to prove the validity of the \\nfollowing expressions:\\n(a) * \\n()\\n()\\n()\\nAB AC ABC A BC\\n¨´ ¨ ¨ ¨ ¨´\\n−\\n[]\\n=\\n(b) \\n()\\nABC A B C\\nc ccc\\n´´ ¨ ¨\\n=\\n(c) \\n() ()\\nAC B B A C\\nc\\n´¨\\n=−−\\n(d) \\n()\\nABC A B C\\nc ccc\\n¨¨ ´ ´\\n=\\n2.32 \\nGive expressions (in terms of sets \\nA\\n, \\nB\\n,\\n and \\nC\\n)\\n \\nfor the sets shown shaded in the following ﬁgures. \\nThe shaded areas in each ﬁgure constitute one set, \\nso give only one expression for each of the four \\nﬁgures.', metadata={'source': 'imagepro.pdf', 'page': 117}),\n",
       " Document(page_content='ﬁgures.\\n(a)* (b) (c) (d)\\nA\\nB\\nC\\n2.33 \\nWith reference to the discussion on sets in Section \\n2.6,\\n do the following:\\n(a) * \\nLet \\nS\\n be a set of real numbers ordered by the \\nrelation \\n“less than or equal to” \\n() .\\n≤\\n Show \\nthat \\nS\\n is a partially ordered set; that is, show \\nthat the reﬂexive, transitive, and antisymmet-\\nric properties hold.\\n(b) * \\nShow that changing the relation “less than or \\nequal to”\\n to “less than” \\n()\\n<\\n produces a strict \\nordered set.\\n(c) \\nNow let \\nS\\n be the set of lower\\n-case letters in \\nthe English alphabet. Show that, under \\n() ,\\n<\\nS\\n is a strict ordered set.\\n2.34 \\nFor any nonzero integers \\nm\\n and \\nn\\n,\\n we say that \\nm\\n \\nDIP4E_GLOBAL_Print_Ready.indb   116\\n6/16/2017   2:03:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 117}),\n",
       " Document(page_content='Problems\\n    \\n117\\nis divisible by \\nn\\n, written \\nmn\\n, if there exists an \\ninteger \\nk\\n such that \\nkn m\\n=\\n.\\n For example, 42 (\\nm\\n) \\nis divisible by 7 (\\nn\\n) because there exists an inte-\\nger \\nk\\n=\\n6\\n such that \\nkn m\\n=\\n.\\n Show that the set of \\npositive integers is a partially ordered set under \\nthe relation “divisible by.” In other words, do the \\nfollowing:\\n(a) * \\nShow that the property of reﬂectivity holds \\nunder this relation.\\n(b) \\nShow that the property of transitivity holds.\\n(c) \\nShow that anti symmetry holds.\\n2.35 \\nIn general, what would the resulting image, \\ngxy\\n(,\\n) ,\\n \\nlook like if we modiﬁed Eq.\\n (2-43), as follows:\\ngxy\\nmn\\nTfr c\\nrc S\\nxy\\n(,)\\n( ,)\\n(,)\\n=\\n[]\\n∑\\n1\\nH\\nwhere \\nT\\n is the intensity transformation function \\nin Fig. 2.38(b)?\\n2.36 \\nWith reference to Table 2.3, provide single, com-\\nposite transformation functions for performing \\nthe following operations:\\n(a) * \\nScaling and translation.\\n(b) * \\nScaling, translation, and rotation.\\n(c)', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='(c) \\nVertical shear, scaling, translation, and rota-\\ntion.\\n(d) \\nDoes the order of multiplication of the indi-\\nvidual matrices to produce a single transfor\\n-\\nmations make a difference? Give an example \\nbased on a scaling/translation transforma-\\ntion to support your answer.\\n2.37 \\nWe know from Eq. (2-45) that an afﬁne transfor-\\nmation of coordinates is given by \\n′\\n′\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\nx\\ny\\nx\\ny\\naa\\na\\naaa\\n11 0 0 1\\n11 12 13\\n21 22 23\\nA\\n⎦ ⎦\\n⎥\\n⎥\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nx\\ny\\n1\\nwhere \\n(,)\\n′′\\nxy\\n are the transformed coordinates, \\n(,)\\nxy\\n are the original coordinates, and the ele-\\nments of \\nA\\n are given in \\nTable 2.3 for various \\ntypes of transformations. The inverse transforma-\\ntion, \\nA\\n−\\n1\\n,\\n to go from the transformed back to the \\noriginal coordinates is just as important for per\\n-\\nforming inverse mappings.\\n(a) * \\nFind the inverse scaling transformation.\\n(b) \\nFind the inverse translation transformation.\\n(c) \\nFind the inverse vertical and horizontal', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='shearing transformations\\n.\\n(d) * \\nFind the inverse rotation transformation.\\n(e) * \\nShow a composite inverse translation/rota-\\ntion transformation.\\n2.38 \\nWhat are the equations, analogous to Eqs. (2-46) \\nand (2-47),\\n that would result from using triangu-\\nlar instead of quadrilateral regions?\\n2.39 \\nDo the following.\\n(a) * \\nProve that the Fourier kernel in Eq. (2-59) is \\nseparable and symmetric\\n.\\n(b) \\nRepeat (a) for the kernel in Eq. (2-60).\\n2.40 * \\nShow that 2-D transforms with separable, sym-\\nmetric kernels can be computed by:\\n (1) comput-\\ning 1-D transforms along the individual rows (col-\\numns) of the input image; and (2) computing 1-D \\ntransforms along the columns (rows) of the result \\nfrom step (1).\\n2.41 \\nA plant produces miniature polymer squares that \\nhave to undergo 100% visual inspection.\\n Inspec-\\ntion is semi-automated. At each inspection sta-\\ntion, a robot places each polymer square over an \\noptical system that produces a magniﬁed image', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='of the square. The image completely ﬁlls a view-\\ning screen of size \\n80 80\\n×\\n mm.\\n Defects appear as \\ndark circular blobs\\n, and the human inspector’s job \\nis to look at the screen and reject any sample that \\nhas one or more dark blobs with a diameter of 0.8 \\nmm or greater, as measured on the scale of the \\nscreen. The manufacturing manager believes that \\nif she can ﬁnd a way to fully automate the process, \\nproﬁts will increase by 50%, and success in this \\nproject will aid her climb up the corporate ladder. \\nAfter extensive investigation, the manager decides \\nthat the way to solve the problem is to view each \\ninspection screen with a CCD TV camera and feed \\nthe output of the camera into an image processing \\nsystem capable of detecting the blobs, measuring \\ntheir diameter, and activating the accept/reject \\nbutton previously operated by a human inspec-\\ntor. She is able to ﬁnd a suitable system, provided \\nthat the smallest defect occupies an area of at \\nleast \\n22\\n×', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='least \\n22\\n×\\n pixels in the digital image. The manager \\nhires you to help her specify the camera and lens \\nDIP4E_GLOBAL_Print_Ready.indb   117\\n6/16/2017   2:03:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 118}),\n",
       " Document(page_content='118\\n    \\nChapter\\n \\n2\\n  \\nDigital Image Fundamentals\\nsystem to satisfy this requirement, using off-the-\\nshelf components\\n. Available off-the-shelf lenses \\nhave focal lengths that are integer multiples of \\n25 mm or 35 mm, up to 200 mm. Available cam-\\neras yield image sizes of \\n512 512\\n×\\n, \\n1024 1024\\n×\\n, \\nor \\n2048 2048\\n×\\n pixels. The \\nindividual\\n imaging \\nelements in these cameras are squares measuring \\n88\\n×\\n m,\\nm\\n and the spaces between imaging ele-\\nments are \\n2 m.\\nm\\nFor this application, the cameras \\ncost much more than the lenses\\n, so you should use \\nthe lowest-resolution camera possible, consistent \\nwith a suitable lens. As a consultant, you have \\nto provide a written recommendation, showing \\nin reasonable detail the analysis that led to your \\nchoice of components. Use the imaging geometry \\nsuggested in Problem 2.6.\\nDIP4E_GLOBAL_Print_Ready.indb   118\\n6/16/2017   2:03:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 119}),\n",
       " Document(page_content='1193\\nIntensity Transformations and \\nSpatial Filtering\\nPreview\\nThe term \\nspatial domain\\n refers to the image plane itself, and image processing methods in this category \\nare based on direct manipulation of pixels in an image. This is in contrast to image processing in a trans-\\nform domain which, as we will discuss in Chapters 4 and 6, involves ﬁrst transforming an image into the \\ntransform domain, doing the processing there, and obtaining the inverse transform to bring the results \\nback into the spatial domain. Two principal categories of spatial processing are intensity transforma-\\ntions and  spatial ﬁltering. Intensity transformations operate on single pixels of an image for tasks such \\nas contrast manipulation and image thresholding. Spatial ﬁltering performs operations on the neighbor-\\nhood of every pixel in an image. Examples of spatial ﬁltering include image smoothing and sharpening.', metadata={'source': 'imagepro.pdf', 'page': 120}),\n",
       " Document(page_content='In the sections that follow, we discuss a number of “classical” techniques for intensity transformations \\nand spatial ﬁltering.  \\nUpon completion of this chapter, readers should:\\n Understand the meaning of spatial domain \\nprocessing, and how it differs from transform \\ndomain processing.\\n Be familiar with the principal techniques used \\nfor intensity transformations.\\n Understand the physical meaning of image \\nhistograms and how they can be manipulated \\nfor image enhancement.\\n Understand the mechanics of spatial ﬁltering, \\nand how spatial ﬁlters are formed.\\n Understand the principles of spatial convolu-\\ntion and correlation.\\n Be familiar with the principal types of spatial \\nﬁlters, and how they are applied.\\n Be aware of the relationships between spatial \\nﬁlters, and the fundamental role of lowpass \\nﬁlters. \\n Understand how to use combinations of \\nenhancement methods in cases where a single \\napproach is insufﬁcient.\\nIt makes all the difference whether one sees darkness through', metadata={'source': 'imagepro.pdf', 'page': 120}),\n",
       " Document(page_content='the light or brightness through the shadows.\\nDavid Lindsay\\nDIP4E_GLOBAL_Print_Ready.indb   119\\n6/16/2017   2:03:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 120}),\n",
       " Document(page_content='120\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n3.1 BACKGROUND \\nAll the image processing techniques discussed in this chapter are implemented in \\nthe spatial domain, which we know from the discussion in Section 2.4 is the plane \\ncontaining the pixels of an image. Spatial domain techniques operate directly on the \\npixels of an image, as opposed, for example, to the frequency domain (the topic of \\nChapter 4) in which operations are performed on the Fourier transform of an image, \\nrather than on the image itself. As you will learn in progressing through the book, \\nsome image processing tasks are easier or more meaningful to implement in the \\nspatial domain, while others are best suited for other approaches. \\nTHE BASICS OF INTENSITY TRANSFORMATIONS AND SPATIAL  \\nFILTERING\\nThe spatial domain processes we discuss in this chapter are based on the expression\\n \\ngxy T f xy\\n(,\\n) (,)\\n=\\n[]\\n \\n(3-1)\\nwhere \\nfx y\\n(,\\n)\\n is an input image, \\ngxy\\n(,\\n)\\n is the output image, and \\nT', metadata={'source': 'imagepro.pdf', 'page': 121}),\n",
       " Document(page_content='T\\n is an operator on \\nf\\n \\ndefined over a \\nneighborhood of point \\n(,)\\nxy\\n. The operator can be applied to the pix-\\nels of a single image (our principal focus in this chapter) or to the pixels of a set of \\nimages\\n, such as performing the elementwise sum of a sequence of images for noise \\nreduction, as discussed in Section 2.6. Figure 3.1 shows the basic implementation of \\nEq. (3-1) on a single image. The point \\n(,)\\nxy\\n00\\n shown is an arbitrary location in the \\nimage, and the small region shown is a \\nneighborhood\\n of \\n(,) ,\\nxy\\n00\\n as explained in Sec-\\ntion 2.6. Typically, the neighborhood is rectangular, centered on \\n(,)\\nxy\\n00\\n, and much \\nsmaller in size than the image.\\nThe process that Fig. 3.1 illustrates consists of moving the center of the neighbor-\\nhood from pixel to pixel, and applying the operator \\nT\\n to the pixels in the neighbor-\\nhood to yield an output value at that location. Thus, for any speciﬁc location \\n(,) ,\\nxy\\n00\\n \\n3.1\\nFIGURE 3.1\\nA \\n33\\n×\\n  \\nneighborhood \\nabout a point', metadata={'source': 'imagepro.pdf', 'page': 121}),\n",
       " Document(page_content='about a point \\n(,)\\nxy\\n00\\n in an image. \\nThe neighborhood \\nis moved from \\npixel to pixel in the \\nimage to generate \\nan output image.  \\nRecall from  \\nChapter 2 that the \\nvalue of a pixel at \\nlocation \\n(,)\\nxy\\n00\\n is\\nfx y\\n(,\\n) ,\\n00\\n the value \\nof the image at that \\nlocation.\\nOrigin\\n00\\n3 3 neighborhood \\nof point ( , )\\nxy\\n×\\nImage \\nf\\ny \\nx\\nx\\n0\\ny\\n0\\n00\\nPixel [its value is ( , )]\\nfx y\\nDIP4E_GLOBAL_Print_Ready.indb   120\\n6/16/2017   2:03:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 121}),\n",
       " Document(page_content='3.1\\n  \\nBackground\\n    \\n121\\nthe value of the output image \\ng\\n at those coordinates is equal to the result of apply-\\ning \\nT\\n to the neighborhood with origin at \\n(,)\\nxy\\n00\\n in \\nf\\n.  For example, suppose that \\nthe neighborhood is a square of size \\n33\\n×\\n and that operator \\nT\\n is deﬁned as \\n“com-\\npute the average intensity of the pixels in the neighborhood.” Consider an arbitrary \\nlocation in an image, say \\n(,) .\\n100\\n150\\n The result at that location in the output image, \\ng\\n(,\\n) ,\\n100 150\\n is the sum of \\nf\\n(,\\n)\\n100 150\\n and its 8-neighbors, divided by 9. The center of \\nthe neighborhood is then moved to the next adjacent location and the procedure \\nis repeated to generate the next value of the output image \\ng\\n.\\n Typically, the process \\nstarts at the top left of the input image and proceeds pixel by pixel in a horizontal \\n(vertical) scan, one row (column) at a time. We will discuss this type of neighbor-\\nhood processing beginning in Section 3.4.\\nThe smallest possible neighborhood is of size \\n11', metadata={'source': 'imagepro.pdf', 'page': 122}),\n",
       " Document(page_content='11\\n×\\n.\\n In this case, \\ng\\n depends only \\non the value of \\nf\\n at a single point \\n(,)\\nxy\\n and \\nT\\n in Eq.\\n (3-1) becomes an \\nintensity\\n (also \\ncalled a \\ngray-level,\\n or \\nmapping\\n) \\ntransformation function\\n of the form\\n \\nsT r\\n=\\n()\\n \\n(3-2)\\nwhere, for simplicity in notation, we use \\ns\\n and \\nr\\n to denote\\n, respectively, the intensity \\nof \\ng\\n and \\nf\\n at any point \\n(,) .\\nxy\\n For example, if \\nTr\\n()\\n has the form in Fig. 3.2(a), the \\nresult of applying the transformation to every pixel in \\nf\\n to generate the correspond-\\ning pixels in \\ng\\n would be to produce an image of higher contrast than the original,\\n by \\ndarkening the intensity levels below \\nk\\n and brightening the levels above \\nk\\n. In this \\ntechnique, sometimes called \\ncontrast stretching\\n (see Section 3.2), values of \\nr\\n lower \\nthan \\nk\\n reduce (darken) the values of \\ns\\n, toward black. The opposite is true for values \\nof \\nr\\n higher than \\nk\\n. Observe how an intensity value \\nr\\n0\\n is mapped to obtain the cor-\\nresponding value \\ns\\n0\\n.', metadata={'source': 'imagepro.pdf', 'page': 122}),\n",
       " Document(page_content='s\\n0\\n.\\n In the limiting case shown in Fig. 3.2(b), \\nTr\\n()\\n produces a two-\\nlevel (binary) image\\n. A mapping of this form is called a \\nthresholding\\n \\nfunction\\n. Some \\nfairly simple yet powerful processing approaches can be formulated with intensity \\ntransformation functions. In this chapter, we use intensity transformations princi-\\npally for image enhancement. In Chapter 10, we will use them for image segmenta-\\ntion. Approaches whose results depend only on the intensity at a point sometimes \\nare called \\npoint processing\\n techniques, as opposed to the \\nneighborhood processing\\n \\ntechniques discussed in the previous paragraph.\\nDepending on the size \\nof a neighborhood and \\nits location, part of the \\nneighborhood may lie \\noutside the image. There \\nare two solutions to this: \\n(1) to ignore the values \\noutside the image, or \\n(2) to pad image, as \\ndiscussed in Section 3.4.  \\nThe second approach is \\npreferred.\\nb a\\nFIGURE 3.2\\nIntensity  \\ntransformation \\nfunctions.  \\n(a) Contrast  \\nstretching', metadata={'source': 'imagepro.pdf', 'page': 122}),\n",
       " Document(page_content='stretching  \\nfunction.  \\n(b) Thresholding \\nfunction.\\nk\\nk\\nr\\n0\\n s\\n0\\n \\n/H11005\\n \\nT\\n(\\nr\\n0\\n)\\nDark Light\\nDark Light\\nDark Light\\nDark Light\\nr \\nr\\ns\\n \\n/H11005\\n \\nT\\n(\\nr\\n)\\ns\\n \\n/H11005\\n \\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nDIP4E_GLOBAL_Print_Ready.indb   121\\n6/16/2017   2:03:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 122}),\n",
       " Document(page_content='122\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nABOUT THE EXAMPLES IN THIS CHAPTER\\nAlthough intensity transformation and spatial filtering methods span a broad range \\nof applications, most of the examples in this chapter are applications to image \\nenhancement. \\nEnhancement\\n is the process of manipulating an image so that the \\nresult is more suitable than the original for a specific application. The word \\nspecific\\n \\nis important, because it establishes at the outset that enhancement techniques are \\nproblem-oriented. Thus, for example, a method that is quite useful for enhancing \\nX-ray images may not be the best approach for enhancing infrared images. There is \\nno general “theory” of image enhancement. When an image is processed for visual \\ninterpretation, the viewer is the ultimate judge of how well a particular method \\nworks. When dealing with machine perception, enhancement is easier to quantify.', metadata={'source': 'imagepro.pdf', 'page': 123}),\n",
       " Document(page_content='For example, in an automated character-recognition system, the most appropriate \\nenhancement method is the one that results in the best recognition rate, leaving \\naside other considerations such as computational requirements of one method \\nversus another. Regardless of the application or method used, image enhancement \\nis one of the most visually appealing areas of image processing. Beginners in image \\nprocessing generally find enhancement applications interesting and relatively sim-\\nple to understand. Therefore, using examples from image enhancement to illustrate \\nthe spatial processing methods developed in this chapter not only saves having an \\nextra chapter in the book dealing with image enhancement but, more importantly, is \\nan effective approach for introducing newcomers to image processing techniques in \\nthe spatial domain. As you progress through the remainder of the book, you will find \\nthat the material developed in this chapter has a scope that is much broader than', metadata={'source': 'imagepro.pdf', 'page': 123}),\n",
       " Document(page_content='just image enhancement.\\n3.2 SOME BASIC INTENSITY TRANSFORMATION FUNCTIONS \\nIntensity transformations are among the simplest of all image processing techniques. \\nAs noted in the previous section, we denote the values of pixels, before and after \\nprocessing, by \\nr\\n and \\ns\\n, respectively. These values are related by a transformation \\nT\\n, \\nas given in Eq. (3-2), that maps a pixel value \\nr\\n into a pixel value \\ns\\n. Because we deal \\nwith digital quantities, values of an intensity transformation function typically are \\nstored in a table, and the mappings from \\nr\\n to \\ns\\n are implemented via table lookups. \\nFor an 8-bit image, a lookup table containing the values of \\nT\\n will have 256 entries.\\nAs an introduction to intensity transformations, consider Fig. 3.3, which shows \\nthree basic types of functions used frequently in image processing: linear (negative \\nand identity transformations), logarithmic (log and inverse-log transformations), \\nand power-law (\\nn\\nth power and \\nn', metadata={'source': 'imagepro.pdf', 'page': 123}),\n",
       " Document(page_content='n\\nth power and \\nn\\nth root transformations). The identity function is \\nthe trivial case in which the input and output intensities are identical.\\nIMAGE NEGATIVES \\nThe negative of an image with intensity levels in the range \\n[, ]\\n01\\nL\\n−\\n is obtained by \\nusing the negative transformation function shown in F\\nig. 3.3, which has the form:\\n \\nsL r\\n=−\\n−\\n1  \\n(3-3)\\n3.2\\nDIP4E_GLOBAL_Print_Ready.indb   122\\n6/16/2017   2:03:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 123}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n123\\nReversing the intensity levels of a digital image in this manner produces the \\nequivalent of a photographic negative. This type of processing is used, for example, \\nin enhancing white or gray detail embedded in dark regions of an image, especially \\nwhen the black areas are dominant in size. Figure 3.4 shows an example. The origi-\\nnal image is a digital mammogram showing a small lesion. Despite the fact that the \\nvisual content is the same in both images, some viewers find it easier to analyze the \\nfine details of the breast tissue using the negative image.\\nIdentity\\n0\\nL\\n/\\n4L\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n \\n1\\nInput intensity levels, \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL\\n \\n/H11002\\n \\n1\\nOutput intensity levels, \\ns\\nLog\\nNegative\\nn\\nth power\\nn\\nth root\\nInverse log\\n(exponential)\\nFIGURE 3.3\\nSome basic  \\nintensity  \\ntransformation  \\nfunctions. Each \\ncurve was scaled  \\nindependently\\n so \\nthat all curves \\nwould ﬁt in the \\nsame graph. Our  \\ninterest here is', metadata={'source': 'imagepro.pdf', 'page': 124}),\n",
       " Document(page_content='interest here is \\non the \\nshapes\\n of \\nthe curves, not \\non their relative \\nvalues. \\nb a\\nFIGURE 3.4\\n(a) A  \\ndigital  \\nmammogram.  \\n(b) Negative \\nimage obtained \\nusing Eq. (3-3). \\n(Image (a)  \\nCourtesy of \\nGeneral Electric \\nMedical Systems.)\\nDIP4E_GLOBAL_Print_Ready.indb   123\\n6/16/2017   2:03:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 124}),\n",
       " Document(page_content='124\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nLOG TRANSFORMATIONS\\nThe general form of the log transformation in Fig. 3.3 is\\n \\nsc r\\n=+\\nlog\\n( )\\n1  \\n(3-4)\\nwhere \\nc\\n is a constant and it is assumed that \\nr\\n≥\\n0.\\n The shape of the log curve in Fig. 3.3 \\nshows that this transformation maps a narrow range of low intensity values in the \\ninput into a wider range of output levels\\n. For example, note how input levels in the \\nrange \\n[, ]\\n04\\nL\\n map to output levels to the range \\n[, ] .\\n03 4\\nL\\n Conversely, higher values \\nof input levels are mapped to a narrower range in the output.\\n We use a transformation \\nof this type to expand the values of dark pixels in an image, while compressing the \\nhigher-level values. The opposite is true of the inverse log (exponential) transformation.\\nAny curve having the general shape of the log function shown in Fig. 3.3 would \\naccomplish this spreading/compressing of intensity levels in an image, but the pow-', metadata={'source': 'imagepro.pdf', 'page': 125}),\n",
       " Document(page_content='er-law transformations discussed in the next section are much more versatile for \\nthis purpose. The log function has the important characteristic that it compresses \\nthe dynamic range of pixel values. An example in which pixel values have a large \\ndynamic range is the Fourier spectrum, which we will discuss in Chapter 4. It is not \\nunusual to encounter spectrum values that range from 0 to \\n10\\n6\\n or higher. Processing \\nnumbers such as these presents no problems for a computer, but image displays can-\\nnot reproduce faithfully such a wide range of values. The net effect is that intensity \\ndetail can be lost in the display of a typical Fourier spectrum.\\nFigure 3.5(a) shows a Fourier spectrum with values in the range 0 to \\n15 1 0\\n6\\n..\\n×\\n \\nW\\nhen these values are scaled linearly for display in an 8-bit system, the brightest \\npixels dominate the display, at the expense of lower (and just as important) values', metadata={'source': 'imagepro.pdf', 'page': 125}),\n",
       " Document(page_content='of the spectrum. The effect of this dominance is illustrated vividly by the relatively \\nsmall area of the image in Fig. 3.5(a) that is not perceived as black. If, instead of \\ndisplaying the values in this manner, we ﬁrst apply Eq. (3-4) (with \\nc\\n=\\n1\\n in this case) \\nto the spectrum values\\n, then the range of values of the result becomes 0 to 6.2. Trans-\\nforming values in this way enables a greater range of intensities to be shown on the \\ndisplay. Figure 3.5(b) shows the result of scaling the intensity range linearly to the \\nb a\\nFIGURE 3.5\\n(a) Fourier  \\nspectrum  \\ndisplayed as a  \\ngrayscale image. \\n(b) Result of  \\napplying the log \\ntransformation \\nin Eq. (3-4) with \\nc\\n=\\n1.\\n Both images \\nare scaled to the \\nrange [0, 255].\\nDIP4E_GLOBAL_Print_Ready.indb   124\\n6/16/2017   2:03:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 125}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n125\\ninterval \\n[, ]\\n0\\n255\\n and showing the spectrum in the same 8-bit display. The level of \\ndetail visible in this image as compared to an unmodiﬁed display of the spectrum \\nis evident from these two images\\n. Most of the Fourier spectra in image processing \\npublications, including this book, have been scaled in this manner.\\nPOWER-LAW (GAMMA) TRANSFORMATIONS\\nPower-law transformations have the form\\n \\nsc r\\n=\\ng\\n \\n(3-5)\\nwhere \\nc\\n and \\ng\\n are positive constants. Sometimes Eq. (3-5) is written as \\nsc r\\n=+\\n()\\ne\\ng\\n \\nto account for offsets (that is, a measurable output when the input is zero). However, \\noffsets typically are an issue of display calibration, and as a result they are normally \\nignored in Eq. (3-5). Figure 3.6 shows plots of \\ns\\n as a function of \\nr\\n for various values \\nof \\ng\\n.\\n As with log transformations, power-law curves with fractional values of \\ng\\n map', metadata={'source': 'imagepro.pdf', 'page': 126}),\n",
       " Document(page_content='g\\n map \\na narrow range of dark input values into a wider range of output values, with the \\nopposite being true for higher values of input levels. Note also in Fig. 3.6 that a fam-\\nily of transformations can be obtained simply by varying \\ng\\n.\\n Curves generated with \\nvalues of \\ng\\n>\\n1\\n have exactly the opposite effect as those generated with values of \\ng\\n<\\n1.\\n When \\nc\\n==\\ng\\n1 Eq. (3-5) reduces to the identity transformation.\\nT\\nhe response of many devices used for image capture, printing, and display obey \\na power law. By convention, the exponent in a power-law equation is referred to as \\ngamma\\n [hence our use of this symbol in Eq. (3-5)]. The process used to correct these \\npower-law response phenomena is called \\ngamma correction\\n or \\ngamma encoding\\n. \\nFor example, cathode ray tube (CRT) devices have an intensity-to-voltage response \\nthat is a power function, with exponents varying from approximately 1.8 to 2.5. As \\nthe curve for \\ng\\n=\\n25\\n.', metadata={'source': 'imagepro.pdf', 'page': 126}),\n",
       " Document(page_content='g\\n=\\n25\\n.\\n in Fig. 3.6 shows, such display systems would tend to produce \\ng\\n \\n/H11005\\n 0.04\\ng\\n/H11005\\n 0.10\\ng\\n/H11005\\n 0.20\\ng\\n/H11005\\n 0.40\\ng\\n/H11005\\n 0.67\\ng\\n/H11005\\n 1\\ng\\n/H11005\\n 1.5\\ng\\n/H11005\\n 2.5\\ng\\n/H11005\\n 5.0\\ng\\n/H11005\\n 10.0\\ng\\n/H11005\\n 25.0\\n0\\nL\\n/\\n4L\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n \\n1\\nInput intensity levels,\\n \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL\\n \\n/H11002\\n \\n1\\nOutput intensity levels, \\ns\\nFIGURE 3.6\\nPlots of the  \\ngamma equation \\nsc r\\n=\\ng\\n for various \\nvalues of \\ng\\n (\\nc\\n = 1 \\nin all cases).\\n Each \\ncurve was scaled  \\nindependently\\n so \\nthat all curves \\nwould ﬁt in the \\nsame graph. Our  \\ninterest here is \\non the \\nshapes\\n of \\nthe curves, not \\non their relative \\nvalues.\\nDIP4E_GLOBAL_Print_Ready.indb   125\\n6/16/2017   2:03:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 126}),\n",
       " Document(page_content='126\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nimages that are darker than intended. Figure 3.7 illustrates this effect. Figure 3.7(a) \\nis an image of an intensity ramp displayed in a monitor with a gamma of 2.5. As \\nexpected, the output of the monitor appears darker than the input, as Fig. 3.7(b) \\nshows.\\nIn this case, gamma correction consists of using the transformation \\nsr r\\n==\\n125 04\\n..\\n \\nto preprocess the image before inputting it into the monitor. Figure 3.7(c) is the result. \\nWhen input into the same monitor, the gamma-corrected image produces an output \\nthat is close in appearance to the original image, as Fig. 3.7(d) shows. A similar analysis \\nas above would apply to other imaging devices, such as scanners and printers, the dif-\\nference being the device-dependent value of gamma (Poynton [1996]).\\nEXAMPLE 3.1 : Contrast enhancement using power-law intensity transformations.', metadata={'source': 'imagepro.pdf', 'page': 127}),\n",
       " Document(page_content='In addition to gamma correction, power-law transformations are useful for general-purpose contrast \\nmanipulation. Figure 3.8(a) shows a magnetic resonance image (MRI) of a human upper thoracic spine \\nwith a fracture dislocation. The fracture is visible in the region highlighted by the circle. Because the \\nimage is predominantly dark, an expansion of intensity levels is desirable. This can be accomplished \\nusing a power-law transformation with a fractional exponent. The other images shown in the ﬁgure were \\nobtained by processing Fig. 3.8(a) with the power-law transformation function of Eq. (3-5). The values \\nSometimes, a higher \\ngamma makes the  \\ndisplayed image look \\nbetter to viewers than \\nthe original because of \\nan increase in contrast. \\nHowever, the objective \\nof gamma correction is to \\nproduce a \\nfaithful\\n display \\nof an input image.\\nb a\\nd c\\nFIGURE 3.7\\n(a) Intensity ramp \\nimage. (b) Image \\nas viewed on a \\nsimulated monitor \\nwith a gamma of \\n2.5. (c) Gamma- \\ncorrected image.', metadata={'source': 'imagepro.pdf', 'page': 127}),\n",
       " Document(page_content='corrected image. \\n(d) Corrected \\nimage as viewed \\non the same \\nmonitor. Compare \\n(d) and (a).\\nOriginal image as viewed on a monitor with\\na gamma of 2.5\\nOriginal image\\nGamma Correction\\nGamma-corrected image\\nGamma-corrected image as viewed on the\\nsame monitor\\nDIP4E_GLOBAL_Print_Ready.indb   126\\n6/16/2017   2:03:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 127}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n127\\nof gamma corresponding to images (b) through (d) are 0.6, 0.4, and 0.3, respectively (\\nc\\n=\\n1\\n in all cases). \\nObserve that as gamma decreased from 0.6 to 0.4,\\n more detail became visible. A further decrease of \\ngamma to 0.3 enhanced a little more detail in the background, but began to reduce contrast to the point \\nwhere the image started to have a very slight “washed-out” appearance, especially in the background. \\nThe best enhancement in terms of contrast and discernible detail was obtained with \\ng\\n=\\n04\\n..\\n A value of \\ng\\n=\\n03\\n.\\n is an approximate limit below which contrast in this particular image would be reduced to an \\nunacceptable level.\\nEXAMPLE 3.2 : Another illustration of power-law transformations.\\nFigure 3.9(a) shows the opposite problem of that presented in Fig. 3.8(a). The image to be processed \\nb a\\nd c\\nFIGURE 3.8\\n(a) Magnetic \\nresonance  \\nimage (MRI) of a \\nfractured human \\nspine (the region \\nof the fracture is', metadata={'source': 'imagepro.pdf', 'page': 128}),\n",
       " Document(page_content='of the fracture is \\nenclosed by the \\ncircle).  \\n(b)–(d) Results of  \\napplying the  \\ntransformation  \\nin Eq. (3-5) \\nwith \\nc\\n=\\n1 and \\ng\\n=\\n06\\n.,\\n 0.4, and \\n0.3,\\n respectively. \\n(Original image \\ncourtesy of Dr. \\nDavid R. Pickens, \\nDepartment of \\nRadiology and \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center.)\\nDIP4E_GLOBAL_Print_Ready.indb   127\\n6/16/2017   2:03:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 128}),\n",
       " Document(page_content='128\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nb a\\nd c\\nFIGURE 3.9\\n(a) Aerial image. \\n(b)–(d) Results \\nof applying the \\ntransformation \\nin Eq. (3-5) with \\ng\\n=\\n30\\n.,\\n 4.0, and \\n5.0, respectively. \\n(\\nc\\n=\\n1\\n in all cases.) \\n \\n(Original image \\ncourtesy of \\nN\\nASA.)\\nnow has a washed-out appearance, indicating that a compression of intensity levels is desirable. This can \\nbe accomplished with Eq. (3-5) using values of \\ng\\n greater than 1. The results of processing Fig. 3.9(a) with \\ng\\n=\\n30\\n.,\\n 4.0, and 5.0 are shown in Figs. 3.9(b) through (d), respectively. Suitable results were obtained \\nusing gamma values of 3.0 and 4.0.\\n The latter result has a slightly more appealing appearance because it \\nhas higher contrast. This is true also of the result obtained with \\ng\\n=\\n50\\n..\\n For example, the airport runways \\nnear the middle of the image appears clearer in F\\nig. 3.9(d) than in any of the other three images. \\nPIECEWISE LINEAR TRANSFORMATION FUNCTIONS', metadata={'source': 'imagepro.pdf', 'page': 129}),\n",
       " Document(page_content='An approach complementary to the methods discussed in the previous three sec-\\ntions is to use piecewise linear functions. The advantage of these functions over those \\ndiscussed thus far is that the form of piecewise functions can be arbitrarily complex. \\nIn fact, as you will see shortly, a practical implementation of some important trans-\\nformations can \\nbe formulated only as piecewise linear functions. The main disadvan-\\ntage of these functions is that their specification requires considerable user input.\\nDIP4E_GLOBAL_Print_Ready.indb   128\\n6/16/2017   2:03:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 129}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n129\\nContrast Stretching\\nLow-contrast images can result from poor illumination, lack of dynamic range in the \\nimaging sensor, or even the wrong setting of a lens aperture during image acquisi-\\ntion. \\nContrast stretching\\n expands the range of intensity levels in an image so that it \\nspans the ideal full intensity range of the recording medium or display device.\\nFigure 3.10(a) shows a typical transformation used for contrast stretching. The \\nlocations of points \\n(, )\\nrs\\n11\\n and \\n(, )\\nrs\\n22\\n control the shape of the transformation function. \\nIf \\nrs\\n11\\n=\\n and \\nrs\\n22\\n=\\n the transformation is a linear function that produces no changes \\nin intensity\\n. If \\nrr\\n12\\n=\\n, \\ns\\n1\\n0\\n=\\n,\\n and \\nsL\\n2\\n1\\n=−\\n the transformation becomes a \\nthreshold-\\ning function\\n that creates a binary image [see F\\nig. 3.2(b)]. Intermediate values of \\n(, )\\nrs\\n11\\n \\nand \\n(,)\\nsr\\n22\\n produce various degrees of spread in the intensity levels of the output', metadata={'source': 'imagepro.pdf', 'page': 130}),\n",
       " Document(page_content='image, thus affecting its contrast. In general, \\nrr\\n12\\n≤\\n and \\nss\\n12\\n≤\\n is assumed so that \\nthe function is single valued and monotonically increasing\\n. This preserves the order \\nof intensity levels, thus preventing the creation of intensity artifacts. Figure 3.10(b) \\nshows an 8-bit image with low contrast. Figure 3.10(c) shows the result of contrast \\nstretching, obtained by setting \\n(, ) ( ,)\\nmin\\nrs r\\n11\\n0\\n=\\n and \\n(, ) ( , ) ,\\nmax\\nrs r L\\n22\\n1\\n=−\\n where \\nr\\nmin\\n and \\nr\\nmax\\n denote the minimum and maximum intensity levels in the input image, \\n0\\nL\\n/\\n4\\nL\\n/\\n23\\nL\\n/\\n4\\nL \\n/H11002\\n 1\\nInput intensities, \\nr\\n0\\nL\\n/\\n4\\nL\\n/\\n2\\n3\\nL\\n/\\n4\\nL \\n/H11002\\n 1\\nOutput intensities, \\ns\\n(\\nr\\n2\\n, \\ns\\n2\\n)\\n(\\nr\\n1\\n, \\ns\\n1\\n)\\nT\\n(\\nr\\n)\\nb a\\nd c\\nFIGURE 3.10\\nContrast stretching.  \\n(a) Piecewise linear \\ntransformation \\nfunction. (b) A low-\\ncontrast electron \\nmicroscope image \\nof pollen, magniﬁed \\n700 times.  \\n(c) Result of  \\ncontrast stretching. \\n(d) Result of  \\nthresholding.  \\n(Original image \\ncourtesy of Dr.  \\nRoger Heady,', metadata={'source': 'imagepro.pdf', 'page': 130}),\n",
       " Document(page_content='Roger Heady, \\nResearch School of \\nBiological Sciences, \\nAustralian National \\nUniversity,  \\nCanberra,  \\nAustralia.)\\nDIP4E_GLOBAL_Print_Ready.indb   129\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 130}),\n",
       " Document(page_content='130\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nrespectively. The transformation stretched the intensity levels linearly to the full \\nintensity range, \\n[, ] .\\n01\\nL\\n−\\n Finally, Fig. 3.10(d) shows the result of using the thresh-\\nolding function,\\n with \\n(, ) ( ,)\\nrs\\nm\\n11\\n0\\n=\\n and \\n(, ) (, ) ,\\nrs\\nm L\\n22\\n1\\n=−\\n where \\nm\\n is the mean \\nintensity level in the image\\n. \\nIntensity-Level Slicing\\nThere are applications in which it is of interest to highlight a specific range of inten-\\nsities in an image. Some of these applications include enhancing features in satellite \\nimagery, such as masses of water, and enhancing flaws in X-ray images. The method, \\ncalled \\nintensity-level slicing\\n, can be implemented in several ways, but most are varia-\\ntions of two basic themes. One approach is to display in one value (say, white) all the \\nvalues in the range of interest and in another (say, black) all other intensities. This', metadata={'source': 'imagepro.pdf', 'page': 131}),\n",
       " Document(page_content='transformation, shown in Fig. 3.11(a), produces a binary image. The second approach, \\nbased on the transformation in Fig. 3.11(b), brightens (or darkens) the desired range \\nof intensities, but leaves all other intensity levels in the image unchanged.\\nEXAMPLE 3.3 : Intensity-level slicing.\\nFigure 3.12(a) is an aortic angiogram near the kidney area (see Section 1.3 for details on this image). The \\nobjective of this example is to use intensity-level slicing to enhance the major blood vessels that appear \\nlighter than the background, as a result of an injected contrast medium. Figure 3.12(b) shows the result \\nof using a transformation of the form in Fig. 3.11(a). The selected band was near the top of the intensity \\nscale because the range of interest is brighter than the background. The net result of this transformation \\nis that the blood vessel and parts of the kidneys appear white, while all other intensities are black. This', metadata={'source': 'imagepro.pdf', 'page': 131}),\n",
       " Document(page_content='type of enhancement produces a binary image, and is useful for studying the shape characteristics of the \\nﬂow of the contrast medium (to detect blockages, for example).\\nIf interest lies in the actual intensity values of the region of interest, we can use the transformation of \\nthe form shown in Fig. 3.11(b). Figure 3.12(c) shows the result of using such a transformation in which \\na band of intensities in the mid-gray region around the mean intensity was set to black, while all other \\nintensities were left unchanged. Here, we see that the gray-level tonality of the major blood vessels and \\npart of the kidney area were left intact. Such a result might be useful when interest lies in measuring the \\nactual ﬂow of the contrast medium as a function of time in a sequence of images.\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\n0\\nAB\\n L \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\ns \\ns \\nr \\nr \\nL \\n/H11002\\n 1\\n0\\nAB\\nL \\n/H11002\\n 1 \\nb a\\nFIGURE 3.11\\n(a) This transfor-\\nmation function \\nhighlights range \\n[,]\\nAB\\n and reduces \\nall other intensities', metadata={'source': 'imagepro.pdf', 'page': 131}),\n",
       " Document(page_content='to a lower level.\\n \\n(b) This function \\nhighlights range \\n[,]\\nAB\\n and leaves \\nother intensities \\nunchanged.\\nDIP4E_GLOBAL_Print_Ready.indb   130\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 131}),\n",
       " Document(page_content='3.2\\n  \\nSome Basic Intensity Transformation Functions\\n    \\n131\\nBit-Plane Slicing\\nPixel values are integers composed of bits. For example, values in a 256-level gray-\\nscale image are composed of 8 bits (one byte). Instead of highlighting intensity-level \\nranges, as 3.3, we could highlight the contribution made to total image appearance \\nby specific bits. As Fig. 3.13 illustrates, an 8-bit image may be considered as being \\ncomposed of eight one-bit planes, with plane 1 containing the lowest-order bit of all \\npixels in the image, and plane 8 all the highest-order bits.\\nFigure 3.14(a) shows an 8-bit grayscale image and Figs. 3.14(b) through (i) are \\nits eight one-bit planes, with Fig. 3.14(b) corresponding to the highest-order bit. \\nObserve that the four higher-order bit planes, especially the ﬁrst two, contain a sig-\\nniﬁcant amount of the visually-signiﬁcant data. The lower-order planes contribute \\nto more  subtle  intensity details  in the image. The original  image has a gray border', metadata={'source': 'imagepro.pdf', 'page': 132}),\n",
       " Document(page_content='whose intensity is 194. Notice that the corresponding borders of some of the bit \\nb a\\nc\\nFIGURE 3.12\\n  (a) Aortic angiogram. (b) Result of using a slicing transformation of the type illustrated in Fig. 3.11(a), \\nwith the range of intensities of interest selected in the upper end of the gray scale. (c) Result of using the transfor-\\nmation in Fig. 3.11(b), with the selected range set near black, so that the grays in the area of the blood vessels and \\nkidneys were preserved. (Original image courtesy of Dr. Thomas R. Gest, University of Michigan Medical School.) \\nOne 8-bit byte\\nBit plane 8\\n(most significant)\\nBit plane 1\\n(least significant)\\nFIGURE 3.13\\nBit-planes of an \\n8-bit image.\\nDIP4E_GLOBAL_Print_Ready.indb   131\\n6/16/2017   2:03:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 132}),\n",
       " Document(page_content='132\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nplanes are black (0), while others are white (1). To see why, consider a pixel in, say, \\nthe middle  of the lower border of Fig. 3.14(a). The corresponding pixels in the bit \\nplanes, starting with the highest-order plane, have values 1 1 0 0 0 0 1 0, which is the \\nbinary representation of decimal 194. The value of any pixel in the original image  \\ncan be similarly reconstructed from its corresponding binary-valued pixels in the bit \\nplanes by converting an 8-bit binary sequence to decimal.\\nThe binary image for the 8th bit plane of an 8-bit image can be obtained by thresh-\\nolding the input image with a transformation function that maps to 0 intensity values \\nbetween 0 and 127, and maps to 1 values between 128 and 255. The binary image in \\nFig. 3.14(b) was obtained in this manner. It is left as an exercise (see Problem 3.3) to \\nobtain the transformation functions for generating the other bit planes.', metadata={'source': 'imagepro.pdf', 'page': 133}),\n",
       " Document(page_content='Decomposing an image into its bit planes is useful for analyzing the relative \\nimportance of each bit in the image, a process that aids in determining the adequacy \\nof the number of bits used to quantize the image. Also, this type of decomposition \\nis useful for image compression (the topic of Chapter 8), in which fewer than all \\nplanes are used in reconstructing an image. For example, Fig. 3.15(a) shows an image \\nreconstructed using bit planes 8 and 7 of the preceding decomposition. The recon-\\nstruction is done by multiplying the pixels of the \\nn\\nth plane by the constant \\n2\\n1\\nn\\n−\\n.\\n This \\nconverts the \\nn\\nth signiﬁcant binary bit to decimal.\\n Each bit plane is multiplied by the \\ncorresponding constant, and all resulting planes are added to obtain the grayscale \\nimage. Thus, to obtain Fig. 3.15(a), we multiplied bit plane 8 by 128, bit plane 7 by 64, \\nand added the two planes. Although the main features of the original image were', metadata={'source': 'imagepro.pdf', 'page': 133}),\n",
       " Document(page_content='restored, the reconstructed image appears ﬂat, especially in the background. This \\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 3.14\\n (a) An 8-bit gray-scale image of size \\n550 1192\\n×\\n pixels. (b) through (i) Bit planes 8 through 1, with bit \\nplane 1 corresponding to the least signiﬁcant bit.\\n Each bit plane is a binary image..\\nDIP4E_GLOBAL_Print_Ready.indb   132\\n6/16/2017   2:03:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 133}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n133\\nis not surprising, because two planes can produce only four distinct intensity  lev-\\nels. Adding plane 6 to the reconstruction helped the situation, as Fig. 3.15(b) shows. \\nNote that the background of this image has perceptible false contouring. This effect \\nis reduced signiﬁcantly by adding the 5th plane to the reconstruction, as Fig. 3.15(c) \\nillustrates. Using more planes in the reconstruction would not contribute signiﬁcant-\\nly to the appearance of this image. Thus, we conclude that, in this example, storing \\nthe four highest-order bit planes would allow us to reconstruct the original image \\nin acceptable detail. Storing these four planes instead of the original image requires \\n50% less storage.\\n3.3 HISTOGRAM PROCESSING \\nLet \\nr\\nk\\n,\\n for\\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n denote the intensities of an \\nL\\n-level digital image\\n, \\nfx y\\n(,\\n)\\n. The \\nunnormaliz\\ned histogram\\n of \\nf\\n is defined as\\n \\nhr n k L\\nkk\\n()\\n, , , ,\\n== −\\nf o r 012 1\\n…\\n \\n(3-6)\\nwhere \\nn\\nk', metadata={'source': 'imagepro.pdf', 'page': 134}),\n",
       " Document(page_content='(3-6)\\nwhere \\nn\\nk\\n is the number of pixels in \\nf\\n with intensity \\nr\\nk\\n,\\n and the subdivisions of the \\nintensity scale are called \\nhistogram bins\\n.\\n Similarly, the \\nnormalized histogram\\n of \\nf\\n is \\ndefined as\\n \\npr\\nhr\\nMN\\nn\\nMN\\nk\\nkk\\n()\\n()\\n==\\n \\n(3-7)\\nwhere, as usual, \\nM\\n and \\nN\\n are the number of image rows and columns\\n, respectively. \\nMostly, we work with normalized histograms, which we refer to simply as \\nhistograms \\nor \\nimage histograms\\n. The sum of \\npr\\nk\\n()\\n for all values of \\nk\\n is always 1.\\n The components \\nof \\npr\\nk\\n()\\n are estimates of the probabilities of intensity levels occurring in an image. \\nAs you will learn in this section,\\n histogram manipulation is a fundamental tool in \\nimage processing. Histograms are simple to compute and are also suitable for fast \\nhardware implementations, thus making histogram-based techniques a popular tool \\nfor real-time image processing. \\nHistogram shape is related to image appearance. For example, Fig. 3.16 shows', metadata={'source': 'imagepro.pdf', 'page': 134}),\n",
       " Document(page_content='images with four basic intensity characteristics: dark, light, low contrast, and high \\ncontrast; the image histograms are also shown. We note in the dark image that the \\nmost populated histogram bins are concentrated on the lower (dark) end of the \\nintensity scale. Similarly, the most populated bins of the light image are biased \\ntoward the higher end of the scale. An image with low contrast has a narrow histo-\\n3.3\\nb a\\nc\\nFIGURE 3.15\\n Image  reconstructed from bit planes: (a) 8 and 7; (b) 8, 7, and 6; (c) 8, 7, 6, and 5.\\nDIP4E_GLOBAL_Print_Ready.indb   133\\n6/16/2017   2:03:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 134}),\n",
       " Document(page_content='134\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\ngram located typically toward the middle of the intensity scale, as Fig. 3.16(c) shows. \\nFor a monochrome image, this implies a dull, washed-out gray look. Finally, we see \\nthat the components of the histogram of the high-contrast image cover a wide range \\nof the intensity scale, and the distribution of pixels is not too far from uniform, with \\nfew bins being much higher than the others. Intuitively, it is reasonable to conclude \\nthat an image whose pixels tend to occupy the entire range of possible intensity lev-\\nels and, in addition, tend to be distributed uniformly, will have an appearance of high \\ncontrast and will exhibit a large variety of gray tones. The net effect will be an image \\nthat shows a great deal of gray-level detail and has a high dynamic range. As you will \\nsee shortly, it is possible to develop a transformation function that can achieve this', metadata={'source': 'imagepro.pdf', 'page': 135}),\n",
       " Document(page_content='effect automatically, using only the histogram of an input image.\\nHISTOGRAM EQUALIZATION\\nAssuming initially continuous intensity values, let the variable \\nr\\n denote the intensi-\\nties of an image to be processed. As usual, we assume that \\nr\\n is in the range \\n[, ] ,\\n01\\nL\\n−\\n \\nwith \\nr\\n=\\n0\\n representing black and \\nrL\\n=−\\n1\\n representing white. For \\nr\\n satisfying these \\nconditions\\n, we focus attention on transformations (intensity mappings) of the form\\n \\nsT r rL\\n=−\\n()\\n0 1\\n≤≤\\n \\n(3-8)\\nHistogram of \\nhigh-contrast image\\nHistogram of \\nlow-contrast image\\nHistogram of \\ndark image\\nHistogram of \\nlight image\\nb\\na\\nc\\nd\\nFIGURE 3.16\\n Four image types and their corresponding histograms. (a) dark; (b) light; (c) low contrast; (d) high con-\\ntrast. The horizontal axis of the histograms are values of \\nr\\nk\\n and the vertical axis are values of \\np\\nr\\nk\\n() .\\nDIP4E_GLOBAL_Print_Ready.indb   134\\n6/16/2017   2:03:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 135}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n135\\nthat produce an output intensity value, \\ns\\n, for a given intensity value \\nr\\n in the input \\nimage. We assume that \\n(a) \\nTr\\n()\\n is a monotonic\\n†\\n increasing function in the interval \\n01\\n≤≤\\nrL\\n−\\n; and\\n(b) \\n01\\n≤≤\\nTr\\nL\\n()\\n−\\n for \\n01\\n≤≤\\nrL\\n−\\n.\\nIn some formulations to be discussed shortly, we use the inverse transformation\\n \\nrT s sL\\n=−\\n−\\n1\\n01\\n()\\n≤≤\\n \\n(3-9)\\nin which case we change condition (a) to:\\n(a\\n/H11032\\n)\\n \\nTr\\n()\\n is a \\nstrictly\\n monotonic increasing function in the interval \\n01\\n≤≤\\nrL\\n−\\n.\\nThe condition in (a) that \\nTr\\n()\\n be monotonically increasing guarantees that output \\nintensity values will never be less than corresponding input values\\n, thus preventing \\nartifacts created by reversals of intensity. Condition (b) guarantees that the range of \\noutput intensities is the same as the input. Finally, condition \\n(a )\\n/H11032\\n guarantees that the \\nmappings from \\ns\\n back to \\nr\\n will be one-to-one\\n, thus preventing ambiguities.', metadata={'source': 'imagepro.pdf', 'page': 136}),\n",
       " Document(page_content='Figure 3.17(a) shows a function that satisﬁes conditions (a) and (b). Here, we see \\nthat it is possible for multiple input values to map to a single output value and still \\nsatisfy these two conditions. That is, a monotonic transformation function performs \\na one-to-one or many-to-one mapping. This is perfectly ﬁne when mapping from \\nr\\n \\nto \\ns\\n. However, Fig. 3.17(a) presents a problem if we wanted to recover the values of \\nr\\n uniquely from the mapped values (inverse mapping can be visualized by revers-\\ning the direction of the arrows). This would be possible for the inverse mapping \\nof \\ns\\nk\\n in Fig. 3.17(a), but the inverse mapping of \\ns\\nq\\n is a range of values, which, of \\ncourse, prevents us in general from recovering the original value of \\nr\\n that resulted \\n†\\n A function \\nTr\\n()\\n is a \\nmonotonic increasing\\n function if \\nTr Tr\\n()\\n()\\n21\\n≥\\n for \\nrr\\n21\\n>\\n. \\nTr\\n()\\n is a \\nstrictly monotonic increas-\\ning\\n function if \\nTr Tr\\n()\\n()\\n21\\n>\\n for \\nrr\\n21\\n>', metadata={'source': 'imagepro.pdf', 'page': 136}),\n",
       " Document(page_content='21\\n>\\n for \\nrr\\n21\\n>\\n. Similar deﬁnitions apply to a monotonic decreasing function.\\nSingle\\nvalue,\\n s\\nk\\nr\\nk\\ns\\nk\\nSingle\\nvalue,\\n s\\nq\\nSingle\\nvalue\\nMultiple\\nvalues\\nr \\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\nT\\n(\\nr\\n)\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\nr \\nT\\n(\\nr\\n)\\n. . .\\nb a\\nFIGURE 3.17\\n(a) Monotonic  \\nincreasing function, \\nshowing how  \\nmultiple values can \\nmap to a single  \\nvalue. (b) Strictly  \\nmonotonic increas-\\ning function. This is \\na one-to-one map-\\nping, both ways.\\nDIP4E_GLOBAL_Print_Ready.indb   135\\n6/16/2017   2:03:14 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 136}),\n",
       " Document(page_content='136\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nin \\ns\\nq\\n.\\n As Fig. 3.17(b) shows, requiring that \\nTr\\n()\\n be strictly monotonic guarantees \\nthat the inverse mappings will be \\nsingle valued\\n (i.e\\n., the mapping is one-to-one in \\nboth directions).This is \\na theoretical requirement that will allow us to derive some \\nimportant histogram processing techniques later in this chapter. Because images are \\nstored using integer intensity values, we are forced to round all results to their near-\\nest integer values. This often results in strict monotonicity not being satisﬁed, which \\nimplies inverse transformations that may not be unique\\n. Fortunately, this problem is \\nnot difﬁcult to handle in the discrete case, as Example 3.7 in this section illustrates.\\nThe intensity of an image may be viewed as a random variable in the interval \\n[, ] .\\n01\\nL\\n−\\n Let \\npr\\nr\\n()\\n and \\nps\\ns\\n()\\n denote the PDFs of intensity values \\nr\\n and \\ns\\n in two dif-\\nferent images\\n. The subscripts on \\np', metadata={'source': 'imagepro.pdf', 'page': 137}),\n",
       " Document(page_content='p\\n indicate that \\np\\nr\\n and \\np\\ns\\n are different functions. A \\nfundamental result from probability theory is that if \\npr\\nr\\n()\\n and \\nTr\\n()\\n are known, and \\nTr\\n()\\n is continuous and differentiable over the range of values of interest, then the \\nPDF of the transformed (mapped) variable \\ns\\n can be obtained as\\n \\nps pr\\ndr\\nds\\nsr\\n() ()\\n=\\n \\n(3-10)\\nThus, we see that the PDF of the output intensity variable, \\ns\\n,\\n is determined by the \\nPDF of the input intensities and the transformation function used [recall that \\nr\\n and \\ns\\n are related by \\nTr\\n()\\n]. \\nA transformation function of particular importance in image processing is\\n \\nsT r L p d\\nr\\nr\\n== −\\n() ( ) ( )\\n1\\n0\\n2\\nww\\n \\n(3-11)\\nwhere \\nw\\n is a dummy variable of integration. The integral on the right side is the \\ncumulative distribution function\\n (CDF) of random variable \\nr\\n.\\n Because PDFs always \\nare positive, and the integral of a function is the area under the function, it follows', metadata={'source': 'imagepro.pdf', 'page': 137}),\n",
       " Document(page_content='that the transformation function of Eq. (3-11) satisfies condition (a). This is because \\nthe area under the function cannot decrease as \\nr\\n increases. When the upper limit in \\nthis equation is \\nrL\\n=−\\n()\\n1\\n the integral evaluates to 1, as it must for a PDF.  Thus, the \\nmaximum value of \\ns\\n is \\nL\\n−\\n1,\\n and condition (b) is satisfied also.\\nW\\ne use Eq. (3-10) to ﬁnd the \\nps\\ns\\n()\\n corresponding to the transformation just dis-\\ncussed.\\n We know from Leibniz’s rule in calculus that the derivative of a deﬁnite \\nintegral with respect to its upper limit is the integrand evaluated at the limit. That is,\\n \\nds\\ndr\\ndT r\\ndr\\nL\\nd\\ndr\\npd\\nLp r\\nr\\nr\\nr\\n=\\n=−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=−\\n()\\n() ( )\\n() ( )\\n1\\n1\\n0\\n2\\nww\\n  \\n(3-12)\\nDIP4E_GLOBAL_Print_Ready.indb   136\\n6/16/2017   2:03:15 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 137}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n137\\nSubstituting this result for \\ndr ds\\n in Eq. (3-10), and noting that all probability values \\nare positive\\n, gives the result\\n \\nps pr\\ndr\\nds\\npr\\nLp r\\nL\\nsL\\nsr\\nr\\nr\\n() ()\\n()\\n() ( )\\n=\\n=\\n−\\n=\\n−\\n−\\n1\\n1\\n1\\n1\\n01\\n≤≤\\n  \\n(3-13)\\nWe recognize the form of \\nps\\ns\\n()\\n in the last line of this equation as a \\nuniform \\nprob-\\nability density function.\\n Thus, performing the intensity transformation in Eq. (3-11) \\nyields a random variable, \\ns\\n, characterized by a uniform PDF. What is important is \\nthat \\nps\\ns\\n()\\n in Eq. (3-13) will \\nalwa\\nys\\n be uniform, \\nindependently\\n of the form of \\npr\\nr\\n() .\\n \\nF\\nigure 3.18 and the following example illustrate these concepts.\\nEXAMPLE 3.4 : Illustration of Eqs. (3-11) and (3-13).\\nSuppose that the (continuous) intensity values in an image have the PDF\\n \\npr\\nr\\nL\\nrL\\nr\\n()\\n()\\n=\\n−\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n2\\n1\\n01\\n0\\n2\\nfor \\notherwise\\n≤≤\\nFrom Eq. (3-11) \\n \\nsT r L p d\\nL\\nd\\nr\\nL\\nrr\\nr\\n== − =\\n−\\n=\\n−\\n() ( ) ( )\\n1\\n2\\n11\\n00\\n2\\n22\\nww w w\\nEq. (3-11)\\nr \\np\\nr\\n(\\nr\\n)\\n0\\nA\\nL \\n/H11002\\n 1\\ns', metadata={'source': 'imagepro.pdf', 'page': 138}),\n",
       " Document(page_content='A\\nL \\n/H11002\\n 1\\ns \\np\\ns\\n(\\ns\\n)\\n0\\nL \\n/H11002\\n 1\\nL \\n/H11002\\n 1\\n1\\nb a\\nFIGURE 3.18\\n  (a) An arbitrary PDF. (b) Result of applying Eq. (3-11) to the input PDF. The \\nresulting PDF is always uniform, independently of the shape of the input.\\nDIP4E_GLOBAL_Print_Ready.indb   137\\n6/16/2017   2:03:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 138}),\n",
       " Document(page_content='138\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nSuppose that we form a new image with intensities, \\ns\\n, obtained using this transformation; that is, the \\ns\\n \\nvalues are formed by squaring the corresponding intensity values of the input image, then dividing them \\nby \\nL\\n−\\n1.\\n We can verify that the PDF of the intensities in the new image, \\nps\\ns\\n() ,\\n is uniform by substituting \\npr\\nr\\n()\\n into Eq. (3-13), and using the fact that \\nsr L\\n=−\\n2\\n1\\n() ;\\n that is,\\n \\nps pr\\ndr\\nds\\nr\\nL\\nds\\ndr\\nr\\nL\\nd\\ndr\\nr\\nL\\nsr\\n() ()\\n()\\n()\\n==\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n−\\n2\\n1\\n2\\n1\\n1\\n2\\n1\\n2\\n2\\n⎥ ⎥\\n=\\n−\\n−\\n=\\n−\\n−\\n1\\n2\\n2\\n1\\n1\\n2\\n1\\n1\\nr\\nL\\nL\\nrL\\n()\\n()\\nThe last step follows because \\nr\\n is nonnegative and \\nL\\n>\\n1.\\n As expected, the result is a uniform PDF.\\nF\\nor discrete values, we work with probabilities and summations instead of prob-\\nability density functions and integrals (but the requirement of monotonicity stated \\nearlier still applies). Recall that the probability of occurrence of intensity level \\nr\\nk\\n in', metadata={'source': 'imagepro.pdf', 'page': 139}),\n",
       " Document(page_content='r\\nk\\n in \\na digital image is approximated by\\n \\npr\\nn\\nMN\\nrk\\nk\\n()\\n=\\n \\n(3-14)\\nwhere \\nMN\\n is the total number of pixels in the image\\n, and \\nn\\nk\\n denotes the number of \\npixels that have intensity \\nr\\nk\\n.\\n As noted in the beginning of this section, \\npr\\nrk\\n() ,\\n with \\nrL\\nk\\n∈−\\n[, ] ,\\n01\\n is commonly referred to as a normalized image histogram.\\nT\\nhe discrete form of the transformation in Eq. (3-11) is\\n \\nsT r L p r k L\\nkk\\nr j\\nj\\nk\\n== − = −\\n=\\n∑\\n() ( ) () , , , ,\\n1 012 1\\n0\\n…\\n \\n(3-15)\\nwhere, as before, \\nL\\n is the number of possible intensity levels in the image (e\\n.g., 256 \\nfor an 8-bit image). Thu\\ns, a processed (output) image is obtained by using Eq. (3-15) \\nto map each pixel in the input image with intensity \\nr\\nk\\n into a corresponding pixel with \\nlevel \\ns\\nk\\n in the output image, This is called a \\nhistogram equalization\\n or \\nhistogram \\nlinearization\\n transformation. It is not difficult to show (see Problem 3.9) that this', metadata={'source': 'imagepro.pdf', 'page': 139}),\n",
       " Document(page_content='transformation satisfies conditions (a) and (b) stated previously in this section.\\nEXAMPLE 3.5 : Illustration of the mechanics of histogram equalization.\\nIt will be helpful to work through a simple example. Suppose that a 3-bit image \\n()\\nL\\n=\\n8\\n of size \\n64 64\\n×\\n \\npixels \\n()\\nMN\\n=\\n4096\\n has the intensity distribution in Table 3.1, where the intensity levels are integers in \\nthe range \\n[, ] [, ] .\\n01\\n0 7\\nL\\n−=\\n The histogram of this image is sketched in Fig. 3.19(a).Values of the histo-\\ngram equalization transformation function are obtained using Eq.\\n (3-15). For instance,\\n \\nsT r p r p r\\nrj r\\nj\\n00 0\\n0\\n0\\n77 1 3 3\\n== = =\\n=\\n∑\\n() () () .\\nDIP4E_GLOBAL_Print_Ready.indb   138\\n6/16/2017   2:03:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 139}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n139\\nSimilarly, \\nsT r\\n11\\n30 8\\n==\\n() . ,\\n \\ns\\n2\\n45 5\\n=\\n.,\\n \\ns\\n3\\n56 7\\n=\\n.,\\n \\ns\\n4\\n62 3\\n=\\n.,\\n \\ns\\n5\\n66 5\\n=\\n.,\\n \\ns\\n6\\n68 6\\n=\\n.,\\n and \\ns\\n7\\n70 0\\n=\\n..\\n This trans-\\nformation function has the staircase shape shown in F\\nig. 3.19(b).\\nAt this point, the \\ns\\n values are fractional because they were generated by summing probability values, \\nso we round them to their nearest integer values in the range \\n[, ] :\\n07\\n \\nssss\\nss\\ns\\n02 4 6\\n13 5\\n13 3 1 45 5 5 62 3 6 68 6 7\\n30 8 3 56 7 6 6\\n=→ =→ =→ =→\\n=→ =→ =\\n.. . .\\n... ..\\n65 7 7 00 7\\n7\\n→= →\\ns\\nThese are the values of the equalized histogram. Observe that the transformation yielded only ﬁve \\ndistinct intensity levels\\n. Because \\nr\\n0\\n0\\n=\\n was mapped to \\ns\\n0\\n1\\n=\\n,\\n there are 790 pixels in the histogram \\nequalized image with this value (see \\nTable 3.1). Also, there are 1023 pixels with a value of \\ns\\n1\\n3\\n=\\n and 850 \\npixels with a value of \\ns\\n2\\n5\\n=\\n.\\n However, both \\nr\\n3\\n and \\nr\\n4\\n were mapped to the same value, 6, so there are \\n()\\n656', metadata={'source': 'imagepro.pdf', 'page': 140}),\n",
       " Document(page_content='()\\n656\\n329 985\\n+=\\n pixels in the equalized image with this value. Similarly, there are \\n()\\n245\\n122 81 448\\n++=\\n \\npixels with a value of 7 in the histogram equalized image\\n. Dividing these numbers by \\nMN\\n=\\n4096\\n yield-\\ned the equalized histogram in F\\nig. 3.19(c).\\nBecause a histogram is an approximation to a PDF, and no new allowed intensity levels are created \\nin the process, perfectly ﬂat histograms are rare in practical applications of histogram equalization using \\nthe method just discussed. Thus, unlike its continuous counterpart, it cannot be proved in general that \\ndiscrete histogram equalization using Eq. (3-15) results in a uniform histogram (we will introduce later in \\nr\\nk\\nn\\nk\\npr n M N\\nrk k\\n()\\n=\\nr\\n0\\n0\\n=\\n790\\n0.19\\nr\\n1\\n1\\n=\\n1023\\n0.25\\nr\\n2\\n2\\n=\\n850\\n0.21\\nr\\n3\\n3\\n=\\n656\\n0.16\\nr\\n4\\n4\\n=\\n329\\n0.08\\nr\\n5\\n5\\n=\\n245\\n0.06\\nr\\n6\\n6\\n=\\n122\\n0.03\\nr\\n7\\n7\\n=\\n81\\n0.02\\nTABLE \\n3.1\\nIntensity  \\ndistribution and \\nhistogram values \\nfor a 3-bit, \\n64 64\\n×\\n \\ndigital image\\n.\\nr\\nk\\n \\np\\nr\\n(\\nr\\nk\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n1.4\\n2.8\\n4.2\\n7.0', metadata={'source': 'imagepro.pdf', 'page': 140}),\n",
       " Document(page_content='.25\\n1.4\\n2.8\\n4.2\\n7.0\\n5.6\\n.05\\n.10\\n.15\\n.25\\n.20\\n01234567\\ns\\nk\\n \\np\\ns\\n(\\ns\\nk\\n)\\n01234567\\nr\\nk\\n \\ns\\nk\\n01234567\\nT\\n(\\nr\\n)\\nb a\\nc\\nFIGURE 3.19\\nHistogram  \\nequalization.  \\n(a) Original  \\nhistogram.  \\n(b) Transformation \\nfunction.  \\n(c) Equalized  \\nhistogram.\\nDIP4E_GLOBAL_Print_Ready.indb   139\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 140}),\n",
       " Document(page_content='140\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nthis section an approach for removing this limitation). However, as you will see shortly, using Eq. (3-15) \\nhas the general tendency to spread the histogram of the input image so that the intensity levels of the \\nequalized image span a wider range of the intensity scale. The net result is contrast enhancement.\\nWe discussed earlier the advantages of having intensity values that span the entire \\ngray scale. The method just derived produces intensities that have this tendency, and \\nalso has the advantage that it is fully automatic. In other words, the process of his-\\ntogram equalization consists entirely of implementing Eq. (3-15), which is based on \\ninformation that can be extracted directly from a given image, without the need for \\nany parameter speciﬁcations. This automatic, “hands-off” characteristic is important.\\nThe inverse transformation from \\ns\\n back to \\nr\\n is denoted by\\n \\nrT s\\nkk\\n=\\n−\\n1\\n()\\n  \\n(3-16)', metadata={'source': 'imagepro.pdf', 'page': 141}),\n",
       " Document(page_content='=\\n−\\n1\\n()\\n  \\n(3-16)\\nIt can be shown (see Problem 3.9) that this inverse transformation satisfies conditions \\n(a\\n/H11032\\n) and (b) defined earlier \\nonly\\n if \\nall\\n intensity levels are present in the input image\\n. \\nThis implies that none of the bins of the image histogram are empty. Although the \\ninverse transformation is not used in histogram equalization, it plays a central role \\nin the histogram-matching scheme developed after the following example.\\nEXAMPLE 3.6 : Histogram equalization.\\nThe left column in Fig. 3.20 shows the four images from Fig. 3.16, and the center column shows the result \\nof performing histogram equalization on each of these images. The ﬁrst three results from top to bottom \\nshow signiﬁcant improvement. As expected, histogram equalization did not have much effect on the \\nfourth image because its intensities span almost the full scale already. Figure 3.21 shows the transforma-', metadata={'source': 'imagepro.pdf', 'page': 141}),\n",
       " Document(page_content='tion functions used to generate the equalized images in Fig. 3.20. These functions were generated using \\nEq. (3-15). Observe that transformation (4) is nearly linear, indicating that the inputs were mapped to \\nnearly equal outputs. Shown is the mapping of an input value \\nr\\nk\\n to a corresponding output value \\ns\\nk\\n.\\n In \\nthis case\\n, the mapping was for image 1 (on the top left of Fig. 3.21), and indicates that a dark value was \\nmapped to a much lighter one, thus contributing to the brightness of the output image.\\nThe third column in Fig. 3.20 shows the histograms of the equalized images. While all the histograms \\nare different, the histogram-equalized images themselves are visually very similar. This is not totally \\nunexpected because the basic difference between the images on the left column is one of contrast, not \\ncontent. Because the images have the same content, the increase in contrast resulting from histogram', metadata={'source': 'imagepro.pdf', 'page': 141}),\n",
       " Document(page_content='equalization was enough to render any intensity differences between the equalized images visually \\nindistinguishable. Given the signiﬁcant range of contrast differences in the original images, this example \\nillustrates the power of histogram equalization as an adaptive, autonomous contrast-enhancement tool.\\nHISTOGRAM MATCHING (SPECIFICATION)\\nAs explained in the last section, histogram equalization produces a transformation \\nfunction that seeks to generate an output image with a uniform histogram. When \\nautomatic enhancement is desired, this is a good approach to consider because the \\nDIP4E_GLOBAL_Print_Ready.indb   140\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 141}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n141\\nFIGURE 3.20\\n Left column: Images from Fig. 3.16. Center column: Corresponding histogram-equalized images. Right \\ncolumn: histograms of the images in the center column (compare with the histograms in Fig. 3.16). \\nDIP4E_GLOBAL_Print_Ready.indb   141\\n6/16/2017   2:03:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 142}),\n",
       " Document(page_content='142\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192 255\\n(2)\\n(1)\\n(3)\\n(4)\\nIntensity values\\n \\nof original images\\nr\\nIntensity values of histogram-equalized images\\ns\\nr\\nk\\ns\\nk\\nFIGURE 3.21\\nTransformation \\nfunctions for histo-\\ngram equalization. \\nTransformations (1) \\nthrough (4) were \\nobtained using  \\nEq. (3-15) and the  \\nhistograms of the \\nimages on the left \\ncolumn of Fig. 3.20. \\nMapping of one \\nintensity value \\nr\\nk\\n in \\nimage 1 to its cor-\\nresponding value \\ns\\nk\\n \\nis shown.\\nresults from this technique are predictable and the method is simple to implement. \\nHowever, there are applications in which histogram equalization is not suitable. In \\nparticular, it is useful sometimes to be able to specify the shape of the histogram that \\nwe wish the processed image to have. The method used to generate images that have \\na specified histogram is called \\nhistogram matching\\n or \\nhistogram\\n \\nspecification\\n.\\nConsider for a moment continuous intensities \\nr', metadata={'source': 'imagepro.pdf', 'page': 143}),\n",
       " Document(page_content='r\\n and \\nz \\nwhich, as before, we treat \\nas random variables with PDFs \\npr\\nr\\n()\\n and \\npz\\nz\\n() ,\\n respectively. Here, \\nr\\n and \\nz\\n denote \\nthe intensity levels of the input and output (processed) images\\n, respectively. We can \\nestimate \\npr\\nr\\n()\\n from the given input image, and \\npz\\nz\\n()\\n is the \\nspeciﬁed\\n PDF that we \\nwish the output image to have\\n.\\nLet \\ns\\n be a random variable with the property\\n \\nsT r L p d\\nr\\nr\\n== −\\n() ( ) ( )\\n1\\n0\\n2\\nww\\n \\n(3-17)\\nwhere \\nw\\n is dummy variable of integration. This is the same as Eq. (3-11), which we \\nrepeat here for convenience\\n.\\nDeﬁne a function \\nG\\n on variable \\nz\\n with the property\\n \\nGz L p d\\nz\\nz\\n() ( ) ()\\n=−\\n1\\n0\\n2\\nvv = s\\n \\n(3-18)\\nwhere \\nv\\n is a dummy variable of integration. It follows from the preceding two equa-\\ntions that \\nGz s Tr\\n()\\n()\\n==\\n and, therefore, that \\nz\\n must satisfy the condition\\n \\nzG s G T r\\n==\\n[]\\n−−\\n11\\n() ()\\n \\n(3-19)\\nDIP4E_GLOBAL_Print_Ready.indb   142\\n6/16/2017   2:03:21 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 143}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n143\\nThe transformation function \\nTr\\n()\\n can be obtained using Eq. (3-17) after \\npr\\nr\\n()\\n has  \\nbeen  estimated using the input image\\n. Similarly, function \\nGz\\n()\\n can be obtained from \\nEq.\\n (3-18) because \\npz\\nz\\n()\\n is given.\\nEquations (3-17) through (3-19) imply that an image whose intensity levels have \\na speciﬁed PDF can be obtained using the following procedure:\\n1. \\nObtain \\npr\\nr\\n()\\n \\nfrom the input image to use in Eq.\\n (3-17).\\n2. \\nUse the speciﬁed PDF, \\npz\\nz\\n() ,\\n in Eq. (3-18) to obtain the function \\nGz\\n()\\n.\\n3. \\nCompute the inverse transformation \\nzGs\\n=\\n−\\n1\\n() ;\\n this is a mapping from \\ns\\n to \\nz\\n, \\nthe latter being the values that have the speciﬁed PDF\\n.\\n4. \\nObtain the output image by ﬁrst equalizing the input image using Eq. (3-17); the \\npixel values in this image are the \\ns\\n values\\n. For each pixel with value \\ns\\n in the equal-\\nized image, perform the inverse mapping \\nzGs\\n=\\n−\\n1\\n()\\n to obtain the corresponding \\npixel in the output image', metadata={'source': 'imagepro.pdf', 'page': 144}),\n",
       " Document(page_content='. When all pixels have been processed with this trans-\\nformation, the PDF of the output image, \\npz\\nz\\n() ,\\n will be equal to the speciﬁed PDF. \\nBecause \\ns\\n is related to \\nr\\n by \\nTr\\n()\\n, it is possible for the mapping that yields \\nz\\n from \\ns\\n \\nto be expressed directly in terms of \\nr\\n.\\n In general, however, finding analytical expres-\\nsions for \\nG\\n−\\n1\\n is not a trivial task. Fortunately, this is not a problem when working \\nwith discrete quantities, as you will see shortly.\\nAs before, we have to convert the continuous result just derived into a discrete \\nform. This means that we work with histograms instead of PDFs. As in histogram \\nequalization, we lose in the conversion the ability to be able to guarantee a result that \\nwill have the exact speciﬁed histogram. Despite this, some very useful results can be \\nobtained even with approximations.\\nThe discrete formulation of Eq. (3-17) is the histogram equalization transforma-\\ntion in Eq. (3-15), which we repeat here for convenience:', metadata={'source': 'imagepro.pdf', 'page': 144}),\n",
       " Document(page_content='sT r L p r k L\\nkk\\nr j\\nj\\nk\\n== − = −\\n=\\n∑\\n() ( ) () , , , ,\\n1 012 1\\n0\\n…\\n \\n(3-20)\\nwhere the components of this equation are as before. Similarly, given a specific value \\nof \\ns\\nk\\n,\\n the discrete formulation of Eq. (3-18) involves computing the transformation \\nfunction\\n \\nGz L p z\\nqz\\ni\\ni\\nq\\n() ( ) ( )\\n=−\\n=\\n∑\\n1\\n0\\n \\n(3-21)\\nfor a value of \\nq\\n so that\\n \\nGz s\\nqk\\n()\\n=\\n \\n(3-22)\\nwhere \\npz\\nzi\\n()\\n is the \\ni\\nth value of the specified histogram.\\n Finally, we obtain the desired \\nvalue \\nz\\nq\\n from the inverse transformation:\\nDIP4E_GLOBAL_Print_Ready.indb   143\\n6/16/2017   2:03:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 144}),\n",
       " Document(page_content='144\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n \\nzG s\\nqk\\n=\\n−\\n1\\n()\\n \\n(3-23)\\nWhen performed over all pixels, this is a mapping from the \\ns\\n values in the histogram-\\nequalized image to the corresponding \\nz\\n values in the output image\\n.\\nIn practice, there is no need to compute the inverse of \\nG\\n. Because we deal with \\nintensity levels that are integers, it is a simple matter to compute all the possible \\nvalues of \\nG\\n using Eq. (3-21) for \\nqL\\n=−\\n012\\n1\\n,,, , .\\n…\\n These values are rounded to their \\nnearest integer values spanning the range \\n[, ]\\n01\\nL\\n−\\n and stored in a lookup table. \\nT\\nhen, given a particular value of \\ns\\nk\\n,\\n we look for the closest match in the table. For \\nexample\\n, if the 27th entry in the table is the closest value to \\ns\\nk\\n,\\n then \\nq\\n=\\n26\\n (recall \\nthat we start counting intensities at 0) and \\nz\\n26\\n is the best solution to Eq. (3-23). \\nThus, the given value \\ns\\nk\\n would map to \\nz\\n26\\n.\\n Because the \\nz\\n’\\ns are integers in the range \\n[, ] ,\\n01\\nL\\n−', metadata={'source': 'imagepro.pdf', 'page': 145}),\n",
       " Document(page_content='[, ] ,\\n01\\nL\\n−\\n it follows that \\nz\\n0\\n0\\n=\\n, \\nzL\\nL\\n−\\n=−\\n1\\n1,\\n and, in general, \\nzq\\nq\\n=\\n.\\n Therefore, \\nz\\n26\\n \\nwould equal intensity value 26. We repeat this procedure to ﬁnd the mapping from \\neach value \\ns\\nk\\n to the value \\nz\\nq\\n that is its closest match in the table. These mappings are \\nthe solution to the histogram-speciﬁcation problem.\\nGiven an input image, a speciﬁed histogram, \\npz\\nzi\\n() ,\\n \\niL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n and recall-\\ning that the \\nss\\nk\\n’\\n are the values resulting from Eq. (3-20), we may summarize the \\nprocedure for discrete histogram speciﬁcation as follows:\\n1. \\nCompute the histogram, \\npr\\nr\\n() ,\\n of the input image, and use it in Eq. (3-20) to map \\nthe intensities in the input image to the intensities in the histogram-equalized \\nimage\\n. Round the \\nresulting values, \\ns\\nk\\n, to the integer range \\n[, ] .\\n01\\nL\\n−\\n2.\\n \\nCompute all values of function \\nGz\\nq\\n()\\n using the Eq. (3-21) for \\nqL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n \\nwhere \\npz\\nzi\\n()', metadata={'source': 'imagepro.pdf', 'page': 145}),\n",
       " Document(page_content='…\\n \\nwhere \\npz\\nzi\\n()\\n are the values of the speciﬁed histogram. Round the values of \\nG\\n to \\nintegers in the range \\n[, ] .\\n01\\nL\\n−\\n Store the rounded values of \\nG\\n in a lookup table\\n.\\n3. \\nFor every value of \\nsk L\\nk\\n,, , , , ,\\n=−\\n012 1\\n…\\n use the stored values of \\nG\\n from Step 2 \\nto ﬁnd the corresponding value of \\nz\\nq\\n so that \\nGz\\nq\\n()\\n is closest to \\ns\\nk\\n.\\n Store these \\nmappings from \\ns\\n to \\nz\\n.\\n When more than one value of \\nz\\nq\\n gives the same match \\n(i.e., the mapping is not unique), choose the smallest value by convention.\\n4. \\nForm the histogram-speciﬁed image by mapping every equalized pixel with val-\\nue \\ns\\nk\\n to the corresponding pixel with value \\nz\\nq\\n in the histogram-speciﬁed image, \\nusing the mappings found in Step 3. \\nAs in the continuous case, the intermediate step of equalizing the input image is \\nconceptual. It can be skipped by combining the two transformation functions, \\nT\\n and \\nG\\n−\\n1\\n, as Example 3.7 below shows.\\nW', metadata={'source': 'imagepro.pdf', 'page': 145}),\n",
       " Document(page_content='W\\ne mentioned at the beginning of the discussion on histogram equalization that, \\nin addition to condition (b), inverse functions (\\nG\\n−\\n1\\n in the present discussion) have to \\nbe strictly monotonic to satisfy condition (a\\n/H11032\\n). In terms of Eq. (3-21), this means that \\nnone of the values \\npz\\nzi\\n()\\n in the speciﬁed histogram can be zero (see Problem 3.9). \\nW\\nhen this condition is not satisﬁed, we use the “work-around” procedure in Step 3. \\nThe following example illustrates this numerically.\\nDIP4E_GLOBAL_Print_Ready.indb   144\\n6/16/2017   2:03:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 145}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n145\\nEXAMPLE 3.7 : Illustration of the mechanics of histogram speciﬁcation.\\nConsider the \\n64 64\\n×\\n hypothetical image from Example 3.5, whose histogram is repeated in Fig. 3.22(a). \\nIt is desired to transform this histogram so that it will have the values speciﬁed in the second column of \\nT\\nable 3.2. Figure 3.22(b) shows this histogram.\\nThe ﬁrst step is to obtain the histogram-equalized values, which we did in Example 3.5:\\n \\nssssssss\\n012 34 5 6 7\\n13566777\\n====== = =\\n;;;;;;;\\nIn the next step, we compute the values of \\nGz\\nq\\n()\\n using the values of \\npz\\nzq\\n()\\n from Table 3.2 in Eq. (3-21): \\n \\nGz Gz Gz Gz\\nGz\\nGz\\n( ). ( ). ( ). ( ).\\n() . () .\\n0246\\n13\\n00 0 00 0 24 5 59 5\\n00 0 1\\n====\\n==\\n0 05 4 55 7 00\\n57\\nGz Gz\\n() . () .\\n==\\nAs in Example 3.5, these fractional values are rounded to integers in the range \\n[, ] :\\n07\\n \\nGz\\nGz\\nGz\\nGz\\nGz\\n() .\\n() .\\n() .\\n() .\\n() .\\n04\\n15\\n2\\n00 0 0\\n24 5 2\\n00 0 0\\n45 5 5\\n00 0\\n=→ =→\\n=→ =→\\n=→\\n→= →\\n=→ =→\\n0\\n5 95 6\\n10 5 1\\n70 0 7\\n6\\n37\\nGz\\nGz\\nGz\\n() .', metadata={'source': 'imagepro.pdf', 'page': 146}),\n",
       " Document(page_content='6\\n37\\nGz\\nGz\\nGz\\n() .\\n() .\\n() .\\nThese results are summarized in Table 3.3. The transformation function, \\nGz\\nq\\n() ,\\n is sketched in Fig. 3.23(c). \\nBecause its ﬁrst three values are equal,\\n \\nG\\n is not strictly monotonic, so condition (a\\n/H11032\\n) is violated. Therefore, \\nwe use the approach outlined in Step 3 of the algorithm to handle this situation. According to this step, \\nwe ﬁnd the smallest value of \\nz\\nq\\n so that the value \\nGz\\nq\\n()\\n is the closest to \\ns\\nk\\n.\\n We do this for every value of \\nr\\nk\\n \\np\\nr\\n(\\nr\\nk\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n.30\\n01234567\\nz\\nq\\n \\np\\nz\\n(\\nz\\nq\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n.30\\n01234567\\nz\\nq\\n \\np\\nz\\n(\\nz\\nq\\n)\\n.05\\n.10\\n.15\\n.20\\n.25\\n01234567\\nz\\nq\\n \\nG\\n(\\nz\\nq\\n)\\n1\\n2\\n3\\n4\\n7\\n6\\n5\\n01234567\\nb a\\nd c\\nFIGURE 3.22\\n(a) Histogram of a \\n3-bit image.  \\n(b) Speciﬁed  \\nhistogram.  \\n(c) Transformation \\nfunction obtained \\nfrom the speciﬁed \\nhistogram.  \\n(d) Result of  \\nhistogram  \\nspeciﬁcation.  \\nCompare the \\nhistograms in (b) \\nand (d).\\nDIP4E_GLOBAL_Print_Ready.indb   145\\n6/16/2017   2:03:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 146}),\n",
       " Document(page_content='146\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nTABLE \\n3.3\\nRounded values \\nof the  \\ntransformation \\nfunction \\nGz\\nq\\n() .\\n \\nz\\nq\\nGz\\nq\\n()\\nz\\n0\\n0\\n=\\n0\\nz\\n1\\n1\\n=\\n0\\nz\\n2\\n2\\n=\\n0\\nz\\n3\\n3\\n=\\n1\\nz\\n4\\n4\\n=\\n2\\nz\\n5\\n5\\n=\\n5\\nz\\n6\\n6\\n=\\n6\\nz\\n7\\n7\\n=\\n7\\ns\\nk\\n to create the required mappings from \\ns\\n to \\nz\\n. For example, \\ns\\n0\\n1\\n=\\n,\\n and we see that \\nGz\\n()\\n,\\n3\\n1\\n=\\n which is \\na perfect match in this case\\n, so we have the correspondence \\nsz\\n03\\n→\\n.\\n Every pixel whose value is 1 in the \\nhistogram equalized image would map to a pixel valued 3 in the histogram-speciﬁed image\\n. Continuing \\nin this manner, we arrive at the mappings in Table 3.4.\\nIn the ﬁnal step of the procedure, we use the mappings in Table 3.4 to map every pixel in the his-\\ntogram equalized image into a corresponding pixel in the newly created histogram-speciﬁed image. \\nThe values of the resulting histogram are listed in the third column of Table 3.2, and the histogram is \\nshown in Fig. 3.22(d). The values of \\npz\\nzq\\n()', metadata={'source': 'imagepro.pdf', 'page': 147}),\n",
       " Document(page_content='pz\\nzq\\n()\\n were obtained using the same procedure as in Example 3.5. \\nF\\nor instance, we see in Table 3.4 that \\ns\\nk\\n=\\n1\\n maps to \\nz\\nq\\n=\\n3,\\n and there are 790 pixels in the histogram-\\nequalized image with a value of 1.\\n Therefore, \\npz\\nz\\n()\\n..\\n3\\n790 4096 0 19\\n==\\nAlthough the ﬁnal result in Fig. 3.22(d) does not match the speciﬁed histogram exactly, the gen-\\neral trend of moving the intensities toward the high end of the intensity scale deﬁnitely was achieved.\\n \\nAs mentioned earlier, obtaining the histogram-equalized image as an intermediate step is useful for \\nz\\nq\\nSpeciﬁed\\npz\\nzq\\n()\\nActual\\npz\\nzq\\n()\\nz\\n0\\n0\\n=\\n0.00\\n0.00\\nz\\n1\\n1\\n=\\n0.00\\n0.00\\nz\\n2\\n2\\n=\\n0.00\\n0.00\\nz\\n3\\n3\\n=\\n0.15\\n0.19\\nz\\n4\\n4\\n=\\n0.20\\n0.25\\nz\\n5\\n5\\n=\\n0.30\\n0.21\\nz\\n6\\n6\\n=\\n0.20\\n0.24\\nz\\n7\\n7\\n=\\n0.15\\n0.11\\nTABLE \\n3.2\\nSpeciﬁed and \\nactual histograms \\n(the values in \\nthe third column \\nare computed in \\nExample 3.7).\\nDIP4E_GLOBAL_Print_Ready.indb   146\\n6/16/2017   2:03:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 147}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n147\\nexplaining the procedure, but this is not necessary. Instead, we could list the mappings from the \\nr\\n’s to \\nthe \\ns\\n’s and from the \\ns\\n’s to the \\nz\\n’s in a three-column table. Then, we would use those mappings to map \\nthe original pixels directly into the pixels of the histogram-speciﬁed image.\\nEXAMPLE 3.8 : Comparison between histogram equalization and histogram speciﬁcation.\\nFigure 3.23(a) shows an image of the Mars moon, Phobos, taken by NASA’s Mars Global Surveyor. \\nFigure 3.23(b) shows the histogram of Fig. 3.23(a). The image is dominated by large, dark areas, result-\\ning in a histogram characterized by a large concentration of pixels in the dark end of the gray scale. At \\nﬁrst glance, one might conclude that histogram equalization would be a good approach to enhance this \\nimage, so that details in the dark areas become more visible. It is demonstrated in the following discus-\\nsion that this is not so.', metadata={'source': 'imagepro.pdf', 'page': 148}),\n",
       " Document(page_content='Figure 3.24(a) shows the histogram equalization transformation [Eq. (3-20)] obtained using the histo-\\ngram in Fig. 3.23(b). The most relevant characteristic of this transformation function is how fast it rises \\nfrom intensity level 0 to a level near 190. This is caused by the large concentration of pixels in the input \\nhistogram having levels near 0. When this transformation is applied to the levels of the input image to \\nobtain a histogram-equalized result, the net effect is to map a very narrow interval of dark pixels into the \\nTABLE \\n3.4\\nMapping of  \\nvalues \\ns\\nk\\n into  \\ncorresponding \\nvalues \\nz\\nq\\n.\\nsz\\nkq\\n→\\n13\\n→\\n34\\n→\\n55\\n→\\n66\\n→\\n77\\n→\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\nb a\\nFIGURE 3.23\\n(a) An image, and \\n(b) its histogram.\\nDIP4E_GLOBAL_Print_Ready.indb   147\\n6/16/2017   2:03:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 148}),\n",
       " Document(page_content='148\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192 255\\nInput intensity\\nOutput intensity\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\nb\\na\\nc\\nFIGURE 3.24\\n(a) Histogram \\nequalization \\ntransformation \\nobtained using \\nthe histogram \\nin Fig. 3.23(b). \\n(b) Histogram \\nequalized image. \\n(c) Histogram of \\nequalized image.\\nupper end of the gray scale of the output image. Because numerous pixels in the input image have levels \\nprecisely in this interval, we would expect the result to be an image with a light, washed-out appearance. \\nAs Fig. 3.24(b) shows, this is indeed the case. The histogram of this image is shown in Fig. 3.24(c). Note \\nhow all the intensity levels are biased toward the upper one-half of the gray scale.\\nBecause the problem with the transformation function in Fig. 3.24(a) was caused by a large con-', metadata={'source': 'imagepro.pdf', 'page': 149}),\n",
       " Document(page_content='centration of pixels in the original image with levels near 0, a reasonable approach is to modify the \\nhistogram of that image so that it does not have this property. Figure 3.25(a) shows a manually speci-\\nﬁed function that preserves the general shape of the original histogram, but has a smoother transition \\nof levels in the dark region of the gray scale. Sampling this function into 256 equally spaced discrete \\nvalues produced the desired speciﬁed histogram. The transformation function, \\nGz\\nq\\n() ,\\n obtained from this \\nhistogram using Eq.\\n (3-21) is labeled transformation (1) in Fig. 3.25(b). Similarly, the inverse transfor-\\nmation \\nGs\\nk\\n−\\n1\\n() ,\\n from Eq. (3-23) (obtained using the step-by-step procedure discussed earlier) is labeled \\ntransformation (2) in F\\nig. 3.25(b). The enhanced image in Fig. 3.25(c) was obtained by applying trans-\\nformation (2) to the pixels of the histogram-equalized image in Fig. 3.24(b). The improvement of the', metadata={'source': 'imagepro.pdf', 'page': 149}),\n",
       " Document(page_content='histogram-speciﬁed image over the result obtained by histogram equalization is evident by comparing \\nthese two images. It is of interest to note that a rather modest change in the original histogram was all \\nthat was required to obtain a signiﬁcant improvement in appearance. Figure 3.25(d) shows the histo-\\ngram of Fig. 3.25(c). The most distinguishing feature of this histogram is how its low end has shifted right \\ntoward the lighter region of the gray scale (but not excessively so), as desired.\\nDIP4E_GLOBAL_Print_Ready.indb   148\\n6/16/2017   2:03:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 149}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n149\\nLOCAL HISTOGRAM PROCESSING\\nThe histogram processing methods discussed thus far are \\nglobal\\n, in the sense that \\npixels are modified by a transformation function based on the intensity distribution \\nof an entire image. This global approach is suitable for overall enhancement, but \\ngenerally fails when the objective is to enhance details over small areas in an image. \\nThis is because the number of pixels in small areas have negligible influence on \\nthe computation of global transformations. The solution is to devise transformation \\nfunctions based on the intensity distribution of pixel neighborhoods.\\nThe histogram processing techniques previously described can be adapted to local \\nenhancement. The procedure is to deﬁne a neighborhood and move its center from \\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\n255\\n192\\n128\\n64\\n0\\n0 64 128 192\\n255\\nInput intensity\\nOutput intensity\\n(2)\\n(1)\\n7.00\\n5.25\\n3.50\\n1.75\\n0\\n0 64 128 192\\n255\\nIntensity\\nNumber of pixels (', metadata={'source': 'imagepro.pdf', 'page': 150}),\n",
       " Document(page_content='Number of pixels ( \\n/H11003 \\n10\\n4\\n)\\nNumber of pixels ( \\n/H11003 \\n10\\n4\\n)\\na\\nb\\nc\\nd\\nFIGURE 3.25\\nHistogram  \\nspeciﬁcation.  \\n(a) Speciﬁed histo-\\ngram.  \\n(b) Transformation \\nGz\\nq\\n() ,\\n labeled (1), \\nand \\nGs\\nk\\n−\\n1\\n() ,\\n  \\nlabeled (2).\\n  \\n(c) Result of  \\nhistogram  \\nspeciﬁcation.  \\n(d) Histogram of \\nimage (c).\\nDIP4E_GLOBAL_Print_Ready.indb   149\\n6/16/2017   2:03:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 150}),\n",
       " Document(page_content='150\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\npixel to pixel in a horizontal or vertical direction. At each location, the histogram of \\nthe points in the neighborhood is computed, and either a histogram equalization or \\nhistogram speciﬁcation transformation function is obtained. This function is used to \\nmap the intensity of the pixel centered in the neighborhood. The center of the neigh-\\nborhood is then moved to an adjacent pixel location and the procedure is repeated. \\nBecause only one row or column of the neighborhood changes in a one-pixel trans-\\nlation of the neighborhood, updating the histogram obtained in the previous loca-\\ntion with the new data introduced at each motion step is possible (see Problem 3.14). \\nThis approach has obvious advantages over repeatedly computing the histogram of \\nall pixels in the neighborhood region each time the region is moved one pixel loca-\\ntion. Another approach used sometimes to reduce computation is to utilize nonover-', metadata={'source': 'imagepro.pdf', 'page': 151}),\n",
       " Document(page_content='lapping regions, but this method usually produces an undesirable “blocky” effect.\\nEXAMPLE 3.9 : Local histogram equalization.\\nFigure 3.26(a) is an 8-bit, \\n512 512\\n×\\n image consisting of ﬁve black squares on a light gray background. \\nT\\nhe image is slightly noisy, but the noise is imperceptible. There are objects embedded in the dark \\nsquares, but they are invisible for all practical purposes. Figure 3.26(b) is the result of global histogram \\nequalization. As is often the case with histogram equalization of smooth, noisy regions, this image shows \\nsigniﬁcant enhancement of the noise. However, other than the noise, Fig. 3.26(b) does not reveal any \\nnew signiﬁcant details from the original. Figure 3.26(c) was obtained using local histogram equaliza-\\ntion of Fig. 3.26(a) with a neighborhood of size \\n33\\n×\\n. Here, we see signiﬁcant detail within all the dark \\nsquares\\n. The intensity values of these objects are too close to the intensity of the dark squares, and their', metadata={'source': 'imagepro.pdf', 'page': 151}),\n",
       " Document(page_content='sizes are too small, to inﬂuence global histogram equalization signiﬁcantly enough to show this level of \\nintensity detail.\\nUSING HISTOGRAM STATISTICS FOR IMAGE ENHANCEMENT\\nStatistics obtained directly from an image histogram can be used for image enhance-\\nment. Let \\nr\\n denote a discrete random variable representing intensity values in the range \\n[, ]\\n01\\nL\\n−\\n, and let \\npr\\ni\\n()\\n denote the normalized histogram component corresponding to \\nintensity value \\nr\\ni\\n.\\n As indicated earlier, we may view \\npr\\ni\\n()\\n as an estimate of the prob-\\nability that intensity \\nr\\ni\\n occurs in the image from which the histogram was obtained.\\nb a\\nc\\nFIGURE 3.26\\n(a) Original  \\nimage. (b) Result \\nof global  \\nhistogram  \\nequalization.  \\n(c) Result of local \\nhistogram  \\nequalization.\\nDIP4E_GLOBAL_Print_Ready.indb   150\\n6/16/2017   2:03:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 151}),\n",
       " Document(page_content='3.3\\n  \\nHistogram Processing\\n    \\n151\\nFor an image with intensity levels in the range \\n[, ] ,\\n01\\nL\\n−\\n the \\nn\\nth moment of \\nr\\n \\nabout its mean,\\n \\nm\\n, is deﬁned as\\n \\nm\\nni\\nn\\ni\\ni\\nL\\nrm p r\\n=−\\n=\\n−\\n∑\\n() ( )\\n0\\n1\\n \\n(3-24)\\nwhere \\nm\\n is given by \\n \\nmr p r\\nii\\ni\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(3-25)\\nThe mean is a measure of average intensity and \\nthe variance (or stan\\ndard deviation,\\n \\ns\\n), given by\\n \\nsm\\n2\\n2\\n2\\n0\\n1\\n== −\\n=\\n−\\n∑\\n() ( )\\nrm p r\\nii\\ni\\nL\\n \\n(3-26)\\nis a measure of image contrast.\\nW\\ne consider two uses of the mean and variance for enhancement purposes. The \\nglobal\\n mean and variance [Eqs. (3-25) and (3-26)] are computed over an entire \\nimage and are useful for gross adjustments in overall intensity and contrast. A more \\npowerful use of these parameters is in local enhancement, where the \\nlocal\\n mean and \\nvariance are used as the basis for making changes that depend on image character-\\nistics in a neighborhood about each pixel in an image.\\nLet \\n(, )\\nxy', metadata={'source': 'imagepro.pdf', 'page': 152}),\n",
       " Document(page_content='Let \\n(, )\\nxy\\n denote the coordinates of any pixel in a given image, and let \\nS\\nxy\\n denote \\na neighborhood of speciﬁed size, centered on \\n(, ) .\\nxy\\n The mean value of the pixels in \\nthis neighborhood is given by the expression\\n \\nmr p r\\nSi S i\\ni\\nL\\nxy\\nxy\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(3-27)\\nwhere \\np\\nS\\nxy\\n is the histogram of the pixels in region \\nS\\nxy\\n.\\n This histogram has \\nL\\n bins\\n, \\ncorresponding to the \\nL\\n possible intensity values in the input image. However, many \\nof the bins will have 0 counts, depending on the size of \\nS\\nxy\\n.\\n For example, if the neigh-\\nborhood is of size \\n33\\n×\\n and \\nL\\n=\\n256\\n,\\n only between 1 and 9 of the 256 bins of the \\nhistogram of the neighborhood will be nonzero (the maximum number of possible \\ndifferent\\n intensities in a \\n33\\n×\\n region is 9, and the minimum is 1). These non-zero \\nvalues will correspond to the number of different intensities in \\nS\\nxy\\n .\\nThe variance of the pixels in the neighborhood is similarly given by\\n \\ns\\nSi S S i\\ni\\nL\\nxy\\nxy xy\\nrm pr\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑', metadata={'source': 'imagepro.pdf', 'page': 152}),\n",
       " Document(page_content='22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\n \\n(3-28)\\nAs before, the local mean is a measure of average intensity in neighborhood \\nS\\nxy\\n,\\n and \\nthe local variance (or standard deviation) is a measure of intensity contrast in that \\nneighborhood.\\nSee the tutorials section \\nin the book website for a \\nreview of probability.\\nWe follow convention \\nin using \\nm\\n for the mean \\nvalue. Do not confuse it \\nwith our use of the same \\nsymbol to denote the \\nnumber of rows in an  \\nm\\n \\n/H11003\\n \\nn\\n neighborhood.\\nDIP4E_GLOBAL_Print_Ready.indb   151\\n6/16/2017   2:03:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 152}),\n",
       " Document(page_content='152\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nAs the following example illustrates, an important aspect of image processing \\nusing the local mean and variance is the ﬂexibility these parameters afford in devel-\\noping simple, yet powerful enhancement rules based on statistical measures that \\nhave a close, predictable correspondence with image appearance.\\nEXAMPLE 3.10 :  Local enhancement using histogram statistics.\\nFigure 3.27(a) is the same image as Fig. 3.26(a), which we enhanced using local histogram equalization. \\nAs noted before, the dark squares contain embedded symbols that are almost invisible. As before, we \\nwant to enhance the image to bring out these hidden features. \\nWe can use the concepts presented in this section to formulate an approach for enhancing low-con-\\ntrast details embedded in a background of similar intensity. The problem at hand is to enhance the low-', metadata={'source': 'imagepro.pdf', 'page': 153}),\n",
       " Document(page_content='contrast detail in the dark areas of the image, while leaving the light background unchanged. \\nA method used to determine whether an area is relatively light or dark at a point \\n(, )\\nxy\\n is to com-\\npare the average local intensity\\n, \\nm\\nS\\nxy\\n,\\n to the average image intensity (the global mean), denoted by \\nm\\nG\\n.\\n We obtain \\nm\\nG\\n using Eq. (3-25) with the histogram of the entire image. Thus, we have the ﬁrst ele-\\nment of our enhancement scheme: We will consider the pixel at \\n(, )\\nxy\\n as a candidate for processing if \\nkm m km\\nGS G\\nxy\\n01\\n≤≤\\n,\\n where \\nk\\n0\\n and \\nk\\n1\\n are nonnegative constants and \\nkk\\n01\\n<\\n.\\n For example, if our focus is \\non areas that are darker than one-quarter of the mean intensity\\n, we would choose \\nk\\n0\\n0\\n=\\n and \\nk\\n1\\n02 5\\n=\\n..\\nBecause we are interested in enhancing areas that have low contrast, we also need a measure to \\ndetermine whether the contrast of an area makes it a candidate for enhancement.\\n We consider the \\npixel at \\n(, )\\nxy\\n as a candidate if \\nkk\\nGS G\\nxy\\n23', metadata={'source': 'imagepro.pdf', 'page': 153}),\n",
       " Document(page_content='kk\\nGS G\\nxy\\n23\\nss s\\n≤≤\\n,\\n where \\ns\\nG\\n is the global standard deviation obtained \\nwith Eq. (3-26) using the histogram of the entire image, and \\nk\\n2\\n and \\nk\\n3\\n are nonnegative constants, with \\nkk\\n23\\n<\\n.\\n For example, to enhance a dark area of low contrast, we might choose \\nk\\n2\\n0\\n=\\n and \\nk\\n3\\n01\\n=\\n..\\n A \\npixel that meets all the preceding conditions for local enhancement is processed by multiplying it by a \\nspeciﬁed constant,\\n \\nC\\n, to increase (or decrease) the value of its intensity level relative to the rest of the \\nimage. Pixels that do not meet the enhancement conditions are not changed.\\nWe summarize the preceding approach as follows. Let \\nfx y\\n(,\\n)\\n denote the value of an image at any \\nimage coordinates \\n(, ) ,\\nxy\\n and let \\ngxy\\n(,\\n)\\n be the corresponding value in the enhanced image at those \\ncoordinates\\n. Then,\\n \\ngxy\\nCf\\nx y k m m km k\\nk\\nfx y\\nGS G GS G\\nxy\\nxy\\n(, )\\n(, )\\n(, )\\n=\\nif  \\nAND  \\not\\n2\\n01 3\\n≤≤\\n≤≤\\nss s\\nh\\nherwise\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n \\n(3-29)\\nb a\\nFIGURE 3.27\\n(a) Original  \\nimage. (b) Result', metadata={'source': 'imagepro.pdf', 'page': 153}),\n",
       " Document(page_content='image. (b) Result \\nof local  \\nenhancement \\nbased on local  \\nhistogram  \\nstatistics.  \\nCompare (b) with \\nFig. 3.26(c).\\nDIP4E_GLOBAL_Print_Ready.indb   152\\n6/16/2017   2:03:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 153}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n153\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , ,\\n…\\n where, as indicated above, \\nC\\n, \\nk\\n0\\n, \\nk\\n1\\n, \\nk\\n2\\n,\\n and \\nk\\n3\\n are \\nspeciﬁed constants, \\nm\\nG\\n is the global mean of the input image, and \\ns\\nG\\n is its standard deviation. Param-\\neters \\nm\\nS\\nxy\\n and \\ns\\nS\\nxy\\n are the local mean and standard deviation, respectively, which change for every loca-\\ntion \\n(, ) .\\nxy\\n As usual, \\nM\\n and \\nN\\n are the number of rows and columns in the input image\\n.\\nFactors such as the values of the global mean and variance relative to values in the areas to be \\nenhanced play a key role in selecting the parameters in Eq. (3-29), as does the range of differences \\nbetween the intensities of the areas to be enhanced and their background. In the case of Fig. 3.27(a), \\nm\\nG\\n=\\n161,\\n \\ns\\nG\\n=\\n103,\\n the maximum intensity values of the image and areas to be enhanced are 228 and \\n10,\\n respectively, and the minimum values are 0 in both cases.', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='We would like for the maximum value of the enhanced features to be the same as the maximum value \\nof the image, so we select \\nC\\n=\\n22\\n8\\n..\\n The areas to be enhanced are quite dark relative to the rest of the \\nimage\\n, and they occupy less than a third of the image area; thus, we expect the mean intensity in the \\ndark areas to be much less than the global mean. Based on this, we let \\nk\\n0\\n0\\n=\\n and \\nk\\n1\\n01\\n=\\n..\\n Because the \\nareas to be enhanced are of very low contrast,\\n we let \\nk\\n2\\n0\\n=\\n.\\n For the upper limit of acceptable values \\nof standard deviation we set \\nk\\n3\\n01\\n=\\n.,\\n which gives us one-tenth of the global standard deviation. Figure \\n3.27(b) is the result of using Eq.\\n (3-29) with these parameters. By comparing this ﬁgure with Fig. 3.26(c), \\nwe see that the method based on local statistics detected the same hidden features as local histogram \\nequalization. But the present approach extracted signiﬁcantly more detail. For example, we see that all', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='the objects are solid, but only the boundaries were detected by local histogram equalization. In addition, \\nnote that the intensities of the objects are not the same, with the objects in the top-left and bottom-right \\nbeing brighter than the others. Also, the horizontal rectangles in the lower left square evidently are of \\ndifferent intensities. Finally, note that the background in both the image and dark squares in Fig. 3.27(b) \\nis nearly the same as in the original image; by comparison, the same regions in Fig. 3.26(c) exhibit more \\nvisible noise and have lost their gray-level content. Thus, the additional complexity required to use local \\nstatistics yielded results in this case that are superior to local histogram equalization.\\n3.4 FUNDAMENTALS OF SPATIAL FILTERING  \\nIn this section, we discuss the use of spatial filters for image processing. Spatial filter-\\ning is used in a broad spectrum of image processing applications, so a solid under-', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='standing of filtering principles is important. As mentioned at the beginning of this \\nchapter, the filtering examples in this section deal mostly with image enhancement. \\nOther applications of spatial filtering are discussed in later chapters. \\nThe name \\nﬁlter\\n is borrowed from frequency domain processing (the topic of \\nChapter 4) where “ﬁltering” refers to passing, modifying, or rejecting speciﬁed fre-\\nquency components of an image. For example, a ﬁlter that passes low frequencies \\nis called a \\nlowpass ﬁlter\\n. The net effect produced by a lowpass ﬁlter is to smooth an \\nimage by blurring it. We can accomplish similar smoothing directly on the image \\nitself by using \\nspatial ﬁlters\\n. \\nSpatial ﬁltering modiﬁes an image by replacing the value of each pixel by a func-\\ntion of the values of the pixel and its neighbors. If the operation performed on the \\nimage pixels is linear, then the ﬁlter is called a \\nlinear spatial ﬁlter\\n. Otherwise, the \\nﬁlter is a \\nnonlinear spatial ﬁlter\\n.', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='.\\n \\nWe will focus attention ﬁrst on linear ﬁlters and then \\nintroduce some basic nonlinear ﬁlters\\n. Section 5.3 contains a more comprehensive \\nlist of nonlinear ﬁlters and their application.\\n3.4\\nSee Section 2.6 regarding \\nlinearity.\\nDIP4E_GLOBAL_Print_Ready.indb   153\\n6/16/2017   2:03:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 154}),\n",
       " Document(page_content='154\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nTHE MECHANICS OF LINEAR SPATIAL FILTERING\\nA linear spatial filter performs a sum-of-products operation between an image \\nf\\n and a \\nfilter kernel\\n, \\nw\\n.\\n The kernel is an array whose size defines the neighborhood \\nof opera-\\ntion,\\n and whose coefficients determine the nature of the filter. Other terms used to \\nrefer to a spatial filter kernel are \\nmask\\n, \\ntemplate\\n, and \\nwindow\\n. We use the term \\nfilter \\nkernel\\n or simply \\nkernel\\n.\\nFigure 3.28 illustrates the mechanics of linear spatial ﬁltering using a \\n33\\n×\\n ker-\\nnel.\\n At any point \\n(, )\\nxy\\n in the image, the response, \\ngxy\\n(,\\n) ,\\n of the ﬁlter is the sum of \\nproducts of the kernel coefﬁcients and the image pixels encompassed by the kernel:\\n \\ngxy\\nf x y\\nf x y\\nfx\\ny\\n( , )( , ) ( , )( , ) ( , )\\n(,)(, )\\n=− − − − +− − +\\n++\\nww\\nw\\n11 1 1 1 0 1\\n00\\n…\\n…\\n+\\n++ +\\nw\\n(, ) ( , )\\n11 1 1\\nfx y\\n \\n(3-30)\\nAs coordinates \\nx\\n and \\ny\\n are varied,', metadata={'source': 'imagepro.pdf', 'page': 155}),\n",
       " Document(page_content='y\\n are varied,\\n the center of the kernel moves from pixel to pixel, \\ngenerating the filtered image, \\ng\\n, in the process.\\n†\\nObserve that the center coefﬁcient of the kernel, \\nw\\n(,\\n)\\n00\\n, aligns with the pixel at \\nlocation \\n(, ) .\\nxy\\n For a kernel of size \\nmn\\n×\\n,\\n we assume that \\nma\\n=+\\n21\\n and \\nnb\\n=+\\n21\\n,\\nwhere \\na\\n and \\nb\\n are nonnegative integers\\n. This means that our focus is on kernels of \\nodd size in both coordinate directions. In general, linear spatial ﬁltering of an image \\nof size \\nMN\\n×\\n with a kernel of size \\nmn\\n×\\n is given by the expression\\n \\ngxy\\nstf x sy t\\ntb\\nb\\nsa\\na\\n(, ) ( ,)( , )\\n=+\\n+\\n=− =−\\n∑ ∑\\nw\\n \\n(3-31)\\nwhere \\nx\\n and \\ny\\n are varied so that the center (origin) of the kernel visits every pixel in \\nf\\n once\\n. For a fixed value of \\n(, ) ,\\nxy\\n Eq. (3-31) implements the \\nsum of products\\n of the \\nform shown in Eq.\\n (3-30), but for a kernel of arbitrary odd size. As you will learn in \\nthe following section, this equation is a central tool in linear filtering.', metadata={'source': 'imagepro.pdf', 'page': 155}),\n",
       " Document(page_content='SPATIAL CORRELATION AND CONVOLUTION\\nSpatial correlation\\n is illustrated graphically in Fig. 3.28, and it is described mathemati-\\ncally by Eq. \\n(3-31). Correlation consists of moving the center of a kernel over an \\nimage, and computing the sum of products at each location. The mechanics of \\nspatial \\nconvolution\\n are the same, except that the correlation kernel is rotated by 180°. Thus, \\nwhen the values of a kernel are symmetric about its center, correlation and convolu-\\ntion yield the same result. The reason for rotating the kernel will become clear in \\nthe following discussion. The best way to explain the differences between the two \\nconcepts is by example. \\nWe begin with a 1-D illustration, in which case Eq. (3-31) becomes\\n \\ngx sf x s\\nsa\\na\\n() ( )( )\\n=+\\n=−\\n∑\\nw\\n \\n(3-32)\\n†\\n  A ﬁltered pixel value typically is assigned to a corresponding location in a new image created to hold the results', metadata={'source': 'imagepro.pdf', 'page': 155}),\n",
       " Document(page_content='of ﬁltering. It is seldom the case that ﬁltered pixels replace the values of the corresponding location in the origi-\\nnal image, as this would change the content of the image while ﬁltering is being performed.\\nIt certainly is possible \\nto work with kernels of \\neven size, or mixed even \\nand odd sizes. However, \\nworking with odd sizes \\nsimpliﬁes indexing and \\nis also more intuitive \\nbecause the kernels have \\ncenters falling on integer \\nvalues, and they are \\nspatially symmetric.\\nDIP4E_GLOBAL_Print_Ready.indb   154\\n6/16/2017   2:03:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 155}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n155\\nFigure 3.29(a) shows a 1-D function, \\nf\\n, and a kernel, \\nw.\\n The kernel is of size \\n15\\n×\\n,\\n so \\na\\n=\\n2\\n and \\nb\\n=\\n0\\n in this case. Figure 3.29(b) shows the starting position used to per-\\nform correlation,\\n in which \\nw\\n is positioned so that its center coefficient is coincident \\nwith the origin of \\nf\\n. \\nT\\nhe first thing we notice is that part of \\nw\\n lies outside \\nf\\n,\\n so the summation is \\nundefined in that area. A solution to this problem is to \\npad\\n function \\nf\\n with enough \\n0’s on either side. In general, if the kernel is of size \\n1\\n×\\nm\\n,\\n we need \\n()\\nm\\n−\\n12\\n zeros \\non either side of \\nf\\n in order to handle the beginning and ending configurations of \\nw\\n \\nwith respect to \\nf\\n.\\n Figure 3.29(c) shows a properly padded function. In this starting \\nconﬁguration, all coefﬁcients of the kernel overlap valid values. \\nZero padding is not the \\nonly padding option, as \\nwe will discuss in detail \\nlater in this chapter.\\nPixel values under kernel', metadata={'source': 'imagepro.pdf', 'page': 156}),\n",
       " Document(page_content='when it is centered on (\\nx\\n, \\ny\\n)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n \\n/H11002 \\n1, \\ny\\n)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n \\n/H11001 \\n1, \\ny\\n)\\nf\\n(\\nx\\n, \\ny\\n \\n/H11002 \\n1)\\nf\\n(\\nx\\n, \\ny\\n \\n/H11001 \\n1)\\nf\\n(\\nx\\n, \\ny\\n)\\nw\\n(\\n/H11002\\n1,\\n/H11002\\n1)\\nw\\n(0,\\n/H11002\\n1)\\nw\\n(\\n/H11002\\n1,0)\\nw\\n(\\n/H11002\\n1,1)\\nw\\n(0,1)\\nw\\n(1,1)\\nw\\n(0,0)\\nw\\n(1,0)\\nw\\n(1,\\n/H11002\\n1)\\nKernel coefficients\\nx\\nImage \\nf\\ny\\nImage origin\\nFilter kernel,\\nMagnified view showing filter kernel\\ncoefficients and corresponding pixels\\nin the image\\nFilter kernel\\nKernel origin\\nImage pixels\\nw\\n(\\ns\\n,\\nt\\n)\\nFIGURE 3.28\\nThe mechanics \\nof linear spatial \\nﬁltering  \\nusing a \\n33\\n×\\n  \\nkernel.\\n The pixels \\nare shown as \\nsquares to sim-\\nplify the graph-\\nics. Note that \\nthe origin of the \\nimage is at the top \\nleft, but the origin \\nof the kernel is at \\nits center. Placing \\nthe origin at the \\ncenter of spatially \\nsymmetric kernels \\nsimpliﬁes writing \\nexpressions for \\nlinear ﬁltering.', metadata={'source': 'imagepro.pdf', 'page': 156}),\n",
       " Document(page_content='linear ﬁltering.\\nDIP4E_GLOBAL_Print_Ready.indb   155\\n6/16/2017   2:03:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 156}),\n",
       " Document(page_content='156\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nThe ﬁrst correlation value is the sum of products in this initial position, computed \\nusing Eq. (3-32) with \\nx\\n=\\n0:\\n  \\ngs f s\\ns\\n() ()( )\\n00\\n0\\n2\\n2\\n=+ =\\n=−\\n∑\\nw\\n \\nT\\nhis value is in the leftmost location of the correlation result in\\n \\nFig. 3.29(g). \\nT\\no obtain the second value of correlation, we shift the relative positions of \\nw\\n and \\nf\\n one pixel location to the right [i.e\\n., we let \\nx\\n=\\n1\\n in Eq. (3-32)] and compute the sum \\nof products again.\\n The result is \\ng\\n()\\n,\\n18\\n=\\n as shown in the leftmost, nonzero location \\nin F\\nig. 3.29(g). When \\nx\\n=\\n2,\\n we obtain \\ng\\n()\\n.\\n22\\n=\\n When \\nx\\n=\\n3,\\n we get \\ng\\n()\\n34\\n=\\n [see Fig. \\n3.29(e)].\\n Proceeding in this manner by varying \\nx \\none shift at a time, we “build” the \\ncorrelation result in Fig. 3.29(g). Note that it took 8 values of \\nx\\n (i.e., \\nx\\n=\\n012\\n7\\n,,, ,\\n…\\n) \\nto fully shift \\nw\\n past \\nf\\n so the\\n center\\n coefﬁcient in \\nw\\n visited \\nevery\\n pixel in \\nf\\n.\\n Sometimes,', metadata={'source': 'imagepro.pdf', 'page': 157}),\n",
       " Document(page_content='f\\n.\\n Sometimes, \\nit is useful to have every element of \\nw\\n visit every pixel in \\nf\\n.\\n For this, we have to start \\n(i)\\n(j)\\n(k)\\n(l)\\n(m)\\n(n)\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n000\\n1\\n00 0\\n000\\n1\\n0000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\n00000\\n1\\n00 000\\n00000\\n1\\n000000\\n0\\n12428\\n82421\\nCorrelation\\nConvolution\\nStarting position alignment\\nPosition after 1 shift\\nFinal position\\n( h ) 00082421 000\\n0\\nExtended (full) correlation result\\n(p)\\n000124280000\\nExtended (full) convolution result\\n(g)\\n0 8 2 4 2 1 0 0\\n        Correlation result\\n(o)\\n01242800\\n        Convolution result\\n000\\n1\\n0000\\nOrigin\\nf\\n82421\\nw\\n rotated 180\\n/H11034\\n000\\n1\\n0000\\nOrigin\\nf\\n2428\\n1\\nw\\nPosition after 3 shifts\\nZero padding\\nStarting position\\nZero padding\\nStarting position\\nStarting position alignment\\nPosition after 1 shift\\nPosition after 3 shifts\\nFinal position\\nFIGURE 3.29\\nIllustration of 1-D \\ncorrelation and \\nconvolution of a \\nkernel, \\nw', metadata={'source': 'imagepro.pdf', 'page': 157}),\n",
       " Document(page_content='kernel, \\nw\\n,  with a \\nfunction \\nf  \\nconsisting of a  \\ndiscrete unit \\nimpulse\\n. Note that \\ncorrelation and \\nconvolution are \\nfunctions of the \\nvariable \\nx\\n, which \\nacts to \\ndisplace\\n \\none function with \\nrespect to the \\nother. For the \\nextended  \\ncorrelation and \\nconvolution \\nresults, the  \\nstarting  \\nconﬁguration \\nplaces the right-\\nmost element of \\nthe kernel to be \\ncoincident with \\nthe origin of \\nf\\n. \\nAdditional  \\npadding must be \\nused.\\nDIP4E_GLOBAL_Print_Ready.indb   156\\n6/16/2017   2:03:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 157}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n157\\nwith the rightmost element of \\nw\\n coincident with the origin of \\nf\\n,\\n and end with the \\nleftmost element of \\nw\\n being coincident the last element of \\nf\\n (additional padding \\nwould be required).\\n Figure Fig. 3.29(h) shows the result of this \\nextended\\n, or \\nfull\\n, cor-\\nrelation. As Fig. 3.29(g) shows, we can obtain the “standard” correlation by cropping \\nthe full correlation in Fig. 3.29(h).\\nThere are two important points to note from the preceding discussion. First, cor-\\nrelation is a function of \\ndisplacement\\n of the ﬁlter kernel relative to the image. In \\nother words, the ﬁrst value of correlation corresponds to zero displacement of the \\nkernel, the second corresponds to one unit displacement, and so on.\\n†\\n The second \\nthing to notice is that correlating a kernel \\nw\\n with a function that contains all 0’s and \\na single 1 yields a \\ncop\\ny\\n of \\nw\\n, but \\nrotated\\n \\nby 180°. A function that contains a single 1', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='with the rest being 0’s is called a \\ndiscrete unit impulse\\n. Correlating a kernel with a dis-\\ncrete unit impulse yields a \\nrotated\\n version of the kernel at the location of the impulse. \\nThe right side of Fig. 3.29 shows the sequence of steps for performing convolution \\n(we will give the equation for convolution shortly). The only difference here is that \\nthe kernel is \\npre-rotated\\n by 180° prior to performing the shifting/sum of products \\noperations. As the\\n \\nconvolution in Fig. 3.29(o) shows, the result of pre-rotating the \\nkernel is that now we have an \\nexact\\n copy of the kernel at the location of the unit \\nimpulse\\n. In fact, a foundation of linear system theory is that convolving a function \\nwith an impulse yields a copy of the function at the location of the impulse. We will \\nuse this property extensively in Chapter 4.\\nThe 1-D concepts just discussed extend easily to images, as Fig. 3.30 shows. For a \\nkernel of size \\nmn\\n×\\n,\\n we pad the image with a minimum of \\n()\\nm\\n−\\n12', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='()\\nm\\n−\\n12\\n rows of 0’s at \\nthe top and bottom and \\n()\\nn\\n−\\n12\\n columns of 0’s on the left and right. In this case, \\nm\\n and \\nn\\n are equal to 3,\\n so we pad \\nf\\n with one row of 0’s above and below and one \\ncolumn of 0’s to the left and right, as Fig. 3.30(b) shows. Figure 3.30(c) shows the \\ninitial position of the kernel for performing correlation, and Fig. 3.30(d) shows the \\nﬁnal result after  the center of \\nw\\n visits every pixel in \\nf\\n,\\n computing a sum of products \\nat each location. As before, the result is a copy of the kernel, rotated by 180°. We will \\ndiscuss the extended correlation result shortly.\\nFor convolution, we pre-rotate the kernel as before and repeat the sliding sum of \\nproducts just explained. Figures 3.30(f) through (h) show the result. You see again \\nthat convolution of a function with an impulse copies the function to the location \\nof the impulse. As noted earlier, correlation and convolution yield the same result if \\nthe kernel values are symmetric about the center.', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='The concept of an impulse is fundamental in linear system theory, and is used in \\nnumerous places throughout the book. A \\ndiscrete impulse of strength\\n (\\namplitude\\n) \\nA\\n \\nlocated at coordinates \\n(,)\\nxy\\n00\\n is deﬁned as\\n \\nd\\n(,\\n)\\nxx yy\\nAx\\nxy y\\n−− =\\n==\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n00\\n00\\n0\\nif  and \\notherwise\\n \\n(3-33)\\n†\\n  In reality, we are shifting \\nf\\n to the left of \\nw\\n every time we increment \\nx\\n in Eq.\\n (3-32). However, it is more intuitive \\nto think of the smaller kernel moving right over the larger array \\nf\\n. The motion of the two is relative, so either \\nway of looking at the motion is acceptable. The reason we increment \\nf\\n and not \\nw\\n is that indexing the equations \\nfor correlation and convolution is much easier (and clearer) this way\\n, especially when working with 2-D arrays.\\nRotating a 1-D kernel \\nby 180° is equivalent to \\nﬂipping the kernel about \\nits axis. \\nIn 2-D, rotation by 180° \\nis equivalent to ﬂipping \\nthe kernel about one axis \\nand then the other.\\nDIP4E_GLOBAL_Print_Ready.indb   157', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='6/16/2017   2:03:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 158}),\n",
       " Document(page_content='158\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFor example, the unit impulse in Fig. 3.29(a) is given by \\nd\\n()\\nx\\n−\\n3\\n in the 1-D version of \\nthe preceding equation.\\n  Similarly, the impulse in Fig. 3.30(a) is given by \\nd\\n(,\\n)\\nxy\\n−−\\n22\\n \\n[remember\\n, the origin is at \\n(,)\\n00\\n].\\n Summarizing the preceding discussion in equation form,\\n the correlation of a \\nkernel \\nw\\n of size \\nmn\\n×\\n with an image \\nfx y\\n(,\\n)\\n, denoted as \\n(w\\n/H22845\\nfx y\\n)( , ),\\n is given by \\nEq.\\n (3-31), which we repeat here for convenience:\\n \\n(w\\nw\\n/H22845\\nfx y s t f xs yt\\ntb\\nb\\nsa\\na\\n)( , )\\n( , ) ( , )\\n=+\\n+\\n=− =−\\n∑ ∑\\n \\n(3-34)\\nBecause our kernels do not depend on \\n(,) ,\\nxy\\n we will sometimes make this fact explic-\\nit by writing the left side of the preceding equation as \\nw\\n/H22845\\nfx y\\n(, ) .\\n Equation (3-34) is \\nevaluated for all values of the displacement variables \\nx\\n and \\ny\\n so that the center point \\nof \\nw\\n visits every pixel in \\nf\\n,\\n†\\n where we assume that \\nf\\n has been padded appropriately. \\n†', metadata={'source': 'imagepro.pdf', 'page': 159}),\n",
       " Document(page_content='†\\n  As we mentioned earlier, the \\nminimum\\n number of required padding elements for a 2-D correlation is \\n()\\nm\\n−\\n12\\n \\nrows above and below \\nf,\\n and \\n()\\nn\\n−\\n12\\n columns on the left and right. With this padding, and assuming that \\nf\\n \\nis of size \\nMN\\n×\\n,\\n the values of \\nx\\n and \\ny\\n required to obtain a complete correlation are \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n This assumes that the starting conﬁguration is such that the \\ncenter\\n of the kernel coincides \\nwith the \\norigin\\n of the image\\n, which we have deﬁned to be at the top, left (see Fig. 2.19). \\nRecall that \\nA\\n = 1 for a \\nunit impulse.\\n00000\\n00\\n1\\n00\\n00000\\n00000\\n00000\\n0000\\n0000\\n0000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\n0000\\n0000\\n0000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\nOrigin\\nRotated \\nw\\n0\\n123\\n0\\n0\\n456\\n0\\n0\\n789\\n0\\n00000\\n00000\\nConvolution result\\n0000000\\n0000000\\n00\\n987\\n00\\n00\\n654\\n00\\n00\\n321\\n00\\n0000000\\n0000000\\nFull correlation result\\n0\\n987\\n0\\n0\\n654\\n0\\n0\\n321\\n0\\n00000\\n00000\\nCorrelation result\\n456\\n789\\n123\\nw\\n(a)\\nPadded \\nf\\n0000000\\n0000000', metadata={'source': 'imagepro.pdf', 'page': 159}),\n",
       " Document(page_content='f\\n0000000\\n0000000\\n0000000\\n000\\n1\\n000\\n0000000\\n0000000\\n0000000\\n(b)\\n0000000\\n0000000\\n00\\n123\\n00\\n00\\n456\\n00\\n00\\n789\\n00\\n0000000\\n0000000\\nFull convolution result\\n(d)\\n(g)\\n(h)\\n(f)\\n(e)\\n(c)\\n456\\n789\\n123\\n654\\n321\\n987\\nInitial position for \\nw\\nf\\nFIGURE 3.30\\nCorrelation \\n(middle row) and \\nconvolution (last \\nrow) of a 2-D \\nkernel with an \\nimage consisting \\nof a discrete unit \\nimpulse. The 0’s \\nare shown in gray \\nto simplify visual \\nanalysis. Note that \\ncorrelation and \\nconvolution are \\nfunctions of \\nx\\n and \\ny\\n. As these  \\nvariable change,  \\nthey  \\ndisplace\\n one  \\nfunction with  \\nrespect to the \\nother. See the \\ndiscussion of Eqs. \\n(3-36) and (3-37) \\nregarding full \\ncorrelation and \\nconvolution.\\nDIP4E_GLOBAL_Print_Ready.indb   158\\n6/16/2017   2:03:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 159}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n159\\nAs explained earlier, \\nam\\n=−\\n() ,\\n12\\n \\nbn\\n=−\\n() ,\\n12\\n and we assume that \\nm\\n and \\nn\\n are \\nodd integers\\n. \\nIn a similar manner, the \\nconvolution\\n of a kernel \\nw\\n of size \\nmn\\n×\\n with an image \\nfx y\\n(,\\n) ,\\n denoted by \\n(w\\n/H22841\\nfx y\\n)(\\n,) ,\\n is deﬁned as\\n \\n(w\\nw\\n/H22841\\nfx y s t f xs yt\\ntb\\nb\\nsa\\na\\n)( , )\\n( , ) ( , )\\n=−\\n−\\n=− =−\\n∑ ∑\\n \\n(3-35)\\nwhere the minus signs align the coordinates of \\nf\\n and \\nw\\n when one of the functions is \\nrotated by \\n180\\n°\\n (see Problem 3.17). This equation implements the sum of products \\nprocess to which we refer throughout the book as \\nlinear spatial filtering\\n.\\n That is, lin-\\near spatial filtering and spatial convolution are synonymous.\\nBecause convolution is commutative (see Table 3.5), it is immaterial whether \\nw\\n \\nor \\nf \\nis rotated,\\n but rotation of the kernel is used by convention. Our kernels do not \\ndepend on \\n(,) ,\\nxy\\n a fact that we sometimes make explicit by writing the left side \\nof Eq.\\n (3-35) as \\nw', metadata={'source': 'imagepro.pdf', 'page': 160}),\n",
       " Document(page_content='(3-35) as \\nw\\n/H22841\\nfx y\\n(, ) .\\n When the meaning is clear, we let the dependence of \\nthe previous two equations on \\nx\\n and \\ny\\n be implied, and use the simpliﬁed notation \\nw\\n/H22845\\nf\\n and \\nw\\n/H22841\\nf\\n.\\n As with correlation, Eq. (3-35) is evaluated for all values of the \\ndisplacement variables \\nx\\n and \\ny\\n so that the center of \\nw\\n visits every pixel in \\nf\\n,\\n which \\nwe assume has been padded. The values of \\nx\\n and \\ny\\n needed to obtain a full convolu-\\ntion are \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n The size of the result is \\nMN\\n×\\n.\\nWe can deﬁne correlation and convolution so that \\nevery\\n element of \\nw\\n(instead of \\njust its center) visits \\nevery\\n pixel in \\nf\\n.\\n This requires that the starting conﬁguration be \\nsuch that the right, lower corner of the kernel coincides with the origin of the image. \\nSimilarly, the ending conﬁguration will be with the top left corner of the kernel coin-\\nciding with the lower right corner of the image. If the kernel and image are of sizes \\nmn\\n×', metadata={'source': 'imagepro.pdf', 'page': 160}),\n",
       " Document(page_content='mn\\n×\\n and \\nMN\\n×\\n,\\n respectively, the padding would have to increase to \\n()\\nm\\n−\\n1\\n pad-\\nding elements above and below the image\\n, and \\n()\\nn\\n−\\n1\\n elements to the left and right. \\nUnder these conditions\\n, the size of the resulting full correlation or convolution array \\nwill be of size \\nSS\\nvh\\n×\\n, where (see Figs. 3.30(e) and (h), and Problem 3.19),\\n \\nSm M\\nv\\n=+−\\n1  \\n(3-36)\\nand\\n \\nSn N\\nh\\n=+ −\\n1  \\n(3-37)\\nOften, spatial ﬁltering algorithms are based on correlation and thus implement \\nEq.\\n (3-34) instead. To use the algorithm for correlation, we input \\nw\\n into it; for con-\\nvolution,\\n we input \\nw\\n rotated by \\n180\\n°\\n.\\n The opposite is true for an algorithm that \\nimplements Eq.\\n (3-35). Thus, either Eq. (3-34) or Eq. (3-35) can be made to perform \\nthe function of the other by rotating the ﬁlter kernel. Keep in mind, however, that \\nthe \\norder\\n of the functions input into a correlation algorithm \\ndoes\\n make a difference, \\nbecause correlation is neither commutative nor associative (see Table 3.5).', metadata={'source': 'imagepro.pdf', 'page': 160}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   159\\n6/16/2017   2:03:42 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 160}),\n",
       " Document(page_content='160\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFigure 3.31 shows two kernels used for smoothing the intensities of an image. To \\nﬁlter an image using one of these kernels, we perform a convolution of the kernel \\nwith the image in the manner just described. When talking about ﬁltering and ker-\\nnels, you are likely to encounter the terms \\nconvolution ﬁlter\\n, \\nconvolution mask,\\n or \\nconvolution kernel\\n to denote ﬁlter kernels of the type we have been discussing. Typi-\\ncally, these terms are used in the literature to denote a spatial ﬁlter kernel, and not \\nto imply necessarily that the kernel is used for convolution. Similarly, “convolving a \\nkernel with an image” often is used to denote the sliding, sum-of-products process \\nwe just explained, and does not necessarily differentiate between correlation and \\nconvolution. Rather, it is used generically to denote either of the two operations.', metadata={'source': 'imagepro.pdf', 'page': 161}),\n",
       " Document(page_content='This imprecise terminology is a frequent source of confusion. In this book, when we \\nuse the term \\nlinear spatial ﬁltering\\n, we mean \\nconvolving a kernel with an image\\n.\\nSometimes an image is ﬁltered (i.e., convolved) sequentially, in stages, using a dif-\\nferent kernel in each stage. For example, suppose than an image \\nf\\n is ﬁltered with a \\nkernel \\nw\\n1\\n,\\n the result ﬁltered with kernel \\nw\\n2\\n,\\n that result ﬁltered with a third kernel, \\nand so on,\\n for \\nQ\\n stages. Because of the commutative property of convolution, this \\nmultistage ﬁltering can be done in a single ﬁltering operation, \\nw\\n/H22841\\nf\\n,\\n where\\n \\nww w w w\\n=\\n123\\n/H22841/H22841/H22841 /H22841\\n/midhorizellipsis\\nQ\\n \\n(3-38)\\nThe size of \\nw\\n is obtained from the sizes of the individual kernels by successive \\napplications of Eqs\\n. (3-36) and (3-37). If all the individual kernels are of size \\nmn\\n×\\n, \\nit follows from these equations that \\nw\\n will be of size \\nWW\\nvh\\n×\\n, where\\n \\nWQm m\\nv\\n=− +\\n×\\n()\\n1\\n \\n(3-39)\\nand\\n \\nWQn n\\nh\\n=− +\\n×\\n()\\n1', metadata={'source': 'imagepro.pdf', 'page': 161}),\n",
       " Document(page_content='h\\n=− +\\n×\\n()\\n1\\n \\n(3-40)\\nThese equations assume that every value of a kernel visits every value of the array \\nresulting from the convolution in the previous step\\n. That is, the initial and ending \\nconfigurations, are as described in connection with Eqs. (3-36) and (3-37).\\nBecause the values of \\nthese kernels are sym-\\nmetric about the center, \\nno rotation is required \\nbefore convolution.\\nWe could not write a \\nsimilar equation for  \\ncorrelation because it is \\nnot commutative.\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n9\\n/H11003\\n1.0000\\n0.3679 0.6065\\n0.3679\\n0.3679\\n0.3679\\n0.6065 0.6065\\n0.6065\\n/H11003\\n4.8976\\n1\\nb a\\nFIGURE 3.31\\nExamples of \\nsmoothing kernels: \\n(a) is a \\nbox\\n kernel; \\n(b) is a \\nGaussian\\n \\nkernel.\\nProperty\\nConvolution\\nCorrelation\\nCommutative\\nfggf\\n/H22841/H22841\\n=\\n—\\nAssociative\\nfg f g h\\nh\\n/H22841/H22841 /H22841/H22841\\n(\\n)\\n=\\n(\\n)\\n—\\nDistributive\\nfg h f g f h\\n/H22841/H22841 /H22841\\n+\\n(\\n)\\n=\\n(\\n)\\n+\\n(\\n)\\nfg h f g f h\\n/H22845/H22845 /H22845\\n+\\n(\\n)\\n=\\n(\\n)\\n+\\n(\\n)\\nTABLE \\n3.5\\nSome fundamen-\\ntal properties of', metadata={'source': 'imagepro.pdf', 'page': 161}),\n",
       " Document(page_content='tal properties of \\nconvolution and \\ncorrelation. A \\ndash means that \\nthe property does \\nnot hold.\\nDIP4E_GLOBAL_Print_Ready.indb   160\\n6/16/2017   2:03:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 161}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n161\\nSEPARABLE FILTER KERNELS\\nAs noted in Section 2.6, a 2-D function \\nGxy\\n(,\\n)\\n is said to be \\nseparable\\n if it can be written \\nas the product of two 1-D functions\\n, \\nGx\\n1\\n()\\n and \\nGx\\n2\\n() ;\\n that is, \\nGxy G xG y\\n(,\\n) () () .\\n=\\n12\\n \\nA spatial filter kernel is a matrix, and a separable kernel is a matrix that can be \\nexpressed as the outer product of two vectors. For example, the \\n23\\n*\\n kernel\\n \\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n111\\n111\\nis separable because it can be expressed as the outer product of the vectors\\n \\ncr\\n==\\n1\\n1\\n1\\n1\\n1\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\nan\\nd\\nThat is,\\n \\ncr\\nT\\n== =\\n1\\n1\\n111\\n111\\n111\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n[]\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nw\\n \\nA separable kernel of size \\nmn\\n×\\n can be expressed as the outer product of two vec-\\ntors\\n, \\nv\\n and \\nw\\n:\\n \\nw\\n=\\nvw\\nT\\n \\n(3-41)\\nwhere \\nv\\n and \\nw\\n are vectors of size \\nm\\n×\\n1\\n and \\nn\\n×\\n1,\\n respectively. For a square kernel \\nof size \\nmm\\n×\\n, we write\\n \\nw\\n=\\nvv\\nT\\n \\n(3-42)', metadata={'source': 'imagepro.pdf', 'page': 162}),\n",
       " Document(page_content='w\\n=\\nvv\\nT\\n \\n(3-42)\\nIt turns out that the product of a column vector and a row vector is the same as the \\n2-D convolution of the vectors (see Problem 3.24).\\n \\nThe importance of separable kernels lies in the computational advantages that \\nresult from the associative property of convolution. If we have a kernel \\nw\\n that can \\nbe decomposed into two simpler kernels\\n, such that \\nww w\\n12\\n=\\n/H22841\\n,\\n then it follows \\nfrom the commutative and associative properties in \\nTable 3.5 that \\n     \\nwf w w w w w w w w\\n/H22841/H22841 /H22841/H22841 /H22841 /H22841 /H22841 /H22841 /H22841\\n====\\n() () ) )\\n((\\n12 21 2 1 1 2\\nff f f\\n \\n(3-43)\\nThis equation says that convolving a separable kernel with an image is the same as \\nconvolving \\nw\\n1\\n with \\nf\\n  first, and then convolving the result with \\nw\\n2\\n. \\nF\\nor an image of size \\nMN\\n×\\n and a kernel of size \\nmn\\n×\\n,\\n implementation of Eq. \\n(3-35) requires on the order of \\nMNmn\\n multiplications and additions. This is because', metadata={'source': 'imagepro.pdf', 'page': 162}),\n",
       " Document(page_content='it follows directly from that equation that \\neac\\nh\\n pixel in the output (ﬁltered) image \\ndepends on \\nall\\n the coefﬁcients in the ﬁlter kernel. But, if the kernel is separable and \\nwe use Eq. (3-43), then the ﬁrst convolution, \\nw\\n1\\n/H22841\\nf\\n,\\n requires on the order of \\nMNm\\n \\nTo be strictly consistent \\nin notation, we should \\nuse uppercase, bold  \\nsymbols for kernels when \\nwe refer to them as  \\nmatrices. However,  \\nkernels are mostly \\ntreated in the book as \\n2-D functions, which we \\ndenote in italics. To avoid \\nconfusion, we continue \\nto use italics for kernels \\nin this short section, with \\nthe understanding that \\nthe two notations are \\nintended to be equivalent \\nin this case.\\nWe assume that the \\nvalues of \\nM\\n and \\nN\\n \\ninclude any padding of \\nf\\n prior to performing \\nconvolution.\\nDIP4E_GLOBAL_Print_Ready.indb   161\\n6/16/2017   2:03:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 162}),\n",
       " Document(page_content='162\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nmultiplications and additions because \\nw\\n1\\n is of size \\nm\\n×\\n1.\\n The result is of size \\nMN\\n×\\n, \\nso the convolution of \\nw\\n2\\n with the result requires \\nMNn\\n such operations, for a total of \\nMN m n\\n()\\n+\\n multiplication and addition operations. Thus, the \\ncomputational advan-\\ntage\\n of performing convolution with a separable\\n, as opposed to a nonseparable, ker-\\nnel is deﬁned as\\n \\nC\\nMNmn\\nMN m n\\nmn\\nmn\\n=\\n+\\n=\\n+\\n()\\n \\n(3-44)\\nFor a kernel of modest size, say \\n11 11\\n×\\n,\\n the computational advantage (and thus exe-\\ncution-time advantage) is a respectable 5.2.\\n For kernels with hundreds of elements, \\nexecution times can be reduced by a factor of a hundred or more, which is significant. \\nWe will illustrate the use of such large kernels in Example 3.16.\\nWe know from matrix theory that a matrix resulting from the product of a column \\nvector and a row vector \\nalways\\n has a rank of 1. By deﬁnition, a separable kernel is', metadata={'source': 'imagepro.pdf', 'page': 163}),\n",
       " Document(page_content='formed by such a product. Therefore, to determine if a kernel is separable, all we \\nhave to do is determine if its rank is 1. Typically, we ﬁnd the rank of a matrix using a \\npre-programmed function in the computer language being used. For example, if you \\nuse MATLAB, function \\nrank\\n will do the job.\\nOnce you have determined that the rank of a kernel matrix is 1, it is not difﬁcult \\nto ﬁnd two vectors \\nv\\n and \\nw\\n such that their outer product, \\nvw\\nT\\n,\\n is equal to the kernel. \\nT\\nhe approach consists of only three\\n \\nsteps:\\n1. \\nFind any nonzero element in the kernel and let \\nE\\n denote its value\\n.\\n2. \\nForm vectors \\nc\\n and \\nr\\n equal,\\n respectively, to the column and row in the kernel \\ncontaining the element found in Step 1.\\n3. \\nWith reference to Eq. (3-41), let \\nvc\\n=\\n and \\nwr\\nT\\nE\\n=\\n.\\nThe reason why this simple three-step method works is that the rows and columns \\nof a matrix whose rank is 1 are linearly dependent.\\n That is, the rows differ only by a', metadata={'source': 'imagepro.pdf', 'page': 163}),\n",
       " Document(page_content='constant multiplier, and similarly for the columns. It is instructive to work through \\nthe mechanics of this procedure using a small kernel (see Problems 3.20 and 3.22).\\nAs we explained above, the objective is to ﬁnd two 1-D kernels, \\nw\\n1\\n and \\nw\\n2\\n,\\n in \\norder to implement 1-D convolution.\\n In terms of the preceding notation, \\nw\\n1\\n==\\ncv\\n \\nand \\nw\\n2\\n==\\nrw\\nE\\nT\\n.\\n For circularly symmetric kernels, the column through the center \\nof the kernel describes the entire kernel;\\n that is, \\nw\\n=\\nvv\\nT\\nc\\n,\\n where \\nc\\n is the value of \\nthe center coefﬁcient.\\n Then, the 1-D components are \\nw\\n1\\n=\\nv\\n and \\nw\\n2\\n=\\nv\\nT\\nc\\n. \\nSOME IMPORTANT COMPARISONS BETWEEN FILTERING IN THE  \\nSPATIAL AND FREQUENCY DOMAINS\\nAlthough filtering in the frequency domain is the topic of Chapter 4, we introduce \\nat this junction some important concepts from the frequency domain that will help \\nyou master the material that follows. \\nThe tie between spatial- and frequency-domain processing is the \\nFourier trans-\\nform', metadata={'source': 'imagepro.pdf', 'page': 163}),\n",
       " Document(page_content='Fourier trans-\\nform\\n. We use the Fourier transform to go from the spatial to the frequency domain; \\nAs we will discuss later \\nin this chapter, the only \\nkernels that are sepa-\\nrable \\nand\\n whose values \\nare circularly symmetric \\nabout the center are \\nGaussian kernels, which \\nhave a nonzero center \\ncoefﬁcient (i.e., \\nc \\n>\\n \\n0 for \\nthese kernels).\\nDIP4E_GLOBAL_Print_Ready.indb   162\\n6/16/2017   2:03:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 163}),\n",
       " Document(page_content='3.4\\n  \\nFundamentals of Spatial Filtering\\n    \\n163\\nto return to the spatial domain we use the \\ninverse Fourier transform\\n. This will be \\ncovered in detail in Chapter 4. The focus here is on two fundamental properties \\nrelating the spatial and frequency domains:\\n1. \\nConvolution, which is the basis for ﬁltering in the spatial domain, is equivalent \\nto multiplication in the frequenc\\ny domain, and vice versa.\\n2. \\nAn impulse of strength \\nA\\n in the spatial domain is a constant of value \\nA\\n in the \\nfrequenc\\ny domain, and vice versa. \\nAs explained in Chapter 4, a function (e.g., an image) satisfying some mild condi-\\ntions can be expressed as the sum of sinusoids of different frequencies and ampli-\\ntudes. Thus, the \\nappearance\\n of an image depends on the frequencies of its sinusoidal \\ncomponents—change the frequencies of those components, and you will change the \\nappearance of the image. What makes this a powerful concept is that it is possible to', metadata={'source': 'imagepro.pdf', 'page': 164}),\n",
       " Document(page_content='associate certain frequency bands with image characteristics. For example, regions \\nof an image with intensities that vary slowly (e.g., the walls in an image of a room) \\nare characterized by sinusoids of low frequencies. Similarly, edges and other sharp \\nintensity transitions are characterized by high frequencies. Thus, reducing the high-\\nfrequency components of an image will tend to blur it.\\nLinear ﬁltering is concerned with ﬁnding suitable ways to modify the frequency \\ncontent of an image. In the spatial domain we do this via convolution ﬁltering. In \\nthe frequency domain we do it with multiplicative ﬁlters. The latter is a much more \\nintuitive approach, which is one of the reasons why it is virtually impossible to truly \\nunderstand spatial ﬁltering without having at least some rudimentary knowledge of \\nthe frequency domain. \\nAn example will help clarify these ideas. For simplicity, consider a 1-D func-', metadata={'source': 'imagepro.pdf', 'page': 164}),\n",
       " Document(page_content='tion (such as an intensity scan line through an image) and suppose that we want to \\neliminate all its frequencies above a cutoff value, \\nu\\n0\\n,\\n while “passing” all frequen-\\ncies below that value\\n. Figure 3.32(a)\\n \\nshows a frequency-domain ﬁlter function for \\ndoing this\\n. (The term \\nﬁlter transfer function\\n is used to denote ﬁlter functions in the \\nfrequency domain—this is analogous to our use of the term “ﬁlter kernel” in the \\nspatial domain.) Appropriately, the function in Fig. 3.32(a) is called a \\nlowpass\\n ﬁlter \\ntransfer function. In fact, this is an \\nideal\\n lowpass ﬁlter function because it eliminates \\nall\\n frequencies above \\nu\\n0\\n,\\n while passing all frequencies below this value.\\n†\\n That is, the \\n†\\n All the frequency domain ﬁlters in which we are interested are symmetrical about the origin and encompass \\nboth positive and negative frequencies, as we will explain in Section 4.3 (see Fig. 4.8). For the moment, we show', metadata={'source': 'imagepro.pdf', 'page': 164}),\n",
       " Document(page_content='only the right side (positive frequencies) of 1-D ﬁlters for simplicity in this short explanation. \\nSee the explanation of \\nEq. (3-33) regarding \\nimpulses. \\nAs we did earlier with \\nspatial ﬁlters, when the \\nmeaning is clear we use \\nthe term \\nﬁlter\\n inter-\\nchangeably with \\nﬁlter \\ntransfer function\\n when \\nworking in the frequency \\ndomain.\\n0\\nu\\nu\\nPassband\\nfrequency\\nStopband\\nFrequency domain\\n1\\nx\\nSpatial domain\\n0\\nu\\nb a\\nFIGURE 3.32\\n(a) Ideal 1-D low-\\npass ﬁlter transfer \\nfunction in the  \\nfrequency domain. \\n(b) Corresponding \\nﬁlter kernel in the \\nspatial domain.\\nDIP4E_GLOBAL_Print_Ready.indb   163\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 164}),\n",
       " Document(page_content='164\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\ntransition of the ﬁlter between low and high frequencies is instantaneous. Such ﬁlter \\nfunctions are not realizable with physical components, and have issues with “ringing” \\nwhen implemented digitally. However, ideal ﬁlters are very useful for illustrating \\nnumerous ﬁltering phenomena, as you will learn in Chapter 4.\\nTo lowpass-ﬁlter a spatial signal in the frequency domain, we ﬁrst convert it to the \\nfrequency domain by computing its Fourier transform, and then \\nmultiply\\n the result \\nby the ﬁlter transfer function in Fig. 3.32(a)\\n \\nto eliminate frequency components with \\nvalues higher than \\nu\\n0\\n.\\n To return to the spatial domain, we take the inverse Fourier \\ntransform of the ﬁltered signal.\\n The result will be a blurred spatial domain function.\\nBecause of the duality between the spatial and frequency domains, we can obtain \\nthe same result in the spatial domain by \\nconvolving\\n the equivalent spatial domain', metadata={'source': 'imagepro.pdf', 'page': 165}),\n",
       " Document(page_content='ﬁlter kernel with the input spatial function. The equivalent spatial ﬁlter kernel \\nis the inverse Fourier transform of the frequency-domain ﬁlter transfer function. \\nFigure 3.32(b) shows the spatial ﬁlter kernel corresponding to the frequency domain \\nﬁlter transfer function in Fig. 3.32(a). The ringing characteristics of the kernel are \\nevident in the ﬁgure. A central theme of digital ﬁlter design theory is obtaining faith-\\nful (and practical) approximations to the sharp cut off of ideal frequency domain \\nﬁlters while reducing their ringing characteristics.\\nA WORD ABOUT HOW SPATIAL FILTER KERNELS ARE CONSTRUCTED\\nWe consider three basic approaches for constructing spatial filters in the following \\nsections of this chapter. One approach is based on formulating filters based on \\nmathematical properties. For example, a filter that computes the average of pixels \\nin a neighborhood blurs an image. Computing an average is analogous to integra-', metadata={'source': 'imagepro.pdf', 'page': 165}),\n",
       " Document(page_content='tion. Conversely, a filter that computes the local derivative of an image sharpens the \\nimage. We give numerous examples of this approach in the following sections.\\nA second approach is based on sampling a 2-D spatial function whose shape has \\na desired property. For example, we will show in the next section that samples from \\na Gaussian function can be used to construct a weighted-average (lowpass) ﬁlter. \\nThese 2-D spatial functions sometimes are generated as the inverse Fourier trans-\\nform of 2-D ﬁlters speciﬁed in the frequency domain. We will give several examples \\nof this approach in this and the next chapter. \\nA third approach is to design a spatial ﬁlter with a speciﬁed frequency response. \\nThis approach is based on the concepts discussed in the previous section, and falls \\nin the area of digital ﬁlter design. A 1-D spatial ﬁlter with the desired response is \\nobtained (typically using ﬁlter design software). The 1-D ﬁlter values can be expressed \\nas a vector \\nv', metadata={'source': 'imagepro.pdf', 'page': 165}),\n",
       " Document(page_content='as a vector \\nv\\n, and a 2-D separable kernel can then be obtained using Eq. (3-42). Or the \\n1-D ﬁlter can be rotated about its center to generate a 2-D kernel that approximates a \\ncircularly symmetric function. We will illustrate these techniques in Section 3.7.\\n3.5 SMOOTHING (LOWPASS) SPATIAL FILTERS  \\nSmoothing\\n (also called \\naveraging\\n) spatial filters are used to reduce sharp transi-\\ntions in intensity. Because random noise typically consists of sharp transitions in \\n3.5\\nDIP4E_GLOBAL_Print_Ready.indb   164\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 165}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n165\\nintensity, an obvious application of smoothing is noise reduction. Smoothing prior \\nto image resampling to reduce aliasing, as will be discussed in Section 4.5, is also \\na common application. Smoothing is used to reduce irrelevant detail in an image, \\nwhere “irrelevant” refers to pixel regions that are small with respect to the size of \\nthe filter kernel. Another application is for smoothing the false contours that result \\nfrom using an insufficient number of intensity levels in an image, as discussed in Sec-\\ntion 2.4. Smoothing filters are used in combination with other techniques for image \\nenhancement, such as the histogram processing techniques discussed in Section 3.3, \\nand unsharp masking, as discussed later in this chapter. We begin the discussion \\nof smoothing filters by considering linear smoothing filters in some detail. We will \\nintroduce nonlinear smoothing filters later in this section.', metadata={'source': 'imagepro.pdf', 'page': 166}),\n",
       " Document(page_content='As we discussed in Section 3.4, linear spatial ﬁltering consists of convolving an \\nimage with a ﬁlter kernel. Convolving a smoothing kernel with an image blurs the \\nimage, with the degree of blurring being determined by the size of the kernel and \\nthe values of its coefﬁcients. In addition to being useful in countless applications of \\nimage processing, lowpass ﬁlters are fundamental, in the sense that other impor-\\ntant ﬁlters, including sharpening (highpass), bandpass, and bandreject ﬁlters, can be \\nderived from lowpass ﬁlters, as we will show in Section 3.7.\\nWe discuss in this section lowpass ﬁlters based on \\nbox\\n and \\nGaussian\\n kernels, \\nboth of which are separable. Most of the discussion will center on Gaussian kernels \\nbecause of their numerous useful properties and breadth of applicability. We will \\nintroduce other smoothing ﬁlters in Chapters 4 and 5.\\nBOX FILTER KERNELS\\nThe simplest, separable lowpass filter kernel is the \\nbox kernel\\n, whose coefficients', metadata={'source': 'imagepro.pdf', 'page': 166}),\n",
       " Document(page_content='have the same value (typically 1). The name “box kernel” comes from a constant \\nkernel resembling a box when viewed in 3-D. We showed a \\n33\\n×\\n box filter in Fig. \\n3.31(a).\\n An \\nmn\\n×\\n box filter is an \\nmn\\n×\\n array of 1’s, with a normalizing constant in \\nfront,\\n whose value is 1 divided by the sum of the values of the coefficients (i.e.,  \\n1\\nmn\\n \\nwhen all the coefficients are 1’\\ns). This normalization, which we apply to all lowpass \\nkernels, has two purposes. First, the average value of an area of constant intensity \\nwould equal that intensity in the filtered image, as it should. Second, normalizing \\nthe kernel in this way prevents introducing a \\nbias\\n during filtering; that is, the sum \\nof the pixels in the original and filtered images will be the same (see Problem 3.31). \\nBecause in a box kernel all rows and columns are identical, the rank of these kernels \\nis 1, which, as we discussed earlier, means that they are separable.\\nEXAMPLE 3.11 :  Lowpass ﬁltering with a box kernel.', metadata={'source': 'imagepro.pdf', 'page': 166}),\n",
       " Document(page_content='Figure 3.33(a) shows a test pattern image of size \\n1024 1024\\n×\\n pixels. Figures 3.33(b)-(d) are the results \\nobtained using box ﬁlters of size\\n \\nmm\\n×\\n with \\nm\\n=\\n31\\n1\\n,,\\n and 21, respectively. For \\nm\\n=\\n3,\\n we note a slight \\noverall blurring of the image\\n, with the image features whose sizes are comparable to the size of the \\nkernel being affected signiﬁcantly more. Such features include the thinner lines in the image and the \\nnoise pixels contained in the boxes on the right side of the image. The ﬁltered image also has a thin gray \\nborder, the result of zero-padding the image prior to ﬁltering. As indicated earlier, padding extends the \\nboundaries of an image to avoid undeﬁned operations when parts of a kernel lie outside the border of \\nDIP4E_GLOBAL_Print_Ready.indb   165\\n6/16/2017   2:03:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 166}),\n",
       " Document(page_content='166\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nthe image during ﬁltering. When zero (black) padding is used, the net result of smoothing at or near the \\nborder is a dark gray border that arises from including black pixels in the averaging process. Using the \\n11 11\\n×\\n kernel resulted in more pronounced blurring throughout the image, including a more prominent \\ndark border\\n. The result with the \\n21 21\\n×\\n kernel shows signiﬁcant blurring of all components of the image, \\nincluding the loss of the characteristic shape of some components\\n, including, for example, the small \\nsquare on the top left and the small character on the bottom left. The dark border resulting from zero \\npadding is proportionally thicker than before. We used zero padding here, and will use it a few more \\ntimes, so that you can become familiar with its effects. In Example 3.14 we discuss two other approaches', metadata={'source': 'imagepro.pdf', 'page': 167}),\n",
       " Document(page_content='to padding that eliminate the dark-border artifact that usually results from zero padding. \\nLOWPASS GAUSSIAN FILTER KERNELS\\nBecause of their simplicity, box filters are suitable for quick experimentation and \\nthey often yield smoothing results that are visually acceptable. They are useful also \\nwhen it is desired to reduce the effect of smoothing on edges (see Example 3.13). \\nHowever, box filters have limitations that make them poor choices in many appli-\\ncations. For example, a defocused lens is often modeled as a lowpass filter, but \\nbox filters are poor approximations to the blurring characteristics of lenses (see \\nProblem 3.33). Another limitation is the fact that box filters favor blurring along \\nperpendicular directions. In applications involving images with a high level of detail, \\nb a\\nd c\\nFIGURE 3.33\\n(a) Test pattern of \\nsize \\n1024 1024\\n×\\n \\npixels\\n.  \\n(b)-(d) Results of \\nlowpass ﬁltering \\nwith box kernels \\nof sizes \\n33\\n×\\n, \\n11 11\\n×\\n,  \\nand \\n21 21\\n×\\n,  \\nrespectively\\n.', metadata={'source': 'imagepro.pdf', 'page': 167}),\n",
       " Document(page_content=',  \\nrespectively\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   166\\n6/16/2017   2:03:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 167}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n167\\nor with strong geometrical components, the directionality of box filters often pro-\\nduces undesirable results. (Example 3.13 illustrates this issue.) These are but two \\napplications in which box filters are not suitable.\\nThe kernels of choice in applications such as those just mentioned are \\ncircularly \\nsymmetric\\n (also called \\nisotropic\\n, meaning their response is independent of orienta-\\ntion). As it turns out, Gaussian kernels of the form\\n \\nw\\n(,) (,)\\nst Gst K e\\nst\\n==\\n−\\n+\\n22\\n2\\n2\\ns\\n \\n(3-45)\\nare the \\nonly\\n circularly symmetric kernels that are also separable (Sahoo [1990]).\\n \\nThus, because Gaussian kernels of this form are separable, Gaussian filters enjoy the \\nsame computational advantages as box filters, but have a host of additional proper-\\nties that make them ideal for image processing, as you will learn in the following \\ndiscussion. Variables \\ns\\n and \\nt\\n in Eq. (3-45), are real (typically discrete) numbers. \\nBy letting \\nrs t\\n=+\\n[]', metadata={'source': 'imagepro.pdf', 'page': 168}),\n",
       " Document(page_content='rs t\\n=+\\n[]\\n22\\n12\\n we can write Eq. (3-45) as\\n \\nGr K e\\nr\\n()\\n=\\n−\\n2\\n2\\n2\\ns\\n(3-46)\\nThis equivalent form simplifies derivation of expressions later in this section. This \\nform also reminds us that the function is circularly symmetric\\n. Variable \\nr\\n is the dis-\\ntance from the center to any point on function \\nG\\n. Figure 3.34 shows values of \\nr\\n for \\nseveral kernel sizes using integer values for \\ns\\n and \\nt\\n. Because we work generally with \\nodd kernel sizes, the centers of such kernels fall on integer values, and it follows that \\nall values of \\nr\\n2\\n are integers also. You can see this by squaring the values in Fig. 3.34 \\nOur interest here is \\nstrictly on the bell shape \\nof the Gaussian function; \\nthus, we dispense with \\nthe traditional multiplier \\nof the Gaussian PDF and \\nuse a general constant, \\nK\\n, instead. Recall that \\ns \\ncontrols the “spread” of a \\nGaussian function about \\nits mean.\\nFIGURE 3.34\\nDistances from \\nthe center for  \\nvarious sizes of \\nsquare kernels.\\n0\\n1\\n2\\n22\\n32\\n42\\n5\\n10\\n17\\n13\\n2\\n3', metadata={'source': 'imagepro.pdf', 'page': 168}),\n",
       " Document(page_content='42\\n5\\n10\\n17\\n13\\n2\\n3\\n4\\n1234\\n25\\n5\\n5\\n10\\n13\\n17\\n25 5\\n1 2 3 4\\n1\\n2\\n3\\n4\\n42\\n17\\n25 5\\n32 5\\n10\\n13\\n22\\n13\\n25\\n5\\n2\\n5\\n10\\n17\\n2\\n5\\n10\\n17\\n22\\n13\\n25\\n5\\n32\\n5\\n10\\n13\\n42\\n17\\n25\\n5\\n2\\n5\\n10\\n17\\n22\\n13\\n25\\n5\\n32\\n5\\n10\\n13\\n42\\n17\\n25\\n5\\n. . . . .\\n. . . . .\\n. . . . .\\n. . . . .\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n2\\n1\\n2\\nm\\n()\\n−\\n*\\nmm\\n9\\n*\\n9\\n33\\n*\\n55\\n*\\n77\\n*\\nDIP4E_GLOBAL_Print_Ready.indb   167\\n6/16/2017   2:03:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 168}),\n",
       " Document(page_content='168\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n(for a formal proof, see Padfield [2011]). Note in particular that the distance squared \\nto the corner points for a kernel of size \\nmm\\n×\\n is\\n \\nr\\nmm\\nmax\\n() ()\\n2\\n2\\n2\\n1\\n2\\n2\\n1\\n2\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n--\\n \\n(3-47)\\nThe kernel in Fig. 3.31(b) was obtained by sampling Eq. (3-45) (with \\nK\\n=\\n1\\n and \\ns\\n=\\n1\\n). Figure 3.35(a) shows a perspective plot of a Gaussian function, and illustrates \\nthat the samples used to generate that kernel were obtained by specifying values of \\ns\\n and \\nt,\\n then \\n“reading” the values of the function at those coordinates. These values \\nare the coefﬁcients of the kernel. Normalizing the kernel by dividing its coefﬁcients \\nby the sum of the coefﬁcients completes the speciﬁcation of the kernel. The reasons \\nfor normalizing the kernel are as discussed in connection with box kernels. Because \\nGaussian kernels are separable, we could simply take samples along a cross section', metadata={'source': 'imagepro.pdf', 'page': 169}),\n",
       " Document(page_content='through the center and use the samples to form vector \\nv\\n in Eq. (3-42), from which \\nwe obtain the 2-D kernel.\\nSeparability is one of many fundamental properties of circularly symmetric \\nGaussian kernels. For example, we know that the values of a Gaussian function at a \\ndistance larger than \\n3\\ns\\n from the mean are small enough that they can be ignored. \\nT\\nhis means that if we select the size of a Gaussian kernel to be \\nLMLM\\n66\\nss\\n×\\n (the nota-\\ntion \\nLM\\nc\\n is used to denote the \\nceiling\\n of \\nc\\n;\\n that is, the smallest integer not less than \\nc\\n), we are assured of getting essentially the same result as if we had used an arbi-\\ntrarily large Gaussian kernel. Viewed another way, this property tells us that there \\nis nothing to be gained by using a Gaussian kernel larger than \\nLMLM\\n66\\nss\\n×\\n for image \\nprocessing\\n. Because typically we work with kernels of odd dimensions, we would use \\nthe smallest \\nodd\\n integer that satisﬁes this condition (e.g., a \\n43 43\\n×\\n kernel if \\ns\\n=\\n7).\\n \\nT', metadata={'source': 'imagepro.pdf', 'page': 169}),\n",
       " Document(page_content='s\\n=\\n7).\\n \\nT\\nwo other fundamental properties of Gaussian functions are that the product \\nand convolution of two Gaussians are Gaussian functions also. Table 3.6 shows the \\nmean and standard deviation of the product and convolution of two 1-D Gaussian \\nfunctions, \\nf\\n and \\ng \\n(remember, because of separability, we only need a 1-D Gauss-\\nian to form a circularly symmetric 2-D function). The mean and standard deviation \\nSmall Gaussian kernels \\ncannot capture the char-\\nacteristic Gaussian bell \\nshape, and thus behave \\nmore like box kernels. As \\nwe discuss below, a prac-\\ntical size for Gaussian \\nkernels is on the order of \\n6\\ns\\n/H11003\\n6\\ns\\n.\\nAs we explained in \\nSection 2.6, the symbols \\n<\\n⋅\\n= \\nand \\n:\\n⋅\\n;\\n denote the \\nceiling\\n \\nand \\nfloor\\n func-\\ntions\\n. That is, the ceiling \\nand floor functions map \\na real number to the \\nsmallest following, or the \\nlargest previous, integer, \\nrespectively.\\nProofs of the results in \\nTable 3.6 are simpliﬁed \\nby working with the  \\nFourier transform and', metadata={'source': 'imagepro.pdf', 'page': 169}),\n",
       " Document(page_content='the frequency domain, \\nboth of which are topics \\nin Chapter 4.\\n0.3679 0.6065 0.3679\\n1.0000\\n0.6065 0.6065\\n0.3679\\n0.3679 0.6065\\n/H11003\\n4.8976\\n1\\ns\\nt\\n1\\n1\\n1\\n/H11002\\n1\\nG\\n(\\ns\\n, \\nt\\n)\\nb a\\nFIGURE 3.35\\n(a) Sampling a  \\nGaussian function \\nto obtain a discrete  \\nGaussian kernel. \\nThe values shown \\nare for \\nK\\n=\\n1 and \\ns\\n=\\n1. (b) Resulting \\n33\\n×\\n kernel [this \\nis the same as Fig. \\n3.31(b)]. \\nDIP4E_GLOBAL_Print_Ready.indb   168\\n6/16/2017   2:03:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 169}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n169\\ncompletely deﬁne a Gaussian, so the parameters in Table 3.6 tell us all there is to \\nknow about the functions resulting from multiplication and convolution of Gauss-\\nians. As indicated by Eqs. (3-45) and (3-46), Gaussian kernels have zero mean, so our \\ninterest here is in the standard deviations. \\nThe convolution result is of particular importance in ﬁltering. For example, we \\nmentioned in connection with Eq. (3-43) that ﬁltering sometimes is done in succes-\\nsive stages, and that the same result can be obtained by one stage of ﬁltering with a \\ncomposite kernel formed as the convolution of the individual kernels. If the kernels \\nare Gaussian, we can use the result in Table 3.6 (which, as noted, generalizes directly \\nto more than two functions) to compute the standard deviation of the composite \\nkernel (and thus completely deﬁne it) without actually having to perform the con-\\nvolution of all the individual kernels.', metadata={'source': 'imagepro.pdf', 'page': 170}),\n",
       " Document(page_content='EXAMPLE 3.12 :  Lowpass ﬁltering with a Gaussian kernel.\\nTo compare Gaussian and box kernel ﬁltering, we repeat Example 3.11 using a Gaussian kernel.  Gauss-\\nian kernels have to be larger than box ﬁlters to achieve the same degree of blurring. This is because, \\nwhereas a box kernel assigns the same weight to all pixels, the values of Gaussian kernel coefﬁcients \\n(and hence their effect) decreases as a function of distance from the kernel center. As explained earlier, \\nwe use a size equal to the closest odd integer to \\nLMLM\\n66\\nss\\n×\\n.\\n Thus, for a Gaussian kernel of size \\n21 21\\n×\\n, \\nwhich is the size of the kernel we used to generate F\\nig. 3.33(d), we need \\ns\\n=\\n35\\n..\\n Figure 3.36(b) shows the \\nresult of lowpass ﬁltering the test pattern with this kernel.\\n Comparing this result with Fig. 3.33(d), we see \\nthat the Gaussian kernel resulted in signiﬁcantly less blurring. A little experimentation would show that \\nwe need \\ns\\n=\\n7', metadata={'source': 'imagepro.pdf', 'page': 170}),\n",
       " Document(page_content='we need \\ns\\n=\\n7\\n to obtain comparable results. This implies a Gaussian kernel of size \\n43 43\\n×\\n.\\n Figure 3.36(c) \\nshows the results of ﬁltering the test pattern with this kernel.\\n Comparing it with Fig. 3.33(d), we see that \\nthe results indeed are very close. \\nWe mentioned earlier that there is little to be gained by using a Gaussian kernel larger than \\nLMLM\\n66\\nss\\n×\\n. \\nT\\no demonstrate this, we ﬁltered the test pattern in Fig. 3.36(a) using a Gaussian kernel with \\ns\\n=\\n7\\n again, \\nbut of size \\n85 85\\n×\\n.\\n Figure 3.37(a) is the same as Fig. 3.36(c), which we generated using the smallest \\nodd kernel satisfying the \\nLM LM\\n66\\n×\\n condition (\\n43 43\\n×\\n,\\n for \\ns\\n=\\n7).\\n Figure 3.37(b) is the result of using the \\n85 85\\n×\\n kernel, which is double the size of the other kernel. As you can see, not discernible additional \\nfg\\nfg\\n×\\nf\\ng\\n/H22841\\nMean\\nStandard deviation\\ns\\nf\\ns\\ng\\nm\\nf\\nm\\ng\\nm\\nmm\\nfg\\nfg\\ngf\\nfg\\n×\\n=\\n+\\n+\\nss\\nss\\n22\\n22\\nmm m\\nfg f g\\n/H22841\\n=+\\ns\\nss\\nss\\nfg\\nfg\\nfg\\n×\\n=\\n+\\n22\\n22\\nss s\\nfg f g\\n/H22841\\n=+\\n22\\nTABLE', metadata={'source': 'imagepro.pdf', 'page': 170}),\n",
       " Document(page_content='=+\\n22\\nTABLE \\n3.6\\n \\nMean and standard deviation of the product \\n()\\n×\\n and convolution \\n(\\n)\\n/H22841\\n of two 1-D Gaussian functions, \\nf\\n \\nand \\ng\\n. These results generalize directly to the product and convolution of more than two 1-D Gaussian functions \\n(see Problem 3.25).\\nDIP4E_GLOBAL_Print_Ready.indb   169\\n6/16/2017   2:03:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 170}),\n",
       " Document(page_content='170\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nblurring occurred. In fact, the difference image in Fig 3.37(c) indicates that the two images are nearly \\nidentical, their maximum difference being 0.75, which is less than one level out of 256 (these are 8-bit \\nimages). \\n EXAMPLE 3.13 :  Comparison of Gaussian and box ﬁlter smoothing characteristics.\\nThe results in Examples 3.11 and 3.12 showed little visual difference in blurring. Despite this, there are \\nsome subtle differences that are not apparent at ﬁrst glance. For example, compare the large letter “a” \\nin Figs. 3.33(d) and 3.36(c); the latter is much smoother around the edges. Figure 3.38 shows this type \\nof different behavior between box and Gaussian kernels more clearly. The image of the rectangle was \\nb a\\nc\\n \\nFIGURE 3.36\\n  (a)A test pattern of size \\n1024 1024\\n×\\n. (b) Result of lowpass ﬁltering the pattern with a Gaussian kernel \\nof size \\n21 21\\n×\\n,\\n with standard deviations \\ns\\n=\\n35\\n..', metadata={'source': 'imagepro.pdf', 'page': 171}),\n",
       " Document(page_content='s\\n=\\n35\\n..\\n (c) Result of using a kernel of size \\n43 43\\n×\\n,\\n with \\ns\\n=\\n7.\\n This result \\nis comparable to F\\nig. 3.33(d). We used \\nK\\n=\\n1 in all cases.\\nb a\\nc\\nFIGURE 3.37\\n (a) Result of ﬁltering Fig. 3.36(a) using a Gaussian kernels of size \\n43 43\\n×\\n,\\n with \\ns\\n=\\n7.\\n (b) Result of using \\na kernel of \\n85 85\\n×\\n, with the same value of \\ns\\n. (c) Difference image.\\nDIP4E_GLOBAL_Print_Ready.indb   170\\n6/16/2017   2:03:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 171}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n171\\nsmoothed using a box and a Gaussian kernel with the sizes and parameters listed in the ﬁgure. These \\nparameters were selected to give blurred rectangles of approximately the same width and height, in \\norder to show the effects of the ﬁlters on a comparable basis. As the intensity proﬁles show, the box ﬁlter \\nproduced linear smoothing, with the transition from black to white (i.e., at an edge) having the shape \\nof a ramp. The important features here are hard transitions at the onset and end of the ramp. We would \\nuse this type of ﬁlter when less smoothing of edges is desired. Conversely, the Gaussian ﬁlter yielded \\nsigniﬁcantly smoother results around the edge transitions. We would use this type of ﬁlter when gener-\\nally uniform smoothing is desired.\\nAs the results in Examples 3.11, 3.12, and 3.13 show, zero padding an image intro-\\nduces dark borders in the ﬁltered result, with the thickness of the borders depending', metadata={'source': 'imagepro.pdf', 'page': 172}),\n",
       " Document(page_content='on the size and type of the ﬁlter kernel used. Earlier, when discussing correlation \\nand convolution, we mentioned two other methods of image padding: \\nmirror \\n(also \\ncalled\\n symmetric\\n) \\npadding\\n, in which values outside the boundary of the image are \\nobtained by mirror-reﬂecting the image across its border; and \\nreplicate padding\\n, in \\nwhich values outside the boundary are set equal to the nearest image border value. \\nThe latter padding is useful when the areas near the border of the image are con-\\nstant. Conversely, mirror padding is more applicable when the areas near the border \\ncontain image details. In other words, these two types of padding attempt to “extend” \\nthe characteristics of an image past its borders. \\nFigure 3.39 illustrates these padding methods, and also shows the effects of more \\naggressive smoothing. Figures 3.39(a) through 3.39(c) show the results of ﬁltering \\nFig. 3.36(a) with a Gaussian kernel of size \\n187 187\\n×\\n elements with \\nK\\n=\\n1\\n and \\ns\\n=\\n31\\n,', metadata={'source': 'imagepro.pdf', 'page': 172}),\n",
       " Document(page_content='1\\n and \\ns\\n=\\n31\\n,\\n \\nusing zero\\n, mirror, and replicate padding, respectively. The differences between the \\nborders of the results with the zero-padded image and the other two are obvious, \\nb a\\nc\\n \\nFIGURE 3.38\\n (a) Image of a white rectangle on a black background, and a horizontal intensity proﬁle along the scan \\nline shown dotted. (b) Result of smoothing this image with a box kernel of size \\n71 71\\n×\\n,\\n and corresponding intensity \\nproﬁle\\n. (c) Result of smoothing the image using a Gaussian kernel of size \\n151 151\\n×\\n,\\n with \\nK\\n=\\n1\\n and \\ns\\n=\\n25.\\n Note \\nthe smoothness of the proﬁle in (c) compared to (b).\\n The image and rectangle are of sizes \\n1024 1024\\n×\\n and \\n768 128\\n×\\n \\npixels\\n, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   171\\n6/16/2017   2:03:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 172}),\n",
       " Document(page_content='172\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nand indicate that mirror and replicate padding yield more visually appealing results \\nby eliminating the dark borders resulting from zero padding.\\nEXAMPLE 3.14 :  Smoothing performance as a function of kernel and image size.\\nThe amount of relative blurring produced by a smoothing kernel of a given size depends directly on \\nimage size. To illustrate, Fig. 3.40(a) shows the same test pattern used earlier, but of size \\n4096 4096\\n×\\n \\npixels\\n, four times larger in each dimension than before. Figure 3.40(b) shows the result of ﬁltering this \\nimage with the same Gaussian kernel and padding used in Fig. 3.39(b). By comparison, the former \\nimage shows considerably less blurring for the same size ﬁlter. In fact, Fig. 3.40(b) looks more like the \\nb a\\nc\\nFIGURE 3.39\\n Result of ﬁltering the test pattern in Fig. 3.36(a) using (a) zero padding, (b) mirror padding, and (c) rep-\\nlicate padding. A Gaussian kernel of size \\n187 187\\n×', metadata={'source': 'imagepro.pdf', 'page': 173}),\n",
       " Document(page_content='187 187\\n×\\n, with \\nK\\n=\\n1 and \\ns\\n=\\n31\\n was used in all three cases.\\nb a\\nc\\nFIGURE 3.40\\n (a) Test pattern of size \\n4096 4096\\n×\\n pixels. (b) Result of ﬁltering the test pattern with the same Gaussian \\nkernel used in F\\nig. 3.39. (c) Result of ﬁltering the pattern using a Gaussian kernel of size \\n745 745\\n×\\n elements, with \\nK\\n=\\n1 and \\ns\\n=\\n124.\\n Mirror padding was used throughout. \\nDIP4E_GLOBAL_Print_Ready.indb   172\\n6/16/2017   2:03:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 173}),\n",
       " Document(page_content='3.5\\n  \\nSmoothing (Lowpass) Spatial Filters\\n    \\n173\\nimage in Fig. 3.36(d), which was ﬁltered using a \\n43 43\\n×\\n Gaussian kernel. In order to obtain results that \\nare comparable to F\\nig. 3.39(b) we have to increase the size and standard deviation of the Gaussian \\nkernel by four, the same factor as the increase in image dimensions. This gives a kernel of (odd) size \\n745 745\\n×\\n (with \\nK\\n=\\n1\\n and \\ns\\n=\\n124).\\n Figure 3.40(c) shows the result of using this kernel with mirror pad-\\nding\\n. This result is quite similar to Fig. 3.39(b). After the fact, this may seem like a trivial observation, but \\nyou would be surprised at how frequently not understanding the relationship between kernel size and \\nthe size of objects in an image can lead to ineffective performance of spatial ﬁltering algorithms. \\nEXAMPLE 3.15 :  Using lowpass ﬁltering and thresholding for region extraction.\\nFigure 3.41(a) is a \\n2566 2758\\n×\\n Hubble Telescope image of the \\nHic\\nkson Compact Group\\n (see ﬁgure', metadata={'source': 'imagepro.pdf', 'page': 174}),\n",
       " Document(page_content='(see ﬁgure \\ncaption), whose intensities were scaled to the range \\n[,] .\\n01\\n Our objective is to illustrate lowpass ﬁltering \\ncombined with intensity thresholding for eliminating irrelevant detail in this image\\n. In the present con-\\ntext, “irrelevant” refers to pixel regions that are small compared to kernel size.\\nFigure 3.41(b) is the result of ﬁltering the original image with a Gaussian kernel of size \\n151 151\\n×\\n \\n(approximately 6% of the image width) and standard deviation \\ns\\n=\\n25\\n. We chose these parameter val-\\nues in order generate a sharper\\n, more selective Gaussian kernel shape than we used in earlier examples. \\nThe ﬁltered image shows four predominantly bright regions. We wish to extract only those regions from \\nthe image. Figure 3.41(c) is the result of thresholding the ﬁltered image with a threshold \\nT\\n=\\n04\\n.\\n (we will \\ndiscuss threshold selection in\\n \\nChapter 10). As the ﬁgure shows, this approach effectively extracted the \\nfour regions of interest,', metadata={'source': 'imagepro.pdf', 'page': 174}),\n",
       " Document(page_content='and eliminated details deemed irrelevant in this application.\\nEXAMPLE 3.16 :  Shading correction using lowpass ﬁltering.\\nOne of the principal causes of image shading is nonuniform illumination. \\nShading correction\\n (also \\ncalled \\nﬂat-ﬁeld correction\\n) is important because shading is a common cause of erroneous measurements, \\ndegraded performance of automated image analysis algorithms, and difﬁculty of image interpretation \\nb a\\nc\\nFIGURE 3.41\\n (a) A \\n2566 2758\\n×\\n Hubble Telescope image of the \\nHic\\nkson Compact Group\\n. (b) Result of lowpass ﬁlter-\\ning with a Gaussian kernel. (c) Result of thresholding the ﬁltered image (intensities were scaled to the range [0, 1]). \\nThe Hickson Compact Group contains dwarf galaxies that have come together, setting off thousands of new star \\nclusters. (Original image courtesy of NASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   173\\n6/16/2017   2:03:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 174}),\n",
       " Document(page_content='174\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nby humans. We introduced shading correction in Example 2.7, where we corrected a shaded image by \\ndividing it by the shading pattern. In that example, the shading pattern was given. Often, that is not the \\ncase in practice, and we are faced with having to estimate the pattern directly from available samples of \\nshaded images. Lowpass ﬁltering is a rugged, simple method for estimating shading patterns.\\nConsider the \\n2048 2048\\n×\\n checkerboard image in Fig. 3.42(a), whose inner squares are of size \\n128 128\\n×\\n \\npixels\\n. Figure 3.42(b) is the result of lowpass ﬁltering the image with a \\n512 512\\n×\\n Gaussian kernel (four \\ntimes the size of the squares),\\n \\nK\\n=\\n1,\\n and \\ns\\n=\\n128\\n (equal to the size of the squares). This kernel is just \\nlarge enough to blur\\n-out the squares (a kernel three times the size of the squares is too small to blur', metadata={'source': 'imagepro.pdf', 'page': 175}),\n",
       " Document(page_content='them out sufﬁciently). This result is a good approximation to the shading pattern visible in Fig. 3.42(a). \\nFinally, Fig. 3.42(c) is the result of dividing (a) by (b). Although the result is not perfectly ﬂat, it deﬁnitely \\nis an improvement over the shaded image. \\nIn the discussion of separable kernels in Section 3.4, we pointed out that the computational advan-\\ntage of separable kernels can be signiﬁcant for large kernels. It follows from Eq. (3-44) that the compu-\\ntational advantage of the kernel used in this example (which of course is separable) is 262 to 1. Thinking \\nof computation time, if it took 30 sec to process a set of images similar to Fig. 3.42(b) using the two 1-D \\nseparable components of the Gaussian kernel, it would have taken 2.2 hrs to achieve the same result \\nusing a nonseparable lowpass kernel, or if we had used the 2-D Gaussian kernel directly, without decom-\\nposing it into its separable parts.\\nORDER-STATISTIC (NONLINEAR) FILTERS', metadata={'source': 'imagepro.pdf', 'page': 175}),\n",
       " Document(page_content='Order-statistic filters are nonlinear spatial filters whose response is based on ordering \\n(ranking) the pixels contained in the region encompassed by the filter. Smoothing is \\nachieved by replacing the value of the center pixel with the value determined by the \\nranking result. The best-known filter in this category is the \\nmedian filter\\n, which, as \\nits name implies, replaces the value of the center pixel by the median of the intensity \\nvalues in the neighborhood of that pixel (the value of the center pixel is included \\nb a\\nc\\nFIGURE 3.42\\n (a) Image shaded by a shading pattern oriented in the \\n−\\n45\\n°\\n direction. (b) Estimate of the shading \\npatterns obtained using lowpass ﬁltering\\n. (c) Result of dividing (a) by (b). (See Section 9.8 for a morphological \\napproach to shading correction).\\nDIP4E_GLOBAL_Print_Ready.indb   174\\n6/16/2017   2:03:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 175}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n175\\nin computing the median). Median filters provide excellent noise reduction capa-\\nbilities for certain types of random noise, with considerably less blurring than lin-\\near smoothing filters of similar size. Median filters are particularly effective in the \\npresence of \\nimpulse noise\\n (sometimes called \\nsalt-and-pepper noise,\\n when it manis-\\nfests itself as white and black dots superimposed on an image).\\nThe \\nmedian\\n, \\nj\\n,\\n of a set of values is such that half the values in the set are less than \\nor equal to \\nj\\n and half are greater than or equal to \\nj\\n.\\n In order to perform median \\nﬁltering at a point in an image\\n, we ﬁrst sort the values of the pixels in the neighbor-\\nhood, determine their median, and assign that value to the pixel in the ﬁltered image \\ncorresponding to the center of the neighborhood. For example, in a \\n33\\n×\\n neighbor-\\nhood the median is the 5th largest value\\n, in a \\n55\\n×\\n neighborhood it is the 13th largest \\nvalue', metadata={'source': 'imagepro.pdf', 'page': 176}),\n",
       " Document(page_content='value\\n, and so on. When several values in a neighborhood are the same, all equal val-\\nues are grouped. For example, suppose that a \\n33\\n×\\n neighborhood has values (10, 20, \\n20,\\n 20, 15, 20, 20, 25, 100). These values are sorted as (10, 15, 20, 20, 20, 20, 20, 25, 100), \\nwhich results in a median of 20. Thus, the principal function of median ﬁlters is to \\nforce points to be more like their neighbors. Isolated clusters of pixels that are light \\nor dark with respect to their neighbors, and whose area is less than \\nm\\n2\\n2\\n (one-half \\nthe ﬁlter area),\\n are forced by an \\nmm\\n×\\n median ﬁlter to have the value of the median \\nintensity of the pixels in the neighborhood (see Problem 3.36).\\nT\\nhe median ﬁlter is by far the most useful order-statistic ﬁlter in image processing, \\nbut is not the only one. The median represents the 50th percentile of a ranked set \\nof numbers, but ranking lends itself to many other possibilities. For example, using \\nthe 100th percentile results in the so-called', metadata={'source': 'imagepro.pdf', 'page': 176}),\n",
       " Document(page_content='max ﬁlter\\n, which is useful for ﬁnding the \\nbrightest points in an image or for eroding dark areas adjacent to light regions. The \\nresponse of a \\n33\\n×\\n max ﬁlter is given by \\nRz k\\nk\\n==\\n{}\\nmax , , , , .\\n123 9\\n…\\n The 0th per-\\ncentile ﬁlter is the \\nmin ﬁlter\\n,\\n used for the opposite purpose. Median, max, min, and \\nseveral other nonlinear ﬁlters will be considered in more detail in Section 5.3.\\nEXAMPLE 3.17 :  Median ﬁltering.\\nFigure 3.43(a) shows an X-ray image of a circuit board heavily corrupted by salt-and-pepper noise. To \\nillustrate the superiority of median ﬁltering over lowpass ﬁltering in situations such as this, we show in \\nFig. 3.43(b) the result of ﬁltering the noisy image with a Gaussian lowpass ﬁlter, and in Fig. 3.43(c) the \\nresult of using a median ﬁlter. The lowpass ﬁlter blurred the image and its noise reduction performance \\nwas poor. The superiority in all respects of median over lowpass ﬁltering in this case is evident.\\n3.6 SHARPENING (HIGHPASS) SPATIAL FILTERS', metadata={'source': 'imagepro.pdf', 'page': 176}),\n",
       " Document(page_content='Sharpening highlights transitions in intensity. Uses of image sharpening range from \\nelectronic printing and medical imaging to industrial inspection and autonomous \\nguidance in military systems. In Section 3.5, we saw that image blurring could be \\naccomplished in the spatial domain by pixel averaging (smoothing) in a neighbor-\\nhood. Because averaging is analogous to integration, it is logical to conclude that \\nsharpening can be accomplished by spatial differentiation. In fact, this is the case, \\nand the following discussion deals with various ways of defining and implementing \\noperators for sharpening by digital differentiation. The strength of the response of \\n3.6\\nDIP4E_GLOBAL_Print_Ready.indb   175\\n6/16/2017   2:03:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 176}),\n",
       " Document(page_content='176\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\na derivative operator is proportional to the magnitude of the intensity discontinuity \\nat the point at which the operator is applied. Thus, image differentiation enhances \\nedges and other discontinuities (such as noise) and de-emphasizes areas with slowly \\nvarying intensities. As noted in Section 3.5, smoothing is often referred to as lowpass \\nfiltering, a term borrowed from frequency domain processing. In a similar manner, \\nsharpening is often referred to as \\nhighpass \\nfiltering. In this case, high frequencies \\n(which are responsible for fine details) are passed, while low frequencies are attenu-\\nated or rejected.\\nFOUNDATION\\nIn the two sections that follow, we will consider in some detail sharpening filters that \\nare based on first- and second-order derivatives, respectively. Before proceeding \\nwith that discussion, however, we stop to look at some of the fundamental properties', metadata={'source': 'imagepro.pdf', 'page': 177}),\n",
       " Document(page_content='of these derivatives in a digital context. To simplify the explanation, we focus atten-\\ntion initially on one-dimensional derivatives. In particular, we are interested in the \\nbehavior of these derivatives in areas of constant intensity, at the onset and end of \\ndiscontinuities (\\nstep\\n and \\nramp\\n \\ndiscontinuities\\n), and along \\nintensity ramps\\n. As you will \\nsee in Chapter 10, these types of discontinuities can be used to model noise points, \\nlines, and edges in an image.\\nDerivatives of a digital function are deﬁned in terms of differences. There are \\nvarious ways to deﬁne these differences. However, we require that any deﬁnition we \\nuse for a \\nﬁrst derivative:\\n \\n1. \\nMust be zero in areas of constant intensity.\\n2. \\nMust be nonzero at the onset of an intensity step or ramp. \\n3. \\nMust be nonzero along intensity ramps. \\nSimilarly\\n, any definition of a \\nsecond derivative\\n \\nb a\\nc\\nFIGURE 3.43', metadata={'source': 'imagepro.pdf', 'page': 177}),\n",
       " Document(page_content='b a\\nc\\nFIGURE 3.43\\n (a) X-ray image of a circuit board, corrupted by salt-and-pepper noise. (b) Noise reduction using a \\n19 19\\n×\\n Gaussian lowpass ﬁlter kernel with \\ns\\n=\\n3.\\n (c) Noise reduction using a \\n77\\n×\\n median ﬁlter. (Original image \\ncourtesy of Mr. Joseph E. Pascente, Lixi, Inc.)\\nDIP4E_GLOBAL_Print_Ready.indb   176\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 177}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n177\\n1. \\nMust be zero in areas of constant intensity. \\n2. \\nMust be nonzero at the onset \\nand\\n end of an intensity step or ramp\\n.\\n3. \\nMust be zero along intensity ramps. \\nW\\ne are dealing with digital quantities whose values are finite. Therefore, the maxi-\\nmum possible intensity change also is finite, and the shortest distance over which \\nthat change can occur is between adjacent pixels. \\nA basic deﬁnition of the \\nﬁrst-order\\n derivative of a one-dimensional function \\nfx\\n()\\n \\nis the difference\\n \\n∂\\n∂\\n=+ −\\nf\\nx\\nfx fx\\n() ( )\\n1\\n \\n(3-48)\\nWe used a partial derivative here in order to keep the notation consistent when we \\nconsider an image function of two variables\\n, \\nfx y\\n(,\\n)\\n, at which time we will be deal-\\ning with partial derivatives along the two spatial axes\\n. Clearly, \\n∂∂ =\\nf x df dx\\n when \\nthere is only one variable in the function;\\n the same is true for the second derivative.\\nWe deﬁne the \\nsecond-order\\n derivative of \\nfx\\n()', metadata={'source': 'imagepro.pdf', 'page': 178}),\n",
       " Document(page_content='fx\\n()\\n as the difference\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\nx\\nfx fx fx\\n() ()( )\\n \\n(3-49)\\nThese two definitions satisfy the conditions stated above, as we illustrate in Fig. 3.44, \\nwhere we also examine the similarities and differences between first- and second-\\norder derivatives of a digital function.\\nT\\nhe values denoted by the small squares in Fig. 3.44(a) are the intensity values \\nalong a horizontal intensity proﬁle (the dashed line connecting the squares is includ-\\ned to aid visualization). The actual numerical values of the scan line are shown inside \\nthe small boxes in 3.44(b). As Fig. 3.44(a) shows, the scan line contains three sections \\nof constant intensity, an intensity ramp, and an intensity step. The circles indicate the \\nonset or end of intensity transitions. The ﬁrst- and second-order derivatives, com-\\nputed using the two preceding deﬁnitions, are shown below the scan line values in \\nFig. 3.44(b), and are plotted in Fig. 3.44(c).When computing the ﬁrst derivative at a \\nlocation \\nx', metadata={'source': 'imagepro.pdf', 'page': 178}),\n",
       " Document(page_content='location \\nx\\n, we subtract the value of the function at that location from the next point, \\nas indicated in Eq. (3-48), so this is a “look-ahead” operation. Similarly, to compute \\nthe second derivative at \\nx\\n, we use the previous and the next points in the computa-\\ntion, as indicated in Eq. (3-49). To avoid a situation in which the previous or next \\npoints are outside the range of the scan line, we show derivative computations in Fig. \\n3.44 from the second through the penultimate points in the sequence.\\nAs we traverse the proﬁle from left to right we encounter ﬁrst an area of constant \\nintensity and, as Figs. 3.44(b) and (c) show, both derivatives are zero there, so condi-\\ntion (1) is satisﬁed by both. Next, we encounter an intensity ramp followed by a step, \\nand we note that the ﬁrst-order derivative is nonzero at the onset of the ramp and \\nthe step; similarly, the second derivative is nonzero at the onset and end of both the', metadata={'source': 'imagepro.pdf', 'page': 178}),\n",
       " Document(page_content='ramp and the step; therefore, property (2) is satisﬁed by both derivatives. Finally, we \\nWe will return to Eq. \\n(3-48) in Section 10.2 and \\nshow how it follows from \\na Taylor series expansion. \\nFor now, we accept it as a \\ndeﬁnition.\\nDIP4E_GLOBAL_Print_Ready.indb   177\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 178}),\n",
       " Document(page_content='178\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nsee that property (3) is satisﬁed also by both derivatives because the ﬁrst derivative \\nis nonzero and the second is zero along the ramp. Note that the sign of the second \\nderivative changes at the onset and end of a step or ramp. In fact, we see in Fig. \\n3.44(c) that in a step transition a line joining these two values crosses the horizontal \\naxis midway between the two extremes. This \\nzero crossing\\n property is quite useful \\nfor locating edges, as you will see in Chapter 10. \\nEdges in digital images often are ramp-like transitions in intensity, in which case \\nthe ﬁrst derivative of the image would result in thick edges because the derivative \\nis nonzero along a ramp. On the other hand, the second derivative would produce a \\ndouble edge one pixel thick, separated by zeros. From this, we conclude that the sec-\\nond derivative enhances ﬁne detail much better than the ﬁrst derivative, a property', metadata={'source': 'imagepro.pdf', 'page': 179}),\n",
       " Document(page_content='ideally suited for sharpening images. Also, second derivatives require fewer opera-\\ntions to implement than ﬁrst derivatives, so our initial attention is on the former. \\nUSING THE SECOND DERIVATIVE FOR IMAGE SHARPENING—THE \\nLAPLACIAN\\nIn this section we discuss the implementation of 2-D, second-order derivatives and \\ntheir use for image sharpening. The approach consists of defining a discrete formu-\\nlation of the second-order derivative and then constructing a filter kernel based on \\nWe will return to the \\nsecond derivative in \\nChapter 10, where we use \\nit extensively for image \\nsegmentation.\\nb\\na\\nc\\nFIGURE 3.44\\n(a) A section of a  \\nhorizontal scan \\nline from an \\nimage, showing \\nramp and step \\nedges, as well as \\nconstant  \\nsegments. \\n(b)Values of the \\nscan line and its \\nderivatives. \\n(c) Plot of the \\nderivatives, show-\\ning a zero cross-\\ning. In (a) and (c) \\npoints were joined \\nby dashed lines as \\na visual aid.\\n4\\n5\\n6\\n3\\n2\\n1\\n0\\nConstant\\nintensity\\nValues of\\nscan line\\n1st derivative', metadata={'source': 'imagepro.pdf', 'page': 179}),\n",
       " Document(page_content='1st derivative\\n2nd derivative\\nIntensity transition\\nIntensity\\nRamp\\nStep\\nx\\nx\\n66\\n00\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n10\\n0\\n000500\\n00\\n/H11002\\n1\\n0\\nZero crossing\\nFirst derivative\\nSecond derivative\\nIntensity\\nx\\n5\\n4\\n3\\n2\\n1\\n0\\n/H11002\\n5\\n/H11002\\n4\\n/H11002\\n3\\n/H11002\\n2\\n/H11002\\n1\\n00\\n0\\n00 1\\n0\\n0005\\n/H11002\\n50\\n00\\n/H11002\\n1\\n6543211111166666\\n6\\nData points\\nDIP4E_GLOBAL_Print_Ready.indb   178\\n6/16/2017   2:03:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 179}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n179\\nthat formulation. As in the case of Gaussian lowpass kernels in Section 3.5, we are \\ninterested here in isotropic kernels, whose response is independent of the direction \\nof intensity discontinuities in the image to which the filter is applied. \\nIt can be shown (Rosenfeld and Kak [1982]) that the simplest isotropic deriva-\\ntive operator (kernel) is the \\nLaplacian\\n, which, for a function (image) \\nfx y\\n(,\\n)\\n of two \\nvariables\\n, is deﬁned as\\n \\n/H11612\\n2\\n2\\n2\\n2\\n2\\nf\\nf\\nx\\nf\\ny\\n=\\n∂\\n∂\\n+\\n∂\\n∂\\n \\n(3-50)\\nBecause derivatives of any order are linear operations, the Laplacian is a linear oper-\\nator\\n. To express this equation in discrete form, we use the definition in Eq. (3-49), \\nkeeping in mind that we now have a second variable. In the \\nx\\n-direction, we have\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\nx\\nfx y fx y fx y\\n(, ) (, ) ( , )\\n \\n(3-51)\\nand, similarly, in the \\ny\\n-direction,\\n we have\\n \\n∂\\n∂\\n=+ +− −\\n2\\n2\\n11 2\\nf\\ny\\nfx y fx y fx y\\n(, ) (, ) (, )\\n  \\n(3-52)', metadata={'source': 'imagepro.pdf', 'page': 180}),\n",
       " Document(page_content='(3-52)\\nIt follows from the preceding three equations that the discrete Laplacian of two \\nvariables is\\n    \\n/H11612\\n2\\n11 11 4\\nfx y fx y fx y fx y fx y fx y\\n( , )( , )( , )( , )( , ) ( , )\\n=+ +− + + + − −\\n \\n(3-53)\\nThis equation can be implemented using convolution with the kernel in Fig. 3.45(a); \\nthus\\n, the filtering mechanics for image sharpening are as described in Section 3.5 for \\nlowpass filtering; we are simply using different coefficients here.\\nThe kernel in Fig. 3.45(a) is isotropic for rotations in increments of 90° with respect \\nto the \\nx\\n- and \\ny\\n-axes. The diagonal directions can be incorporated in the deﬁnition of \\nthe digital Laplacian by adding four more terms to Eq. (3-53). Because each diagonal \\nterm would contains a \\n−\\n2\\nfx\\ny\\n(, )\\n term, the total subtracted from the difference terms \\nb\\na\\nc\\nd\\nFIGURE 3.45\\n  (a) Laplacian kernel used to implement Eq. (3-53). (b) Kernel used to implement', metadata={'source': 'imagepro.pdf', 'page': 180}),\n",
       " Document(page_content='an extension of this equation that includes the diagonal terms. (c) and (d) Two other Lapla-\\ncian kernels.\\n/H11002\\n4 1\\n00\\n0\\n0\\n1\\n1\\n1\\n/H11002\\n8 1\\n11\\n1\\n1\\n1\\n1\\n1\\n4\\n/H11002\\n1\\n00\\n0\\n0\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n8\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   179\\n6/16/2017   2:04:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 180}),\n",
       " Document(page_content='180\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nnow would be \\n−\\n8\\nfx\\ny\\n(, ) .\\n Figure 3.45(b) shows the kernel used to implement this \\nnew deﬁnition.\\n This kernel yields isotropic results in increments of 45°. The kernels \\nin Figs. 3.45(c) and (d) also are used to compute the Laplacian. They are obtained \\nfrom deﬁnitions of the second derivatives that are the negatives of the ones we used \\nhere. They yield equivalent results, but the difference in sign must be kept in mind \\nwhen combining a Laplacian-ﬁltered image with another image.\\nBecause the Laplacian is a derivative operator, it highlights sharp intensity tran-\\nsitions in an image and de-emphasizes regions of slowly varying intensities. This \\nwill tend to produce images that have grayish edge lines and other discontinuities, \\nall superimposed on a dark, featureless background. Background features can be \\n“recovered” while still preserving the sharpening effect of the Laplacian by adding', metadata={'source': 'imagepro.pdf', 'page': 181}),\n",
       " Document(page_content='the Laplacian image to the original. As noted in the previous paragraph, it is impor-\\ntant to keep in mind which deﬁnition of the Laplacian is used. If the deﬁnition used \\nhas a negative center coefﬁcient, then we \\nsubtract\\n the Laplacian image from the \\noriginal to obtain a sharpened result. Thus, the basic way in which we use the Lapla-\\ncian for image sharpening is\\n \\ngxy f xy c f xy\\n(,\\n) (, ) (, )\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n/H11612\\n2\\n \\n(3-54)\\nwhere \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n are the input and sharpened images, respectively. We let \\nc\\n=−\\n1\\n if the Laplacian kernels in Fig. 3.45(a) or (b) is used, and \\nc\\n=\\n1\\n if either of the \\nother two kernels is used.\\nEXAMPLE 3.18 :  Image sharpening using the Laplacian.\\nFigure 3.46(a) shows a slightly blurred image of the North Pole of the moon, and Fig. 3.46(b) is the result \\nof ﬁltering this image with the Laplacian kernel in Fig. 3.45(a) directly. Large sections of this image are', metadata={'source': 'imagepro.pdf', 'page': 181}),\n",
       " Document(page_content='black because the Laplacian image contains both positive and negative values, and all negative values \\nare clipped at 0 by the display. \\nFigure 3.46(c) shows the result obtained using Eq. (3-54), with \\nc\\n=−\\n1,\\n because we used the kernel in \\nF\\nig. 3.45(a) to compute the Laplacian. The detail in this image is unmistakably clearer and sharper than \\nin the original image. Adding the Laplacian to the original image restored the overall intensity varia-\\ntions in the image. Adding the Laplacian increased the contrast at the locations of intensity discontinui-\\nties. The net result is an image in which small details were enhanced and the background tonality was \\nreasonably preserved. Finally, Fig. 3.46(d) shows the result of repeating the same procedure but using \\nthe kernel in Fig. 3.45(b). Here, we note a signiﬁcant improvement in sharpness over Fig. 3.46(c). This is \\nnot unexpected because using the kernel in Fig. 3.45(b) provides additional differentiation (sharpening)', metadata={'source': 'imagepro.pdf', 'page': 181}),\n",
       " Document(page_content='in the diagonal directions. Results such as those in Figs. 3.46(c) and (d) have made the Laplacian a tool \\nof choice for sharpening digital images.\\nBecause Laplacian images tend to be dark and featureless, a typical way to scale these images for dis-\\nplay is to use Eqs. (2-31) and (2-32). This brings the most negative value to 0 and displays the full range \\nof intensities. Figure 3.47 is the result of processing Fig. 3.46(b) in this manner. The dominant features of \\nthe image are edges and sharp intensity discontinuities. The background, previously black, is now gray as \\na result of scaling. This grayish appearance is typical of Laplacian images that have been scaled properly.\\nDIP4E_GLOBAL_Print_Ready.indb   180\\n6/16/2017   2:04:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 181}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n181\\nObserve in Fig. 3.45 that the coefﬁcients of each kernel sum to zero. Convolution-\\nbased ﬁltering implements a sum of products, so when a derivative kernel encom-\\npasses a constant region in a image, the result of convolution in that location must be \\nzero. Using kernels whose coefﬁcients sum to zero accomplishes this. \\nIn Section 3.5, we normalized smoothing kernels so that the sum of their coef-\\nﬁcients would be one. Constant areas in images ﬁltered with these kernels would \\nbe constant also in the ﬁltered image. We also found that the sum of the pixels in \\nthe original and ﬁltered images were the same, thus preventing a bias from being \\nintroduced by ﬁltering (see Problem 3.31). When convolving an image with a kernel \\nb a\\nd c\\nFIGURE 3.46\\n(a) Blurred  \\nimage of the \\nNorth Pole of the \\nmoon.  \\n(b) Laplacian  \\nimage obtained \\nusing the kernel \\nin Fig. 3.45(a).  \\n(c) Image  \\nsharpened  \\nusing Eq. (3-54) \\nwith \\nc\\n=−\\n1. \\n(d) Image', metadata={'source': 'imagepro.pdf', 'page': 182}),\n",
       " Document(page_content='=−\\n1. \\n(d) Image  \\nsharpened using \\nthe same  \\nprocedure\\n, but \\nwith the kernel \\nin Fig. 3.45(b). \\n(Original  \\nimage courtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   181\\n6/16/2017   2:04:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 182}),\n",
       " Document(page_content='182\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nwhose coefﬁcients sum to zero, it turns out that the pixels of the ﬁltered image \\nwill \\nsum to zero\\n \\nalso\\n (see Problem 3.32). This implies that images ﬁltered with such ker-\\nnels will have negative values, and sometimes will require additional processing to \\nobtain suitable visual results. Adding the ﬁltered image to the original, as we did in \\nEq. (3-54), is an example of such additional processing. \\nUNSHARP MASKING AND HIGHBOOST FILTERING\\nSubtracting an unsharp (smoothed) version of an image from the original image is \\nprocess that has been used since the 1930s by the printing and publishing industry to \\nsharpen images. This process, called \\nunsharp masking\\n, consists of the following steps:\\n1. \\nBlur the original image.\\n2. \\nSubtract the blurred image from the original (the resulting difference is called \\nthe \\nmask\\n.)\\n3. \\nAdd the mask to the original.\\nLetting \\nfx y\\n(, )', metadata={'source': 'imagepro.pdf', 'page': 183}),\n",
       " Document(page_content='Letting \\nfx y\\n(, )\\n denote the blurred image, the mask in equation form is given by:\\n \\ngx y f x y f x y\\nmask\\n(, ) (, ) (, )\\n=−\\n \\n(3-55)\\nThen we add a weighted portion of the mask back to the original image:\\n \\ngxy f xy k g xy\\n(,\\n) (, ) (, )\\n=+\\nmask\\n \\n(3-56)\\nThe photographic pro-\\ncess of unsharp masking \\nis based on creating a \\nblurred positive and \\nusing it along with the \\noriginal negative to \\ncreate a sharper image. \\nOur interest is in the \\ndigital\\n equivalent of this \\nprocess.\\nFIGURE 3.47\\nThe Laplacian  \\nimage from  \\nFig. 3.46(b), scaled \\nto the full [0, 255] \\nrange of intensity \\nvalues. Black pixels \\ncorrespond to the \\nmost negative  \\nvalue in the  \\nunscaled  \\nLaplacian image, \\ngrays are inter-\\nmediate values, \\nand white pixels \\ncorresponds to the \\nhighest positive \\nvalue.\\nDIP4E_GLOBAL_Print_Ready.indb   182\\n6/16/2017   2:04:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 183}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n183\\nwhere we included a weight, \\nkk\\n()\\n,\\n≥\\n0\\n for generality. When \\nk\\n=\\n1\\n we have unsharp \\nmasking\\n, as defined above. When \\nk\\n>\\n1,\\n the process is referred to as \\nhighboost filter\\n-\\ning\\n. Choosing \\nk\\n<\\n1 reduces the contribution of the unsharp mask.\\nF\\nigure 3.48 illustrates the mechanics of unsharp masking. Part (a) is a horizontal \\nintensity proﬁle across a vertical ramp edge that transitions from dark to light. Fig-\\nure 3.48(b) shows the blurred scan line superimposed on the original signal (shown \\ndashed). Figure 3.48(c) is the mask, obtained by subtracting the blurred signal from \\nthe original. By comparing this result with the section of Fig. 3.44(c) corresponding \\nto the ramp in Fig. 3.44(a), we note that the unsharp mask in Fig. 3.48(c) is similar \\nto what we would obtain using a second-order derivative. Figure 3.48(d) is the ﬁnal \\nsharpened result, obtained by adding the mask to the original signal. The points', metadata={'source': 'imagepro.pdf', 'page': 184}),\n",
       " Document(page_content='at which a change of slope occurs in the signal are now emphasized (sharpened). \\nObserve that negative values were added to the original. Thus, it is possible for the \\nﬁnal result to have negative intensities if the original image has any zero values, or if \\nthe value of \\nk\\n is chosen large enough to emphasize the peaks of the mask to a level \\nlarger than the minimum value in the original signal. Negative values cause dark \\nhalos around edges that can become objectionable if \\nk\\n is too large. \\nEXAMPLE 3.19 :  Unsharp masking and highboost ﬁltering.\\nFigure 3.49(a) shows a slightly blurred image of white text on a dark gray background. Figure 3.49(b) \\nwas obtained using a Gaussian smoothing ﬁlter of size \\n31 31\\n×\\n with \\ns\\n=\\n5.\\n As explained in our earlier \\ndiscussion of Gaussian lowpass kernels\\n, the size of the kernel we used here is the smallest odd integer \\nno less than \\n66\\nss\\n×\\n.\\n Figure 3.49(c) is the unsharp mask, obtained using Eq. (3-55). To obtain the im-\\nOriginal signal', metadata={'source': 'imagepro.pdf', 'page': 184}),\n",
       " Document(page_content='Original signal\\nBlurred signal\\nUnsharp mask\\nSharpened signal\\nb\\na\\nc\\nd\\nFIGURE 3.48\\n1-D illustration of \\nthe mechanics of \\nunsharp masking.  \\n(a) Original \\nsignal. (b) Blurred \\nsignal with original \\nshown dashed for \\nreference.  \\n(c) Unsharp mask. \\n(d) Sharpened  \\nsignal, obtained by \\nadding (c) to (a).\\nDIP4E_GLOBAL_Print_Ready.indb   183\\n6/16/2017   2:04:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 184}),\n",
       " Document(page_content='184\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nage in Fig. 3.49(d) was used the unsharp masking expression room Eq. (3-56) with \\nk\\n=\\n1.\\n This image is \\nsigniﬁcantly sharper than the original image in F\\nig. 3.49(a), but we can do better, as we show in the fol-\\nlowing paragraph. \\nFigure 3.49(e) shows the result of using Eq. (3-56) with \\nk\\n=\\n45\\n..\\n This value is almost at the extreme of \\nwhat we can use without introducing some serious artifacts in the image\\n. The artifacts are dark, almost \\nblack, halos around the border of the characters. This is caused by the lower “blip” in Fig. 3.48(d) be-\\ncoming negative, as we explained earlier. When scaling the image so that it only has positive values for \\ndisplay, the negative values are either clipped at 0, or scaled so that the most negative values become 0, \\ndepending on the scaling method used. In either case, the blips will be the darkest values in the image.', metadata={'source': 'imagepro.pdf', 'page': 185}),\n",
       " Document(page_content='The results in Figs. 3.49(d) and 3.49(e) would be difﬁcult to generate using the traditional ﬁlm pho-\\ntography explained earlier, and it illustrates the power and versatility of image processing in the context \\nof digital photography.\\nUSING FIRST-ORDER DERIVATIVES FOR IMAGE SHARPENING—THE \\nGRADIENT\\nFirst derivatives in image processing are implemented using the magnitude of the \\ngradient. The \\ngradient\\n of an image \\nf\\n at coordinates \\n(, )\\nxy\\n is defined as the two-\\ndimensional column vector\\n \\n/H11612\\nff\\ng\\ng\\nf\\nx\\nf\\ny\\nx\\ny\\n≡=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n=\\n∂\\n∂\\n∂\\n∂\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n⎥\\ngrad( )\\n \\n(3-57)\\nWe will discuss the  \\ngradient in more detail \\nin Section 10.2. Here, \\nwe are interested only \\nin using it for image \\nsharpening.\\nb a\\nc\\ne d\\nFIGURE 3.49\\n (a) Original image of size \\n600 259\\n×\\n pixels. (b) Image blurred using a \\n31 31\\n×\\n Gaussian lowpass ﬁlter with \\ns\\n=\\n5.\\n (c) Mask. (d) Result of unsharp masking using Eq. (3-56) with \\nk\\n=\\n1.\\n (e) Result of highboost ﬁltering with \\nk\\n=\\n45\\n..', metadata={'source': 'imagepro.pdf', 'page': 185}),\n",
       " Document(page_content='k\\n=\\n45\\n..\\n \\nDIP4E_GLOBAL_Print_Ready.indb   184\\n6/16/2017   2:04:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 185}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n185\\nThis vector has the important geometrical property that it points in the direction of \\nthe greatest rate of change of \\nf\\n at location \\n(, ) .\\nxy\\n \\nT\\nhe \\nmagnitude\\n (\\nlength\\n) of vector \\n/H11612\\nf\\n,\\n denoted as \\nMxy\\n(,\\n)\\n (the vector norm nota-\\ntion \\n/H11612\\nf\\n is also used frequently), where\\n \\nMxy f f g g\\nxy\\n(, )\\n=\\n()\\n=+\\n/H11612/H11612\\n=m a g\\n22\\n \\n(3-58)\\nis the \\nvalue\\n at \\n(, )\\nxy\\n of the \\nrate of c\\nhange\\n in the direction of the gradient vector. Note \\nthat \\nMxy\\n(,\\n)\\n is an image of the same size as the original, created when \\nx\\n and \\ny\\n are \\nallowed to vary over all pixel locations in \\nf\\n.\\n It is common practice to refer to this \\nimage as the\\n gradient image\\n (or simply as the \\ngradient\\n when the meaning is clear).\\nBecause the components of the gradient vector are derivatives, they are linear \\noperators. However, the magnitude of this vector is not, because of the squaring and', metadata={'source': 'imagepro.pdf', 'page': 186}),\n",
       " Document(page_content='square root operations. On the other hand, the partial derivatives in Eq. (3-57) are \\nnot rotation invariant, but the magnitude of the gradient vector is. \\nIn some implementations, it is more suitable computationally to approximate the \\nsquares and square root operations by absolute values:\\n \\nMxy g g\\nxy\\n(, )\\n≈+\\n@@@@\\n \\n(3-59)\\nThis expression still preserves the relative changes in intensity, but the isotropic \\nproperty is lost in general.\\n However, as in the case of the Laplacian, the isotropic \\nproperties of the discrete gradient defined in the following paragraph are preserved \\nonly for a limited number of rotational increments that depend on the kernels used \\nto approximate the derivatives. As it turns out, the most popular kernels used to \\napproximate the gradient are isotropic at multiples of 90°. These results are inde-\\npendent of whether we use Eq. (3-58) or (3-59), so nothing of significance is lost in \\nusing the latter equation if we choose to do so.', metadata={'source': 'imagepro.pdf', 'page': 186}),\n",
       " Document(page_content='As in the case of the Laplacian, we now deﬁne discrete approximations to the \\npreceding equations, and from these formulate the appropriate kernels. In order \\nto simplify the discussion that follows, we will use the notation in Fig. 3.50(a) to \\ndenote the intensities of pixels in a \\n33\\n×\\n region. For example, the value of the center \\npoint,\\n \\nz\\n5\\n,\\n denotes the value of \\nfx y\\n(,\\n)\\n at an arbitrary location, \\n(, ) ;\\nxy\\n \\nz\\n1\\n denotes the \\nvalue of \\nfx y\\n(,\\n) ;\\n−−\\n11\\n and so on. As indicated in Eq. (3-48), the simplest approxi-\\nmations to a ﬁrst-order derivative that satisfy the conditions stated at the beginning \\nof this section are \\ngz z\\nx\\n=−\\n()\\n85\\n and \\ngz z\\ny\\n=−\\n() .\\n65\\n Two other deﬁnitions, proposed \\nby Roberts [1965] in the early development of digital image processing, use cross \\ndifferences:\\n \\ngz z gz z\\nxy\\n=−\\n=−\\n() ()\\n95\\n86\\nand\\n \\n(3-60)\\nIf we use Eqs. (3-58) and (3-60), we compute the gradient image as\\n \\nMxy z z z z\\n(, ) ( ) ( )\\n=−+−\\n⎡\\n⎣\\n⎤\\n⎦\\n95\\n2\\n86\\n2\\n12\\n \\n(3-61)', metadata={'source': 'imagepro.pdf', 'page': 186}),\n",
       " Document(page_content='2\\n86\\n2\\n12\\n \\n(3-61)\\nThe vertical bars denote \\nabsolute values. \\nDIP4E_GLOBAL_Print_Ready.indb   185\\n6/16/2017   2:04:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 186}),\n",
       " Document(page_content='186\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nIf we use Eqs. (3-59) and (3-60), then\\n \\nMxy z z z z\\n(,\\n)\\n≈− + −\\n@@ @@\\n95 86\\n \\n(3-62)\\nwhere it is understood that \\nx\\n and \\ny\\n vary over the dimensions of the image in the \\nmanner described earlier\\n. The difference terms needed in Eq. (3-60) can be imple-\\nmented using the two kernels in Figs. 3.50(b) and (c). These kernels are referred to \\nas the \\nRoberts cross-gradient operators\\n. \\nAs noted earlier, we prefer to use kernels of odd sizes because they have a unique, \\n(integer) center of spatial symmetry. The smallest kernels in which we are interested \\nare of size \\n33\\n×\\n.\\n Approximations to \\ng\\nx\\n and \\ng\\ny\\n using a \\n33\\n×\\n neighborhood centered \\non \\nz\\n5\\n are as follows:\\n \\ng\\nf\\nx\\nzz z zz z\\nx\\n=\\n∂\\n∂\\n=+ + − + +\\n() ( )\\n78 9 12 3\\n22\\n \\n(3-63)\\nand\\n \\ng\\nf\\ny\\nzz z zz z\\ny\\n=\\n∂\\n∂\\n=+ + − + +\\n() ( )\\n36 9 14 7\\n22\\n \\n(3-64)\\nThese equations can be implemented using the kernels in Figs. 3.50(d) and (e). The', metadata={'source': 'imagepro.pdf', 'page': 187}),\n",
       " Document(page_content='difference between the third and first rows of the \\n33\\n×\\n image region approximates the \\npartial derivative in the \\nx\\n-direction,\\n and is implemented using the kernel in Fig. 3.50(d). \\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n000\\n121\\n/H11002\\n1\\n01\\n/H11002\\n2\\n0\\n2\\n/H11002\\n1\\n01\\n0\\n/H11002\\n1\\n10\\n/H11002\\n1\\n0\\n01\\nz\\n1\\nz\\n2\\nz\\n3\\nz\\n4\\nz\\n5\\nz\\n6\\nz\\n7\\nz\\n8\\nz\\n9\\na\\nb\\nc\\nd\\ne\\nFIGURE 3.50\\n(a) A \\n33\\n×\\n region \\nof an image\\n, \\nwhere the \\nz\\ns are \\nintensity values.  \\n(b)–(c) Roberts \\ncross-gradient \\noperators.  \\n(d)–(e) Sobel \\noperators. All the \\nkernel  \\ncoefﬁcients sum \\nto zero, as expect-\\ned of a derivative \\noperator. \\nDIP4E_GLOBAL_Print_Ready.indb   186\\n6/16/2017   2:04:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 187}),\n",
       " Document(page_content='3.6\\n  \\nSharpening (Highpass) Spatial Filters\\n    \\n187\\nThe difference between the third and first columns approximates the partial deriva-\\ntive in the \\ny\\n-direction and is implemented using the kernel in Fig. 3.50(e). The partial \\nderivatives at all points in an image are obtained by convolving the image with these \\nkernels. We then obtain the magnitude of the gradient as before. For example, substitut-\\ning \\ng\\nx\\n and \\ng\\ny\\n into Eq. (3-59) yields\\n \\nMxy g g z z z z z z\\nzz\\nxy\\n(, )\\n( ) ( )\\n(\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n=+ + − + +\\n[]\\n⎡\\n⎣\\n++ +\\n22\\n1\\n2\\n78 9 12 3\\n2\\n36\\n22\\n2\\nz\\nzz z z\\n91 4 7\\n2\\n1\\n2\\n2\\n)( )\\n−++\\n[]\\n⎤\\n⎦\\n \\n(3-65)\\nThis equation indicates that the value of \\nM \\nat any image coordinates \\n(,)\\nxy\\n is given \\nby squaring values of the convolution of the two kernels with image \\nf \\nat those coor\\n-\\ndinates, summing the two results, and taking the square root. \\nThe kernels in Figs. 3.50(d) and (e) are called the \\nSobel operators\\n. The idea behind', metadata={'source': 'imagepro.pdf', 'page': 188}),\n",
       " Document(page_content='. The idea behind \\nusing a weight value of 2 in the center coefﬁcient is to achieve some smoothing by \\ngiving more importance to the center point (we will discuss this in more detail in \\nChapter 10). The coefﬁcients in all the kernels in Fig. 3.50 sum to zero, so they would \\ngive a response of zero in areas of constant intensity, as expected of a derivative \\noperator. As noted earlier, when an image is convolved with a kernel whose coef-\\nﬁcients sum to zero, the elements of the resulting ﬁltered image sum to zero also, so \\nimages convolved with the kernels in Fig. 3.50 will have negative values in general.\\nThe computations of \\ng\\nx\\n and \\ng\\ny\\n are linear operations and are implemented using \\nconvolution, as noted above. The nonlinear aspect of sharpening with the gradient is \\nthe computation of \\nMxy\\n(,\\n)\\n involving squaring and square roots, or the use of abso-\\nlute values\\n, all of which are nonlinear operations. These operations are performed', metadata={'source': 'imagepro.pdf', 'page': 188}),\n",
       " Document(page_content='after the linear process (convolution) that yields \\ng\\nx\\n and \\ng\\ny\\n.\\nEXAMPLE 3.20 :  Using the gradient for edge enhancement.\\nThe gradient is used frequently in industrial inspection, either to aid humans in the detection of defects \\nor, what is more common, as a preprocessing step in automated inspection. We will have more to say \\nabout this in Chapter 10. However, it will be instructive now to consider a simple example to illustrate \\nhow the gradient can be used to enhance defects and eliminate slowly changing background features. \\nFigure 3.51(a) is an optical image of a contact lens, illuminated by a lighting arrangement designed \\nto highlight imperfections, such as the two edge defects in the lens boundary seen at 4 and 5 o’clock. \\nFigure 3.51(b) shows the gradient obtained using Eq. (3-65) with the two Sobel kernels in Figs. 3.50(d) \\nand (e). The edge defects are also quite visible in this image, but with the added advantage that constant', metadata={'source': 'imagepro.pdf', 'page': 188}),\n",
       " Document(page_content='or slowly varying shades of gray have been eliminated, thus simplifying considerably the computational \\ntask required for automated inspection. The gradient can be used also to highlight small specs that may \\nnot be readily visible in a gray-scale image (specs like these can be foreign matter, air pockets in a sup-\\nporting solution, or miniscule imperfections in the lens). The ability to enhance small discontinuities in \\nan otherwise ﬂat gray ﬁeld is another important feature of the gradient.\\nDIP4E_GLOBAL_Print_Ready.indb   187\\n6/16/2017   2:04:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 188}),\n",
       " Document(page_content='188\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\n3.7 HIGHPASS, BANDREJECT, AND BANDPASS FILTERS FROM LOW-\\nPASS FILTERS  \\nSpatial and frequency-domain linear ﬁlters are classiﬁed into four broad categories: \\nlowpass and highpass ﬁlters, which we introduced in Sections 3.5 and 3.6, and \\nband-\\npass \\nand \\nbandreject\\n ﬁlters, which we introduce in this section. We mentioned at the \\nbeginning of Section 3.5 that the other three types of filters can be constructed from \\nlowpass filters. In this section we explore methods for doing this. Also, we illustrate \\nthe third approach discussed at the end of Section 3.4 for obtaining spatial filter ker-\\nnels. That is, we use a filter design software package to generate 1-D filter functions. \\nThen, we use these to generate 2-D separable filters functions either via Eq.(3-42), \\nor by rotating the 1-D functions about their centers to generate 2-D kernels. The', metadata={'source': 'imagepro.pdf', 'page': 189}),\n",
       " Document(page_content='rotated versions are approximations of circularly symmetric (isotropic) functions.\\nFigure 3.52(a) shows the transfer function of a 1-D ideal lowpass ﬁlter in the \\nfrequency domain [this is the same as Fig. 3.32(a)]. We know from earlier discus-\\nsions in this chapter that lowpass ﬁlters attenuate or delete high frequencies, while \\npassing low \\nfrequencies. A \\nhighpass ﬁlter\\n behaves in exactly the opposite manner. \\nAs Fig. 3.52(b) shows, a highpass ﬁlter deletes or attenuates all frequencies below a \\ncut-off value, \\nu\\n0\\n,\\n and passes all frequencies above this value. Comparing Figs. 3.52(a) \\nand (b),\\n we see that a highpass ﬁlter transfer function is obtained by subtracting a \\nlowpass function from 1. This operation is in the frequency domain. As you know \\nfrom Section 3.4, a constant in the frequency domain is an impulse in the spatial \\ndomain. Thus, we obtain a highpass ﬁlter kernel in the spatial domain by subtracting', metadata={'source': 'imagepro.pdf', 'page': 189}),\n",
       " Document(page_content='a lowpass ﬁlter kernel from a unit impulse with the same center as the kernel. An \\nimage ﬁltered with this kernel is the same as an image obtained by subtracting a low-\\npass-ﬁltered image from the original image. The unsharp mask deﬁned by Eq. (3-55) \\nis precisely this operation. Therefore, Eqs. (3-54) and (3-56) implement equivalent \\noperations (see Problem 3.42).\\nFigure 3.52(c) shows the transfer function of a bandreject ﬁlter. This transfer \\nfunction can be constructed from the sum of a lowpass and a highpass function with \\n3.7\\nRecall from the discus-\\nsion of Eq. (3-33) that a \\nunit impulse is an array \\nof 0’s with a single 1.\\nb a\\nFIGURE 3.51\\n(a) Image of a \\ncontact lens (note \\ndefects on the \\nboundary at 4 and \\n5 o’clock).  \\n(b) Sobel gradient. \\n(Original image \\ncourtesy of  \\nPerceptics  \\nCorporation.) \\nDIP4E_GLOBAL_Print_Ready.indb   188\\n6/16/2017   2:04:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 189}),\n",
       " Document(page_content='3.7\\n  \\nHighpass, Bandreject, and Bandpass Filters from Lowpass Filters\\n    \\n189\\ndifferent cut-off frequencies (the highpass function can be constructed from a \\ndif-\\nferent\\n lowpass function). The bandpass ﬁlter transfer function in Fig. 3.52(d) can be \\nobtained by subtracting the bandreject function from 1 (a unit impulse in the spatial \\ndomain). Bandreject ﬁlters are also referred to as \\nnotch\\n ﬁlters, but the latter tend \\nto be more locally oriented, as we will show in Chapter 4. Table 3.7 summarizes the \\npreceding discussion. \\nThe key point in Fig. 3.52 and Table 3.7 is that all transfer functions shown can \\nbe obtained starting with a lowpass ﬁlter transfer function. This is important. It is \\nimportant also to realize that we arrived at this conclusion via simple graphical \\ninterpretations in the frequency domain. To arrive at the same conclusion based on \\nconvolution in the spatial domain would be a much harder task.', metadata={'source': 'imagepro.pdf', 'page': 190}),\n",
       " Document(page_content='EXAMPLE 3.21 :  Lowpass, highpass, bandreject, and bandpass ﬁltering.\\nIn this example we illustrate how we can start with a 1-D lowpass ﬁlter transfer function generated \\nusing a software package, and then use that transfer function to generate spatial ﬁlter kernels based on \\nthe concepts introduced in this section. We also examine the spatial ﬁltering properties of these kernels.\\n1\\n1\\n0\\nu\\nu\\nPassband\\nStopband\\nHighpass filter\\n1\\nu\\nPassband\\nStopband\\n1\\nu\\n2\\nu\\nStopband\\nBandpass filter\\n0\\nu\\nu\\nPassband Stopband\\nLowpass filter\\n1\\nu\\nPassband\\nStopband\\n1\\nu\\n2\\nu\\nPassband\\nBandreject filter\\nb a\\nd c\\nFIGURE 3.52\\nTransfer functions \\nof ideal 1-D ﬁlters \\nin the frequency \\ndomain (\\nu\\n denotes \\nfrequency). \\n(a) Lowpass ﬁlter. \\n(b) Highpass ﬁlter.  \\n(c) Bandreject ﬁlter.  \\n(d) Bandpass ﬁlter. \\n(As before, we \\nshow only positive \\nfrequencies for \\nsimplicity.)\\nFilter type\\nSpatial kernel in terms of lowpass kernel, \\nlp\\nLowpass\\nlp x y\\n(,)\\nHighpass\\nh pxy xy l pxy\\n(,) (,) (,)\\n=−\\nd\\nBandreject\\nb rxy l p xy h p xy\\nl', metadata={'source': 'imagepro.pdf', 'page': 190}),\n",
       " Document(page_content='l\\np xy xy l p xy\\n(,) (,) (,)\\n(,) (,) (,)\\n=+\\n=+ −\\n[]\\n12\\n12\\nd\\nBandpass\\nb pxy xy b rxy\\nxy\\nl p xy xy l p xy\\n(,) (,) (,)\\n(,) (,) (,) (,)\\n=−\\n=− + −\\n[]\\n⎡\\nd\\ndd\\n12\\n⎣ ⎣\\n⎤\\n⎦\\nTABLE \\n3.7\\nSummary of the \\nfour principal  \\nspatial ﬁlter types \\nexpressed in \\nterms of low-\\npass ﬁlters. The \\ncenters of the \\nunit impulse and \\nthe ﬁlter kernels \\ncoincide.\\nDIP4E_GLOBAL_Print_Ready.indb   189\\n6/16/2017   2:04:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 190}),\n",
       " Document(page_content='190\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nFigure 3.53 shows a so-called \\nzone plate\\n image that is used frequently for testing the characteristics of \\nﬁltering approaches. There are various versions of zone plates; the one in Fig. 3.53 was generated using \\nthe equation\\n \\nzxy\\nx y\\n(,) c o s\\n=+ +\\n(\\n)\\n⎡\\n⎣\\n⎤\\n⎦\\n1\\n2\\n1\\n22\\n \\n(3-66)\\nwith \\nx\\n and \\ny\\n varying in the range \\n[. , . ] ,\\n−\\n82\\n82\\n in increments of 0.0275. This resulted in an image of size \\n597 597\\n×\\n pixels. The bordering black region was generated by setting to 0 all pixels with distance great-\\ner than 8.2 from the image center\\n. The key characteristic of a zone plate is that its spatial frequency \\nincreases as a function of distance from the center, as you can see by noting that the rings get narrower \\nthe further they are from the center. This property makes a zone plate an ideal image for illustrating the \\nbehavior of the four ﬁlter types just discussed.', metadata={'source': 'imagepro.pdf', 'page': 191}),\n",
       " Document(page_content='Figure 3.54(a) shows a 1-D, 128-element spatial lowpass ﬁlter function designed using MATLAB \\n[compare with Fig. 3.32(b)]. As discussed earlier, we can use this 1-D function to construct a 2-D, separa-\\nble lowpass ﬁlter kernel based on Eq. (3-42), or we can rotate it about its center to generate a 2-D, isotro-\\npic kernel. The kernel in Fig. 3.54(b) was obtained using the latter approach. Figures 3.55(a) and (b) are \\nthe results of ﬁltering the image in Fig. 3.53 with the separable and isotropic kernels, respectively. Both \\nﬁlters passed the low frequencies of the zone plate while attenuating the high frequencies signiﬁcantly. \\nObserve, however, that the separable ﬁlter kernel produced a “squarish” (non-radially symmetric) result \\nin the passed frequencies. This is a consequence of ﬁltering the image in perpendicular directions with \\na separable kernel that is not isotropic. Using the isotropic kernel yielded a result that is uniform in all', metadata={'source': 'imagepro.pdf', 'page': 191}),\n",
       " Document(page_content='radial directions. This is as expected, because both the ﬁlter and the image are isotropic. \\nFIGURE 3.53\\nA zone plate \\nimage of size \\n597 597\\n×\\n pixels.\\n0\\n0.04\\n0.06\\n0.12\\n-\\n0.02\\n03 26 4 9 6\\n128\\nb a\\nFIGURE 3.54\\n(a) A 1-D spatial \\nlowpass ﬁlter \\nfunction. (b) 2-D \\nkernel obtained \\nby rotating the \\n1-D proﬁle about \\nits center.\\nDIP4E_GLOBAL_Print_Ready.indb   190\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 191}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n191\\nFigure 3.56 shows the results of ﬁltering the zone plate with the four ﬁlters described in Table 3.7. We \\nused the 2-D lowpass kernel in Fig. 3.54(b) as the basis for the highpass ﬁlter, and similar lowpass ker-\\nnels for the bandreject ﬁlter. Figure 3.56(a) is the same as Fig. 3.55(b), which we repeat for convenience. \\nFigure 3.56(b) is the highpass-ﬁltered result. Note how effectively the low frequencies were ﬁltered out. \\nAs is true of highpass-ﬁltered images, the black areas were caused by negative values being clipped at 0 \\nby the display. Figure 3.56(c) shows the same image scaled using Eqs. (2-31) and (2-32). Here we see \\nclearly that only high frequencies were passed by the ﬁlter. Because the highpass kernel was constructed \\nusing the same lowpass kernel that we used to generate Fig. 3.56(a), it is evident by comparing the two \\nresults that the highpass ﬁlter passed the frequencies that were attenuated by the lowpass ﬁlter.', metadata={'source': 'imagepro.pdf', 'page': 192}),\n",
       " Document(page_content='Figure 3.56(d) shows the bandreject-ﬁltered image, in which the attenuation of the mid-band of \\nfrequencies is evident. Finally, Fig. 33.56(e) shows the result of bandpass ﬁltering. This image also has \\nnegative values, so it is shown scaled in Fig. 3.56(f). Because the bandpass kernel was constructed by \\nsubtracting the bandreject kernel from a unit impulse, we see that the bandpass ﬁlter passed the fre-\\nquencies that were attenuated by the bandreject ﬁlter. We will give additional examples of bandpass and \\nbandreject ﬁltering in Chapter 4.\\n3.8 COMBINING SPATIAL ENHANCEMENT METHODS  \\nWith a few exceptions, such as combining blurring with thresholding (Fig. 3.41), we \\nhave focused attention thus far on individual spatial-domain processing approaches. \\nFrequently, a given task will require application of several complementary tech-\\nniques in order to achieve an acceptable result. In this section, we illustrate how to', metadata={'source': 'imagepro.pdf', 'page': 192}),\n",
       " Document(page_content='combine several of the approaches developed thus far in this chapter to address a \\ndifficult image enhancement task.\\nThe image in Fig. 3.57(a) is a nuclear whole body bone scan, used to detect dis-\\neases such as bone infections and tumors. Our objective is to enhance this image by \\nsharpening it and by bringing out more of the skeletal detail. The narrow dynamic \\nrange of the intensity levels and high noise content make this image difﬁcult to \\nenhance. The strategy we will follow is to utilize the Laplacian to highlight ﬁne detail, \\nand the gradient to enhance prominent edges. For reasons that will be explained \\nshortly, a smoothed version of the gradient image will be used to mask the Laplacian \\n3.8\\nIn this context, masking \\nrefers to multiplying two \\nimages, as in \\nFig. 2.34\\n. \\nThis is not be confused \\nwith the mask used in \\nunsharp masking.\\nb a\\nFIGURE 3.55\\n(a) Zone plate  \\nimage ﬁltered \\nwith a separable \\nlowpass kernel. \\n(b) Image ﬁltered \\nwith the isotropic \\nlowpass kernel in', metadata={'source': 'imagepro.pdf', 'page': 192}),\n",
       " Document(page_content='lowpass kernel in \\nFig. 3.54(b).\\nDIP4E_GLOBAL_Print_Ready.indb   191\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 192}),\n",
       " Document(page_content='192\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nimage. Finally, we will attempt to increase the dynamic range of the intensity levels \\nby using an intensity transformation.\\nFigure 3.57(b) shows the Laplacian of the original image, obtained using the \\nkernel in Fig. 3.45(d). This image was scaled (for display only) using the same \\ntechnique as in Fig. 3.47. We can obtain a sharpened image at this point simply by \\nadding Figs. 3.57(a) and (b), according to Eq. (3-54). Just by looking at the noise \\nlevel in Fig. 3.57(b), we would expect a rather noisy sharpened image if we added \\nFigs. 3.57(a) and (b). This is conﬁrmed by the result in Fig. 3.57(c). One way that \\ncomes immediately to mind to reduce the noise is to use a median ﬁlter. However, \\nmedian ﬁltering is an aggressive nonlinear process capable of removing image fea-\\ntures. This is unacceptable in medical image processing.\\nAn alternate approach is to use a mask formed from a smoothed version of the', metadata={'source': 'imagepro.pdf', 'page': 193}),\n",
       " Document(page_content='gradient of the original image. The approach is based on the properties of ﬁrst- and \\nb a\\nc\\ne d\\nf\\nFIGURE 3.56\\nSpatial ﬁltering of the zone plate image. (a) Lowpass result; this is the same as Fig. 3.55(b). (b) Highpass result. \\n(c) Image (b) with intensities scaled. (d) Bandreject result. (e) Bandpass result. (f) Image (e) with intensities scaled. \\nDIP4E_GLOBAL_Print_Ready.indb   192\\n6/16/2017   2:04:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 193}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n193\\nb a\\nd c\\nFIGURE 3.57\\n(a) Image of whole \\nbody bone scan.  \\n(b) Laplacian of (a). \\n(c) Sharpened image \\nobtained by adding \\n(a) and (b).  \\n(d) Sobel gradient of \\nimage  (a). (Original \\nimage courtesy of \\nG.E. Medical Sys-\\ntems.)\\nDIP4E_GLOBAL_Print_Ready.indb   193\\n6/16/2017   2:04:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 194}),\n",
       " Document(page_content='194\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nsecond-order derivatives we discussed when explaining Fig. 3.44. The Laplacian, is \\na second-order derivative operator and has the deﬁnite advantage that it is superior \\nfor enhancing ﬁne detail. However, this causes it to produce noisier results than \\nthe gradient. This noise is most objectionable in smooth areas, where it tends to be \\nmore visible. The gradient has a stronger response in areas of signiﬁcant intensity \\ntransitions (ramps and steps) than does the Laplacian. The response of the gradient \\nto noise and ﬁne detail is lower than the Laplacian’s and can be lowered further by \\nsmoothing the gradient with a lowpass ﬁlter. The idea, then, is to smooth the gradient \\nand multiply it by the Laplacian image. In this context, we may view the smoothed \\ngradient as a mask image. The product will preserve details in the strong areas, while', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='reducing noise in the relatively ﬂat areas. This process can be interpreted roughly as \\ncombining the best features of the Laplacian and the gradient. The result is added to \\nthe original to obtain a ﬁnal sharpened image.\\nFigure 3.57(d) shows the Sobel gradient of the original image, computed using \\nEq. (3-59). Components \\ng\\nx\\n and \\ng\\ny\\n were obtained using the kernels in Figs. 3.50(d) \\nand (e), respectively. As expected, the edges are much more dominant in this image \\nthan in the Laplacian image. The smoothed gradient image in Fig. 3.57(e) was \\nobtained by using a box ﬁlter of size \\n55\\n×\\n.\\n The fact that Figs. 3.57(d) and (e) are \\nmuch brighter than F\\nig. 3.57(b) is further evidence that the gradient of an image \\nwith signiﬁcant edge content has values that are higher in general than in a Lapla-\\ncian image.\\nFigure 3.57(f) shows the product of the Laplacian and smoothed gradient image. \\nNote the dominance of the strong edges and the relative lack of visible noise, which', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='is the reason for masking the Laplacian with a smoothed gradient image. Adding the \\nproduct image to the original resulted in the sharpened image in Fig. 3.57(g). The \\nincrease in sharpness of detail in this image over the original is evident in most parts \\nof the image, including the ribs, spinal cord, pelvis, and skull. This type of improve-\\nment would not have been possible by using the Laplacian or the gradient alone.\\nThe sharpening procedure just discussed did not affect in an appreciable way the \\ndynamic range of the intensity levels in an image. Thus, the ﬁnal step in our enhance-\\nment task is to increase the dynamic range of the sharpened image. As we discussed \\nin some detail in Sections 3.2 and 3.3, there are several intensity transformation \\nfunctions that can accomplish this objective. Histogram processing is not a good \\napproach on images whose histograms are characterized by dark and light compo-', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='nents, which is the case here. The dark characteristics of the images with which we \\nare dealing lend themselves much better to a power-law transformation. Because \\nwe wish to spread the intensity levels, the value of \\ng\\n in Eq. (3-5) has to be less than 1. \\nAfter a few trials with this equation, we arrived at the result in Fig. 3.57(h), obtained \\nwith \\ng\\n=\\n0\\n5\\n.\\n and \\nc\\n=\\n1.\\n Comparing this image with Fig. 3.57(g), we note that signiﬁ-\\ncant new detail is visible in F\\nig. 3.57(h). The areas around the wrists, hands, ankles, \\nand feet are good examples of this. The skeletal bone structure also is much more \\npronounced, including the arm and leg bones. Note the faint deﬁnition of the outline \\nof the body, and of body tissue. Bringing out detail of this nature by expanding the \\ndynamic range of the intensity levels also enhanced noise, but Fig. 3.57(h) is a signiﬁ-\\ncant visual improvement over the original image.\\nDIP4E_GLOBAL_Print_Ready.indb   194\\n6/16/2017   2:04:11 PM', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='www.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 195}),\n",
       " Document(page_content='3.8\\n  \\nCombining Spatial Enhancement Methods\\n    \\n195\\nf e\\nh\\ng\\nFIGURE 3.57\\n(Continued)\\n \\n(e) Sobel image \\nsmoothed with a \\n55\\n×\\n box ﬁlter.  \\n(f) Mask image \\nformed by the \\nproduct of (b)  \\nand (e).\\n  \\n(g) Sharpened \\nimage obtained \\nby the adding \\nimages (a) and (f). \\n(h) Final result \\nobtained by  \\napplying a power-\\nlaw transformation \\nto (g). Compare \\nimages (g) and (h) \\nwith (a). (Original \\nimage courtesy \\nof G.E. Medical \\nSystems.) \\nDIP4E_GLOBAL_Print_Ready.indb   195\\n6/16/2017   2:04:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 196}),\n",
       " Document(page_content='196\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nSummary, References, and Further Reading\\n  \\nThe material in this chapter is representative of current techniques used for intensity transformations and spatial \\nﬁltering. The topics were selected for their value as fundamental material that would serve as a foundation in an \\nevolving ﬁeld. Although most of the examples used in this chapter deal with image enhancement, the techniques \\npresented are perfectly general, and you will encounter many of them again throughout the remaining chapters in \\ncontexts unrelated to enhancement.\\nThe material in Section 3.1 is from Gonzalez [1986]. For additional reading on the material in Section 3.2, see \\nSchowengerdt [2006] and Poyton [1996]. Early references on histogram processing (Section 3.3) are Gonzalez and \\nFittes [1977], and Woods and Gonzalez [1981]. Stark [2000] gives some interesting generalizations of histogram \\nequalization for adaptive contrast enhancement.', metadata={'source': 'imagepro.pdf', 'page': 197}),\n",
       " Document(page_content='For complementary reading on linear spatial ﬁltering (Sections 3.4-3.7), see Jain [1989], Rosenfeld and Kak \\n[1982], Schowengerdt [2006], Castleman [1996], and Umbaugh [2010]. For an interesting approach for generating \\nGaussian kernels with integer coefﬁcients see Padﬁeld [2011]. The book by Pitas and Venetsanopoulos [1990] is a \\ngood source for additional reading on median and other nonlinear spatial ﬁlters.\\nFor details on the software aspects of many of the examples in this chapter, see Gonzalez, Woods, and Eddins \\n[2009].\\nProblems\\n  \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com). \\n3.1 \\nGive a single intensity transformation function \\nfor spreading the intensities of an image so the \\nlowest intensity is 0 and the highest is \\nL\\n−\\n1.\\n3.2 \\nDo the following:\\n(a) * \\nGive a continuous function for implement-\\ning the contrast stretching transformation in \\nF', metadata={'source': 'imagepro.pdf', 'page': 197}),\n",
       " Document(page_content='F\\nig. 3.2(a). In addition to \\nm\\n, your function \\nmust include a parameter, \\nE\\n, for control-\\nling the slope of the function as it transi-\\ntions from low to high intensity values. Your \\nfunction should be normalized so that its \\nminimum and maximum values are 0 and 1, \\nrespectively.\\n(b) \\nSketch a family of transformations as a \\nfunction of parameter \\nE\\n,\\n for a ﬁxed value \\nmL\\n=\\n2,\\n where \\nL\\n is the number of intensity \\nlevels in the image\\n..\\n3.3 \\nDo the following:\\n(a) * \\nPropose a \\nset\\n of intensity-slicing transforma-\\ntion functions capable of producing all the \\nindividual bit planes of an 8-bit monochrome \\nimage\\n. For example, applying to an image a \\ntransformation function with the property \\nTr\\n()\\n=\\n0\\n if \\nr\\n is 0 or even,\\n and \\nTr\\n()\\n=\\n1\\n if \\nr\\n is \\nodd,\\n produces an image of the least signiﬁ-\\ncant bit plane (see Fig. 3.13). (\\nHint:\\n Use an \\n8-bit truth table to determine the form of \\neach transformation function.)\\n(b) \\nHow many intensity transformation functions', metadata={'source': 'imagepro.pdf', 'page': 197}),\n",
       " Document(page_content='would there be for 16-bit images?\\n(c) \\nIs the basic approach in (a) limited to images \\nin which the number of intensity levels is an \\ninteger power of 2,\\n or is the method general \\nfor any number of \\nintege\\nr intensity levels?\\n(d) \\nIf the method is general, how would it be dif-\\nferent from your solution in (a)?\\n3.4 \\nDo the following:\\n(a) \\nPropose a method for extracting the bit planes \\nof an image based on converting the value of \\nits pixels to binary\\n. \\n(b) \\nFind all the bit planes of the following 4-bit \\nimage:\\n \\n01 8 6\\n22 1 1\\n11 51 41 2\\n36 91 0\\n3.5 \\nIn general:\\n(a) * \\nWhat effect would setting to zero the lower-\\nDIP4E_GLOBAL_Print_Ready.indb   196\\n6/16/2017   2:04:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 197}),\n",
       " Document(page_content='Problems\\n    \\n197\\norder bit planes have on the histogram of an \\nimage?\\n(b) \\nWhat would be the effect on the histogram \\nif we set to zero the higher\\n-order bit planes \\ninstead?\\n3.6 \\nExplain why the discrete histogram equalization \\ntechnique does not yield a ﬂat histogram in gen-\\neral.\\n3.7 \\nSuppose that a digital image is subjected to histo-\\ngram equalization.\\n Show that a second pass of his-\\ntogram equalization (on the histogram-equalized \\nimage) will produce exactly the same result as the \\nﬁrst pass.\\n3.8 \\nAssuming continuous values, show by an exam-\\nple that it is possible to have a case in which the \\ntransformation function given in Eq.\\n (3-11) satis-\\nﬁes conditions (a) and (b) discussed in Section 3.3, \\nbut its inverse may fail condition (a\\n/H11032\\n).\\n3.9 \\nDo the following:\\n(a) \\nShow that the discrete transformation func-\\ntion given in Eq.\\n (3-15) for histogram equal-\\nization satisﬁes conditions (a) and (b) stated \\nat the beginning of Section 3.3.\\n(b) *', metadata={'source': 'imagepro.pdf', 'page': 198}),\n",
       " Document(page_content='(b) * \\nShow that the inverse discrete transforma-\\ntion in Eq.\\n (3-16) satisﬁes conditions (a\\n/H11032\\n) \\nand (b) in Section 3.3 \\nonly if \\nnone of the \\nintensity levels \\nr\\nk\\n, \\nkL\\n=−\\n012\\n1\\n,,, , ,\\n…\\n are \\nmissing in the original image\\n.\\n3.10 \\nTwo images, \\nfx y\\n(,\\n)\\n and \\ngxy\\n(,\\n)\\n have unnormalized \\nhistograms \\nh\\nf\\n and \\nh\\ng\\n.\\n Give the conditions (on the \\nvalues of the pixels in \\nf\\n and \\ng\\n) under which you \\ncan determine the histograms of images formed \\nas follows:\\n(a) * \\nfx\\ny g x y\\n(,) (,)\\n+\\n(b) \\nfx y g x y\\n(,) (,)\\n−\\n(c) \\nfx y g x y\\n(,) (,)\\n×\\n(d) \\nfx y g x y\\n(,) (,)\\n÷\\nShow how the histograms would be formed in \\neach case\\n. The arithmetic operations are element-\\nwise operations, as deﬁned in Section 2.6.\\n3.11 \\nAssume continuous intensity values, and sup-\\npose that the intensity values of an image have \\nthe PDF \\np\\nrr L\\nr\\n() ( )\\n=−\\n21\\n2\\n for \\n01\\n≤≤\\nrL\\n−\\n,\\n and \\npr\\nr\\n()\\n=\\n0 for other values of \\nr\\n. \\n(a) * \\nFind the transformation function that will \\nmap the input intensity values\\n, \\nr', metadata={'source': 'imagepro.pdf', 'page': 198}),\n",
       " Document(page_content=', \\nr\\n, into values, \\ns\\n, of a histogram-equalized image.\\n(b) * \\nFind the transformation function that (when \\napplied to the histogram-equalized intensi-\\nties\\n, \\ns\\n) will produce an image whose intensity \\nPDF is \\npz z L\\nz\\n() ( )\\n=−\\n31\\n23\\n for \\n01\\n≤≤\\nzL\\n−\\n \\nand \\npz\\nz\\n()\\n=\\n0 for other values of \\nz\\n.\\n(c) \\nExpress the transformation function from (b) \\ndirectly in terms of \\nr\\n,\\n the intensities of the \\ninput image.\\n3.12 \\nAn image with intensities in the range \\n[,]\\n01\\n has \\nthe PDF\\n, \\npr\\nr\\n() ,\\n shown in the following ﬁgure. It \\nis desired to transform the intensity levels of this \\nimage so that they will have the speciﬁed \\npz\\nz\\n()\\n \\nshown in the ﬁgure\\n. Assume continuous quantities, \\nand ﬁnd the transformation (expressed in terms \\nof \\nr\\n and \\nz\\n) that will accomplish this.\\n2\\n1\\n2\\n1\\np\\nr\\n(\\nr\\n)\\np\\nz\\n(\\nz\\n)\\nr\\nz\\n3.13 * \\nIn Fig. 3.25(b), the transformation function \\nlabeled (2) \\n[\\nGs\\nk\\n−\\n1\\n()\\n from Eq. (3-23)] is the mirror image of \\n(1) [\\nGz\\nq\\n()\\n in Eq. (3-21)] about a line joining the', metadata={'source': 'imagepro.pdf', 'page': 198}),\n",
       " Document(page_content='two end points\\n. Does this property always hold \\nfor these two transformation functions? Explain.\\n3.14 * \\nThe local histogram processing method discussed \\nin Section 3.3 requires that a histogram be com-\\nputed at each neighborhood location.\\n Propose \\na method for updating the histogram from one \\nneighborhood to the next, rather than computing \\na new histogram each time.\\n3.15 \\nWhat is the behavior of Eq. (3-35) when \\nab\\n==\\n0?\\n \\nExplain.\\n3.16 \\nYou are given a computer chip that is capable of \\nperforming linear ﬁltering in real time\\n, but you \\nare not told whether the chip performs correla-\\ntion or convolution. Give the details of a test you \\nwould perform to determine which of the two \\noperations the chip performs.\\n3.17 * \\nWe mentioned in Section 3.4 that to perform con-\\nDIP4E_GLOBAL_Print_Ready.indb   197\\n6/16/2017   2:04:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 198}),\n",
       " Document(page_content='198\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nvolution we rotate the kernel by \\n180\\n°\\n.\\n The rota-\\ntion is \\n“built” into Eq. (3-35). Figure 3.28 corre-\\nsponds to correlation. Draw the part of the ﬁgure \\nenclosed by the large ellipse, but with \\nw\\n rotated   \\n180\\n°\\n.\\n Expand Eq. (3-35) for a general \\n33\\n×\\n kernel \\nand show that the result of your expansion corre-\\nsponds to your ﬁgure\\n. This shows graphically that \\nconvolution and correlation differ by the rotation \\nof the kernel.\\n3.18 \\nYou are given the following kernel and image:\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n121\\n242\\n121\\n00000\\n00100\\n00100\\n00100\\n00000\\nf\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n(a) * \\nGive a sketch of the area encircled by the \\nlarge ellipse in F\\nig. 3.28 when the kernel is \\ncentered at point \\n(,)\\n23\\n (2nd row, 3rd col) of \\nthe image shown above\\n. Show speciﬁc values \\nof \\nw\\n and \\nf\\n.\\n(b) * \\nCompute the convolution \\nw\\n/H22841\\nf\\n using the \\nminimum\\n zero padding needed.\\n Show the', metadata={'source': 'imagepro.pdf', 'page': 199}),\n",
       " Document(page_content='Show the \\ndetails of your computations when the ker-\\nnel is centered on point \\n(,)\\n23\\n of \\nf\\n;\\n and then \\nshow the ﬁnal full convolution result.\\n(c) \\nRepeat (b), but for correlation, \\nw\\n/H22845\\nf\\n.\\n3.19 * \\nProve the validity of Eqs. (3-36) and (3-37).\\n3.20 \\nThe kernel, \\nw\\n,  in Problem 3.18 is separable.\\n(a) * \\nBy inspection, ﬁnd two kernels, \\nw\\n1\\n and \\nw\\n2\\n so \\nthat \\nww w\\n=\\n12\\n/H22841\\n.\\n(b) \\nUsing the image in Problem 3.18, compute \\nw\\n1\\n/H22841\\nf\\n using the \\nminimum\\n zero padding (see \\nF\\nig. 3.30). Show the details of your compu-\\ntation when the kernel is centered at point \\n(,)\\n23\\n (2nd row, 3rd col) of \\nf\\n and then show \\nthe full convolution.\\n(c) \\nCompute the convolution of \\nw\\n2\\n with the \\nresult from (b). Show the details of your \\ncomputation when the kernel is centered at \\npoint \\n(,)\\n33\\n of the result from (b), and then \\nshow the full convolution.\\n Compare with the \\nresult in Problem 3.18(b).\\n3.21 \\nGiven the following kernel and image:\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n⎥\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤', metadata={'source': 'imagepro.pdf', 'page': 199}),\n",
       " Document(page_content='=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎤\\n⎦\\n121\\n242\\n121\\n11111\\n11111\\n11111\\n11111\\n11111\\nf\\n⎥ ⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n(a) \\nGive the convolution of the two.\\n(b) \\nDoes your result have a bias?\\n3.22 \\nAnswer the following:\\n(a) * \\nIf \\nv\\n=\\n[]\\n121\\nT\\n and \\nw\\nT\\n=\\n[]\\n2113 ,\\n is the \\nkernel formed by \\nvw\\nT\\n separable?\\n(b) \\nThe following kernel is separable. Find \\nw\\n1\\n \\nand \\nw\\n2\\n such that \\nww w\\n=\\n12\\n/H22841\\n.\\nw\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n131\\n262\\n3.23 \\nDo the following:\\n(a) * \\nShow that the Gaussian kernel, \\nGst\\n(,\\n) ,\\n in \\nEq.\\n (3-45) is separable. (\\nHint:\\n Read the ﬁrst \\nparagraph in the discussion of separable ﬁl-\\nter kernels in Section 3.4.)\\n(b) \\nBecause \\nG\\n is separable and circularly sym-\\nmetric\\n, it can be expressed in the form \\nG\\nT\\n=\\nvv\\n.\\n Assume that the kernel form in \\nEq.\\n (3-46) is used, and that the function is \\nsampled to yield an \\nmm\\n×\\n kernel. What is \\nv\\n \\nin this case?\\n3.24 * \\nShow that the product of a column vector with a \\nrow vector is equivalent to the 2-D convolution \\nof the two vectors\\n. The vectors do not have to', metadata={'source': 'imagepro.pdf', 'page': 199}),\n",
       " Document(page_content='be of the same length. You may use a graphical \\napproach (as in Fig. 3.30) to support the explana-\\ntion of your proof.\\n3.25 \\nGiven \\nK\\n,\\n 1-D Gaussian kernels, \\ngg g\\nK\\n12\\n,,, ,\\n…\\n with \\narbitrary means and standard deviations:\\n(a) * \\nDetermine what the entries in the third col-\\numn of \\nTable 3.6 would be for the product \\ngg g\\nK\\n12\\n×× ×\\n/midhorizellipsis\\n. \\n(b) \\nWhat would the fourth column look like for \\nthe convolution \\ngg g\\nK\\n12\\n/H22841/H22841 /H22841\\n/midhorizellipsis\\n?\\n(\\nHint:\\n It is easier to work with the variance;\\n the \\nstandard deviation is just the square root of your \\nresult.)\\nDIP4E_GLOBAL_Print_Ready.indb   198\\n6/16/2017   2:04:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 199}),\n",
       " Document(page_content='Problems\\n    \\n199\\n3.26 \\nThe two images shown in the following ﬁgure are \\nquite different,\\n but their histograms are the same. \\nSuppose that each image is blurred using a \\n33\\n×\\n \\nbox kernel.\\n(a) * \\nWould the histograms of the blurred images \\nstill be equal? Explain.\\n(b) \\nIf your answer is no, either sketch the two \\nhistograms or give two tables detailing the \\nhistogram components\\n.\\n3.27 \\nAn image is ﬁltered four times using a Gaussian \\nkernel of size \\n33\\n×\\n with a standard deviation of \\n1.0.\\n Because of the associative property of con-\\nvolution, we know that equivalent results can be \\nobtained using a single Gaussian kernel formed \\nby convolving the individual kernels.\\n(a) * \\nWhat is the size of the single Gaussian ker-\\nnel?\\n(b) \\nWhat is its standard deviation?\\n3.28 \\nAn image is ﬁltered with three Gaussian lowpass \\nkernels of sizes \\n33\\n×\\n, \\n55\\n×\\n,\\n and \\n77\\n×\\n,\\n and stan-\\ndard deviations 1.5,\\n 2, and 4, respectively. A com-\\nposite ﬁlter, \\nw\\n,\\n is formed as the convolution of', metadata={'source': 'imagepro.pdf', 'page': 200}),\n",
       " Document(page_content='these three ﬁlters\\n.\\n(a) * \\nIs the resulting ﬁlter Gaussian? Explain.\\n(b) \\nWhat is its standard deviation?\\n(c) \\nWhat is its size?\\n3.29 * \\nDiscuss the limiting effect of repeatedly ﬁltering \\nan image with a \\n33\\n×\\n lowpass ﬁlter kernel. You \\nmay ignore border effects\\n.\\n3.30 \\nIn Fig. 3.42(b) the corners of the estimated shad-\\ning pattern appear darker or lighter than their \\nsurrounding areas\\n. Explain the reason for this.\\n3.31 * \\nAn image is ﬁltered with a kernel whose coefﬁ-\\ncients sum to 1.\\n Show that the sum of the pixel \\nvalues in the original and ﬁltered images is the \\nsame.\\n3.32 \\nAn image is ﬁltered with a kernel whose coefﬁ-\\ncients sum to 0.\\n Show that the sum of the pixel \\nvalues in the ﬁltered image also is 0.\\n3.33 \\nA single point of light can be modeled by a digital \\nimage consisting of all 0’\\ns, with a 1 in the location \\nof the point of light. If you view a single point of \\nlight through a defocused lens, it will appear as a \\nfuzzy blob whose size depends on the amount by', metadata={'source': 'imagepro.pdf', 'page': 200}),\n",
       " Document(page_content='which the lens is defocused. We mentioned in Sec-\\ntion 3.5 that ﬁltering an image with a box kernel \\nis a poor model for a defocused lens, and that a \\nbetter approximation is obtained with a Gauss-\\nian kernel. Using the single-point-of-light analogy, \\nexplain why this is so.\\n3.34 \\nIn the original image used to generate the three \\nblurred images shown,\\n the vertical bars are 5 pix-\\nels wide, 100 pixels high, and their separation is \\n20 pixels. The image was blurred using square box \\nkernels of sizes 23, 25, and 45 elements on the side, \\nrespectively. The vertical bars on the left, lower \\npart of (a) and (c) are blurred, but a clear separa-\\ntion exists between them. \\n(a)\\n(b)\\n(c)\\nHowever, the bars have merged in image (b), de-\\nspite the fact that the kernel used to generate this \\nimage is much smaller than the kernel that pro-\\nduced image (c). Explain the reason for this.\\n3.35 \\nConsider an application such as in Fig. 3.41, in \\nwhich it is desired to eliminate objects smaller', metadata={'source': 'imagepro.pdf', 'page': 200}),\n",
       " Document(page_content='than those enclosed by a square of size \\nqq\\n×\\n pix-\\nels\\n. Suppose that we want to reduce the average \\nDIP4E_GLOBAL_Print_Ready.indb   199\\n6/16/2017   2:04:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 200}),\n",
       " Document(page_content='200\\n    \\nChapter\\n \\n3\\n  \\nIntensity Transformations and Spatial Filtering\\nintensity of those objects to one-tenth of their \\noriginal average value\\n. In this way, their intensity \\nwill be closer to the intensity of the background \\nand they can be eliminated by thresholding. Give \\nthe (odd) size of the smallest box kernel that will \\nyield the desired reduction in average intensity in \\nonly one pass of the kernel over the image. \\n3.36 \\nWith reference to order-statistic ﬁlters (see Sec-\\ntion 3.5):\\n(a) * \\nWe mentioned that isolated clusters of dark \\nor light (with respect to the background) pix-\\nels whose area is less than one-half the area \\nof a median ﬁlter are forced to the median \\nvalue of the neighbors by the ﬁlter\\n. Assume \\na ﬁlter of size \\nnn\\n×\\n (\\nn\\n odd) and explain why \\nthis is so\\n.\\n(b) \\nConsider an image having various sets of \\npixel clusters\\n. Assume that all points in a \\ncluster are lighter or darker than the back-\\nground (but not both simultaneously in the', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='same cluster), and that the area of each clus-\\nter is less than or equal to \\nn\\n2\\n2.\\n In terms of \\nn\\n,\\n under what condition would one or more \\nof these clusters cease to be isolated in the \\nsense described in part (a)?\\n3.37 \\nDo the following:\\n(a) * \\nDevelop a procedure for computing the median \\nof an \\nnn\\n×\\n neighborhood.\\n(b) \\nPropose a technique for updating the median \\nas the center of the neighborhood is moved \\nfrom pixel to pixel.\\n \\n3.38 \\nIn a given application, a smoothing kernel is \\napplied to input images to reduce noise\\n, then a \\nLaplacian kernel is applied to enhance ﬁne details. \\nWould the result be the same if the order of these \\noperations is reversed?\\n3.39 * \\nShow that the Laplacian deﬁned in Eq. (3-50) is \\nisotropic (invariant to rotation).\\n Assume continu-\\nous quantities. From Table 2.3, coordinate rota-\\ntion by an angle \\nu\\n is given by \\n   \\nxx y yx y\\n/H11032/H11032\\n=−\\n= +\\ncos sin\\nsin cos\\nuu uu\\n  and  \\nwhere \\n(,)\\nxy\\n and \\n(,)\\nxy\\n/H11032/H11032\\n are the unrotated and', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='rotated coordinates\\n, respectively.\\n3.40 * \\nYou saw in Fig. 3.46 that the Laplacian with a \\n−\\n8 \\nin the center yields sharper results than the one \\nwith a \\n−\\n4 in the center. Explain the reason why.\\n3.41 * \\nGive a \\n33\\n×\\n kernel for performing unsharp mask-\\ning in a single pass through an image\\n. Assume that \\nthe average image is obtained using a box ﬁlter of \\nsize \\n33\\n×\\n.\\n3.42 \\nShow that subtracting the Laplacian from an im-\\nage gives a result that is proportional to the un-\\nsharp mask in Eq.\\n (3-55). Use the deﬁnition for \\nthe Laplacian given in Eq. (3-53).\\n3.43 \\nDo the following:\\n(a) * \\nShow that the magnitude of the gradient giv-\\nen in Eq.\\n (3-58) is an isotropic operation (see \\nthe statement of Problem 3.39).\\n(b) \\nShow that the isotropic property is lost in \\ngeneral if the gradient is computed using \\nEq.\\n (3-59).\\n3.44 \\nAre any of the following highpass (sharpening)\\nkernels separable? F\\nor those that are, ﬁnd vectors \\nv\\n and \\nw\\n such that \\nvw\\nT\\n equals the kernel(s).\\n(a)', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='(a) \\nThe Laplacian kernels in Figs. 3.45(a) and (b).\\n(b) \\nThe Roberts cross-gradient kernels shown in \\nF\\nigs. 3.50(b) and (c).\\n(c) * \\nThe Sobel kernels in Figs. 3.50(d) and (e).\\n3.45 \\nIn a character recognition application, text pages \\nare reduced to binary using a thresholding trans-\\nformation function of the form in F\\nig. 3.2(b). This \\nis followed by a procedure that thins the charac-\\nters until they become strings of binary 1’s on a \\nbackground of 0’s. Due to noise, binarization and \\nthinning result in broken strings of characters \\nwith gaps ranging from 1 to 3 pixels. One way \\nto “repair” the gaps is to run a smoothing kernel \\nover the binary image to blur it, and thus create \\nbridges of nonzero pixels between gaps.\\n(a) * \\nGive the (odd) size of the smallest box ker-\\nnel capable of performing this task.\\n(b) \\nAfter bridging the gaps, the image is thresh-\\nolded to convert it back to binary form.\\n For \\nyour answer in (a), what is the minimum val-', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='ue of the threshold required to accomplish \\nthis, without causing the segments to break \\nup again?\\nDIP4E_GLOBAL_Print_Ready.indb   200\\n6/16/2017   2:04:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 201}),\n",
       " Document(page_content='Problems\\n    \\n201\\n3.46 \\nA manufacturing company purchased an imag-\\ning system whose function is to either smooth \\nor sharpen images\\n. The results of using the sys-\\ntem on the manufacturing ﬂoor have been poor, \\nand the plant manager suspects that the system \\nis not smoothing and sharpening images the way \\nit should. You are hired as a consultant to deter-\\nmine if the system is performing these functions \\nproperly. How would you determine if the system \\nis working correctly? (\\nHint: \\nStudy the statements \\nof Problems 3.31 and 3.32).\\n3.47 \\nA CCD TV camera is used to perform a long-term \\nstudy b\\ny \\nobserving the same area 24 hours a day, \\nfor \\n30 days. Digital images are captured and transmit-\\nted to a central location every 5 minutes. The illu-\\nmination of the scene changes from natural day-\\nlight to artiﬁcial lighting. At no time is the scene \\nwithout illumination, so it is always possible to \\nobtain an acceptable image. Because the range of', metadata={'source': 'imagepro.pdf', 'page': 202}),\n",
       " Document(page_content='illumination is such that it is always in the linear \\noperating range of the camera, it is decided not \\nto employ any compensating mechanisms on the \\ncamera itself. Rather, it is decided to use image \\nprocessing techniques to post-process, and thus \\nnormalize, the images to the equivalent of con-\\nstant illumination. Propose a method to do this. \\nYou are at liberty to use any method you wish, \\nbut state clearly all the assumptions you made in \\narriving at your design. \\nDIP4E_GLOBAL_Print_Ready.indb   201\\n6/16/2017   2:04:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 202}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 203}),\n",
       " Document(page_content='2034\\nFiltering in the Frequency \\nDomain\\nPreview\\nAfter a brief historical introduction to the Fourier transform and its importance in image processing, we \\nstart from basic principles of function sampling, and proceed step-by-step to derive the one- and two-\\ndimensional discrete Fourier transforms. Together with convolution, the Fourier transform is a staple of \\nfrequency-domain processing. During this development, we also touch upon several important aspects \\nof sampling, such as aliasing, whose treatment requires an understanding of the frequency domain and \\nthus are best covered in this chapter. This material is followed by a formulation of ﬁltering in the fre-\\nquency domain, paralleling the spatial ﬁltering techniques discussed in Chapter 3. We conclude the \\nchapter with a derivation of the equations underlying the fast Fourier transform (FFT), and discuss its \\ncomputational advantages. These advantages make frequency-domain ﬁltering practical and, in many', metadata={'source': 'imagepro.pdf', 'page': 204}),\n",
       " Document(page_content='instances, superior to ﬁltering in the spatial domain.\\nUpon completion of this chapter, readers should:\\n Understand the meaning of frequency domain \\nﬁltering, and how it differs from ﬁltering in the \\nspatial domain.\\n Be familiar with the concepts of sampling,  func- \\ntion reconstruction, and aliasing.\\n Understand convolution in the frequency \\ndomain, and how it is related to ﬁltering.\\n Know how to obtain frequency domain ﬁlter \\nfunctions from spatial kernels, and vice versa.\\n Be able to construct ﬁlter transfer functions \\ndirectly in the frequency domain.\\n Understand why image padding is important.\\n Know the steps required to perform ﬁltering \\nin the frequency domain.\\n Understand when frequency domain ﬁltering \\nis superior to ﬁltering in the spatial domain.\\n Be familiar with other ﬁltering techniques in \\nthe frequency domain, such as unsharp mask-\\ning and homomorphic ﬁltering.\\n Understand the origin and mechanics of the \\nfast Fourier transform, and how to use it effec-', metadata={'source': 'imagepro.pdf', 'page': 204}),\n",
       " Document(page_content='tively in image processing. \\nFilter:\\n A device or material for suppressing or minimizing waves or \\noscillations of certain frequencies.\\nFrequency:\\n The number of times that a periodic function repeats \\nthe same sequence of values during a unit variation of the  \\nindependent variable.\\nWebster’s New Collegiate Dictionary\\nDIP4E_GLOBAL_Print_Ready.indb   203\\n6/16/2017   2:04:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 204}),\n",
       " Document(page_content='204\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.1 BACKGROUND  \\nWe begin the discussion with a brief outline of the origins of the Fourier transform \\nand its impact on countless branches of mathematics, science, and engineering.\\nA BRIEF HISTORY OF THE FOURIER SERIES AND TRANSFORM\\nThe French mathematician Jean Baptiste Joseph Fourier was born in 1768 in the \\ntown of Auxerre, about midway between Paris and Dijon. The contribution for \\nwhich he is most remembered was outlined in a memoir in 1807, and later pub-\\nlished in 1822 in his book, \\nLa Théorie Analitique de la Chaleur\\n (\\nThe Analytic Theory \\nof Heat\\n). This book was translated into English 55 years later by Freeman (see \\nFreeman [1878]). Basically, Fourier’s contribution in this field states that any peri-\\nodic function can be expressed as the sum of sines and/or cosines of different fre-\\nquencies, each multiplied by a different coefficient (we now call this sum a \\nFourier \\nseries', metadata={'source': 'imagepro.pdf', 'page': 205}),\n",
       " Document(page_content='Fourier \\nseries\\n). It does not matter how complicated the function is; if it is periodic and satis-\\nfies some mild mathematical conditions, it can be represented by such a sum. This \\nis taken for granted now but, at the time it first appeared, the concept that compli-\\ncated functions could be represented as a sum of simple sines and cosines was not \\nat all intuitive (see Fig. 4.1). Thus, it is not surprising that Fourier’s ideas were met \\ninitially with skepticism.\\nFunctions that are not periodic (but whose area under the curve is ﬁnite) can be \\nexpressed as the integral of sines and/or cosines multiplied by a weighting function. \\nThe formulation in this case is the \\nFourier transform\\n, and its utility is even greater \\nthan the Fourier series in many theoretical and applied disciplines. Both representa-\\ntions share the important characteristic that a function, expressed in either a Fourier \\nseries or transform, can be reconstructed (recovered) completely via an inverse pro-', metadata={'source': 'imagepro.pdf', 'page': 205}),\n",
       " Document(page_content='cess, with no loss of information. This is one of the most important characteristics of \\nthese representations because it allows us to work in the \\nFourier domain\\n (generally \\ncalled the \\nfrequency domain\\n) and then return to the original domain of the function \\nwithout losing any information. Ultimately, it is the utility of the Fourier series and \\ntransform in solving practical problems that makes them widely studied and used as \\nfundamental tools. \\nThe initial application of Fourier’s ideas was in the ﬁeld of heat diffusion, where \\nthey allowed formulation of differential equations representing heat ﬂow in such \\na way that solutions could be obtained for the ﬁrst time. During the past century, \\nand especially in the past 60 years, entire industries and academic disciplines have \\nﬂourished as a result of Fourier’s initial ideas. The advent of digital computers and \\nthe “discovery” of a fast Fourier transform (FFT) algorithm in the early 1960s revo-', metadata={'source': 'imagepro.pdf', 'page': 205}),\n",
       " Document(page_content='lutionized the ﬁeld of signal processing. These two core technologies allowed for the \\nﬁrst time practical processing of a host of signals of exceptional importance, ranging \\nfrom medical monitors and scanners to modern electronic communications.\\nAs you learned in Section 3.4, it takes on the order of \\nMNmn \\noperations (multi-\\nplications and additions) to ﬁlter an \\nM\\nN\\n×\\n image with a kernel of size \\nmn\\n×\\n ele-\\nments\\n. If the kernel is separable, the number of operations is reduced to \\nMN m n\\n()\\n.\\n+\\n \\nIn Section 4.11,\\n you will learn that it takes on the order of \\n2\\n2\\nMN MN\\nlog\\n operations \\nto perform the equivalent ﬁltering process in the frequenc\\ny domain, where the 2 in \\nfront arises from the fact that we have to compute a forward and an inverse FFT. \\n4.1\\nDIP4E_GLOBAL_Print_Ready.indb   204\\n6/16/2017   2:04:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 205}),\n",
       " Document(page_content='4.1\\n  \\nBackground\\n    \\n205\\nTo get an idea of the relative computational advantages of ﬁltering in the frequency  \\nversus the spatial domain, consider square images and kernels, of sizes \\nMM\\n×\\n and \\nmm\\n×\\n,\\n respectively. The\\n computational advantage\\n (as a function of kernel size) of \\nﬁltering one such image with the FFT as opposed to using a nonseparable kernel is \\ndeﬁned as\\n \\nCm\\nMm\\nMM\\nm\\nM\\nn\\n()\\nlog\\nlog\\n=\\n=\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n4\\n \\n(4-1)\\nIf the kernel is separable, the advantage becomes\\n \\nCm\\nMm\\nMM\\nm\\nM\\ns\\n()\\nlog\\nlog\\n=\\n=\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n \\n(4-2)\\nIn either case, when \\nCm\\n()\\n>\\n1\\n the advantage (in terms of fewer computations) \\nbelongs to the FFT approach;\\n otherwise the advantage favors spatial filtering.\\nFIGURE 4.1\\nThe function at \\nthe bottom is the \\nsum of the four \\nfunctions above it. \\nFourier’s idea in \\n1807 that periodic \\nfunctions could be \\nrepresented as a \\nweighted sum of \\nsines and cosines \\nwas met with \\nskepticism. \\nDIP4E_GLOBAL_Print_Ready.indb   205\\n6/16/2017   2:04:18 PM', metadata={'source': 'imagepro.pdf', 'page': 206}),\n",
       " Document(page_content='www.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 206}),\n",
       " Document(page_content='206\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nFigure 4.2(a) shows a plot of \\nCm\\nn\\n()\\n as a function of \\nm\\n for an image of intermedi-\\nate size \\n() .\\nM\\n=\\n2048\\n The inset table shows a more detailed look for smaller kernel \\nsizes\\n. As you can see, the FFT has the advantage for kernels of sizes \\n77\\n×\\n and larger. \\nT\\nhe advantage grows rapidly as a function of \\nm\\n, being over 200 for \\nm\\n=\\n101\\n,\\n and \\nclose to 1000 for \\nm\\n=\\n201.\\n To give you a feel for the meaning of this advantage, if \\nﬁltering a bank of images of size \\n2048 2048\\n×\\n takes 1 minute with the FFT, it would \\ntake on the order of 17 hours to ﬁlter the same set of images with a nonseparable \\nkernel of size \\n201 201\\n×\\n elements. This is a signiﬁcant difference, and is a clear indica-\\ntor of the importance of frequenc\\ny-domain processing using the FFT.\\nIn the case of separable kernels, the computational advantage is not as dramatic, \\nbut it is still meaningful. The “cross over” point now is around \\nm\\n=\\n27\\n,\\n and when', metadata={'source': 'imagepro.pdf', 'page': 207}),\n",
       " Document(page_content='m\\n=\\n27\\n,\\n and when \\nm\\n=\\n101\\n the difference between frequency- and spatial-domain ﬁltering is still man-\\nageable\\n. However, you can see that with \\nm\\n=\\n201\\n the advantage of using the FFT \\napproaches a factor of 10,\\n which begins to be signiﬁcant. Note in both graphs that \\nthe FFT is an overwhelming favorite for large spatial kernels. \\nOur focus in the sections that follow is on the Fourier transform and its properties. \\nAs we progress through this chapter, it will become evident that Fourier techniques \\nare useful in a broad range of image processing applications. We conclude the chap-\\nter with a discussion of the FFT.\\nABOUT THE EXAMPLES IN THIS CHAPTER\\nAs in Chapter 3, most of the image filtering examples in this chapter deal with image \\nenhancement. For example, smoothing and sharpening are traditionally associated \\nwith image enhancement, as are techniques for contrast manipulation. By its very \\nnature, beginners in digital image processing find enhancement to be interesting', metadata={'source': 'imagepro.pdf', 'page': 207}),\n",
       " Document(page_content='and relatively simple to understand. Therefore, using examples from image enhance-\\nment in this chapter not only saves having an extra chapter in the book but, more \\nimportantly, is an effective tool for introducing newcomers to filtering techniques in \\nthe frequency domain. We will use frequency domain processing methods for other \\napplications in Chapters 5, 7, 8, 10, and 11.\\nThe computational \\nadvantages given by Eqs. \\n(4-1) and (4-2) do not \\ntake into account the fact \\nthat the FFT performs \\noperations between \\ncomplex numbers, and \\nother secondary (but \\nsmall in comparison) \\ncomputations discussed \\nlater in the chapter. Thus, \\ncomparisons should be \\ninterpreted only as  \\nguidelines,\\nC\\ns\\n(\\nm\\n) \\n/H11003 \\n10\\nC\\nn\\n(\\nm\\n) \\n/H11003 \\n10\\n3\\n5\\n10\\n15\\n20\\n25\\n3\\n511\\n1023\\n767\\n255\\nm\\nC\\nn\\n(\\nm\\n)\\nm\\n1\\n2\\n3\\n4\\n5\\n3\\n7\\n11\\n15\\n21\\n27\\nm\\nC\\ns\\n(\\nm\\n)\\n3\\n511\\n1023\\n767\\n255\\nm\\nM = \\n2048\\nM = \\n2048\\n3\\n7\\n11\\n15\\n21\\n27\\n101\\n201\\n0.2\\n1.1\\n2.8\\n5.1\\n10.0\\n16.6\\n232\\n918\\n101\\n201\\n0.1\\n0.3\\n0.5\\n0.7\\n0.9\\n1.2\\n4.6\\n9.1\\nb a\\nFIGURE 4.2', metadata={'source': 'imagepro.pdf', 'page': 207}),\n",
       " Document(page_content='9.1\\nb a\\nFIGURE 4.2\\n(a) Computational \\nadvantage of the \\nFFT over non-\\nseparable spatial \\nkernels.  \\n(b) Advantage over \\nseparable kernels. \\nThe numbers for \\nCm\\n()\\n in the inset \\ntables are not to be \\nmultiplied by the \\nfactors of 10 shown \\nfor the curves\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   206\\n6/16/2017   2:04:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 207}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n207\\n4.2 PRELIMINARY CONCEPTS  \\nWe pause briefly to introduce several of the basic concepts that underlie the mate-\\nrial in later sections. \\nCOMPLEX NUMBERS\\nA complex number, \\nC\\n, is defined as \\n \\nCRj I\\n=+\\n \\n(4-3)\\nwhere \\nR\\n and \\nI\\n are real numbers and \\nj\\n=−\\n1.\\n Here, \\nR\\n denotes the \\nreal part\\n of the \\ncomplex number and \\nI\\n its\\n imaginary part\\n.\\n Real numbers are a subset of complex \\nnumbers in which \\nI\\n=\\n0.\\n The \\nconjugate\\n of a complex number \\nC\\n,\\n denoted \\nC\\n*\\n,\\n is \\ndefined as\\n \\nCR j I\\n*\\n=−\\n \\n(4-4)\\nComplex numbers can be viewed geometrically as points on a plane (called the \\ncom-\\nplex plane\\n) whose abscissa is the \\nreal axis\\n (values of \\nR\\n) and whose ordinate is the \\nimaginary axis\\n (values of \\nI\\n).\\n That is, the complex number \\nRj I\\n+\\n is point \\n(,)\\nRI\\n in the \\ncoordinate system of the complex plane\\n.\\nSometimes it is useful to represent complex numbers in polar coordinates,\\n \\nCC j\\n=+\\n(cos sin )\\nuu\\n \\n(4-5)\\nwhere \\nCR I\\n=+\\n22', metadata={'source': 'imagepro.pdf', 'page': 208}),\n",
       " Document(page_content='where \\nCR I\\n=+\\n22\\n is the length of the vector extending from the origin of the \\ncomplex plane to point \\n(,) ,\\nRI\\n and \\nu\\n is the angle between the vector and the real axis. \\nDrawing a diagram of the real and complex axes with the vector in the first quadrant \\nwill show that \\ntan ( )\\nu\\n=\\nIR\\n or \\nu\\n=\\narctan( ).\\nIR\\n The arctan function returns angles \\nin the range \\n[, ] .\\n−\\npp\\n22\\n \\nBut,\\n because \\nI\\n and \\nR\\n can be positive and negative inde-\\npendently, we need to be able to obtain angles in the full range \\n[, ] .\\n−\\npp\\n We do this \\nby keeping track of the sign of \\nI\\n and \\nR\\n when computing \\nu\\n.\\n Many programming \\nlanguages do this automatically via so called \\nfour\\n-quadrant arctangent functions\\n. For \\nexample, MATLAB provides the function \\natan2(Imag, Real)\\n for this purpose.\\nUsing \\nEuler’s formula\\n,\\n \\nej\\nj\\nu\\nuu\\n=+\\ncos sin\\n \\n(4-6)\\nwhere \\ne\\n=\\n2\\n71828\\n. ...,\\n gives the following familiar representation of complex num-\\nbers in polar coordinates\\n,\\n \\nCC e\\nj\\n=\\nu\\n \\n(4-7)\\nwhere \\nC\\n and \\nu', metadata={'source': 'imagepro.pdf', 'page': 208}),\n",
       " Document(page_content='where \\nC\\n and \\nu\\n are as defined above. For example, the polar representation of the \\ncomplex number \\n12\\n+\\nj\\n is \\n5\\ne\\nj\\nu\\n,\\n where \\nu\\n=\\n63\\n4 .\\n°\\n or 1.1 radians. The preceding equa-\\ntions are applicable also to complex functions\\n. A complex function, \\nF\\n()\\n,\\nu\\n of a real \\nvariable \\nu\\n,\\n can be expressed as the sum \\nFRj I\\n()\\n() (\\nuuu ) ,\\n=+\\n where \\nRu\\n()\\n and \\nIu\\n()\\n are \\nthe real and imaginary component functions of \\nFu\\n()\\n.\\n As previously noted, the com-\\nplex conjugate is \\nFu R u j I u\\n*\\n() () () ,\\n=−\\n the magnitude is \\nFu Ru Iu\\n() [ () () ] ,\\n=+\\n22\\n12\\n \\n4.2\\nDIP4E_GLOBAL_Print_Ready.indb   207\\n6/16/2017   2:04:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 208}),\n",
       " Document(page_content='208\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nand the angle is \\nu\\n( ) arctan[ ( ) ( )].\\nuI u R u\\n=\\n We will return to complex functions sev-\\neral times in the course of this and the next chapter\\n.\\nFOURIER SERIES\\nAs indicated in the previous section, a function \\nft\\n()\\n of a continuous variable, \\nt\\n, \\nthat is periodic with a period,\\n \\nT\\n, can be expressed as the sum of sines and cosines \\nmultiplied by appropriate coefﬁcients. This sum, known as a \\nFourier\\n \\nseries\\n, has the \\nform\\n \\nft ce\\nn\\nj\\nn\\nT\\nt\\nn\\n()\\n=\\n=−\\n∑\\n2\\np\\n/H11009\\n/H11009\\n \\n(4-8)\\nwhere\\n \\nc\\nT\\nfte d t n\\nn\\nj\\nn\\nT\\nt\\nT\\nT\\n==\\n−\\n−\\n1\\n012\\n2\\n2\\n2\\n2\\n( )\\n, , ,...\\np\\nfor \\n±±\\n \\n(4-9)\\nare the coefficients. The fact that Eq. (4-8) is an expansion of sines and cosines fol-\\nlows from Euler’\\ns formula, Eq. (4-6).\\nIMPULSES AND THEIR SIFTING PROPERTIES\\nCentral to the study of linear systems and the Fourier transform is the concept of an \\nimpulse and its sifting property. A \\nunit impulse\\n of a continuous variable \\nt\\n, located at \\nt\\n=\\n0,', metadata={'source': 'imagepro.pdf', 'page': 209}),\n",
       " Document(page_content='t\\n=\\n0,\\n and denoted \\nd\\n()\\n,\\nt\\n is defined as\\n \\nd\\n()\\nt\\nt\\nt\\n=\\n=\\n⎧\\n⎨\\n⎩\\n/H11009\\nif\\n \\nif \\n0\\n00\\n≠\\n \\n(4-10)\\nand is constrained to satisfy the identity\\n \\n-\\n/H11009\\n/H11009\\n2\\nd\\n()\\ntd t\\n=\\n1  \\n(4-11)\\nPhysically, if we interpret \\nt\\n as time\\n, an impulse may be viewed as a spike of infinity \\namplitude and zero duration, having unit area. An impulse has the so-called \\nsifting \\nproperty\\n with respect to integration,\\n \\n-\\n/H11009\\n/H11009\\n2\\nft td t f\\n() () ( )\\nd\\n=\\n0  \\n(4-12)\\nprovided that \\nft\\n()\\n is continuous at \\nt\\n=\\n0,\\n a condition typically satisfied in practice. \\nSifting simply yields the \\nvalue\\n of the function \\nft\\n()\\n at the \\nlocation\\n of the impulse (i.e\\n., \\nat \\nt\\n=\\n0\\n in the previous equation). A more general statement of the sifting property \\ninvolves an impulse located at an arbitrary point,\\n \\nt\\n0\\n,\\n denoted as, \\nd\\n()\\n.\\ntt\\n−\\n0\\n In this case, \\n \\n-\\n/H11009\\n/H11009\\n2\\nft t t d t ft\\n() ( ) ( )\\nd\\n−=\\n00\\n \\n(4-13)\\nAn impulse is not a \\nfunction in the usual \\nsense. A more accurate \\nname is a', metadata={'source': 'imagepro.pdf', 'page': 209}),\n",
       " Document(page_content='name is a \\ndistribution\\n \\nor \\ngeneralized\\n \\nfunction\\n. \\nHowever, one often \\nﬁnds in the literature the \\nnames \\nimpulse function\\n, \\ndelta function\\n, and \\nDirac \\ndelta function\\n, despite the \\nmisnomer.\\nTo \\nsift\\n means literally to \\nseparate, or to separate \\nout, by putting something \\nthrough a sieve.\\nDIP4E_GLOBAL_Print_Ready.indb   208\\n6/16/2017   2:04:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 209}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n209\\nwhich simply gives the value of the function at the location of the impulse. For \\nexample, if \\nft t\\n(\\n) cos( ),\\n=\\n using the impulse \\ndp\\n()\\nt\\n−\\n in Eq. (4-13) yields the result \\nf\\n()\\nc o s () .\\npp\\n== −\\n1  The power of the sifting concept will become evident shortly.\\nOf particular interest later in this section is an \\nimpulse train\\n, \\nst\\nT\\n/H9004\\n() ,\\n deﬁned as the \\nsum of inﬁnitely many impulses \\n/H9004\\nT\\n units apart:\\n \\nst t k T\\nT\\nk\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n() ( )\\n=−\\n=−\\n∑\\nd\\n \\n(4-14)\\nFigure 4.3(a) shows a single impulse located at \\ntt\\n=\\n0\\n,\\n and Fig. 4.3(b) shows an \\nimpulse train.\\n Impulses for continuous variables are denoted by up-pointing arrows \\nto simulate infinite height and zero width. For discrete variables the height is finite, \\nas we will show next.\\nLet \\nx\\n represent a \\ndiscrete\\n variable. As you learned in Chapter 3, the unit discrete \\nimpulse, \\nd\\n()\\n,\\nx\\n serves the same purposes in the context of discrete systems as the \\nimpulse', metadata={'source': 'imagepro.pdf', 'page': 210}),\n",
       " Document(page_content='impulse \\nd\\n()\\nt\\n does when working with continuous variables. It is deﬁned as\\n \\nd\\n()\\nx\\nx\\nx\\n=\\n=\\n⎧\\n⎨\\n⎩\\n10\\n00\\nif\\n \\nif \\n≠\\n \\n(4-15)\\nClearly, this definition satisfies the discrete equivalent of Eq. (4-11):\\n \\nd\\n()\\nx\\nx\\n=\\n=−\\n∑\\n1\\n/H11009\\n/H11009\\n \\n(4-16)\\nThe sifting property for discrete variables has the form\\n \\nfx x f\\nx\\n()() ()\\nd\\n=\\n=−\\n∑\\n0\\n/H11009\\n/H11009\\n \\n(4-17)\\nb a\\nd c\\nFIGURE 4.3\\n(a) Continuous \\nimpulse located \\nat \\ntt\\n=\\n0\\n. (b) An \\nimpulse train \\nconsisting of  \\ncontinuous  \\nimpulses\\n. (c) Unit \\ndiscrete impulse \\nlocated at \\nxx\\n=\\n0\\n. \\n(d) An \\nimpulse \\ntrain consisting \\nof discrete unit \\nimpulses.\\nt \\n0\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\n3\\n/H9004\\nT\\nx\\n1\\nx\\n0\\n0\\nd\\n(\\nx\\n \\n/H11002\\n \\nx\\n0\\n)\\nd\\n(\\nx\\n)\\n. . .\\n. . .\\n/H11002\\n3\\n/H9004\\nT\\nt\\nt\\n0\\n0\\nd\\n(\\nt\\n \\n/H11002\\n \\nt\\n0\\n)\\nd\\n(\\nt\\n)\\nx \\n0\\ns\\n/H9004\\nX\\n(\\nx\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\n/H9004\\nX\\n/H11002/H9004\\nX\\n/H11002\\n2\\n/H9004\\nX\\n/H11002\\n3\\n/H9004\\nX\\n2\\n/H9004\\nX\\n3\\n/H9004\\nX\\n1\\nDIP4E_GLOBAL_Print_Ready.indb   209', metadata={'source': 'imagepro.pdf', 'page': 210}),\n",
       " Document(page_content='6/16/2017   2:04:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 210}),\n",
       " Document(page_content='210\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor, more generally using a discrete impulse located at \\nxx\\n=\\n0\\n (see Eq. 3-33),\\n \\nfx x x fx\\nx\\n()( ) ( )\\nd\\n−=\\n=−\\n∑\\n00\\n/H11009\\n/H11009\\n \\n(4-18)\\nAs before, we see that the sifting property yields the value of the function at the \\nlocation of the impulse\\n. Figure 4.3(c) shows the unit discrete impulse diagrammati-\\ncally, and Fig. 4.3(d) shows a train of discrete unit impulses, Unlike its continuous \\ncounterpart, the discrete impulse is an ordinary function.\\nTHE FOURIER TRANSFORM OF FUNCTIONS OF ONE CONTINUOUS \\nVARIABLE\\nThe \\nFourier transform\\n of a continuous function \\nft\\n()\\n of a continuous variable, \\nt\\n, \\ndenoted \\n/H5219\\nft\\n()\\n,\\n{}\\n is defined by the equation\\n \\n/H5219\\nft\\nft\\ned\\nt\\njt\\n()\\n()\\n{}\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\npm\\n \\n(4-19)\\nwhere \\nm\\n is a continuous variable also.\\n†\\n Because \\nt\\n is integrated out, \\n/H5219\\nft\\n()\\n{}\\nis a func-\\ntion only of \\nm\\n.\\n That is \\n/H5219\\nft\\nF\\n() ( ) ;\\n{}\\n=\\nm\\n therefore, we write the Fourier transform of', metadata={'source': 'imagepro.pdf', 'page': 211}),\n",
       " Document(page_content='ft\\n()\\n as \\n \\nF\\nft\\ned\\nt\\njt\\n()\\n()\\nm\\npm\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\n \\n(4-20)\\nConversely, given \\nF\\n()\\n,\\nm\\n we can obtain \\nft\\n()\\n back using the \\ninverse F\\nourier transform\\n, \\nwritten as\\n \\nft F e d\\njt\\n() ( )\\n=\\n-\\n/H11009\\n/H11009\\n2\\nmm\\npm\\n2\\n \\n(4-21)\\nwhere we made use of the fact that variable \\nm\\n is integrated out in the inverse \\ntransform and wrote simply \\nft\\n()\\n,\\n rather than the more cumbersome notation \\nft F\\n()\\n( ) .\\n=\\n{}\\n−\\n/H5219\\n1\\nm\\n Equations (4-20) and (4-21) comprise the so-called \\nF\\nourier \\ntransform pair\\n, often denoted as \\nft F\\n()\\n( ) .\\n⇔\\nm\\n The double arrow indicates that the \\nexpression on the right is obtained by taking the \\nforwar\\nd\\n Fourier transform of the \\nexpression on the left, while the expression on the left is obtained by taking the \\ninverse\\n Fourier transform of the expression on the right.\\nUsing Euler’s formula, we can write Eq. (4-20) as\\n \\nFf tt j t d t\\n()\\n( ) c o s ( ) s i n ( )\\nmp\\nm p\\nm\\n=−\\n[]\\n-\\n/H11009\\n/H11009\\n2\\n22\\n \\n(4-22)\\n†', metadata={'source': 'imagepro.pdf', 'page': 211}),\n",
       " Document(page_content='2\\n22\\n \\n(4-22)\\n†\\n Conditions for the existence of the Fourier transform are complicated to state in general (Champeney [1987]), \\nbut a sufﬁcient condition for its existence is that the integral of the absolute value of \\nft\\n()\\n,\\n or the integral of the \\nsquare of \\nft\\n()\\n,\\n be ﬁnite. Existence is seldom an issue in practice, except for idealized signals, such as sinusoids \\nthat extend forever\\n. These are handled using generalized impulses. Our primary interest is in the discrete Fourier \\ntransform pair which, as you will see shortly, is guaranteed to exist for all ﬁnite functions. \\nEquation (4-21) indicates \\nthe important fact men-\\ntioned in Section 4.1 that \\na function can be recov-\\nered from its transform.\\nBecause \\nt\\n is integrated \\nout in this equation, the \\nonly variable left is \\nm\\n, \\nwhich is the frequency of \\nthe sine and cosine terms.\\nDIP4E_GLOBAL_Print_Ready.indb   210\\n6/16/2017   2:04:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 211}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n211\\nIf \\nft\\n()\\n is real, we see that its transform in general is complex. Note that the Fourier \\ntransform is an expansion of \\nft\\n()\\n multiplied by sinusoidal terms whose frequencies \\nare determined by the values of \\nm\\n.\\n Thus, because the only variable left after integra-\\ntion is frequenc\\ny, we say that the domain of the Fourier transform is the \\nfrequency \\ndomain\\n. We will discuss the frequency domain and its properties in more detail later \\nin this chapter. In our discussion, \\nt\\n can represent any continuous variable, and the \\nunits of the frequency variable \\nm\\n depend on the units of \\nt\\n.\\n For example, if \\nt\\n repre-\\nsents time in seconds, the units of \\nm\\n are cycles/sec or Hertz (Hz). If \\nt\\n represents \\ndistance in meters\\n, then the units of \\nm\\n are cycles/meter, and so on. In other words, \\nthe units of the frequenc\\ny domain are cycles per unit of the independent variable of \\nthe input function.', metadata={'source': 'imagepro.pdf', 'page': 212}),\n",
       " Document(page_content='the input function.\\nEXAMPLE 4.1 :  Obtaining the Fourier transform of a simple continuous function.\\nThe Fourier transform of the function in Fig. 4.4(a) follows from Eq. (4-20):\\n \\nF f t e dt Ae dt\\nA\\nj\\ne\\njt\\njt\\njt\\nW\\nW\\n() ( )\\nm\\npm\\npm\\npm\\npm\\n==\\n=\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n−−\\n−\\n−\\n−\\n-\\n/H11009\\n/H11009\\n22\\n22\\n2\\n2\\n2\\n2\\nW W\\nW\\njW\\njW\\njW jW\\nA\\nj\\nee\\nA\\nj\\nee\\nAW\\n2\\n2\\n2\\n2\\n=\\n−\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n−\\npm\\npm\\np\\npm pm\\npm pm\\nsin(\\nm m\\npm\\nW\\nW\\n)\\n()\\nt\\n0\\nf\\n(\\nt\\n)\\nA\\n0\\n/H11002\\nW\\n/\\n2\\nW\\n/\\n2\\n0\\n \\n/H11002\\n1\\n/\\nW\\n 1\\n/\\nW\\n \\n/H11002\\n1\\n/\\nW\\n 1\\n/\\nW\\nF\\n(\\nm\\n)\\nAW\\n/H11341\\nF\\n(\\nm\\n)\\n/H11341\\nAW\\n \\n/H11002\\n2\\n/\\nW\\n . . .\\n 2\\n/\\nW\\n . . .\\n \\n/H11002\\n2\\n/\\nW\\n . . .\\n 2\\n/\\nW\\n . . .\\nm\\nm\\nb a\\nc\\nFIGURE 4.4\\n (a) A box function, (b) its Fourier transform, and (c) its spectrum. All functions extend to inﬁnity in both \\ndirections. Note the inverse relationship between the width, \\nW\\n, of the function and the zeros of the transform.\\nDIP4E_GLOBAL_Print_Ready.indb   211\\n6/16/2017   2:04:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 212}),\n",
       " Document(page_content='212\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere we used the trigonometric identity \\nsin ( ) .\\nu\\nuu\\n=−\\n−\\nee j\\njj\\n2\\n In this case, the complex terms of the \\nF\\nourier transform combined nicely into a real sine function. The result in the last step of the preceding \\nexpression is known as the \\nsinc\\n function, which has the general form\\n \\nsinc( )\\nsin( )\\n()\\nm\\nm\\nm\\n=\\np\\np\\n \\n(4-23)\\nwhere \\nsinc( )\\n01\\n=\\n and \\nsinc( )\\nm\\n=\\n0 for all other \\ninteger\\n values of \\nm\\n.\\n Figure 4.4(b) shows a plot of \\nF\\n()\\n.\\nm\\nIn general, the Fourier transform contains complex terms, and it is customary for display purposes to \\nwork with the magnitude of the transform (a real quantity),\\n which is called the \\nFourier spectrum\\n or the \\nfrequency spectrum\\n:\\n \\nFA W\\nW\\nW\\n()\\nsin( )\\n()\\nm\\npm\\npm\\n=\\nFigure 4.4(c) shows a plot of \\nF\\n()\\nm\\n as a function of frequency. The key properties to note are (1) that \\nthe locations of the zeros of both \\nF\\n()\\nm\\n and \\nF\\n()\\nm\\n are inversely proportional to the width,\\nW\\n,', metadata={'source': 'imagepro.pdf', 'page': 213}),\n",
       " Document(page_content='W\\n,\\n of the “box” \\nfunction; (2) that the height of the lobes decreases as a function of distance from the origin; and (3) that \\nthe function extends to inﬁnity for both positive and negative values of \\nm\\n.\\n As you will see later, these \\nproperties are quite helpful in interpreting the spectra of two dimensional F\\nourier transforms of images.\\nEXAMPLE 4.2 :  Fourier transform of an impulse and an impulse train.\\nThe Fourier transform of a unit impulse located at the origin follows from Eq. (4-20):\\n \\n/H5219\\ndm\\nd\\nd\\npm\\npm\\np\\nm\\n() ( ) ()\\n()\\nt F\\nt e dt e t dt e\\njt\\njt\\nj\\n{}\\n==\\n=\\n=\\n−− −\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n22 2\\nwhere we used the sifting property from Eq. (4-12). Thus, we see that the Fourier transform of an \\nimpulse located at the origin of the spatial domain is a constant in the frequency domain (we discussed \\nthis brieﬂy in Section 3.4 in connection with Fig. 3.30). \\nSimilarly, the Fourier transform of an impulse located at \\ntt\\n=\\n0\\n is\\n \\n/H5219\\ndm\\nd\\nd\\npm\\npm\\n() ( ) ()\\n()', metadata={'source': 'imagepro.pdf', 'page': 213}),\n",
       " Document(page_content='pm\\npm\\n() ( ) ()\\n()\\nt t F t te d t e t td t\\njt\\njt\\n−\\n{}\\n== − = −=\\n−−\\n00\\n22\\n0\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\ne e\\njt\\n−\\n2\\n0\\npm\\n \\nwhere we used the sifting property from Eq. (4-13). The term \\ne\\njt\\n−\\n2\\n0\\npm\\n represents a unit circle centered on \\nthe origin of the complex plane, as you can easily see by using Euler’s formula to expand the exponential \\ninto its sine and cosine components.\\nIn Section 4.3, we will use the Fourier transform of a periodic impulse train. Obtaining this transform \\nis not as straightforward as we just showed for individual impulses. However, understanding how to \\nderive the transform of an impulse train is important, so we take the time to derive it here. We start by \\nnoting that the only basic difference in the form of Eqs. (4-20) and (4-21) is the sign of the exponential. \\nThus, if a function \\nft\\n()\\n has the Fourier transform \\nF\\n()\\n,\\nm\\n then evaluating this function at \\nt\\n, \\nFt\\n()\\n,\\n must \\nhave the transform \\nf\\n()\\n.\\n−\\nm\\n Using this \\nsymmetry', metadata={'source': 'imagepro.pdf', 'page': 213}),\n",
       " Document(page_content='symmetry\\n property and given,\\n as we showed above, that the Fou-\\nrier transform of an impulse \\nd\\n()\\ntt\\n−\\n0\\n is \\ne\\njt\\n−\\n2\\n0\\npm\\n,\\n it follows that the function \\ne\\njt\\n−\\n2\\n0\\npm\\n has the transform \\nDIP4E_GLOBAL_Print_Ready.indb   212\\n6/16/2017   2:04:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 213}),\n",
       " Document(page_content='4.2\\n  \\nPreliminary Concepts\\n    \\n213\\ndm\\n() .\\n−−\\nt\\n0\\n By letting \\n−=\\nta\\n0\\n,\\n it follows that the transform of \\ne\\nja t\\n2\\np\\n is \\ndm d m\\n()\\n( ) ,\\n−+ = −\\naa\\n where the last \\nstep is true because \\nd\\n is zero unless \\nm\\n=\\na\\n, which is the same condition for either \\ndm\\n()\\n−+\\na\\n or \\ndm\\n()\\n.\\n−\\na\\n \\nT\\nhe impulse train \\nst\\nT\\n/H9004\\n()\\n in Eq. (4-14) is periodic with period \\n/H9004\\nT\\n,\\n so it can be expressed as a Fourier \\nseries:\\n \\nst c e\\nTn\\nj\\nn\\nT\\nt\\nn\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n()\\n=\\n=−\\n∑\\n2\\np\\nwhere\\n \\nc\\nT\\nst e d t\\nnT\\nj\\nn\\nT\\nt\\nT\\nT\\n=\\n−\\n1\\n2\\n2\\n2\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n2\\n()\\n−\\np\\nWith reference to Fig. 4.3(b), we see that the integral in the interval \\n[, ]\\n−\\n/H9004/H9004\\nTT\\n22\\n encompasses only \\nthe impulse located at the origin.\\n Therefore, the preceding equation becomes\\n \\nc\\nT\\nte d t\\nT\\ne\\nT\\nn\\nj\\nn\\nT\\nt\\nT\\nT\\n==\\n=\\n−\\n−\\n11\\n1\\n2\\n2\\n2\\n0\\n/H9004/H9004\\n/H9004\\n/H9004\\n/H9004\\n/H9004\\n2\\nd\\np\\n()\\nwhere we used the sifting property of \\nd\\n()\\n.\\nt\\n The Fourier series then becomes\\n \\nst\\nT\\ne\\nT\\nj\\nn\\nT\\nt\\nn\\n/H9004\\n/H9004', metadata={'source': 'imagepro.pdf', 'page': 214}),\n",
       " Document(page_content='T\\nt\\nn\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n()\\n=\\n=−\\n∑\\n1\\n2\\np\\nOur objective is to obtain the Fourier transform of this expression. Because summation is a linear pro-\\ncess, obtaining the Fourier transform of a sum is the same as obtaining the sum of the transforms of the \\nindividual components of the sum. These components are exponentials, and we established earlier in \\nthis example that\\n \\n/H5219\\nUV\\ne\\nn\\nT\\nj\\nn\\nT\\nt\\n2\\np\\ndm\\n/H9004\\n/H9004\\n=−\\nQR\\nSo, \\nS\\n()\\n,\\nm\\n the Fourier transform of the periodic impulse train, is\\n \\nSs t\\nT\\ne\\nT\\ne\\nT\\nj\\nn\\nT\\nt\\nn\\nj\\nn\\nT\\nt\\nn\\n() ( )\\nm\\npp\\n=\\n{}\\n===\\n=−\\n=−\\n∑∑\\n/H5219/H5219\\n/H5219\\n/H9004\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n/H9004/H9004\\nUV U V\\n11 1\\n22\\n/H9004\\n/H9004/H9004\\n/H11009\\n/H11009\\nT\\nn\\nT\\nn\\ndm\\nQR\\n−\\n=−\\n∑\\nThis fundamental result tells us that the Fourier transform of an impulse train with period \\n/H9004\\nT\\n is also \\nan impulse train,\\n whose period is \\n1\\n/H9004\\nT\\n.\\n This \\ninverse proportionality\\n between the periods of \\nst\\nT\\n/H9004\\n()\\n and \\nS\\n()\\nm', metadata={'source': 'imagepro.pdf', 'page': 214}),\n",
       " Document(page_content='()\\n and \\nS\\n()\\nm\\n is analogous to what we found in Fig. 4.4 in connection with a box function and its transform. This \\ninverse relationship plays a fundamental role in the remainder of this chapter\\n.\\nCONVOLUTION\\nWe showed in Section 3.4 that convolution of two functions involves flipping (rotat-\\ning by 180°) one function about its origin and sliding it past the other. At each dis-\\nplacement in the sliding process, we perform a computation, which, for discrete \\nvariables, is a sum of products [see Eq. (3-35)]. In the present discussion, we are \\nAs in Section 3.4, the \\nfact that convolution of a \\nfunction with an impulse \\nshifts the origin of the \\nfunction to the location of \\nthe impulse is also true for \\ncontinuous convolution. \\n(See Figs. 3.29 and 3.30.)\\nDIP4E_GLOBAL_Print_Ready.indb   213\\n6/16/2017   2:04:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 214}),\n",
       " Document(page_content='214\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ninterested in the convolution of two \\ncontinuous\\n functions, \\nft\\n()\\n and \\nht\\n()\\n, of one con-\\ntinuous variable\\n, \\nt\\n, so we have to use integration instead of a summation. The con-\\nvolution of these two functions, denoted as before by the operator \\n/H22841\\n,\\n is defined as\\n \\n() ( ) ( ) ( )\\nfh\\nt f h t d\\n/H22841\\n=−\\n-\\n/H11009\\n/H11009\\n2\\ntt t\\n \\n(4-24)\\nwhere the minus sign accounts for the flipping just mentioned, \\nt\\n is the \\ndisplacement\\n \\nneeded to slide one function past the other\\n, and \\nt\\n is a dummy variable that is inte-\\ngrated out.\\n We assume for now that the functions extend from \\n−\\n/H11009\\n to \\n/H11009\\n.\\nW\\ne illustrated the basic mechanics of convolution in Section 3.4, and we will do \\nso again later in this chapter and in Chapter 5. At the moment, we are interested in \\nﬁnding the Fourier transform of Eq. (4-24). We start with Eq. (4-19):\\n \\n/H5219\\n()\\n( ) ( ) ( )\\n()\\nfh t\\nf h t d e d t\\nf\\njt\\n/H22841\\n{}\\n=−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n−\\n--', metadata={'source': 'imagepro.pdf', 'page': 215}),\n",
       " Document(page_content='⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n−\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\ntt t\\nt\\npm\\n2\\n- -\\n/H11009\\n/H11009\\n2\\nht e d t d\\njt\\n()\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n−\\ntt\\npm\\n2\\nThe term inside the brackets is the Fourier transform of \\nht\\n()\\n.\\n−\\nt\\n We will show later \\nin this chapter that \\n/H5219\\nht\\nH e\\nj\\n() ( ) ,\\n−\\n{}\\n=\\n−\\ntm\\npmt\\n2\\n where \\nH\\n()\\nm\\n is the Fourier transform \\nof \\nht\\n()\\n.\\n Using this in the preceding equation gives us\\n \\n/H5219\\n()\\n( ) ( ) ( )\\n() ( )\\nfh t f H e d\\nHf\\ned\\nj\\nj\\n/H22841\\n{}\\n=\\n⎡\\n⎣\\n⎤\\n⎦\\n=\\n−\\n−\\n-\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n2\\n2\\ntm t\\nmt\\npmt\\npmt\\n2\\n2\\nt t\\nmm\\nm\\n=\\n=\\nHF\\nHF\\n()()\\n() ( )\\ni\\nwhere “ \\ni\\n ” indicates multiplication. As noted earlier, if we refer to the domain of \\nt\\n \\nas the spatial domain, and the domain of \\nm\\n as the frequency domain, the preceding \\nequation tells us that the F\\nourier transform of the convolution of two functions in \\nthe spatial domain is equal to the product in the frequency domain of the Fourier', metadata={'source': 'imagepro.pdf', 'page': 215}),\n",
       " Document(page_content='transforms of the two functions. Conversely, if we have the product of the two trans-\\nforms, we can obtain the convolution in the spatial domain by computing the inverse \\nFourier transform. In other words, \\nfh\\n/H22841\\n and \\nHF\\ni\\n are a Fourier transform \\npair\\n.\\n This \\nresult is one-half of the \\nconvolution theorem\\n and is written as\\n \\n() ( ) () ( )\\nfh t H F\\n/H22841\\n⇔\\ni\\nm\\n \\n(4-25)\\nAs noted earlier, the double arrow indicates that the expression on the right is \\nobtained by taking the \\nforwar\\nd\\n Fourier transform of the expression on the left, while \\nRemember, convolution \\nis commutative, so the \\norder of the functions in \\nconvolution expressions \\ndoes not matter. \\nDIP4E_GLOBAL_Print_Ready.indb   214\\n6/16/2017   2:04:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 215}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled  Functions\\n    \\n215\\nthe expression on the left is obtained by taking the \\ninverse\\n Fourier transform of the \\nexpression on the right.\\nFollowing a similar development would result in the other half of the convolution \\ntheorem:\\n \\n() ( ) ( ) ( )\\nfh\\nt H F\\ni\\n⇔\\n/H22841\\nm\\n \\n(4-26)\\nwhich states that convolution in the frequency domain is analogous to multiplica-\\ntion in the spatial domain,\\n the two being related by the forward and inverse Fourier \\ntransforms, respectively. As you will see later in this chapter, the convolution theo-\\nrem is the foundation for filtering in the frequency domain.\\n4.3 SAMPLING AND THE FOURIER TRANSFORM OF SAMPLED  \\nFUNCTIONS  \\nIn this section, we use the concepts from Section 4.2 to formulate a basis for express-\\ning sampling mathematically. Starting from basic principles, this will lead us to the \\nFourier transform of sampled functions. That is, the discrete Fourier transform.\\nSAMPLING', metadata={'source': 'imagepro.pdf', 'page': 216}),\n",
       " Document(page_content='SAMPLING\\nContinuous functions have to be converted into a sequence of discrete values before \\nthey can be processed in a computer. This requires sampling and quantization, as \\nintroduced in Section 2.4. In the following discussion, we examine sampling in more \\ndetail.\\nConsider a continuous function, \\nft\\n()\\n,\\n that we wish to sample at uniform intervals, \\n/H9004\\nT\\n,\\n of the independent variable \\nt \\n(see F\\nig. 4.5). We assume initially that the function \\nextends from \\n−\\n/H11009\\n to \\n/H11009\\n with respect to \\nt\\n.\\n One way to model sampling is to multiply \\nft\\n()\\n by a sampling function equal to a train of impulses \\n/H9004\\nT\\n units apart. That is,\\n \\n/tildenosp\\nft\\nfts t ft t nT\\nT\\nn\\n() () () () ( )\\n== −\\n=−\\n∑\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\nd\\n \\n(4-27)\\nwhere \\n/tildenosp\\nft\\n()\\n denotes the sampled function. Each component of this summation is an \\nimpulse weighted by the value of \\nft\\n()\\n at the location of the impulse, as Fig. 4.5(c) \\nshows\\n. The \\nvalue', metadata={'source': 'imagepro.pdf', 'page': 216}),\n",
       " Document(page_content='shows\\n. The \\nvalue\\n of each sample is given by the “strength” of the weighted impulse, \\nwhich we obtain by integration. That is, the value, \\nf\\nk\\n,\\n of an arbitrary sample in the \\nsampled sequence is given by\\n \\nff t t k T d t\\nfk\\nT\\nk\\n=−\\n=\\n-\\n/H11009\\n/H11009\\n/H9004\\n/H9004\\n2\\n() ( )\\n()\\nd\\n \\n(4-28)\\nwhere we used the sifting property of \\nd\\n in Eq. (4-13). Equation (4-28) holds for any \\ninteger value \\nk\\n=−\\n−\\n..., , , , , ,....\\n2 1012\\n Figure 4.5(d) shows the result, which con-\\nsists of equally spaced samples of the original function.\\nThese two expressions \\nalso hold for discrete \\nvariables, with the \\nexception that the right \\nside of Eq. (4-26) is \\nmultiplied by (1/\\nM\\n), \\nwhere \\nM\\n is the number \\nof discrete samples (see \\nProblem 4.18).\\n4.3\\nTaking samples \\nΔ\\nΤ\\n units \\napart implies a \\nsampling \\nrate\\n equal to 1/\\nΔ\\nΤ\\n. If the \\nunits of \\nΔ\\nΤ\\n are seconds, \\nthen the sampling rate is \\nin samples/s. If the units \\nof \\nΔ\\nΤ\\n are meters, then \\nthe sampling rate is in \\nsamples/m, and so on.', metadata={'source': 'imagepro.pdf', 'page': 216}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   215\\n6/16/2017   2:04:33 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 216}),\n",
       " Document(page_content='216\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nTHE FOURIER TRANSFORM OF SAMPLED FUNCTIONS\\nLet \\nF\\n()\\nm\\n denote the Fourier transform of a continuous function \\nft\\n()\\n.\\n As discussed \\nin the previous section,\\n the corresponding sampled function, \\n/tildenosp\\nft\\n()\\n,\\n is the product of \\nft\\n()\\n and an impulse train. We know from the convolution theorem that the Fourier \\ntransform of the product of two functions in the spatial domain is the convolution \\nof the transforms of the two functions in the frequenc\\ny domain. Thus, the Fourier \\ntransform of the sampled function is:\\n \\n/tildenosp\\n/tildenosp\\nFf\\nt f t s t\\nFS\\nT\\n( ) () () ()\\n() ( )\\nm\\nm\\n=\\n{}\\n=\\n{}\\n=\\n/H5219/H5219\\n/H9004\\n/H22841\\n \\n(4-29)\\nwhere, from Example 4.2,\\n \\nS\\nT\\nn\\nT\\nn\\n()\\nmd m\\n=−\\n=−\\n∑\\n1\\n/H9004/H9004\\n/H11009\\n/H11009\\nQR\\n \\n(4-30)\\nt\\n0\\nf\\n(\\nt\\n)\\nt\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\nt\\nf\\n(\\nt\\n)\\ns\\n/H9004\\nT\\n(\\nt\\n)\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n/H9004\\nT\\n/H11002/H9004\\nT', metadata={'source': 'imagepro.pdf', 'page': 217}),\n",
       " Document(page_content='T\\n/H11002/H9004\\nT\\n/H11002\\n2\\n/H9004\\nT\\n2\\n/H9004\\nT\\nk\\nf\\nk\\n \\n/H11005\\n \\nf\\n(\\nk\\n/H9004\\nT\\n)\\n. . .\\n. . .\\n0\\n. . .\\n. . .\\n1\\n/H11002\\n1\\n/H11002\\n22\\n. .\\n. . \\nb\\na\\nc\\nd\\nFIGURE 4.5\\n(a) A continuous \\nfunction. (b) Train \\nof impulses used to \\nmodel sampling.  \\n(c) Sampled  \\nfunction formed as \\nthe product of (a) \\nand (b). (d) Sample \\nvalues obtained by \\nintegration and  \\nusing the sifting \\nproperty of  \\nimpulses. (The \\ndashed line in (c) is \\nshown for refer-\\nence. It is not part \\nof the data.)\\nDIP4E_GLOBAL_Print_Ready.indb   216\\n6/16/2017   2:04:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 217}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n217\\nis the Fourier transform of the impulse train \\nst\\nT\\n/H9004\\n() .\\n We obtain the convolution of \\nF\\n()\\nm\\n and \\nS\\n()\\nm\\n directly from the 1-D definition of convolution in Eq. (4-24):\\n \\n/tildenosp\\nFF S F\\nSd\\nT\\nF\\nn\\nT\\nn\\n() ( ) () ( )( )\\n()\\nmmt m t t\\ntd m t\\n== −\\n=−\\n−\\n=−\\n/H22841\\n-\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004/H9004\\n/H11009\\n/H11009\\n2\\n2\\n1\\nQR\\n∑ ∑\\n∑\\n∑\\n=−\\n−\\n=−\\n=−\\n=−\\nd\\nT\\nF\\nn\\nT\\nd\\nT\\nF\\nn\\nT\\nn\\nn\\nt\\ntdm t t\\nm\\n1\\n1\\n/H9004/H9004\\n/H9004/H9004\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n-\\n2\\n()\\nQR\\nQR\\n \\n(4-31)\\nwhere the final step follows from the sifting property of the impulse, Eq. (4-13).\\nT\\nhe summation in the last line of Eq. (4-31) shows that the Fourier transform \\n/tildenosp\\nF\\n()\\nm\\n of the sampled function \\n/tildenosp\\nft\\n()\\n is an \\ninﬁnite\\n, \\nperiodic\\n sequence of \\ncopies\\n of the \\ntransform of the original,\\n continuous function. The separation between copies is \\ndetermined by the value of \\n1\\n/H9004\\nT\\n.', metadata={'source': 'imagepro.pdf', 'page': 218}),\n",
       " Document(page_content='1\\n/H9004\\nT\\n.\\n Observe that although \\n/tildenosp\\nft\\n()\\n is a sampled function, \\nits transform,\\n \\n/tildenosp\\nF\\n()\\n,\\nm\\n is \\ncontinuous\\n because it consists of copies of \\nF\\n()\\n,\\nm\\n which is a \\ncontinuous function.\\nF\\nigure 4.6 is a graphical summary of the preceding results.\\n†\\n Figure 4.6(a) is a \\nsketch of the Fourier transform, \\nF\\n()\\n,\\nm\\n of a function \\nft\\n()\\n, and Fig. 4.6(b) shows the \\ntransform,\\n \\n/tildenosp\\nF\\n()\\n,\\nm\\n of the sampled function, \\n/tildenosp\\nft\\n()\\n.\\n As mentioned in the previous sec-\\ntion,\\n the quantity \\n1\\n/H9004\\nT\\n is the \\nsampling rate\\n used to generate the sampled function.\\n \\nSo, in Fig. 4.6(b) the sampling rate was high enough to provide sufﬁcient separation \\nbetween the periods, and thus preserve the integrity (i.e., perfect copies) of \\nF\\n()\\n.\\nm\\n In \\nF\\nig. 4.6(c), the sampling rate was just enough to preserve \\nF\\n()\\n,\\nm\\n but in Fig. 4.6(d), the \\nsampling rate was below the minimum required to maintain distinct copies of \\nF\\n()\\n,\\nm', metadata={'source': 'imagepro.pdf', 'page': 218}),\n",
       " Document(page_content='F\\n()\\n,\\nm\\n \\nand thus failed to preserve the original transform.\\n Figure 4.6(b) is the result of an \\nover-sampled\\n signal, while Figs. 4.6(c) and (d) are the results of \\ncritically sampling\\n \\nand \\nunder-sampling \\nthe signal, respectively. These concepts are the basis that will \\nhelp you grasp the fundamentals of the sampling theorem, which we discuss next.\\nTHE SAMPLING THEOREM\\nWe introduced the idea of sampling intuitively in Section 2.4. Now we consider sam-\\npling formally, and establish the conditions under which a continuous function can \\nbe recovered uniquely from a set of its samples.\\nA function \\nft\\n()\\n whose Fourier transform is zero for values of frequencies outside \\na ﬁnite interval (band) \\n[, ]\\nmax max\\n−\\nmm\\n about the origin is called a \\nband-limited \\nfunc-\\ntion.\\n Figure 4.7(a), which is a magniﬁed section of Fig. 4.6(a), is such a function. Simi-\\nlarly, Fig. 4.7(b) is a more detailed view of the transform of the critically sampled \\n†', metadata={'source': 'imagepro.pdf', 'page': 218}),\n",
       " Document(page_content='†\\n  For the sake of clarity in sketches of Fourier transforms in Fig. 4.6, and other similar ﬁgures in this chapter, we \\nignore the fact that Fourier transforms typically are complex functions. Our interest here is on concepts.\\nDIP4E_GLOBAL_Print_Ready.indb   217\\n6/16/2017   2:04:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 218}),\n",
       " Document(page_content='218\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nfunction [see Fig. 4.6(c)]. A higher value of \\n/H9004\\nT\\n would cause the periods in \\n/tildenosp\\nF\\n()\\nm\\n to \\nmerge;\\n a lower value would provide a clean separation between the periods.\\nWe can recover \\nft\\n()\\n from its samples if we can isolate a single copy of \\nF\\n()\\nm\\n from \\nthe periodic sequence of copies of this function contained in \\n/tildenosp\\nF\\n()\\n,\\nm\\n the transform of \\nthe sampled function \\n/tildenosp\\nft\\n()\\n.\\n Recall from the discussion in the previous section that \\n/tildenosp\\nF\\n()\\nm\\n is a \\ncontinuous\\n, \\nperiodic\\n function with period \\n1\\n/H9004\\nT\\n.\\n Therefore, all we need is \\none complete period to characterize the entire transform.\\n In other words, we can \\nrecover \\nft\\n()\\n from that single period by taking its inverse Fourier transform.\\nExtracting from \\n/tildenosp\\nF\\n()\\nm\\n a single period that is equal to \\nF\\n()\\nm\\n is possible if the sepa-\\nration between copies is sufﬁcient (see F\\nig. 4.6). In terms of Fig. 4.7(b), sufﬁcient', metadata={'source': 'imagepro.pdf', 'page': 219}),\n",
       " Document(page_content='separation is guaranteed if \\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n or\\n \\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n \\n(4-32)\\nThis equation indicates that a continuous, band-limited function can be recovered \\ncompletely from a set of its samples if the samples are acquired at a rate exceeding \\nRemember, the sampling \\nrate is the number of \\nsamples taken per unit of \\nthe independent variable. \\n. . .\\n. . .\\n. . .\\n. . .\\n0\\nF\\n(\\nm\\n)\\nm\\nF\\n(\\nm\\n)\\n~\\nF\\n(\\nm\\n)\\n~\\nF\\n(\\nm\\n)\\n~\\nm\\nm\\nm\\n. . .\\n. . .\\n0\\n0\\n0\\n1\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n/H11002\\n3\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n3\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\n. . .\\n. . .\\nb\\na\\nc\\nd\\nFIGURE 4.6\\n(a) Illustrative \\nsketch of the \\nFourier transform \\nof a band-limited \\nfunction.  \\n(b)–(d) Trans-\\nforms of the \\ncorresponding \\nsampled functions \\nunder the  \\nconditions of \\nover-sampling, \\ncritically  \\nsampling, and \\nunder-sampling, \\nrespectively. \\nDIP4E_GLOBAL_Print_Ready.indb   218', metadata={'source': 'imagepro.pdf', 'page': 219}),\n",
       " Document(page_content='6/16/2017   2:04:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 219}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n219\\ntwice the highest frequency content of the function. This exceptionally important \\nresult is known as the \\nsampling theorem\\n.\\n†\\n We can say based on this result that no \\ninformation is lost if a continuous, band-limited function is represented by samples \\nacquired at a rate greater than twice the highest frequency content of the function. \\nConversely, we can say that the \\nmaximum\\n frequency that can be “captured” by sam-\\npling a signal at a rate \\n1\\n/H9004\\nT\\n is \\nm\\nmax\\n.\\n=\\n12\\n/H9004\\nT\\n A sampling rate \\nexactly\\n equal to twice \\nthe highest frequenc\\ny is called the \\nNyquist rate\\n. Sampling at exactly the Nyquist rate \\nsometimes is sufficient for perfect function recovery, but there are cases in which \\nthis leads to difficulties, as we will illustrate later in Example 4.3. This is the reason \\nwhy the sampling theorem specifies that sampling must exceed the Nyquist rate.', metadata={'source': 'imagepro.pdf', 'page': 220}),\n",
       " Document(page_content='Figure 4.8 illustrates the procedure for recovering \\nF\\n()\\nm\\n from \\n/tildenosp\\nF\\n()\\nm\\n when a function \\nis sampled at a rate higher than the Nyquist rate\\n. The function in Fig. 4.8(b) is deﬁned \\nby the equation\\n \\nH\\nT\\n()\\nmax\\nmax\\nm\\nmm m\\n=\\n−\\n⎧\\n⎨\\n⎩\\n/H9004\\n≤≤\\n0 otherwise\\n \\n(4-33)\\nWhen multiplied by the periodic sequence in Fig. 4.8(a), this function isolates the \\nperiod centered on the origin.\\n Then, as Fig. 4.8(c) shows, we obtain \\nF\\n()\\nm\\n by multiply-\\ning \\n/tildenosp\\nF\\n()\\nm\\n by \\nH\\n()\\n:\\nm\\n†\\n  The sampling theorem is a cornerstone of digital signal processing theory. It was ﬁrst formulated in 1928 by \\nHarry Nyquist, a Bell Laboratories scientist and engineer. Claude E. Shannon, also from Bell Labs, proved the \\ntheorem formally in 1949. The renewed interest in the sampling theorem in the late 1940s was motivated by the \\nemergence of early digital computing systems and modern communications, which created a need for methods \\ndealing with digital (sampled) data.\\nThe \\nΔ\\nΤ \\nin Eq. (4-33)', metadata={'source': 'imagepro.pdf', 'page': 220}),\n",
       " Document(page_content='Δ\\nΤ \\nin Eq. (4-33) \\ncancels out the 1/\\nΔ\\nΤ\\n in \\nEq. (4-31).\\n0\\nF\\n(\\nm\\n)\\nm\\n0\\nF\\n(\\nm\\n)\\nm\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n1\\n2\\n/H9004\\nT\\n–––\\n/H11002\\n1\\n/H9004\\nT\\n––\\n1\\n2\\n/H9004\\nT\\n–––\\n~\\n. . .\\n. . .\\nb\\na\\nFIGURE 4.7\\n(a) Illustrative \\nsketch of the \\nFourier  \\ntransform of a \\nband-limited \\nfunction.  \\n(b) Transform \\nresulting from \\ncritically sampling \\nthat band-limited  \\nfunction. \\nDIP4E_GLOBAL_Print_Ready.indb   219\\n6/16/2017   2:04:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 220}),\n",
       " Document(page_content='220\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nFH F\\n()\\n()()\\nmm m\\n=\\n/tildenosp\\n \\n(4-34)\\nOnce we have \\nF\\n()\\n,\\nm\\n we can recover \\nft\\n()\\n using the inverse Fourier transform:\\n \\nft F e d\\njt\\n() ( )\\n=\\n-\\n/H11009\\n/H11009\\n2\\nmm\\npm\\n2\\n \\n(4-35)\\nEquations (4-33) through (4-35) prove that, theoretically, it is possible to recover a \\nband-limited function from samples obtained at a rate exceeding twice the highest \\nfrequenc\\ny content of the function. As we will discuss in the following section, the \\nrequirement that \\nft\\n()\\n must be band-limited implies in general that \\nft\\n()\\n must extend \\nfrom \\n−\\n/H11009\\n to \\n/H11009\\n, a condition that cannot be met in practice. As you will see shortly, \\nhaving to limit the duration of a function prevents perfect recovery of the function \\nfrom its samples\\n, except in some special cases.\\nFunction \\nH\\n()\\nm\\n is called a \\nlo\\nwpass ﬁlter\\n because it passes frequencies in the low', metadata={'source': 'imagepro.pdf', 'page': 221}),\n",
       " Document(page_content='end of the frequency range, but it eliminates (ﬁlters out) higher frequencies. It is \\ncalled also an \\nideal lowpass ﬁlter\\n because of its instantaneous transitions in ampli-\\ntude (between 0 and \\n/H9004\\nT\\n at location \\n−\\nm\\nmax\\n and the reverse at \\nm\\nmax\\n), a characteristic \\nthat cannot be implemented physically in hardware. We can simulate ideal ﬁlters \\nin software, but even then there are limitations (see Section 4.8). Because they are \\ninstrumental in recovering (reconstructing) the original function from its samples, \\nﬁlters used for the purpose just discussed are also called \\nreconstruction ﬁlters\\n.\\nIn Fig. 3.32 we sketched \\nthe radial cross sections \\nof ﬁlter transfer functions \\nusing only positive fre-\\nquencies, for simplicity. \\nNow you can see that \\nfrequency domain ﬁlter \\nfunctions encompass \\nboth positive and nega-\\ntive frequencies. \\nF\\n(\\nm\\n)\\n~\\nm\\n. . .\\n. . .\\n0\\n1\\n/\\n/H9004\\nT\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\nH\\n(\\nm\\n)\\nm\\n0\\n~\\nF\\n(\\nm\\n) \\n/H11005\\n \\nH\\n(\\nm\\n)\\nF', metadata={'source': 'imagepro.pdf', 'page': 221}),\n",
       " Document(page_content='/H11005\\n \\nH\\n(\\nm\\n)\\nF\\n(\\nm\\n)\\nm\\n0\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n. . .\\n. . .\\nb\\na\\nc\\nFIGURE 4.8\\n(a) Fourier \\ntransform of a \\nsampled,  \\nband-limited  \\nfunction.  \\n(b) Ideal lowpass \\nﬁlter transfer \\nfunction.  \\n(c) The product \\nof (b) and (a), \\nused to extract \\none period of the \\ninﬁnitely periodic \\nsequence in (a). \\nDIP4E_GLOBAL_Print_Ready.indb   220\\n6/16/2017   2:04:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 221}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n221\\nALIASING\\nLiterally, the word \\nalias\\n means “a false identity.” In the field of signal processing, \\naliasing refers to sampling phenomena that cause different signals to become indis-\\ntinguishable from one another after sampling; or, viewed another way, for one signal \\nto “masquerade” as another. \\nConceptually, the relationship between sampling and aliasing is not difﬁcult to \\ngrasp. The foundation of aliasing phenomena as it relates to sampling is that we \\ncan describe a digitized function \\nonly\\n by the values of its samples. This means that \\nit is possible for two (or more) totally \\ndifferent\\n continuous functions to coincide at \\nthe values of their respective samples, but we would have no way of knowing the \\ncharacteristics of the functions between those samples. To illustrate, Fig. 4.9 shows \\ntwo completely different sine functions sampled at the same rate. As you can see', metadata={'source': 'imagepro.pdf', 'page': 222}),\n",
       " Document(page_content='in Figs. 4.9(a) and (c), there are numerous places where the sampled values are the \\nsame in the two functions, resulting in identical sampled functions, as Figs. 4.9(b) \\nand (d) show. \\nTwo continuous functions having the characteristics just described are called an \\naliased pair\\n, and such pairs are indistinguishable after sampling. Note that the reason \\nthese functions are aliased is because we used a sampling rate that is too coarse. That \\nis, the functions were \\nunder-sampled\\n. It is intuitively obvious that if sampling were \\nreﬁned, more and more of the differences between the two continuous functions \\nwould be revealed in the sampled signals. The principal objective of the following \\ndiscussion is to answer the question: What is the minimum sampling rate required \\nto avoid (or reduce) aliasing? This question has both a theoretical and a practical \\nanswer and, in the process of arriving at the answers, we will establish the conditions \\nunder which aliasing occurs.', metadata={'source': 'imagepro.pdf', 'page': 222}),\n",
       " Document(page_content='We can use the tools developed earlier in this section to formally answer the \\nquestion we just posed. All we have to do is ask it in a different form: What happens \\nAlthough we show \\nsinusoidal functions for \\nsimplicity, aliasing occurs \\nbetween any arbitrary \\nsignals whose values are \\nthe same at the sample \\npoints.\\nb a\\nd c\\nFIGURE 4.9\\nThe functions in \\n(a) and (c) are \\ntotally different, \\nbut their digi-\\ntized versions in \\n(b) and (d) are \\nidentical. Aliasing \\noccurs when the \\nsamples of two or \\nmore functions \\ncoincide, but the \\nfunctions are dif-\\nferent elsewhere. \\nDIP4E_GLOBAL_Print_Ready.indb   221\\n6/16/2017   2:04:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 222}),\n",
       " Document(page_content='222\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nif a band-limited function is sampled at less than the Nyquist rate (i.e., at less than \\ntwice its highest frequency)? This is precisely the under-sampled situation discussed \\nearlier in this section and mentioned in the previous paragraph. \\nFigure 4.10(a) is the same as Fig. 4.6(d); it shows schematically the Fourier trans-\\nform of an under-sampled, band-limited function. This ﬁgure illustrates that the net \\neffect of lowering the sampling rate below the Nyquist rate is that the periods of the \\nFourier transform now overlap, and it becomes impossible to isolate a single period \\nof the transform, regardless of the ﬁlter used. For instance, using the ideal lowpass \\nﬁlter in Fig. 4.10(b) would result in a transform that is corrupted by frequencies from \\nadjacent periods, as Fig. 4.10(c) shows. The inverse transform would then yield a \\nfunction, \\nft\\na\\n() ,\\n different from the original. That is, \\nft\\na\\n()', metadata={'source': 'imagepro.pdf', 'page': 223}),\n",
       " Document(page_content='ft\\na\\n()\\n would be an aliased function \\nbecause it would contain frequenc\\ny components not present in the original. Using \\nour earlier terminology, \\nft\\na\\n()\\n would masquerade as a different function. It is pos-\\nsible for aliased functions to bear no resemblance whatsoever to the functions from \\nwhich they originated.\\nUnfortunately\\n, except in some special cases mentioned below, aliasing is always \\npresent in sampled signals. This is because, even if the original sampled function is \\nband-limited, inﬁnite frequency components are introduced the moment we limit \\nthe duration of the function, which we always have to do in practice. As an illustra-\\ntion, suppose that we want to limit the duration of a band-limited function, \\nft\\n()\\n,\\n to a \\nﬁnite interval,\\n say \\n[, ] .\\n0\\nT\\n We can do this by multiplying \\nft\\n()\\n by the function\\n \\nht\\ntT\\n()\\n=\\n⎧\\n⎨\\n⎩\\n10\\n0\\n≤≤\\not\\nherwise\\n \\n(4-36)\\nThis function has the same basic shape as Fig. 4.4(a), whose Fourier transform, \\nH\\n()\\n,\\nm\\n \\nhas frequenc', metadata={'source': 'imagepro.pdf', 'page': 223}),\n",
       " Document(page_content=',\\nm\\n \\nhas frequenc\\ny components extending to infinity in both directions, as Fig. 4.4(b) shows. \\nFrom the convolution theorem, we know that the transform of the product \\nht f t\\n()\\n()\\n \\nis the convolution in the frequenc\\ny domain of the transforms \\nF\\n()\\nm\\n and \\nH\\n()\\n.\\nm\\n Even \\nif \\nF\\n()\\nm\\n is band-limited, convolving it with \\nH\\n()\\nm\\n , which involves sliding one function \\nacross the other\\n, will yield a result with frequency components extending to infinity \\nin both directions (see Problem 4.12). From this we conclude that no function of \\nfinite\\n duration can be band-limited. Conversely, a function that is band-limited must \\nextend from \\n−\\n/H11009\\n to \\n/H11009\\n.\\n†\\n \\nAlthough aliasing is an inevitable fact of working with sampled records of ﬁnite \\nlength,\\n the effects of aliasing can be reduced by smoothing (lowpass ﬁltering) the \\ninput function to attenuate its higher frequencies. This process, called \\nanti-aliasing\\n, \\nhas to be done \\nbefore', metadata={'source': 'imagepro.pdf', 'page': 223}),\n",
       " Document(page_content='before\\n the function is sampled because aliasing is a sampling issue \\nthat cannot be “undone after the fact” using computational techniques.\\n† An important special case is when a function that extends from \\n−\\n/H11009\\n to \\n/H11009\\n is band-limited \\nand\\n periodic\\n. In this \\ncase, the function can be truncated and still be band-limited, \\nprovided\\n that the truncation encompasses \\nexactly\\n \\nan integral number of periods. A single truncated period (and thus the function) can be represented by a set of \\ndiscrete samples satisfying the sampling theorem, taken over the truncated interval.\\nIf we cannot isolate one \\nperiod of the transform, \\nwe cannot recover the \\nsignal without aliasing,\\nDIP4E_GLOBAL_Print_Ready.indb   222\\n6/16/2017   2:04:40 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 223}),\n",
       " Document(page_content='4.3\\n  \\nSampling and the Fourier Transform of Sampled Functions\\n    \\n223\\nEXAMPLE 4.3 : Aliasing.\\nFigure 4.11 shows a classic illustration of aliasing. A pure sine wave extending inﬁnitely in both direc-\\ntions has a single frequency so, obviously, it is band-limited. Suppose that the sine wave in the ﬁgure \\n(ignore the large dots for now) has the equation \\nft t\\n(\\n) sin( ),\\n=\\np\\n and that the horizontal axis corresponds \\nto time\\n, \\nt\\n, in seconds. The function crosses the axis at \\nt\\n=\\n01\\n2\\n,, , .\\n±±\\n…\\nRecall that a function \\nft\\n()\\n is \\nperiodic\\n with \\nperiod\\n \\nP\\n if \\nft P ft\\n()\\n( )\\n+=\\n for all values of \\nt\\n.\\n The period \\nis the number (including fractions) of units of the independent variable that it takes for the function \\nto complete one cycle. The \\nfrequency\\n of a \\nperiodic\\n function is the number of periods (cycles) that the \\nfunction completes in one unit of the independent variable. Thus, the frequency of a periodic function \\nis the \\nreciprocal', metadata={'source': 'imagepro.pdf', 'page': 224}),\n",
       " Document(page_content='is the \\nreciprocal\\n of the period. As before, the sampling rate is the number of samples taken per unit of \\nthe independent variable.\\n In the present example, the independent variable is time, and its units are seconds. The period, \\nP\\n, \\nof \\nsin( )\\np\\nt\\n is 2 s, and its frequency is \\n1\\nP\\n,\\n or \\n12\\n cycles/s. According to the sampling theorem, we can \\nrecover this signal from a set of its samples if the sampling rate exceeds twice the highest frequenc\\ny \\nof the signal. This means that a sampling rate greater than 1 sample/s \\n()\\n21 21\\n×=\\n is required to \\n. . .\\n. . .\\nm\\n \\n0\\n/H11002\\n3\\n/\\n/H9004\\nT\\n/H11002\\n1\\n/\\n/H9004\\nT\\n/H11002\\n2\\n/\\n/H9004\\nT\\n3\\n/\\n/H9004\\nT\\n1\\n/\\n/H9004\\nT\\n2\\n/\\n/H9004\\nT\\nF\\n(\\nm\\n)\\n~\\n0\\nH\\n(\\nm\\n)\\nm\\n \\n0\\n~\\nF\\n(\\nm\\n) \\n/H11005\\n \\nH\\n(\\nm\\n)\\nF\\n(\\nm\\n)\\nm\\n \\n0\\nm\\nmax\\nm\\nmax\\n/H11002\\nm\\nmax\\n/H11002\\nm\\nmax\\n. . .\\n. . .\\n. . .\\n. . .\\n/H9004\\nT\\nb\\na\\nc\\nFIGURE 4.10\\n (a) Fourier transform of an under-sampled, band-limited function. (Interference between adjacent peri-', metadata={'source': 'imagepro.pdf', 'page': 224}),\n",
       " Document(page_content='ods is shown dashed). (b) The same ideal lowpass ﬁlter used in Fig. 4.8. (c) The product of (a) and (b).The interfer-\\nence from adjacent periods results in aliasing that prevents perfect recovery of \\nF\\n()\\nm\\n and, consequently, of \\nft\\n()\\n.\\n \\nDIP4E_GLOBAL_Print_Ready.indb   223\\n6/16/2017   2:04:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 224}),\n",
       " Document(page_content='224\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nrecover the signal. Viewed another way, the separation, \\n/H9004\\nT\\n,\\n between samples has to be less than 1 s. \\nObserve that sampling this signal at \\nexactly\\n twice the frequenc\\ny (1 sample/s), with samples taken at \\nt\\n=\\n01\\n2\\n,, ,,\\n±±\\n…\\n results in \\n…\\n…\\nsin( ), sin( ), sin( ) ,\\n−\\npp\\n0\\n all of which are 0. This illustrates the reason \\nwhy the sampling theorem requires a sampling rate that exceeds twice the highest frequenc\\ny of the \\nfunction, as mentioned earlier.\\nThe large dots in Fig. 4.11 are samples taken uniformly at a rate \\nbelow\\n the required 1 sample/s (i.e., \\nthe samples are taken \\nmore\\n than 1 s apart; in fact, the separation between samples exceeds 2 s). The \\nsampled signal \\nlooks\\n like a sine wave, but its frequency is about one-tenth the frequency of the original \\nfunction. This sampled signal, having a frequency well below anything present in the original continu-', metadata={'source': 'imagepro.pdf', 'page': 225}),\n",
       " Document(page_content='ous function, is an example of aliasing. If the signal had been sampled at a rate slightly exceeding the \\nNyquist rate, the samples would not look like a sine wave at all (see Problem 4.6).\\nFigure 4.11 also illustrates how aliasing can be extremely problematic in musical recordings by intro-\\nducing frequencies not present in the original sound. In order to mitigate this, signals with frequencies \\nabove half the sampling rate \\nmust\\n be ﬁltered out to reduce the effect of aliased signals introduced into \\ndigital recordings. This is the reason why digital recording equipment contains lowpass ﬁlters speciﬁcally \\ndesigned to remove frequency components above half the sampling rate used by the equipment. \\nIf we were given just the samples in Fig. 4.11, another issue illustrating the seriousness of aliasing is \\nthat we would have no way of knowing that these samples are not a true representation of the original', metadata={'source': 'imagepro.pdf', 'page': 225}),\n",
       " Document(page_content='function. As you will see later in this chapter, aliasing in images can produce similarly misleading results.\\nFUNCTION RECONSTRUCTION (RECOVERY) FROM SAMPLED DATA\\nIn this section, we show that reconstructing a function from a set of its samples \\nreduces in practice to interpolating between the samples. Even the simple act of \\ndisplaying an image requires reconstruction of the image from its samples by the dis-\\nplay medium. Therefore, it is important to understand the fundamentals of sampled \\ndata reconstruction. Convolution is central to developing this understanding, dem-\\nonstrating again the importance of this concept.\\nThe discussion of Fig. 4.8 and Eq. (4-34) outlines the procedure for perfect recov-\\nery of a band-limited function from its samples using frequency domain methods. \\n. . .\\n. . .\\nt \\n/H9004\\nT\\n04\\n. . .\\n123 5\\n. . .\\nFIGURE\\n \\n4.11\\n Illustration of aliasing. The under-sampled function (dots) looks like a sine wave having a frequency', metadata={'source': 'imagepro.pdf', 'page': 225}),\n",
       " Document(page_content='much lower than the frequency of the continuous signal. The period of the sine wave is 2 s, so the zero crossings of \\nthe horizontal axis occur every second. \\n/H9004\\nT\\n is the separation between samples. \\nDIP4E_GLOBAL_Print_Ready.indb   224\\n6/16/2017   2:04:41 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 225}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n225\\nUsing the convolution theorem, we can obtain the equivalent result in the spatial \\ndomain. From Eq. (4-34), \\nFH F\\n()\\n()() ,\\nmm m\\n=\\n/tildenosp\\n so it follows that\\n \\nft F\\nHF\\nht\\nft\\n() ( )\\n()()\\n()\\n()\\n=\\n{}\\n=\\n{}\\n=\\n−\\n−\\n/H5219\\n/H5219\\n1\\n1\\nm\\nmm\\n/tildenosp\\n/tildenosp\\n/H22841\\n \\n(4-37)\\nwhere, as before, \\n/tildenosp\\nft\\n()\\n denotes the sampled function, and the last step follows from \\nthe convolution theorem,\\n Eq. (4-25). It can be shown (see Problem 4.13), that sub-\\nstituting Eq. (4-27) for \\n/tildenosp\\nft\\n()\\n into Eq. (4-37), and then using Eq. (4-24), leads to the \\nfollowing spatial domain expression for \\nft\\n()\\n:\\n \\nft fnT t nT T\\nn\\n() ( ) ( )\\n=−\\n[]\\n=−\\n∑\\n/H9004/H9004 /H9004\\n/H11009\\n/H11009\\nsinc\\n \\n(4-38)\\nwhere the sinc function is defined in Eq. (4-23). This result is not unexpected because \\nthe inverse F\\nourier transform of the ideal (box) filter, \\nH\\n()\\n,\\nm\\n is a sinc function (see \\nExample 4.1).', metadata={'source': 'imagepro.pdf', 'page': 226}),\n",
       " Document(page_content='Example 4.1).\\n Equation (4-38) shows that the perfectly reconstructed function, \\nft\\n()\\n,\\n \\nis an infinite sum of sinc functions weighted by the sample values\\n. It has the impor-\\ntant property that the reconstructed function is identically equal to the sample val-\\nues at multiple integer increments of \\n/H9004\\nT\\n.\\n That is, for any \\ntk T\\n=\\n/H9004\\n,\\n where \\nk\\n is an inte-\\nger\\n, \\nft\\n()\\n is equal to the \\nk\\nth sample\\n, \\nfk T\\n()\\n.\\n/H9004\\n This follows from Eq. (4-38) because \\nsinc( )\\n01\\n=\\n and \\nsinc( )\\nm\\n=\\n0\\n for any other integer value of \\nm\\n.\\n Between sample points, \\nvalues of \\nft\\n()\\n are \\ninterpolations\\n formed by the sum of the sinc functions\\n. \\nEquation (4-38) requires an inﬁnite number of terms for the interpolations \\nbetween samples. In practice, this implies that we have to look for approximations \\nthat are \\nﬁnite\\n interpolations between the samples. As we discussed in Section 2.6, the \\nprincipal interpolation approaches used in image processing are nearest-neighbor,', metadata={'source': 'imagepro.pdf', 'page': 226}),\n",
       " Document(page_content='bilinear, and bicubic interpolation. We will discuss the effects of interpolation on \\nimages in Section 4.5.\\n4.4 THE DISCRETE FOURIER TRANSFORM OF ONE VARIABLE  \\nOne of the principal goals of this chapter is the derivation of the \\ndiscrete Fourier \\ntransform\\n (DFT) starting from basic principles. The material up to this point may \\nbe viewed as the foundation of those basic principles, so now we have in place the \\nnecessary tools to derive the DFT.\\nOBTAINING THE DFT FROM THE CONTINUOUS TRANSFORM OF A \\nSAMPLED FUNCTION\\nAs we discussed in Section 4.3, the Fourier transform of a sampled, band-limited func-\\ntion extending from \\n−\\n/H11009\\n to \\n/H11009\\n is a \\ncontinuous\\n, \\nperiodic\\n function that also extends from \\n−\\n/H11009\\n to \\n/H11009\\n.\\n In practice, we work with a finite number of samples, and the objective of \\nthis section is to derive the DFT of such finite sample sets\\n.\\nEquation (4-31) gives the transform, \\n/tildenosp\\nF\\n()\\n,\\nm\\n of sampled data in terms of the trans-', metadata={'source': 'imagepro.pdf', 'page': 226}),\n",
       " Document(page_content='form of the original function,\\n but it does not give us an expression for \\n/tildenosp\\nF\\n()\\nm\\n in terms \\nSee Section 2.4 regard-\\ning interpolation.\\n4.4\\nDIP4E_GLOBAL_Print_Ready.indb   225\\n6/16/2017   2:04:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 226}),\n",
       " Document(page_content='226\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nof the sampled function \\n/tildenosp\\nft\\n()\\n itself. We ﬁnd that expression directly from the deﬁni-\\ntion of the F\\nourier transform in Eq. (4-19):\\n \\n/tildenosp\\n/tildenosp\\nFf\\nt e d t\\njt\\n() ( )\\nm\\npm\\n=\\n−\\n-\\n/H11009\\n/H11009\\n2\\n2\\n \\n(4-39)\\nBy substituting Eq. (4-27) for \\n/tildenosp\\nft\\n()\\n,\\n we obtain\\n \\n/tildenosp\\n/tildenosp\\nF\\nf t e dt\\nf t t n T e dt\\njt\\njt\\nn\\n( ) ()\\n() ( )\\nmd\\npm\\npm\\n== −\\n=\\n−−\\n=−\\n∑\\n--\\n-\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n22\\n22\\n/H11009 /H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004\\n/H11009\\n/H11009\\n/H9004\\n2\\nn\\njt\\nn\\njn T\\nn\\nft t nTe d t\\nfe\\n=−\\n−\\n−\\n=−\\n∑\\n∑\\n−\\n=\\n() ( )\\nd\\npm\\npm\\n2\\n2\\n \\n(4-40)\\nThe last step follows from Eq. (4-28) and the sifting property of the impulse. \\nAlthough \\nf\\nn\\n is a discrete function, its Fourier transform, \\n/tildenosp\\nF\\n()\\n,\\nm\\n is continuous and \\ninfinitely periodic with period \\n1\\n/H9004\\nT\\n,\\n as we know from Eq. (4-31). Therefore, all we \\nneed to characterize \\n/tildenosp\\nF\\n()\\nm', metadata={'source': 'imagepro.pdf', 'page': 227}),\n",
       " Document(page_content='/tildenosp\\nF\\n()\\nm\\n is one period, and sampling one period of this function is \\nthe basis for the DFT\\n.\\nSuppose that we want to obtain \\nM\\n equally spaced samples of \\n/tildenosp\\nF\\n()\\nm\\n taken over the \\none period interval from \\nm\\n=\\n0\\n to \\nm\\n=\\n1\\n/H9004\\nT\\n (see Fig. 4.8). This is accomplished by \\ntaking the samples at the following frequencies:\\n \\nm\\n== −\\nm\\nMT\\nmM\\n/H9004\\n012 1\\n,,, ,\\n…\\n \\n(4-41)\\nSubstituting this result for \\nm\\n into Eq. (4-40) and letting \\nF\\nm\\n denote the result yields\\n \\nFf e m M\\nmn\\nn\\nM\\njm n M\\n== −\\n=\\n−\\n−\\n∑\\n0\\n1\\n2\\n012 1\\np\\n,,, ,\\n…\\n \\n(4-42)\\nThis expression is the \\ndiscrete F\\nourier transform\\n we are seeking.\\n†\\n Given a set \\n{}\\nf\\nm\\n \\nconsisting of \\nM\\n samples of \\nft\\n()\\n,\\n Eq. (4-42) yields a set \\n{}\\nF\\nm\\n of \\nM\\n complex values \\ncorresponding to the discrete Fourier transform of the input sample set. Conversely, \\n†\\n Referring back to Fig. 4.6(b), note that the interval \\n[, ]\\n01\\n/H9004\\nT\\n over which we sampled one period of \\n/tildenosp\\nF\\n()\\nm\\n covers', metadata={'source': 'imagepro.pdf', 'page': 227}),\n",
       " Document(page_content='F\\n()\\nm\\n covers \\ntwo adjacent half periods of the transform (but with the lowest half of period appearing at higher frequencies).\\n \\nThis means that the data in \\nF\\nm\\n requires re-ordering to obtain samples that are ordered from the lowest to the \\nhighest frequency of the period. This is the price paid for the notational convenience of taking the samples at \\nmM\\n=−\\n01\\n2 1\\n,, , , ,\\n…\\n instead of using samples on either side of the origin, which would require the use of nega-\\ntive notation.\\n The procedure used to order the transform data will be discussed in Section 4.6.  \\nDIP4E_GLOBAL_Print_Ready.indb   226\\n6/16/2017   2:04:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 227}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n227\\ngiven \\n{} ,\\nF\\nm\\n we can recover the sample set \\n{}\\nf\\nm\\n by using the \\ninverse discrete Fourier \\ntransform\\n (IDFT)\\n \\nf\\nM\\nFe n M\\nnm\\nm\\nM\\njm n M\\n== −\\n=\\n−\\n∑\\n1\\n012 1\\n0\\n1\\n2\\np\\n,,, ,\\n…\\n \\n(4-43)\\nIt is not difficult to show (see Problem 4.15) that substituting Eq. (4-43) for \\nf\\nn\\n into \\nEq. (4-42) gives the identity \\nFF\\nmm\\n≡\\n.\\n Similarly, substituting Eq. (4-42) into Eq. (4-43) \\nfor \\nF\\nm\\n yields \\nff\\nnn\\n≡\\n.\\n This implies that Eqs. (4-42) and (4-43) constitute a \\ndiscrete \\nF\\nourier transform pair\\n. Furthermore, these identities indicate that the forward and \\ninverse Fourier transforms exist for any set of samples whose values are finite. Note \\nthat neither expression depends explicitly on the sampling interval \\n/H9004\\nT\\n,\\n nor on the \\nfrequenc\\ny intervals of Eq. (4-41). Therefore, the DFT pair is applicable to \\nany\\n finite \\nset of discrete samples taken uniformly.\\nWe used \\nm\\n and \\nn', metadata={'source': 'imagepro.pdf', 'page': 228}),\n",
       " Document(page_content='We used \\nm\\n and \\nn\\n in the preceding development to denote discrete variables \\nbecause it is typical to do so for derivations. However, it is more intuitive, especially \\nin two dimensions, to use the notation \\nx\\n and \\ny\\n for image coordinate variables and \\nu\\n and \\nv\\n for frequency variables, where these are understood to be integers.\\n†\\n Then, \\nEqs. (4-42) and (4-43) become \\n \\nFu fxe u M\\nju x M\\nx\\nM\\n() ()\\n,, , ,\\n== −\\n−\\n=\\n−\\n∑\\n2\\n0\\n1\\n012 1\\np\\n…\\n \\n(4-44)\\nand\\n \\nfx\\nM\\nFue x M\\nju x M\\nu\\nM\\n() ()\\n,, , ,\\n== −\\n=\\n−\\n∑\\n1\\n012 1\\n2\\n0\\n1\\np\\n…\\n \\n(4-45)\\nwhere we used functional notation instead of subscripts for simplicity. Comparing \\nEqs\\n. (4-42) through (4-45), you can see that \\nFu F\\nm\\n()\\n≡\\n and \\nfx f\\nn\\n() .\\n≡\\n From this point \\non,\\n we use Eqs. (4-44) and (4-45) to denote the 1-D DFT pair. As in the continuous \\ncase, we often refer to Eq. (4-44) as the \\nforward\\n DFT of \\nfx\\n()\\n,\\n and to Eq. (4-45) as \\nthe \\ninverse\\n DFT of \\nFu\\n()\\n.\\n As before, we use the notation \\nfx Fu\\n()\\n()\\n⇔\\n to denote a \\nF', metadata={'source': 'imagepro.pdf', 'page': 228}),\n",
       " Document(page_content='⇔\\n to denote a \\nF\\nourier transform pair. Sometimes you will encounter in the literature the \\n1\\nM\\n term \\nin front of Eq.\\n (4-44) instead. That does not affect the proof that the two equations \\nform a Fourier transform pair (see Problem 4.15). \\nKnowledge that \\nfx\\n()\\n and \\nFu\\n()\\n are a transform pair is useful in proving relation-\\nships between functions and their transforms\\n. For example, you are asked in Prob-\\nlem 4.17 to show that \\nfx x Fue\\nju x M\\n() ( )\\n−⇔\\n−\\n0\\n2\\n0\\np\\n is a Fourier transform pair. That is, \\nyou have to show that the DFT of \\nfx x\\n()\\n−\\n0\\n is \\nFue\\nju x M\\n()\\n−\\n2\\n0\\np\\n and, conversely, that \\nthe \\ninverse\\n DFT of \\nFue\\nju x M\\n()\\n−\\n2\\n0\\np\\n is \\nfx x\\n()\\n.\\n−\\n0\\n Because this is done by substituting \\n†\\n  We have been careful in using \\nt\\n for \\ncontinuous\\n spatial variables and \\nm\\n for the corresponding \\ncontinuous\\n fre-\\nquenc\\ny variables. From this point on, we will use \\nx\\n and \\nu\\n to denote 1-D \\ndiscrete\\n spatial and frequency variables,', metadata={'source': 'imagepro.pdf', 'page': 228}),\n",
       " Document(page_content='respectively. When working in 2-D, we will use \\n(, )\\ntz\\n, and \\n(,) ,\\nmn\\n to denote continuous spatial and frequency \\ndomain variables\\n, respectively. Similarly, we will use \\n(, )\\nxy\\n and \\n(,)\\nuv\\n to denote their discrete counterparts. \\nDIP4E_GLOBAL_Print_Ready.indb   227\\n6/16/2017   2:04:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 228}),\n",
       " Document(page_content='228\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ndirectly into Eqs. (4-44) and (4-45), and you will have proved already that these two \\nequations constitute a Fourier transform pair (Problem 4.15), if you prove that one \\nside of “\\n⇔\\n” is the DFT (IDFT) of the other, then it must be true the other side is the \\nIDFT (DFT) of the side you just proved.\\n It turns out that having the option to prove \\none side or the other often simpliﬁes proofs signiﬁcantly.\\n \\nThis is true also of the 1-D \\ncontinuous and 2-D continuous and discrete F\\nourier transform pairs.\\nIt can be shown (see Problem 4.16) that both the forward and inverse discrete \\ntransforms are inﬁnitely periodic, with period \\nM\\n. That is,\\n \\nFu Fu k M\\n()\\n( )\\n=+\\n \\n(4-46)\\nand\\n \\nfx fx k M\\n()\\n( )\\n=+\\n \\n(4-47)\\nwhere \\nk\\n is an integer\\n.\\nThe discrete equivalent of the 1-D convolution in Eq. (4-24) is\\n \\nfx\\nfm h x m x M\\nhx\\nm\\nM\\n()\\n( )( ) ,, , ,\\n()\\n/H22841\\n=\\n−= −\\n=\\n−\\n∑\\n0\\n1\\n012 1\\n…\\n \\n(4-48)', metadata={'source': 'imagepro.pdf', 'page': 229}),\n",
       " Document(page_content='1\\n012 1\\n…\\n \\n(4-48)\\nBecause in the preceding formulations the functions are periodic, their convolu-\\ntion also is periodic\\n. Equation (4-48) gives one period of the periodic convolution. \\nFor this reason, this equation often is referred to as \\ncircular convolution\\n. This is a \\ndirect result of the periodicity of the DFT and its inverse. This is in contrast with the \\nconvolution you studied in Section 3.4, in which values of the displacement, \\nx\\n, were \\ndetermined by the requirement of sliding one function completely past the other, \\nand were not fixed to the range \\n[, ]\\n01\\nM\\n−\\n as in circular convolution. We will discuss \\nthis difference and its significance in Section 4.6 and in F\\nig. 4.27.\\nFinally, we point out that the convolution theorem given in Eqs. (4-25) and (4-26) \\nis applicable also to discrete variables, with the exception that the right side of \\nEq. (4-26) is multiplied by \\n1\\nM\\n (Problem 4.18).\\nRELATIONSHIP BETWEEN THE SAMPLING AND FREQUENCY  \\nINTERVALS\\nIf \\nfx\\n()', metadata={'source': 'imagepro.pdf', 'page': 229}),\n",
       " Document(page_content='INTERVALS\\nIf \\nfx\\n()\\n consists of \\nM\\n samples of a function \\nft\\n()\\n taken \\n/H9004\\nT\\n units apart, the length of \\nthe record comprising the set \\nfx x M\\n()\\n, ,, , , ,\\n{}\\n=−\\n012 1\\n…\\n is\\n \\nTM T\\n=\\n/H9004\\n \\n(4-49)\\nThe corresponding spacing, \\n/H9004\\nu\\n, in the frequency domain follows from Eq. (4-41):\\n \\n/H9004\\n/H9004\\nu\\n==\\n11\\nMT T\\n \\n(4-50)\\nDIP4E_GLOBAL_Print_Ready.indb   228\\n6/16/2017   2:04:48 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 229}),\n",
       " Document(page_content='4.4\\n  \\nThe Discrete Fourier Transform of One Variable\\n    \\n229\\nThe entire frequency range spanned by the \\nM\\n components of the DFT is then\\n \\nRM u\\nT\\n==\\n/H9004\\n/H9004\\n1\\n \\n(4-51)\\nThus, we see from Eqs. (4-50) and (4-51) that the resolution in frequency, \\n/H9004\\nu\\n,\\n of \\nthe DFT depends inversely on the length (duration,\\n if \\nt\\n is time) of the record, \\nT\\n, \\nover which the continuous function, \\nft\\n()\\n, is sampled; and the range of frequencies \\nspanned by the DFT depends on the sampling interval \\n/H9004\\nT\\n.\\n Keep in mind these \\ninverse\\n relationships between \\n/H9004\\nu\\n and \\n/H9004\\nT\\n.\\nEXAMPLE 4.4 :  The mechanics of computing the DFT.\\nFigure 4.12(a) shows four samples of a continuous function, \\nft\\n()\\n,\\n taken \\n/H9004\\nT\\n units apart. Figure 4.12(b) \\nshows the samples in the \\nx\\n-domain.\\n The values of \\nx\\n are 0, 1, 2, and 3, which refer to the number of the \\nsamples in sequence, counting up from 0. For example, \\nff tT\\n()\\n( ) ,\\n22\\n0\\n=+\\n/H9004\\n the third sample of \\nft\\n()\\n.', metadata={'source': 'imagepro.pdf', 'page': 230}),\n",
       " Document(page_content='ft\\n()\\n.\\nFrom Eq. (4-44), the ﬁrst value of \\nFu\\n()\\n [i.e., \\nF\\n()\\n]\\n0  is\\n \\nFf x f f f f\\nx\\n() () () () () ()\\n0 0123 1 2 4 4 1 1\\n0\\n3\\n== + + +\\n[]\\n=+++=\\n=\\n∑\\nThe next value of \\nFu\\n()\\n is\\n \\nFf x e e e e e j\\njx\\nx\\njj j\\n() ( )\\n()\\n11\\n2\\n4\\n4\\n3\\n2\\n21 4\\n0\\n3\\n02 3 2\\n==\\n+ +\\n+ =\\n−\\n+\\n−\\n=\\n−− −\\n∑\\npp\\np\\np\\nSimilarly, \\nFj\\n()\\n( )\\n21 0\\n=− +\\n and \\nFj\\n()\\n( ) .\\n33 2\\n=− +\\n Observe that \\nall\\n values of \\nfx\\n()\\n are used in computing \\neac\\nh\\n value of \\nFu\\n()\\n.\\nIf we were given \\nFu\\n()\\n instead, and were asked to compute its inverse, we would proceed in the same \\nmanner\\n, but using the inverse Fourier transform. For instance, \\n \\nfF u e F u j j\\nju\\nuu\\n() ()\\n()\\n()\\n0\\n1\\n4\\n1\\n4\\n1\\n4\\n11 3 2 1 3 2\\n1\\n4\\n20\\n0\\n3\\n0\\n3\\n== =\\n−\\n+\\n−\\n−\\n−\\n[]\\n=\\n==\\n∑∑\\np\\n[\\n[]\\n41\\n=\\nwhich agrees with Fig. 4.12(b). The other values of \\nfx\\n()\\n are obtained in a similar manner.\\n1\\n02\\n3\\nt\\nf\\n(\\nt\\n)\\n0\\n1\\n2\\n3\\n4\\n5\\nf\\n(\\nx\\n)\\n0\\n1\\n2\\n3\\n4\\n5\\nx \\nt\\n0\\nt\\n0\\n \\n/H11001\\n \\n/H9004\\nTt\\n0\\n \\n/H11001\\n 2\\n/H9004\\nTt\\n0\\n \\n/H11001\\n 3\\n/H9004\\nT\\n0\\nb a\\nFIGURE 4.12\\n(a) A continuous \\nfunction sampled \\n/H9004\\nT', metadata={'source': 'imagepro.pdf', 'page': 230}),\n",
       " Document(page_content='/H9004\\nT\\n units apart. \\n(b) Samples in the \\nx\\n-domain.\\n  \\nVariable \\nt\\n is  \\ncontinuous, while \\nx\\n is discrete.\\nDIP4E_GLOBAL_Print_Ready.indb   229\\n6/16/2017   2:04:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 230}),\n",
       " Document(page_content='230\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.5 EXTENSIONS TO FUNCTIONS OF TWO VARIABLES  \\nIn the following discussion we extend to two variables the concepts introduced in \\nthe previous sections of this chapter.\\nTHE 2-D IMPULSE AND ITS SIFTING PROPERTY\\nThe impulse, \\nd\\n(,\\n) ,\\ntz\\n of two continuous variables, \\nt\\n and \\nz\\n,\\n is defined as before:\\n \\nd\\n(,\\n)\\ntz\\ntz\\n=\\n==\\n⎧\\n⎨\\n⎩\\n10\\n0\\nif \\notherwise\\n \\n(4-52)\\nand\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nd\\n(, )\\nt z dtdz\\n=\\n1  \\n(4-53)\\nAs in the 1-D case, the 2-D impulse exhibits the \\nsifting property\\n under integration,\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nftz tzd t d z f\\n(, ) (, ) ( , )\\nd\\n=\\n00\\n \\n(4-54)\\nor. more generally for an impulse located at \\n(, ) ,\\ntz\\n00\\n \\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nf t z t tz zd t d z f tz\\n(, ) ( , ) ( , )\\nd\\n−− =\\n00 0 0\\n \\n(4-55)\\nAs before, we see that the sifting property yields the value of the function at the \\nlocation of the impulse\\n.\\nFor discrete variables \\nx\\n and \\ny', metadata={'source': 'imagepro.pdf', 'page': 231}),\n",
       " Document(page_content='x\\n and \\ny\\n, the 2-D discrete unit impulse is deﬁned as\\n \\nd\\n(,\\n)\\nxy\\nxy\\n=\\n==\\n⎧\\n⎨\\n⎩\\n10\\n0\\nif \\notherwise\\n \\n(4-56)\\nand its sifting property is\\n \\nfx y x y f\\ny x\\n(,)(,) (,)\\nd\\n=\\n=− =−\\n∑ ∑\\n00\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n \\n(4-57)\\nwhere \\nfx y\\n(,\\n)\\n is a function of discrete variables \\nx\\n and \\ny\\n.\\n For an impulse located at \\ncoordinates \\n(,)\\nxy\\n00\\n (see Fig. 4.13) the sifting property is\\n \\nf x y x xy y f xy\\ny x\\n(,)( , ) ( , )\\nd\\n−− =\\n=− =−\\n∑ ∑\\n00 0 0\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n \\n(4-58)\\nWhen working with an image of finite dimensions, the limits in the two preceding \\nequations are replaced by the dimensions of the image\\n.\\n4.5\\nDIP4E_GLOBAL_Print_Ready.indb   230\\n6/16/2017   2:04:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 231}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n231\\nTHE 2-D CONTINUOUS FOURIER TRANSFORM PAIR\\nLet \\nftz\\n(,\\n)\\n be a continuous function of two continuous variables, \\nt\\n and \\nz\\n.\\n The two-\\ndimensional, continuous Fourier transform pair is given by the expressions\\n \\nFf\\nt\\nz\\ne d\\nt\\nd\\nz\\njt z\\n(,) ( ,)\\n()\\nmn\\npm n\\n=\\n−+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2\\n \\n(4-59)\\nand\\n \\nftz F e d d\\njt z\\n(, )\\n( , )\\n()\\n=\\n+\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\nmn\\nm n\\npm n\\n2\\n \\n(4-60)\\nwhere \\nm\\n and \\nn\\n are the frequency variables. When referring to images, \\nt\\n and \\nz\\n are \\ninterpreted to be continuous \\nspatial\\n variables\\n. As in the 1-D case, the domain of the \\nvariables \\nm\\n and \\nn\\n defines the \\ncontinuous frequency domain\\n.\\nEXAMPLE 4.5 :  Obtaining the Fourier transform of a 2-D box function.\\nFigure 4.14(a) shows the 2-D equivalent of the 1-D box function in Example 4.1. Following a procedure \\nsimilar to the one used in that example gives the result \\n \\nFf\\nt\\nz\\ne d\\nt\\nd\\nz A\\ne\\njt z\\nj\\nT\\nT\\nZ\\nZ\\n(,) ( ,)\\n()\\n(\\nmn', metadata={'source': 'imagepro.pdf', 'page': 232}),\n",
       " Document(page_content='Z\\n(,) ( ,)\\n()\\n(\\nmn\\npm n\\npm\\n==\\n−+\\n−\\n−−\\n--\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n22\\n2 2\\n22\\n2\\n2\\n2\\n2\\nt\\ntz\\ndt dz\\nATZ\\nT\\nT\\nZ\\nZ\\n+\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\nn\\npm\\npm\\npn\\npn\\n)\\nsin( )\\n()\\nsin( )\\n()\\nFigure 4.14(b) shows a portion of the spectrum about the origin. As in the 1-D case, the locations of the \\nzeros in the spectrum are inversely proportional to the values of \\nT\\n and \\nZ\\n.\\n In this example, \\nT\\n is larger \\nthan \\nZ\\n, so the spectrum is the more “contracted” along the \\nm\\n-axis.\\n \\n2-D SAMPLING AND THE 2-D SAMPLING THEOREM\\nIn a manner similar to the 1-D case, sampling in two dimensions can be modeled \\nusing a sampling function (i.e., a 2-D impulse train):\\nx\\n0\\ny\\n0\\n x\\nd\\n(\\nx \\n/H11002\\n x\\n0\\n, \\ny \\n/H11002\\n y\\n0\\n)\\ny \\n1\\nd\\nFIGURE 4.13\\n2-D unit discrete  \\nimpulse. Variables \\nx\\n and \\ny\\n are  \\ndiscrete, and \\nd\\n is \\nzero everywhere \\nexcept at  \\ncoordinates \\n(,) ,\\nxy\\n00\\n where its \\nvalue is 1.\\nDIP4E_GLOBAL_Print_Ready.indb   231\\n6/16/2017   2:04:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 232}),\n",
       " Document(page_content='232\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nst z t m T z n Z\\nTZ\\nn m\\n/H9004/H9004\\n/H11009\\n/H11009\\n/H11009\\n/H11009\\n/H9004/H9004\\n(, )\\n( , )\\n=− −\\n=− =−\\n∑ ∑\\nd\\n \\n(4-61)\\nwhere \\n/H9004\\nT\\n and \\n/H9004\\nZ\\n are the separations between samples along the \\nt\\n- and \\nz\\n-axis of \\nthe continuous function \\nftz\\n(,\\n) .\\n Equation (4-61) describes a set of periodic impulses \\nextending infinitely along the two axes (see F\\nig. 4.15). As in the 1-D case illustrated \\nin Fig. 4.5, multiplying \\nftz\\n(,\\n)\\n by \\nst z\\nTZ\\n/H9004/H9004\\n(, )\\n yields the sampled function. \\nFunction \\nftz\\n(,\\n)\\n is said to be \\nband limited\\n if its F\\nourier transform is 0 outside a \\nrectangle established in the frequency domain by the intervals \\nmm\\n−\\n[]\\nmax max\\n,\\n and \\nnn\\n−\\n[]\\nmax max\\n,;\\n that is,\\n \\nF\\n(,)\\nmax\\nmax\\nmn\\nm m n n\\n=\\n0 for    and  \\n≥≥\\n \\n(4-62)\\nThe \\ntwo-dimensional sampling theorem\\n states that a continuous\\n, band-limited func-\\ntion \\nftz\\n(,\\n)\\n can be recovered with no error from a set of its samples if the sampling', metadata={'source': 'imagepro.pdf', 'page': 233}),\n",
       " Document(page_content='intervals are\\n \\n/H9004\\nT\\n<\\n1\\n2\\nm\\nmax\\n \\n(4-63)\\nand\\n \\n/H9004\\nZ\\n<\\n1\\n2\\nn\\nmax\\n \\n(4-64)\\nor, expressed in terms of the sampling rate, if\\nZ\\nT\\nT\\n/\\n2\\nZ\\n/\\n2\\n t\\nf\\n(\\nt\\n, \\nz\\n)\\nz\\nm\\nn\\nATZ\\n/H11341\\nF\\n(\\nm\\n, \\nn\\n)\\n/H11341\\nA\\nb a\\nFIGURE 4.14\\n(a) A 2-D function \\nand (b) a section \\nof its spectrum. \\nThe box is longer \\nalong the \\nt\\n-axis, \\nso the spectrum is \\nmore contracted \\nalong the \\nm\\n-axis.\\n  t\\n/H9004\\nZ\\n/H9004\\nT\\ns\\n/H9004\\nT\\n/H9004\\nZ\\n(\\nt\\n, \\nz\\n)\\n. . .\\n. . .\\n. . .\\n. . .\\nz \\nFIGURE 4.15\\n2-D impulse train.\\nDIP4E_GLOBAL_Print_Ready.indb   232\\n6/16/2017   2:04:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 233}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n233\\n1\\n2\\n/H9004\\nT\\n>\\nm\\nmax\\n(4-65)\\nand\\n \\n1\\n2\\n/H9004\\nZ\\n>\\nn\\nmax\\n(4-66)\\nStated another way, we say that no information is lost if a 2-D, band-limited, con-\\ntinuous function is represented by samples acquired at rates greater than twice the \\nhighest frequenc\\ny content of the function in both the \\nm\\n- and \\nn\\n-\\ndirections.\\nFigure 4.16 shows the 2-D equivalents of Figs. 4.6(b) and (d). A 2-D ideal ﬁl-\\nter transfer function has the form illustrated in F\\nig. 4.14(a) (but in the frequency \\ndomain). The dashed portion of Fig. 4.16(a) shows the location of the ﬁlter function \\nto achieve the necessary isolation of a single period of the transform for recon-\\nstruction of a band-limited function from its samples, as in Fig. 4.8. From Fig 4.10, \\nwe know that if the function is under-sampled, the periods overlap, and it becomes \\nimpossible to isolate a single period, as Fig. 4.16(b) shows. Aliasing would result \\nunder such conditions.\\nALIASING IN IMAGES', metadata={'source': 'imagepro.pdf', 'page': 234}),\n",
       " Document(page_content='ALIASING IN IMAGES\\nIn this section, we extend the concept of aliasing to images, and discuss in detail sev-\\neral aspects of aliasing related to image sampling and resampling.\\nExtensions from 1-D Aliasing\\nAs in the 1-D case, a continuous function \\nftz\\n(,\\n)\\n of two continuous variables, \\nt\\n and \\nz\\n, \\ncan be band-limited in general only if it extends infinitely in both coordinate direc-\\ntions\\n. The very act of limiting the spatial duration of the function (e.g., by multiply-\\ning it by a box function) introduces corrupting frequency components extending to \\ninfinity in the frequency domain, as explained in Section 4.3 (see also Problem 4.12). \\nBecause we cannot sample a function infinitely, aliasing is always present in digital \\nimages, just as it is present in sampled 1-D functions. There are two principal mani-\\nfestations of aliasing in images: spatial aliasing and temporal aliasing. \\nSpatial aliasing', metadata={'source': 'imagepro.pdf', 'page': 234}),\n",
       " Document(page_content='Spatial aliasing\\n \\nis caused by under-sampling, as discussed in Section 4.3, and tends to be more visible \\nm\\nm\\nv\\nv\\nmax\\nv\\nm\\nmax\\nFootprint of a\\n2-D ideal lowpass\\n(box) filter\\nb a\\nFIGURE 4.16\\nTwo-dimensional \\nFourier  \\ntransforms of (a) an \\nover-sampled, and \\n(b) an under-sam-\\npled, band-limited \\nfunction. \\nDIP4E_GLOBAL_Print_Ready.indb   233\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 234}),\n",
       " Document(page_content='234\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n(and objectionable) in images with repetitive patterns. \\nTemporal aliasing\\n is related \\nto time intervals between images of a sequence of dynamic images. One of the most \\ncommon examples of temporal aliasing is the “wagon wheel” effect, in which wheels \\nwith spokes in a sequence of images (for example, in a movie) appear to be rotating \\nbackwards. This is caused by the frame rate being too low with respect to the speed \\nof wheel rotation in the sequence, and is similar to the phenomenon described in \\nFig. 4.11, in which under sampling produced a signal that appeared to be of much \\nlower frequency than the original. \\nOur focus in this chapter is on spatial aliasing. The key concerns with spatial alias-\\ning in images are the introduction of artifacts such as jaggedness in line features, spu-\\nrious highlights, and the appearance of frequency patterns not present in the original', metadata={'source': 'imagepro.pdf', 'page': 235}),\n",
       " Document(page_content='image. Just as we used Fig. 4.9 to explain aliasing in 1-D functions, we can develop \\nan intuitive grasp of the nature of aliasing in images using some simple graphics. The \\nsampling grid in the center section of Fig. 4.17 is a 2-D representation of the impulse \\ntrain in Fig. 4.15. In the grid, the little white squares correspond to the location of the \\nimpulses (where the image is sampled) and black represents the separation between \\nsamples. Superimposing the sampling grid on an image is analogous to multiplying \\nthe image by an impulse train, so the same sampling concepts we discussed in con-\\nnection with the impulse train in Fig. 4.15 are applicable here. The focus now is to \\nanalyze graphically the interaction between sampling rate (the separation of the \\nsampling points in the grid) and the frequency of the 2-D signals being sampled.\\nFigure 4.17 shows a sampling grid partially overlapping three 2-D signals (regions', metadata={'source': 'imagepro.pdf', 'page': 235}),\n",
       " Document(page_content='of an image) of low, mid, and high spatial frequencies (relative to the separation \\nbetween sampling cells in the grid). Note that the level of spatial “detail” in the \\nregions is proportional to frequency (i.e., higher-frequency signals contain more \\nbars). The sections of the regions inside the sampling grip are rough manifestations \\nof how they would appear after sampling. As expected, all three digitized regions \\nSampling grid\\nLow frequency\\nMid frequency\\nHigh frequency\\nFIGURE 4.17\\nVarious aliasing \\neffects resulting \\nfrom the  \\ninteraction  \\nbetween the \\nfrequency of 2-D \\nsignals and the \\nsampling rate \\nused to digitize \\nthem. The regions \\noutside the \\nsampling grid are \\ncontinuous and \\nfree of aliasing.\\nDIP4E_GLOBAL_Print_Ready.indb   234\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 235}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n235\\nexhibit aliasing to some degree, but the effects are dramatically different, worsening \\nas the discrepancy between detail (frequency) and sampling rate increases. The low-\\nfrequency region is rendered reasonably well, with some mild jaggedness around \\nthe edges. The jaggedness increases as the frequency of the region increases to the \\nmid-range because the sampling rate is the same. This edge distortion (appropriately \\ncalled \\njaggies\\n) is common in images with strong line and/or edge content.\\nThe digitized high-frequency region in the top right of Fig. 4.17 exhibits totally \\ndifferent and somewhat surprising behavior. Additional stripes (of lower frequen-\\ncy) appear in the digitized section, and these stripes are rotated signiﬁcantly with \\nrespect to the direction of the stripes in the continuous region. These stripes are an \\nalias of a totally different signal. As the following example shows, this type of behav-', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='ior can result in images that appear “normal” and yet bear no relation to the original.\\nEXAMPLE 4.6 :  Aliasing in images.\\nConsider an imaging system that is perfect, in the sense that it is noiseless and produces an exact digi-\\ntal image of what it sees, but the number of samples it can take is ﬁxed at \\n96 96\\n×\\n pixels. For simplicity, \\nassume that pixels are little squares of unit width and length.\\n We want to use this system to digitize \\ncheckerboard images of alternating black and white squares. Checkerboard images can be interpreted \\nas periodic, extending inﬁnitely in both dimensions, where one period is equal to adjacent black/white \\npairs. If we specify “valid” digitized images as being those extracted from an inﬁnite sequence in such \\na way that the image contains an integer multiple of periods, then, based on our earlier comments, we \\nknow that properly sampled periodic images will be free of aliasing. In the present example, this means', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='that the sizes of the squares must be such that dividing 96 by the size yields an even number. This will \\ngive an integer number of periods (pairs of black/white squares). The smallest size of squares under the \\nstated conditions is 1 pixel.\\nThe principal objective of this example is to examine what happens when checkerboard images with \\nsquares of sizes less than 1 pixel on the side are presented to the system. This will correspond to the \\nundersampled case discussed earlier, which will result in aliasing. A horizontal or vertical scan line of the \\ncheckerboard images results in a 1-D square wave, so we can focus the analysis on 1-D signals.\\nTo understand the capabilities of our imaging system in terms of sampling, recall from the discussion \\nof the 1-D sampling theorem that, given the sampling rate, the maximum frequency allowed before \\naliasing occurs in the sampled signal has to be less than one-half the sampling rate. Our sampling rate is', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='ﬁxed, at one sample per unit of the independent variable (the units are pixels). Therefore, the maximum \\nfrequency our signal can have in order to avoid aliasing is 1/2 cycle/pixel. \\nWe can arrive at the same conclusion by noting that the most demanding image our system can \\nhandle is when the squares are 1 unit (pixel) wide, in which case the period (cycle) is two pixels. The \\nfrequency is the reciprocal of the period, or 1/2 cycle/pixel, as in the previous paragraph. \\nFigures 4.18(a) and (b) show the result of sampling checkerboard images whose squares are of sizes \\n16 16\\n×\\n and \\n66\\n×\\n pixels, respectively. The frequencies of scan lines in either direction of these two images \\nare 1/32 and 1/6 c\\nycles/pixel. These are well below the 1/2 cycles/pixel allowed for our system. Because, as \\nmentioned earlier, the images are perfectly registered in the ﬁeld of view of the system, the results are free \\nof aliasing, as expected.', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='When the size of the squares is reduced to slightly less than one pixel, a severely aliased image results, \\nas Fig. 4.18(c) shows (the squares used were approximately of size \\n09 5 09 5\\n..\\n×\\n pixels). Finally, reducing \\nDIP4E_GLOBAL_Print_Ready.indb   235\\n6/16/2017   2:04:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 236}),\n",
       " Document(page_content='236\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthe size of the squares to slightly less than 0.5 pixels on the side yielded the image in Fig. 4.18(d). In \\nthis case, the aliased result looks like a normal checkerboard pattern. In fact, this image would result \\nfrom sampling a checkerboard image whose squares are 12 pixels on the side. This last image is a good \\nreminder that aliasing can create results that may be visually quite misleading. \\nThe effects of aliasing can be reduced by slightly defocusing the image to be digi-\\ntized so that high frequencies are attenuated. As explained in Section 4.3, anti-alias-\\ning ﬁltering has to be done at the “front-end,” \\nbefore\\n the image is sampled. There \\nare no such things as after-the-fact software anti-aliasing ﬁlters that can be used to \\nreduce the effects of aliasing caused by violations of the sampling theorem. Most \\ncommercial digital image manipulation packages do have a feature called “anti-', metadata={'source': 'imagepro.pdf', 'page': 237}),\n",
       " Document(page_content='aliasing.” However, as illustrated in Example 4.8 below, this term is related to blur-\\nring a digital image to reduce additional aliasing artifacts caused by resampling. The \\nterm does not apply to reducing aliasing in the original sampled image. A signiﬁcant \\nnumber of commercial digital cameras have true anti-aliasing ﬁltering built in, either \\nin the lens or on the surface of the sensor itself. Even nature uses this approach to \\nreduce the effects of aliasing in the human eye, as the following example shows.\\nEXAMPLE 4.7 :  Nature obeys the limits of the sampling theorem.\\nWhen discussing Figs. 2.1 and 2.2, we mentioned that cones are the sensors responsible for sharp vision. \\nCones are concentrated in the fovea, in line with the visual axis of the lens, and their concentration is \\nmeasured in degrees off that axis. A standard test of visual acuity (the ability to resolve ﬁne detail) in', metadata={'source': 'imagepro.pdf', 'page': 237}),\n",
       " Document(page_content='humans is to place a pattern of alternating black and white stripes in one degree of the visual ﬁeld. If the \\ntotal number of stripes exceeds 120 (i.e., a frequency of 60 cycles/degree), experimental evidence shows \\nthat the observer will perceive the image as a single gray mass. That is, the lens in the eye automatically \\nlowpass ﬁlters spatial frequencies higher than 60 cycles/degree. Sampling in the eye is done by the cones, \\nso, based on the sampling theorem, we would expect the eye to have on the order of 120 cones/degree \\nin order to avoid the effects of aliasing. As it turns out, that is exactly what we have! \\nb a\\nd c\\nFIGURE 4.18\\nAliasing. In (a) and \\n(b) the squares are \\nof sizes 16 and 6 \\npixels on the side. \\nIn (c) and (d) the \\nsquares are of sizes \\n0.95 and 0.48 pixels, \\nrespectively. Each \\nsmall square in (c) \\nis one pixel. Both \\n(c) and (d) are \\naliased. Note how \\n(d) masquerades as \\na “normal” image.\\nDIP4E_GLOBAL_Print_Ready.indb   236\\n6/16/2017   2:04:59 PM', metadata={'source': 'imagepro.pdf', 'page': 237}),\n",
       " Document(page_content='www.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 237}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n237\\nImage Resampling and Interpolation\\nAs in the 1-D case, perfect reconstruction of a band-limited image function from a set \\nof its samples requires 2-D convolution in the spatial domain with a sinc function.  As \\nexplained in Section 4.3, this theoretically perfect reconstruction requires interpola-\\ntion using infinite summations which, in practice, forces us to look for approximate \\ninterpolation methods. One of the most common applications of 2-D interpolation \\nin image processing is in image resizing (zooming and shrinking). Zooming may \\nbe viewed as over-sampling, while shrinking may be viewed as under-sampling. The \\nkey difference between these two operations and the sampling concepts discussed \\nin previous sections is that we are applying zooming and shrinking to digital images.\\nWe introduced interpolation in Section 2.4. Our interest there was to illustrate the', metadata={'source': 'imagepro.pdf', 'page': 238}),\n",
       " Document(page_content='performance of nearest neighbor, bilinear, and bicubic interpolation. In this section, \\nthe focus is on sampling and anti-aliasing issues. Aliasing generally is introduced \\nwhen an image is scaled, either by zooming or by shrinking. For example, a special \\ncase of nearest neighbor interpolation is zooming by \\npixel replication\\n, which we use \\nto increase the size of an image an integer number of times. To double the size of \\nan image, we duplicate each column. This doubles the image size in the horizontal \\ndirection. Then, we duplicate each row of the enlarged image to double the size in \\nthe vertical direction. The same procedure is used to enlarge the image any integer \\nnumber of times. The intensity level assignment of each pixel is predetermined by \\nthe fact that new locations are exact duplicates of old locations. In this crude method \\nof enlargement, one of the principal aliasing effects is the introduction of jaggies', metadata={'source': 'imagepro.pdf', 'page': 238}),\n",
       " Document(page_content='on straight lines that are not horizontal or vertical. The effects of aliasing in image \\nenlargement often are reduced signiﬁcantly by using more sophisticated interpola-\\ntion, as we discussed in Section 2.4. We show in the following example that aliasing \\ncan also be a serious problem in image shrinking. \\nEXAMPLE 4.8 :  Illustration of aliasing in resampled natural images.\\nThe effects of aliasing generally are worsened when the size of a digital image is reduced. Figure 4.19(a) \\nis an image containing regions purposely selected to illustrate the effects of aliasing (note the thinly \\nspaced parallel lines in all garments worn by the subject). There are no objectionable aliasing artifacts \\nin Fig. 4.19(a), indicating that the sampling rate used initially was sufﬁcient to mitigate visible aliasing. \\nIn Fig. 4.19(b), the image was reduced to 33% of its original size using row/column deletion. The', metadata={'source': 'imagepro.pdf', 'page': 238}),\n",
       " Document(page_content='effects of aliasing are quite visible in this image (see, for example, the areas around scarf and the sub-\\nject’s knees). Images (a) and (b) are shown in the same size because the reduced image was brought \\nback to its original size by pixel replication (the replication did not alter appreciably the effects of alias-\\ning just discussed. \\nThe digital “equivalent” of the defocusing of continuous images mentioned earlier for reducing alias-\\ning, is to attenuate the high frequencies of a \\ndigital\\n image by smoothing it with a lowpass ﬁlter before \\nresampling. Figure 4.19(c) was processed in the same manner as Fig. 4.19(b), but the original image was \\nsmoothed using a \\n55\\n×\\n spatial averaging ﬁlter (see Section 3.5) before reducing its size. The improve-\\nment over F\\nig. 4.19(b) is evident. The image is slightly more blurred than (a) and (b), but aliasing is no \\nlonger objectionable.\\nDIP4E_GLOBAL_Print_Ready.indb   237\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 238}),\n",
       " Document(page_content='238\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nAliasing and Moiré Patterns\\nIn optics, a \\nmoiré pattern\\n is a secondary, visual phenomenon produced, for example, \\nby superimposing two gratings of approximately equal spacing. These patterns are \\ncommon, everyday occurrences. For instance, we see them in overlapping insect win-\\ndow screens and on the interference between TV raster lines and striped or high-\\nly textured materials in the background, or worn by individuals. In digital image \\nprocessing, moiré-like patterns arise routinely when sampling media print, such as \\nnewspapers and magazines, or in images with periodic components whose spacing \\nis comparable to the spacing between samples. It is important to note that moiré \\npatterns are more general than sampling artifacts. For instance, Fig. 4.20 shows the \\nmoiré effect using vector drawings that have not been digitized. Separately, the pat-', metadata={'source': 'imagepro.pdf', 'page': 239}),\n",
       " Document(page_content='terns are clean and void of interference. However, the simple acts of superimposing \\none pattern on the other creates a pattern with frequencies not present in either of \\nthe original patterns. Note in particular the moiré effect produced by two patterns \\nof dots, as this is the effect of interest in the following discussion.\\nEXAMPLE 4.9 :  Sampling printed media.\\nNewspapers and other printed materials use so called \\nhalftone dots\\n, which are black dots or ellipses \\nwhose sizes and various grouping schemes are used to simulate gray tones. As a rule, the following num-\\nbers are typical: newspapers are printed using 75 halftone dots per inch (dpi), magazines use 133 dpi, and \\nThe term \\nmoiré\\n is a \\nFrench word (not the \\nname of a person) that \\nappears to have  \\noriginated with weavers, \\nwho ﬁrst noticed what \\nappeared to be interfer-\\nence patterns visible on \\nsome fabrics. The root \\nof the word is from the \\nword \\nmohair\\n, a cloth \\nmade from Angora goat \\nhairs.\\nb a\\nc\\nFIGURE 4.19', metadata={'source': 'imagepro.pdf', 'page': 239}),\n",
       " Document(page_content='b a\\nc\\nFIGURE 4.19\\n Illustration of aliasing on resampled natural images. (a) A digital image of size \\n772 548\\n×\\n pixels with visu-\\nally negligible aliasing\\n. (b) Result of resizing the image to 33% of its original size by pixel deletion and then restor-\\ning it to its original size by pixel replication. Aliasing is clearly visible. (c) Result of blurring the image in (a) with an \\naveraging ﬁlter prior to resizing. The image is slightly more blurred than (b), but aliasing is not longer objectionable. \\n(Original image courtesy of the Signal Compression Laboratory, University of California, Santa Barbara.) \\nDIP4E_GLOBAL_Print_Ready.indb   238\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 239}),\n",
       " Document(page_content='4.5\\n  \\nExtensions to Functions of Two Variables\\n    \\n239\\nb a\\nc\\ne d\\nf\\nFIGURE 4.20\\nExamples of the \\nmoiré effect. \\nThese are vector \\ndrawings, not \\ndigitized patterns. \\nSuperimposing \\none pattern on the \\nother is analogous \\nto multiplying the \\npatterns. \\nhigh-quality brochures use 175 dpi. Figure 4.21 shows what happens when a newspaper image is (under) \\nsampled at 75 dpi. The sampling lattice (which is oriented vertically and horizontally) and dot patterns \\non the newspaper image (oriented at \\n±°\\n45\\n) interact to create a uniform moiré-like pattern that makes \\nthe image look blotchy\\n. (We will discuss a technique in Section 4.10 for reducing the effects of moiré \\npatterns in under-sampled print media.)\\nFIGURE 4.21\\nA newspaper \\nimage digitized at \\n75 dpi. Note the \\nmoiré-like pattern \\nresulting from \\nthe interaction \\nbetween the \\n±°\\n45\\n \\norientation of the \\nhalf-tone dots and \\nthe north-south \\norientation of the \\nsampling elements \\nused to digitized \\nthe image.', metadata={'source': 'imagepro.pdf', 'page': 240}),\n",
       " Document(page_content='the image.\\nDIP4E_GLOBAL_Print_Ready.indb   239\\n6/16/2017   2:04:59 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 240}),\n",
       " Document(page_content='240\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nTHE 2-D DISCRETE FOURIER TRANSFORM AND ITS INVERSE\\nA development similar to the material in Sections 4.3 and 4.4 would yield the follow-\\ning 2-D discrete Fourier transform (DFT):\\n \\nFf\\nx\\ny\\ne\\njx M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nuv\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n \\n(4-67)\\nwhere \\nfx y\\n(,\\n)\\n is a digital image of size \\nMN\\n×\\n.\\n As in the 1-D case, Eq. (4-67) must be \\nevaluated for values of the discrete variables \\nu\\n and \\nv\\n in the ranges \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n \\nand \\nv\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n†\\n Given the transform \\nF\\n(,\\n) ,\\nuv\\n we can obtain \\nfx y\\n(,\\n)\\n by using the \\ninverse discrete \\nF\\nourier transform\\n (IDFT):\\n \\nfx y\\nMN\\nFu e\\nju x My N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nv\\nv\\nv u\\np\\n \\n(4-68)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As in the 1-D case, [Eqs. (4-44) \\nand (4-45)],\\n Eqs. (4-67) and (4-68) constitute a 2-D \\ndiscrete Fourier transform pair,\\n \\nfx y F\\n(,\\n) (,) .\\n⇔\\nuv', metadata={'source': 'imagepro.pdf', 'page': 241}),\n",
       " Document(page_content='(,\\n) (,) .\\n⇔\\nuv\\n (The proof is a straightforward extension of the 1-D case in Prob-\\nlem 4.15.) \\nThe rest of this chapter is based on properties of these two equations and \\ntheir use for image filtering in the frequency domain. The comments made in con-\\nnection with Eqs. (4-44) and (4-45) are applicable to Eqs. (4-67) and (4-68); that is, \\nknowing that \\nfx y\\n(,\\n)\\n and \\nF\\n(,\\n)\\nuv\\n are a Fourier transform pair can be quite useful in \\nproving relationships between functions and their transforms\\n. \\n4.6 SOME PROPERTIES OF THE 2-D DFT AND IDFT \\nIn this section, we introduce several properties of the 2-D discrete Fourier transform \\nand its inverse.\\nRELATIONSHIPS BETWEEN SPATIAL AND FREQUENCY INTERVALS\\nThe relationships between spatial sampling and the corresponding frequency \\ndomain intervals are as explained in Section 4.4. Suppose that a continuous func-\\ntion \\nftz\\n(,\\n)\\n is sampled to form a digital image, \\nfx y\\n(,\\n) ,\\n consisting of \\nMN\\n×\\n samples \\ntaken in the \\nt\\n- and \\nz\\n-directions', metadata={'source': 'imagepro.pdf', 'page': 241}),\n",
       " Document(page_content='z\\n-directions\\n, respectively. Let \\n/H9004\\nT\\n and \\n/H9004\\nZ\\n denote the separations \\nbetween samples (see F\\nig. 4.15). Then, the separations between the corresponding \\ndiscrete, frequency domain variables are given by \\n \\n/H9004\\n/H9004\\nu\\n=\\n1\\nMT\\n \\n(4-69)\\n†\\n As mentioned in Section 4.4, keep in mind that in this chapter we use \\n(, )\\ntz\\n and \\n(,)\\nmn\\n to denote 2-D \\ncontinuous\\n \\nspatial and frequenc\\ny-domain variables. In the 2-D \\ndiscrete\\n case, we use \\n(, )\\nxy\\n for spatial variables and \\n(,)\\nuv\\n for \\nfrequenc\\ny-domain variables, all of which are discrete.\\nSometimes you will ﬁnd \\nin the literature the  \\n1\\n/H20862\\nMN constant in front \\nof the DFT instead of \\nthe IDFT. At times, \\nthe square root of this \\nconstant is included in \\nfront of the forward \\nand inverse transforms, \\nthus creating a more \\nsymmetrical pair. Any \\nof these formulations is \\ncorrect, provided they \\nare used consistently.\\n4.6\\nDIP4E_GLOBAL_Print_Ready.indb   240\\n6/16/2017   2:05:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 241}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n241\\nand\\n \\n/H9004\\n/H9004\\nv\\n=\\n1\\nNZ\\n \\n(4-70)\\nrespectively. Note the important property that the separations between samples in \\nthe frequenc\\ny domain are inversely proportional both to the spacing between spa-\\ntial samples \\nand\\n to the number of samples.\\nTRANSLATION AND ROTATION\\nThe validity of the following Fourier transform pairs can be demonstrated by direct \\nsubstitution into Eqs. (4-67) and (4-68) (see Problem 4.27):\\n \\nfx ye\\nFu u\\njx M y N\\n(,)\\n( , )\\n()\\n2\\n00\\n00\\np\\nuv\\nvv\\n+\\n⇔− −\\n \\n(4-71)\\nand\\n \\nfx x y y F e\\njx M y N\\n(,) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nuv\\nuv\\np\\n \\n(4-72)\\nThat is, multiplying \\nfx y\\n(,\\n)\\n by the exponential shown shifts the origin of the DFT to \\n(,)\\nuv\\n00\\n and, conversely, multiplying \\nF\\n(,\\n)\\nuv\\n by the negative of that exponential shifts \\nthe origin of \\nfx y\\n(,\\n)\\n to \\n(,) .\\nxy\\n00\\n As we illustrate in Example 4.13, translation has no \\neffect on the magnitude (spectrum) of \\nF\\n(,\\n) .\\nuv\\nUsing the polar coordinates\\n \\nxr yr u\\n==\\n= =\\ncos', metadata={'source': 'imagepro.pdf', 'page': 242}),\n",
       " Document(page_content='xr yr u\\n==\\n= =\\ncos\\nsin\\ncos\\nsin\\nuu v w v w\\nv\\n \\nresults in the following transform pair:\\n \\nfr F\\n(,\\n) ( , )\\nuu v wu\\n+⇔ +\\n00\\n \\n(4-73)\\nwhich indicates that rotating \\nfx y\\n(,\\n)\\n by an angle \\nu\\n0\\n rotates \\nF\\n(,\\n)\\nuv\\n by the same angle. \\nConversely\\n, rotating \\nF\\n(,\\n)\\nuv\\n rotates \\nfx y\\n(,\\n)\\n by the same angle.\\nPERIODICITY\\nAs in the 1-D case, the 2-D Fourier transform and its inverse are infinitely periodic\\nin the \\nu\\n and \\nv\\n directions; that is,\\n \\nF F kM F kN F kM kN\\n(,\\n) ( ,) (, ) ( , )\\nuv u v uv u v\\n=+ = + =+ +\\n12 1\\n2\\n \\n(4-74)\\nand\\n \\nfx y fx k M y fx y k N fx k M y k N\\n(,\\n) ( ,) (, ) ( , )\\n=+ = + =+ +\\n12 1\\n2\\n \\n(4-75)\\nwhere \\nk\\n1\\n and \\nk\\n2\\n are integers.\\nThe periodicities of the transform and its inverse are important issues in the \\nimplementation of DFT-based algorithms. Consider the 1-D spectrum in Fig. 4.22(a). \\nAs explained in Section 4.4\\n \\n[see the footnote to Eq.\\n \\n(4-42)], the transform data in the \\ninterval from 0 to \\nM\\n−\\n1\\n consists of two half periods meeting at point \\nM\\n2,\\n but with', metadata={'source': 'imagepro.pdf', 'page': 242}),\n",
       " Document(page_content='M\\n2,\\n but with \\nRecall that we use the \\nsymbol “\\n⇔\\n” to denote \\nFourier transform pairs. \\nThat is, the term on the \\nright is the transform \\nof the term on the left, \\nand the term on the left \\nis the inverse Fourier \\ntransform of the term on \\nthe right.\\nDIP4E_GLOBAL_Print_Ready.indb   241\\n6/16/2017   2:05:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 242}),\n",
       " Document(page_content='242\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthe lower part of the period appearing at higher frequencies. For display and ﬁlter-\\ning purposes, it is more convenient to have in this interval a complete period of the \\ntransform in which the data are contiguous and ordered properly, as in Fig. 4.22(b). \\nIt follows from Eq. (4-71) that\\nfxe Fu u\\nju x M\\n()\\n( )\\n()\\n2\\n0\\np\\n⇔−\\n0\\nIn other words, multiplying \\nfx\\n()\\n by the exponential term shown shifts the transform \\ndata so that the origin,\\n \\nF\\n()\\n,\\n0\\n is moved to \\nu\\n0\\n.\\n If we let \\nuM\\n0\\n2\\n=\\n,\\n the exponential \\nterm becomes \\ne\\njx\\np\\n, which is equal to \\n()\\n−\\n1\\nx\\n because \\nx\\n is an integer. In this case,\\nb\\na\\nd c\\nFIGURE 4.22\\nCentering the \\nFourier transform. \\n(a) A 1-D DFT \\nshowing an inﬁnite \\nnumber of peri-\\nods. (b) Shifted \\nDFT obtained \\nby multiplying \\nfx\\n()\\n by \\n()\\n−\\n1\\nx\\n \\nbefore computing \\nFu\\n()\\n.\\n (c) A 2-D \\nDFT showing an \\ninﬁnite number of \\nperiods\\n. The area \\nwithin the dashed \\nrectangle is the \\ndata array, \\nF\\n(,\\n) ,\\nuv', metadata={'source': 'imagepro.pdf', 'page': 243}),\n",
       " Document(page_content='F\\n(,\\n) ,\\nuv\\n \\nobtained with \\nEq.\\n (4-67) with \\nan image \\nfx y\\n(,\\n)\\n \\nas the input.\\n This \\narray consists of \\nfour quarter peri-\\nods. (d) Shifted \\narray obtained \\nby multiplying \\nfx y\\n(,\\n)\\n by \\n()\\n−\\n+\\n1\\nxy\\n \\nbefore computing \\nF\\n(,\\n) .\\nuv\\n The data \\nnow contains one \\ncomplete\\n, centered \\nperiod, as in (b). \\n/H11002\\nM\\n/\\n2\\nM\\n/\\n2 \\n/H11002\\n 1\\n0\\n0\\nM\\n/\\n2\\nM\\n/\\n2\\nM\\n \\n/H11002\\n 1\\nM\\n \\n/H11002\\n 1\\nM\\nTwo adjacent half \\nperiods meet here.\\nF\\n(\\nu\\n)\\nF\\n(\\nu\\n)\\nu\\nu\\nTwo adjacent half\\nperiods meet here.\\nOne period (\\nM\\n samples)\\nM\\n/\\n2\\nM\\n \\n/H11002\\n 1\\n(0, 0)\\nN\\n/\\n2\\nN\\n \\n/H11002\\n 1\\nu\\nv\\nv\\nu\\nN\\n/\\n2\\nN\\n \\n/H11002\\n 1\\nM\\n/\\n2\\n \\nM\\n/H11002\\n 1\\n(0, 0)\\n/H11005\\n \\nM\\n \\n/H11003\\n \\nN\\n data array computed by the DFT with             as input \\n(,)\\nfx y\\n/H11005\\n \\nM\\n \\n/H11003\\n \\nN\\n data array computed by the DFT with                         as input \\n(,) (1 )\\nxy\\nfx y\\n+\\n−\\n= Periods of the DFT\\nFour adjacent quarter\\nperiods meet here\\n(0,0)\\nF\\nDIP4E_GLOBAL_Print_Ready.indb   242\\n6/16/2017   2:05:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 243}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n243\\n \\nfx Fu M\\nx\\n() ( ) ( / )\\n−⇔ −\\n12\\nThat is, multiplying \\nfx\\n()\\n by \\n()\\n−\\n1\\nx\\n shifts the data so that \\nFu\\n()\\n is centered on the inter-\\nval \\n[, ] ,\\n01\\nM\\n−\\n which corresponds to Fig. 4.22(b), as desired. \\nIn 2-D the situation is more difﬁcult to graph,\\n but the principle is the same, as \\nFig. 4.22(c) shows. Instead of two half periods, there are now four quarter periods \\nmeeting at the point \\n(,) .\\nMN\\n22\\n As in the 1-D case, we want to shift the data so \\nthat \\nF\\n(,\\n)\\n00\\n is at \\n(,) .\\nMN\\n22\\n Letting \\n(,)( , )\\nuM N\\n00\\n22\\nv\\n=\\n in Eq. (4-71) results in \\nthe expression\\n \\nfx y Fu M N\\nxy\\n(,) ( ) ( , )\\n−⇔ − −\\n+\\n12 2\\nv\\n \\n(4-76)\\nUsing this equation shifts the data so that \\nF\\n(,\\n)\\n00\\n is moved to the center of \\nthe \\nfrequency rectangle\\n (i.e\\n., the rectangle deﬁned by the intervals \\n[, ]\\n01\\nM\\n−\\n and \\n[, ]\\n01\\nN\\n−\\n in the frequency domain). Figure 4.22(d) shows the result. \\nK', metadata={'source': 'imagepro.pdf', 'page': 244}),\n",
       " Document(page_content='K\\neep in mind that in all our discussions, coordinate values in both the spatial and \\nfrequency domains are integers. As we explained in Section 2.4 (see Fig. 2.19) if, as \\nin our case), the origin of an \\nMN\\n×\\n image or transform is at \\n(,) ,\\n00\\n then the center of \\nthat image or transform is at \\nfloor floor\\n() , () .\\nMN\\n22\\n()\\n This expression is applicable \\nto both even and odd values of \\nM\\n and \\nN\\n. For example, the center of an array of size \\n20 15\\n×\\n is at point \\n(, ) .\\n10\\n7\\n Because we start counting from 0, these are the 11th and \\n8th points in the ﬁrst and second coordinate axes of the array\\n, respectively.\\nSYMMETRY PROPERTIES\\nAn important result from functional analysis is that any real \\nor\\n complex function, \\nw\\n(,\\n) ,\\nxy\\n can be expressed as the sum of an even and an odd part, each of which can \\nbe real or complex:\\n \\nww w\\n(,\\n) (,) (,)\\nxy xy xy\\neo\\n=+\\n \\n(4-77)\\nwhere the \\neven\\n and \\nodd\\n parts are defined as\\n \\nw\\nww\\ne\\nxy\\nxy\\nxy\\n(,)\\n(,) (\\n,)\\n≜\\n+− −\\n2\\n \\n(4-78)\\nand\\n \\nw\\nww\\no\\nxy\\nxy\\nxy', metadata={'source': 'imagepro.pdf', 'page': 244}),\n",
       " Document(page_content='w\\nww\\no\\nxy\\nxy\\nxy\\n(,)\\n(,) (\\n,)\\n≜\\n−− −\\n2\\n \\n(4-79)\\nfor all valid values of \\nx\\n and \\ny\\n.\\n Substituting Eqs. (4-78) and (4-79) into Eq. (4-77) gives \\nthe identity \\nww\\n(,\\n) (,) ,\\nxy xy\\n≡\\n thus proving the validity of the latter equation. It fol-\\nlows from the preceding definitions that\\n \\nww\\nee\\nxy x y\\n(,) ( , )\\n=− −\\n \\n(4-80)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   243\\n6/16/2017   2:05:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 244}),\n",
       " Document(page_content='244\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nww\\noo\\nxy x y\\n(,) ( , )\\n=− − −\\n \\n(4-81)\\nEven functions are said to be \\nsymmetric\\n and odd functions \\nantisymmetric\\n.\\n Because \\nall indices in the DFT and IDFT are nonnegative integers, when we talk about sym-\\nmetry (antisymmetry) we are referring to symmetry (antisymmetry) about the \\ncen-\\nter point\\n of a sequence, in which case the definitions of even and odd become:\\n \\nww\\nee\\nxy M xN y\\n(,) ( , )\\n=− −\\n \\n(4-82)\\nand\\n \\nww\\noo\\nxy M xN y\\n(,) ( , )\\n=− − −\\n \\n(4-83)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As usual, \\nM\\n and \\nN\\n are the number \\nof rows and columns of a 2-D array\\n.\\nWe know from elementary mathematical analysis that the product of two even or \\ntwo odd functions is even, and that the product of an even and an odd function is \\nodd. In addition, the only way that a discrete function can be odd is if all its samples \\nsum to zero. These properties lead to the important result that\\n \\nww\\neo\\ny\\nN\\nx\\nM\\nxy xy\\n(,) (,)\\n=\\n=\\n−', metadata={'source': 'imagepro.pdf', 'page': 245}),\n",
       " Document(page_content='xy xy\\n(,) (,)\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n0\\n1\\n0\\n1\\n \\n(4-84)\\nfor any two discrete even and odd functions \\nw\\ne\\n and \\nw\\no\\n.\\n In other words, because the \\nargument of Eq.\\n (4-84) is odd, the result of the summations is 0. The functions can \\nbe real or complex.\\nEXAMPLE 4.10 :  Even and odd functions.\\nAlthough evenness and oddness are visualized easily for continuous functions, these concepts are not as \\nintuitive when dealing with discrete sequences. The following illustrations will help clarify the preceding \\nideas. Consider the 1-D sequence \\n \\nff ff f\\n=\\n{}\\n=\\n{}\\n( ) ,( ) ,( ) ,( ) , , ,\\n0123 2 1 1 1\\nin which \\nM\\n=\\n4.\\n To test for evenness, the condition \\nfx f x\\n()\\n( )\\n=−\\n4\\n must be satisﬁed for \\nx\\n=\\n012\\n3\\n,,,.\\n \\nT\\nhat is, we require that\\n \\nff ff ff ff\\n(\\n) () , () () , () () , () ()\\n04 13 22 31\\n====\\nBecause \\nf\\n()\\n4\\n is outside the range being examined and can be any value, the value of \\nf\\n()\\n0\\n is immaterial \\nin the test for evenness', metadata={'source': 'imagepro.pdf', 'page': 245}),\n",
       " Document(page_content='. We see that the next three conditions are satisﬁed by the values in the array, so \\nthe sequence is even. In fact, we conclude that \\nany\\n 4-point even sequence has to have the form\\n \\nabcb\\n,,\\n,\\n{}\\n \\nThat is, only the second and last points must be equal in a 4-point even sequence. In general, when \\nM\\n \\nis an even number, a 1-D even sequence has the property that the points at locations 0 and \\nM\\n2\\n have \\nIn the context of this dis-\\ncussion, the \\nlocations\\n of \\nelements in a sequence \\nare denoted by integers. \\nTherefore, the same \\nobservations made a few \\nparagraphs back about \\nthe centers of arrays of \\neven and odd \\nsizes\\n are \\napplicable to sequences. \\nBut, do not confuse the \\nconcepts of even/odd \\nnumbers\\n and even/odd \\nfunctions\\n.\\nTo convince yourself that \\nthe samples of an odd \\nfunction sum to zero, \\nsketch one period of \\na 1-D sine wave about \\nthe origin or any other \\ninterval spanning one \\nperiod.\\nDIP4E_GLOBAL_Print_Ready.indb   244\\n6/16/2017   2:05:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 245}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n245\\narbitrary values. When \\nM\\n is odd, the ﬁrst point of an even sequence is still arbitrary, but the others form \\npairs with equal values.\\nOdd sequences have the interesting property that their ﬁrst term, \\nw\\no\\n(,) ,\\n00\\n is always 0, a fact that fol-\\nlows directly from Eq.\\n (4-79). Consider the 1-D sequence\\n \\ngg gg g\\n=\\n{}\\n=−\\n{}\\n( ) ,( ) ,( ) ,( ) , ,,\\n0123 0 1 0 1\\nWe can conﬁrm that this is an odd sequence by noting that the terms in the sequence satisfy the condi-\\ntion \\ngx g x\\n()\\n( )\\n=− −\\n4\\n for \\nx\\n=\\n12\\n3\\n,,.\\n All we have to do for \\nx\\n=\\n0\\n is to check that \\ng\\n()\\n.\\n00\\n=\\n We check the \\nother terms using the deﬁnition.\\n For example, \\ngg\\n()\\n() .\\n13\\n=−\\n Any 4-point odd sequence has the form\\n \\n00\\n,,\\n,\\n−\\n{}\\nbb\\nIn general, when \\nM\\n is an even number\\n, a 1-D odd sequence has the property that the points at locations \\n0 and \\nM\\n2\\n are always zero. When \\nM\\n is odd,\\n the ﬁrst term still has to be 0, but the remaining terms form', metadata={'source': 'imagepro.pdf', 'page': 246}),\n",
       " Document(page_content='pairs with equal value but opposite signs.\\nThe preceding discussion indicates that evenness and oddness of sequences depend also on the length \\nof the sequences. For example, we showed already that the sequence \\n01 0 1\\n,,\\n,\\n−\\n{}\\n is odd. However, the \\nsequence \\n01 0 1 0\\n,,\\n, ,\\n−\\n{}\\n is neither odd nor even, although the “basic” structure appears to be odd. This \\nis an important issue in interpreting DFT results. We will show later in this section that the DFTs of even \\nand odd functions have some very important characteristics. Thus, it often is the case that understanding \\nwhen a function is odd or even plays a key role in our ability to interpret image results based on DFTs.\\nThe same basic considerations hold in 2-D. For example, the \\n66\\n×\\n 2-D array with center at location \\n(,) ,\\n33\\n shown bold in the ﬁgure [remember, we start counting at \\n(,) ] ,\\n00\\n \\n0 000 0 0\\n0\\n000 0 0\\n00 1010\\n00 2 20\\n00 1010\\n0 000 0 0\\n−\\n−\\n−\\n0', metadata={'source': 'imagepro.pdf', 'page': 246}),\n",
       " Document(page_content='0 000 0 0\\n−\\n−\\n−\\n0\\nis odd, as you can prove using Eq. (4-83). However, adding another row or column of 0’s would give \\na result that is neither odd nor even.\\n In general, inserting a 2-D array of \\neven dimensions\\n into a larger \\narray of zeros, \\nalso\\n of even dimensions, preserves the symmetry of the smaller array, provided that the \\ncenters coincide. Similarly, a 2-D array of \\nodd dimensions\\n can be inserted into a larger array of zeros of \\nodd dimensions\\n without affecting the symmetry. Note that the inner structure of the preceding array is \\na Sobel kernel (see Fig. 3.50). We return to this kernel in Example 4.15, where we embed it in a larger \\narray of zeros for ﬁltering purposes.\\nArmed with the preceding concepts, we can establish a number of important sym-\\nmetry properties of the DFT and its inverse. A property used frequently is that the \\nFourier transform of a \\nreal\\n function, \\nfx y\\n(,\\n) ,\\n is \\nconjugate symmetric\\n:\\nConjugate symmetry \\nis also called \\nhermitian \\nsymmetry', metadata={'source': 'imagepro.pdf', 'page': 246}),\n",
       " Document(page_content='hermitian \\nsymmetry\\n. The term \\nantihermitian\\n is used \\nsometimes to refer to \\nconjugate antisymmetry.\\nDIP4E_GLOBAL_Print_Ready.indb   245\\n6/16/2017   2:05:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 246}),\n",
       " Document(page_content='246\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nFF\\n*\\n(,) ( , )\\nuv u v\\n=− −\\n \\n(4-85)\\nWe show the validity of this equation as follows:\\n \\nFu f x y e\\nfx y\\ny\\nN\\nx\\nM\\nju x My N\\n*(\\n)\\n*\\n*\\n(,) (,)\\n(,\\nv\\nv\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n=\\n=\\n−\\n=\\n−\\n−+\\n∑ ∑\\n0\\n1\\n0\\n1\\n2\\np\\n) )\\n(,)\\n()\\n([\\ny\\nN\\nx\\nM\\nju x My N\\ny\\nN\\nx\\nM\\nj\\ne\\nfx ye\\n=\\n−\\n=\\n−\\n+\\n=\\n−\\n=\\n−\\n−−\\n∑ ∑\\n∑ ∑\\n=\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\n2\\np\\np\\nv\\nu\\nuxM yN\\nFu\\n][ ] )\\n(,)\\n+−\\n=− −\\nv\\nv\\nwhere the third step follows from the fact that \\nfx y\\n(,\\n)\\n is real. A similar approach \\ncan be used to prove that,\\n if \\nfx y\\n(,\\n)\\n is \\nimaginary\\n,\\n its Fourier transform is conjugate \\nantisymmetric\\n; that is, \\nFF\\n*\\n(,) ( , ) .\\n−− = −\\nuv u v\\n \\nT\\nable 4.1 lists symmetries and related properties of the DFT that are useful in \\ndigital image processing. Recall that the double arrows indicate Fourier transform \\npairs; that is, for any row in the table, the properties on the right are satisﬁed by the \\nFourier transform of the function having the properties listed on the left, and vice', metadata={'source': 'imagepro.pdf', 'page': 247}),\n",
       " Document(page_content='versa. For example, entry 5 reads: The DFT of a real function \\nfx y\\n(,\\n) ,\\n in which \\n(,)\\nxy\\n \\nSpatial Domain\\n†\\nFrequency Domain\\n†\\n1)\\nfx y\\n(,\\n)\\n real\\n⇔\\nFF\\n*\\n(,) ( , )\\nuv u v\\n=− −\\n2)\\nfx y\\n(,\\n)\\n imaginary\\n⇔\\nFF\\n*\\n(,) ( , )\\n−− = −\\nuv u v\\n3)\\nfx y\\n(,\\n)\\n real\\n⇔\\nR\\nIu\\n(,) (,)\\nuv v\\n even;  odd\\n4)\\nfx y\\n(,\\n)\\n imaginary\\n⇔\\nRI u\\n(,\\n) (,)\\nuv v\\n odd;  even\\n5)\\nfx y\\n(,\\n)\\n−−\\n real\\n⇔\\nF\\n*\\n(,)\\nuv\\n complex\\n6)\\nfx y\\n(,\\n)\\n−−\\n complex\\n⇔\\nF\\n(,\\n)\\n−−\\nuv\\n complex\\n7)\\nfx y\\n*\\n(,)\\n complex\\n⇔\\nF\\n*\\n(,)\\n−−\\nuv\\n complex\\n8)\\nfx y\\n(,\\n)\\n real and even\\n⇔\\nF\\n(,\\n)\\nuv\\n real and even\\n9)\\nfx y\\n(,\\n)\\n real and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n imaginary and odd\\n10)\\nfx y\\n(,\\n)\\n imaginary and even\\n⇔\\nF\\n(,\\n)\\nuv\\n imaginary and even\\n11)\\nfx y\\n(,\\n)\\n imaginary and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n real and odd\\n12)\\nfx y\\n(,\\n)\\n complex and even\\n⇔\\nF\\n(,\\n)\\nuv\\n complex and even\\n13)\\nfx y\\n(,\\n)\\n complex and odd\\n⇔\\nF\\n(,\\n)\\nuv\\n complex and odd\\nTABLE \\n4.1\\nSome symmetry \\nproperties of the \\n2-D DFT and its \\ninverse. \\nR\\n(,\\n)\\nuv\\n \\nand \\nI\\n(,\\n)\\nuv\\n are \\nthe real and \\nimaginary parts of \\nF\\n(,\\n) ,\\nuv', metadata={'source': 'imagepro.pdf', 'page': 247}),\n",
       " Document(page_content='F\\n(,\\n) ,\\nuv\\n  \\nrespectively\\n. \\nUse of the word \\ncomplex\\n indicates \\nthat a function \\nhas nonzero real \\nand imaginary \\nparts. \\n†\\nRecall that \\nx\\n, y, \\nu\\n, and \\nv\\n are \\ndiscrete\\n (integer) variables\\n, with \\nx\\n and \\nu\\n in the range \\n[, ] ,\\n01\\nM\\n−\\n and \\ny\\n and \\nv\\n in \\nthe range \\n[, ] .\\n01\\nN\\n−\\n To say that a complex function is \\neven\\n means that its real \\nand\\n imaginary parts are even,\\n and \\nsimilarly for an \\nodd\\n complex function. As before, “\\n⇔\\n” indicates a Fourier transform pair.\\nDIP4E_GLOBAL_Print_Ready.indb   246\\n6/16/2017   2:05:13 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 247}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n247\\nis replaced by \\n(,) ,\\n−−\\nxy\\n is \\nF\\n*\\n(,) ,\\nuv\\n the complex conjugate of the DFT of \\nfx y\\n(,\\n) .\\n \\nConversely\\n, the IDFT of \\nF\\n*\\n(,)\\nuv\\n is \\nfx y\\n(,\\n) .\\n−−\\nEXAMPLE 4.11 :  1-D illustrations of the properties in Table 4.1.\\nThe 1-D sequences (functions) and their transforms in Table 4.2 are short examples of the properties \\nlisted in Table 4.1. For example, in property 3 we see that a real function with elements \\n1234\\n,,\\n,\\n{}\\n has a \\nFourier transform whose real part, \\n1 0222\\n,,,\\n,\\n−−−\\n{}\\n is even and whose imaginary part, \\n020 2\\n,,\\n, ,\\n−\\n{}\\n is \\nodd. Property 8 tells us that a real even function has a transform that is real and even also. Property 12 \\nshows that an even complex function has a transform that is also complex and even. The other listings \\nin the table are analyzed in a similar manner.\\nEXAMPLE 4.12 :  Proving some of the DFT symmetry properties from Table 4.1.', metadata={'source': 'imagepro.pdf', 'page': 248}),\n",
       " Document(page_content='In this example, we prove several of the properties in Table 4.1 to help you develop familiarity with \\nmanipulating these important properties, and to establish a basis for solving some of the problems at \\nthe end of the chapter. We prove only the properties on the right given the properties on the left. The \\nconverse is proved in a manner similar to the proofs we give here. \\nConsider property 3, which reads: If \\nfx y\\n(,\\n)\\n is a real function, the real part of its DFT is even and the \\nimaginary part is odd.\\n We prove this property as follows: \\nF\\n(,\\n)\\nuv\\n is complex in general,  so it can be expressed \\nas the sum of a real and an imaginary part:\\n \\nFR j I\\n(,\\n) (,) (,) .\\nuv uv uv\\n=+\\n Then, \\nFR j I\\n*\\n(,) (,) (,) .\\nuv uv uv\\n=−\\n \\nAlso\\n, \\nFR j I\\n(,\\n) (,) (,) .\\n−− = −− + −−\\nuv uv uv\\n But, as we proved earlier for Eq. (4-85), if \\nfx y\\n(,\\n)\\n is real then \\nFF\\n*\\n(,) ( , ) ,\\nuv u v\\n=− −\\n which, based on the preceding two equations, means that \\nRR\\n(,\\n) ( , )\\nuv u v\\n=− −\\n and \\nII\\n(,\\n) ( , ) .\\nuv u v', metadata={'source': 'imagepro.pdf', 'page': 248}),\n",
       " Document(page_content='(,\\n) ( , ) .\\nuv u v\\n=− − −\\n In view of the deﬁnitions in Eqs. (4-80) and (4-81), this proves that \\nR\\n is an even \\nfunction and that \\nI\\n is an odd function.\\nNext,\\n we prove property 8. If \\nfx y\\n(,\\n)\\n is real, we know from property 3 that the real part of \\nF\\n(,\\n)\\nuv\\n is \\neven,\\n so to prove property 8 all we have to do is show that if \\nfx y\\n(,\\n)\\n is real and even then the imaginary \\npart of \\nF\\n(,\\n)\\nuv\\n is 0 (i.e., \\nF\\n is real).\\n The steps are as follows:\\nProperty\\nf\\n(\\nx\\n)\\nF\\n(\\nu\\n)\\n3\\n1234\\n,,\\n,\\n{}\\n⇔\\n1 0 02 22 02 2\\n+\\n(\\n)\\n−+\\n(\\n)\\n−+\\n(\\n)\\n−−\\n(\\n)\\n{}\\njjjj\\n,,,\\n4\\n1234\\njj\\njj\\n,,,\\n{}\\n⇔\\n02 5 5 5 0 5 5 5\\n+\\n(\\n)\\n−\\n(\\n)\\n−\\n(\\n)\\n−−\\n(\\n)\\n{}\\n., .., ., ..\\njj j j\\n8\\n2111\\n,,,\\n{}\\n⇔\\n5111\\n,,,\\n{}\\n9\\n01 0 1\\n,,\\n,\\n−\\n{}\\n⇔\\n00 02 00 02\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n−\\n(\\n)\\n{}\\njjjj\\n,,,\\n10\\n2111\\njjjj\\n,,,\\n{}\\n⇔\\n5\\njjjj\\n,,,\\n{}\\n11\\n01 0 1\\njj\\nj j\\n,, ,\\n−\\n{}\\n⇔\\n02 0 2\\n,, ,\\n−\\n{}\\n12\\n44 32 02 32\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n{}\\njjjj\\n,,,\\n⇔\\n1 01 0 42 22 42\\n+\\n(\\n)\\n+\\n(\\n)\\n−+\\n(\\n)\\n+\\n(\\n)\\n{}\\njj jj\\n,, ,\\n13\\n00 1 1 00 1\\n+\\n(\\n)\\n+\\n(\\n)\\n+\\n(\\n)\\n−−\\n(\\n)\\n{}\\njj j j\\n,, ,\\n⇔\\n00 22 00 22\\n+\\n(', metadata={'source': 'imagepro.pdf', 'page': 248}),\n",
       " Document(page_content='⇔\\n00 22 00 22\\n+\\n(\\n)\\n−\\n(\\n)\\n+\\n(\\n)\\n−+\\n(\\n)\\n{}\\njjj j\\n,,,\\nTABLE \\n4.2\\n1-D examples of \\nsome of the prop-\\nerties in Table 4.1.\\nDIP4E_GLOBAL_Print_Ready.indb   247\\n6/16/2017   2:05:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 248}),\n",
       " Document(page_content='248\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\n/H5219\\nfx y F fx ye\\nfx y\\nju x My N\\ny\\nN\\nx\\nM\\nr\\n(,) (,) (,)\\n(,\\n()\\n{}\\n==\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\n2\\n0\\n1\\n0\\n1\\np\\n) )\\n(,)\\n()\\n[]\\n=\\n[]\\n=\\n−\\n=\\n−\\n−+\\n=\\n−\\n=\\n−\\n−\\n∑ ∑\\n∑ ∑\\ny\\nN\\nx\\nM\\nju x My N\\nr\\ny\\nN\\nx\\nM\\ne\\nfx ye\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n0\\n1\\np\\nv\\nj\\nju x M j y N\\ne\\n22\\npp\\n() ()\\n−\\nv\\n \\nWe can expand the last line of this expression in terms of even and odd parts\\n \\nFj\\nj\\ny\\nN\\nx\\nM\\ny\\n(,)\\nuv\\n=\\n[]\\n−\\n[]\\n−\\n[]\\n=\\n[]\\n=\\n−\\n=\\n−\\n=\\n∑ ∑\\neven even odd even odd\\neven\\n0\\n1\\n0\\n1\\n0 0\\n1\\n0\\n1\\n0\\n2\\nN\\nx\\nM\\ny\\nj\\n−\\n=\\n−\\n=\\n∑ ∑\\n⋅− ⋅ − ⋅\\n[]\\n=⋅\\n[]\\neven even even odd odd odd\\neven even\\nN N\\nx\\nM\\ny\\nN\\nx\\nM\\ny\\nN\\nj\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n=\\n−\\n∑\\n∑∑\\n∑∑\\n−⋅\\n[]\\n−⋅\\n[]\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n2 even odd even even\\nx x\\nM\\n=\\n−\\n∑\\n=\\n0\\n1\\nreal.\\nThe ﬁrst step follows from Euler’s equation, and the fact that the cos and sin are even and odd functions, \\nrespectively\\n. We also know from property 8 that, in addition to being real, \\nfx y\\n(,\\n)\\n is an even function. \\nT', metadata={'source': 'imagepro.pdf', 'page': 249}),\n",
       " Document(page_content='T\\nhe only term in the penultimate line containing imaginary components is the second term, which is 0 \\naccording to Eq. (4-84). Therefore, if \\nfx y\\n(,\\n)\\n is real and even, then \\nF\\n(,\\n)\\nuv\\n is real. As noted earlier, \\nF\\n(,\\n)\\nuv\\n \\nis even also because \\nfx y\\n(,\\n)\\n is real. This concludes the proof.\\nF\\ninally, we demonstrate the validity of property 6. From the deﬁnition of the DFT,\\n \\nℑ− −\\n{}\\n=− −\\n=\\n−\\n=\\n−\\n−+\\n∑ ∑\\nfx y fx y e\\ny\\nN\\nx\\nM\\nju x M y N\\n(,) (,)\\n()\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nWe are not making a change of variable here. We are evaluating the DFT of \\nfx y\\n(,\\n) ,\\n−−\\n so we sim-\\nply insert this function into the equation,\\n as we would any other function. Because of periodicity, \\nfx y f Mx Ny\\n(,\\n) ( , ) .\\n−− = − −\\n If we now deﬁne \\nmMx\\n=−\\n and \\nnNy\\n=−\\n, then\\n \\nℑ− −\\n{}\\n=\\n=\\n−\\n=\\n−\\n−− + −\\n∑ ∑\\nfx y f m n e\\nn\\nN\\nm\\nM\\nju M m M N n N\\n(,) ( , )\\n([ ] [ ] )\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nTo convince yourself that the summations are correct, try a 1-D transform and expand a few terms by \\nhand. Because \\nexp[ ( )] ,\\n−=\\nj\\n21\\np\\ninteger', metadata={'source': 'imagepro.pdf', 'page': 249}),\n",
       " Document(page_content='−=\\nj\\n21\\np\\ninteger\\n it follows that\\n \\nℑ− −\\n{}\\n==\\n−\\n−\\n=\\n−\\n=\\n−\\n+\\n∑ ∑\\nfx y f m n e\\nFu\\nn\\nN\\nm\\nM\\njm u M n N\\n(,) ( , )\\n(,)\\n()\\n0\\n1\\n0\\n1\\n2\\np\\nv\\nv\\nThis concludes the proof.\\nDIP4E_GLOBAL_Print_Ready.indb   248\\n6/16/2017   2:05:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 249}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n249\\nFOURIER SPECTRUM AND PHASE ANGLE\\nBecause the 2-D DFT is complex in general, it can be expressed in polar form:\\n \\nFu R j I\\nFu e\\nju\\n(,) (,) (,)\\n(,)\\n(,)\\nvu vu v\\nv\\nv\\n=+\\n=\\nf\\n \\n(4-86)\\nwhere the magnitude\\n \\nFu R u I u\\n(,) (,) (,)\\n/\\nvv v\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n12\\n \\n(4-87)\\nis called the \\nF\\nourier\\n (or \\nfrequency\\n) \\nspectrum\\n, and\\n \\nf\\n( , ) arctan\\n(,)\\n(,)\\nu\\nIu\\nRu\\nv\\nv\\nv\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n \\n(4-88)\\nis the \\nphase angle \\nor\\n phase spectrum\\n.\\n Recall from the discussion in Section 4.2 that \\nthe arctan must be computed using a four-quadrant arctangent function, such as \\nMATLAB’s \\natan2(Imag, Real)\\n function.\\nFinally, the \\npower spectrum\\n is deﬁned as\\n \\nPu Fu\\nRu Iu\\n(,) (,)\\n(,) (\\n,)\\nvv\\nvv\\n=\\n=+\\n2\\n22\\n \\n(4-89)\\nAs before, \\nR\\n and \\nI\\n are the real and imaginary parts of \\nF\\n(,\\n) ,\\nuv\\n and all computations  \\nare carried out for the discrete variables \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nM\\n and \\nv\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n \\nT\\nherefore, \\nF\\n(,) ,\\nuv\\n \\nf\\n(,\\n) ,\\nuv\\n and \\nP\\n(,\\n)\\nuv', metadata={'source': 'imagepro.pdf', 'page': 250}),\n",
       " Document(page_content='uv\\n and \\nP\\n(,\\n)\\nuv\\n are arrays of size \\nMN\\n×\\n.\\nThe Fourier transform of a real function is conjugate symmetric [see Eq. (4-85)], \\nwhich implies that the spectrum has \\neven\\n symmetry about the origin:\\n \\nFu F u\\n(,) ( , )\\nvv\\n=− −\\n \\n(4-90)\\nThe phase angle exhibits \\nodd\\n symmetry about the origin:\\n \\nff\\n(,\\n) ( , )\\nuu\\nvv\\n=− − −\\n \\n(4-91)\\nIt follows from Eq. (4-67) that\\n \\nFf\\nx\\ny\\ny\\nN\\nx\\nM\\n(,) (, )\\n00\\n0\\n1\\n0\\n1\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n \\nwhich indicates that the zero-frequency term of the DFT is proportional to the aver-\\nage of \\nfx y\\n(,\\n) .\\n That is, \\n \\nFM N\\nMN\\nfx y\\nMNf\\ny\\nN\\nx\\nM\\n(,)\\n(, )\\n00\\n1\\n0\\n1\\n0\\n1\\n=\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n \\n(4-92)\\nDIP4E_GLOBAL_Print_Ready.indb   249\\n6/16/2017   2:05:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 250}),\n",
       " Document(page_content='250\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere \\nf\\n (a scalar) denotes the average value of \\nfx y\\n(,\\n) .\\n Then, \\n \\nFM N f\\n(,)\\n00\\n=\\n \\n(4-93)\\nBecause the proportionality constant \\nMN\\n usually is large\\n, \\nF\\n(,)\\n00\\n typically is the \\nlargest component of the spectrum by a factor that can be several orders of magni-\\ntude larger than other terms\\n. Because frequency components \\nu\\n and \\nv\\n are zero at the \\norigin,\\n \\nF\\n(,\\n)\\n00\\n sometimes is called the \\ndc component\\n of the transform.\\n This terminol-\\nogy is from electrical engineering, where “dc” signifies direct current (i.e., current of \\nzero frequency).\\nEXAMPLE 4.13 :  The spectrum of a rectangle.\\nFigure 4.23(a) shows an image of a rectangle and Fig. 4.23(b) shows its spectrum, whose values were \\nscaled to the range \\n[, ]\\n0\\n255\\n and displayed in image form. The origins of both the spatial and frequency \\ndomains are at the top left.\\n This is the right-handed coordinate system convention we deﬁned in Fig. 2.19.', metadata={'source': 'imagepro.pdf', 'page': 251}),\n",
       " Document(page_content='Two things are apparent in Fig. 4.23(b). As expected, the area around the origin of the transform con-\\ntains the highest values (and thus appears brighter in the image). However, note that the four corners \\nx\\nu\\nuu\\ny\\nv\\nv\\nv\\nb a\\nd c\\nFIGURE 4.23\\n(a) Image.  \\n(b) Spectrum, \\nshowing small, \\nbright areas in the \\nfour corners (you \\nhave to look care-\\nfully to see them).  \\n(c) Centered  \\nspectrum.  \\n(d) Result after a \\nlog transformation. \\nThe zero crossings \\nof the spectrum \\nare closer in the \\nvertical direction \\nbecause the rectan-\\ngle in (a) is longer \\nin that direction. \\nThe right-handed  \\ncoordinate  \\nconvention used in \\nthe book places the \\norigin of the spatial \\nand frequency \\ndomains at the top \\nleft (see Fig. 2.19). \\nDIP4E_GLOBAL_Print_Ready.indb   250\\n6/16/2017   2:05:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 251}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n251\\nof the spectrum contain similarly high values. The reason is the periodicity property discussed in the \\nprevious section. To center the spectrum, we simply multiply the image in (a) by \\n()\\n−\\n+\\n1\\nxy\\n before comput-\\ning the DFT, as indicated in Eq. (4-76). Figure 4.23(c) shows the result, which clearly is much easier to \\nvisualize (note the symmetry about the center point). Because the dc term dominates the values of the \\nspectrum, the dynamic range of other intensities in the displayed image are compressed. To bring out \\nthose details, we used the log transformation deﬁned in Eq. (3-4) with \\nc\\n=\\n1.\\n Figure 4.23(d) shows the \\ndisplay of \\nlog( ( , ) ).\\n1\\n+\\nF\\nuv\\n The increased rendition of detail is evident. Most spectra shown in this and \\nsubsequent chapters are scaled in this manner\\n. \\nIt follows from Eqs. (4-72) and (4-73) that the spectrum is insensitive to image translation (the abso-', metadata={'source': 'imagepro.pdf', 'page': 252}),\n",
       " Document(page_content='lute value of the exponential term is 1), but it rotates by the same angle of a rotated image. Figure \\n4.24 illustrates these properties. The spectrum in Fig. 4.24(b) is identical to the spectrum in Fig. 4.23(d). \\nb a\\nd c\\nFIGURE 4.24\\n(a) The rectangle \\nin Fig. 4.23(a) \\ntranslated.  \\n(b) Corresponding  \\nspectrum.  \\n(c) Rotated  \\nrectangle.  \\n(d) Corresponding \\n spectrum. \\nThe spectrum of \\nthe translated  \\nrectangle is \\nidentical to the \\nspectrum of the \\noriginal image in \\nFig. 4.23(a). \\nb a\\nc\\nFIGURE 4.25\\nPhase angle  \\nimages of  \\n(a) centered,  \\n(b) translated, \\nand (c) rotated \\nrectangles.\\nDIP4E_GLOBAL_Print_Ready.indb   251\\n6/16/2017   2:05:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 252}),\n",
       " Document(page_content='252\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nClearly, the images in Figs. 4.23(a) and 4.24(a) are different so, if their Fourier spectra are the same, \\nthen, based on Eq. (4-86), their phase angles must be different. Figure 4.25 conﬁrms this. Figures 4.25(a) \\nand (b) are the phase angle arrays (shown as images) of the DFTs of Figs. 4.23(a) and 4.24(a). Note the \\nlack of similarity between the phase images, in spite of the fact that the only differences between their \\ncorresponding images is simple translation. In general, visual analysis of phase angle images yields little \\nintuitive information. For instance, because of its 45° orientation, one would expect intuitively that the \\nphase angle in Fig. 4.25(a) should correspond to the rotated image in Fig. 4.24(c), rather than to the \\nimage in Fig. 4.23(a). In fact, as Fig. 4.25(c) shows, the phase angle of the rotated image has a strong \\norientation that is much less than 45°.', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='The components of the spectrum of the DFT determine the amplitudes of the \\nsinusoids that combine to form an image. At any given frequency in the DFT of \\nan image, a large amplitude implies a greater prominence of a sinusoid of that fre-\\nquency in the image. Conversely, a small amplitude implies that less of that sinu-\\nsoid is present in the image. Although, as Fig. 4.25 shows, the contribution of the \\nphase components is less intuitive, it is just as important. The phase is a measure of \\ndisplacement of the various sinusoids with respect to their origin. Thus, while the \\nmagnitude of the 2-D DFT is an array whose components determine the intensities \\nin the image, the corresponding phase is an array of angles that carry much of the \\ninformation about where discernible objects are located in the image. The following \\nexample illustrates these ideas in more detail. \\nEXAMPLE 4.14 :  Contributions of the spectrum and phase angle to image formation.', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='Figure 4.26(b) shows as an image the phase-angle array, \\nf\\n(,\\n) ,\\nuv\\n of the DFT of Fig. 4.26(a), computed \\nusing Eq.\\n (4-88). Although there is no detail in this array that would lead us by visual analysis to associ-\\nate it with the structure of its corresponding image, the information in this array is crucial in determin-\\ning shape features of the image. To illustrate this, we reconstructed the boy’s image using only its phase \\nangle. The reconstruction consisted of computing the inverse DFT of Eq. (4-86) using \\nf\\n(,\\n) ,\\nuv\\n but setting \\nF\\n(,) .\\nuv\\n=\\n1\\n Figure Fig. 4.26(c) shows the result (the original result had much less contrast than is shown; \\nto bring out details important in this discussion,\\n we scaled the result using Eqs. (2-31) and (2-32), and \\nthen enhanced it using histogram equalization). However, even after enhancement, it is evident that \\nmuch of the intensity information has been lost (remember, that information is carried by the spectrum,', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='which we did not use in the reconstruction). However, the \\nshape\\n features in 4.26(c) are unmistakably \\nfrom Fig. 4.26(a). This illustrates vividly the importance of the phase angle in determining shape char-\\nacteristics in an image. \\nFigure 4.26(d) was obtained by computing the inverse DFT Eq. (4-86), but using only the spectrum. \\nThis means setting the exponential term to 1, which in turn implies setting the phase angle to 0. The \\nresult is not unexpected. It contains only intensity information, with the dc term being the most domi-\\nnant. There is no shape information in the image because the phase was set to zero.\\nFinally, Figs. 4.26(e) and (f) show yet again the dominance of the phase in determining the spatial \\nfeature content of an image. Figure 4.26(e) was obtained by computing the inverse DFT of Eq. (4-86) \\nusing the spectrum of the rectangle from Fig. 4.23(a) and the phase angle from the boy’s image. The', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='boy’s features clearly dominate this result. Conversely, the rectangle dominates Fig. 4.26(f), which was \\ncomputed using the spectrum of the boy’s image and the phase angle of the rectangle.\\nDIP4E_GLOBAL_Print_Ready.indb   252\\n6/16/2017   2:05:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 253}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n253\\nTHE 2-D DISCRETE CONVOLUTION THEOREM\\nExtending Eq. (4-48) to two variables results in the following expression for 2-D \\ncircular convolution\\n:\\n \\n() ( , ) ( , ) ( ,)\\nfh\\nx y f m n h xm y n\\nn\\nN\\nm\\nM\\n/H22841\\n=−\\n−\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n \\n(4-94)\\nfor \\nxM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nyN\\n=−\\n012\\n1\\n,,, , .\\n…\\n As in Eq. (4-48), Eq. (4-94) gives \\none period of a 2-D periodic sequence\\n. The 2-D convolution theorem is give by\\n \\n() ( , ) () ( , )\\nfh\\nx y F H u\\n/H22841\\n⇔\\ni\\nv\\n \\n(4-95)\\nYou will ﬁnd it helpful \\nto review Eq. (4-48), \\nand the comments made \\nthere regarding circular \\nconvolution, as opposed \\nto the convolution we \\nstudied in Section 3.4.\\nb a\\nc\\ne d\\nf\\nFIGURE 4.26\\n (a) Boy image. (b) Phase angle. (c) Boy image reconstructed using only its phase angle (all shape features \\nare there, but the intensity information is missing because the spectrum was not used in the reconstruction). (d) Boy', metadata={'source': 'imagepro.pdf', 'page': 254}),\n",
       " Document(page_content='image reconstructed using only its spectrum. (e) Boy image reconstructed using its phase angle and the spectrum of \\nthe rectangle in Fig. 4.23(a). (f) Rectangle image reconstructed using its phase and the spectrum of the boy’s image. \\nDIP4E_GLOBAL_Print_Ready.indb   253\\n6/16/2017   2:05:28 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 254}),\n",
       " Document(page_content='254\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nand, conversely,\\n \\n() ( , ) ( ) ( , )\\nfhx y\\nMN\\nFH\\ni\\n⇔\\n1\\n/H22841\\nuv\\n \\n(4-96)\\nwhere \\nF\\n and \\nH\\n are the F\\nourier transforms of \\nf\\n and \\nh\\n, respectively, obtained using \\nEq. (4-67). As before, the double arrow is used to indicate that the left and right sides \\nof the expressions constitute a Fourier transform pair. Our interest in the remainder \\nof this chapter is in Eq. (4-95), which states that the Fourier transform of the spatial \\nconvolution of \\nf\\n and \\nh\\n, is the product of their transforms. Similarly, the inverse DFT \\nof the product \\n() ( , )\\nFH\\ni\\nuv\\n yields \\n() ( , ) .\\nfh\\nx y\\n/H22841\\n \\nEquation (4-95) is the foundation of linear ﬁltering in the frequency domain and, \\nas we will explain in Section 4.7, is the basis for all the ﬁltering techniques discussed \\nin this chapter. As you will recall from Chapter 3, spatial convolution is the foun-', metadata={'source': 'imagepro.pdf', 'page': 255}),\n",
       " Document(page_content='dation for spatial ﬁltering, so Eq. (4-95) is the tie that establishes the equivalence \\nbetween spatial and frequency-domain ﬁltering, as we have mentioned several times \\nbefore. \\nUltimately, we are interested in the results of convolution in the spatial domain, \\nwhere we analyze images. However, the convolution theorem tell us that we have \\ntwo ways of computing the spatial convolution of two functions. We can do it directly \\nin the spatial domain with Eq. (3-35), using the approach described in Section 3.4 \\nor, according to Eq. (4-95), we can compute the Fourier transform of each function, \\nmultiply the transforms, and compute the inverse Fourier transform. Because we are \\ndealing with discrete quantities, computation of the Fourier transforms is carried \\nout using a DFT algorithm. This automatically implies periodicity, which means that \\nwhen we take the inverse Fourier transform of the product of the two transforms we', metadata={'source': 'imagepro.pdf', 'page': 255}),\n",
       " Document(page_content='would get a circular (i.e., periodic) convolution, one period of which is given by Eq. \\n(4-94). The question is: under what conditions will the direct spatial approach and \\nthe inverse Fourier transform method yield the same result? We arrive at the answer \\nby looking at a 1-D example ﬁrst, and then extending the results to two variables.\\nThe left column of Fig. 4.27 implements convolution of two functions, \\nf\\n and \\nh\\n, \\nusing the 1-D equivalent of Eq. (3-35), which, because the two functions are of same \\nsize, is written as\\n \\n() ( )\\n()( )\\nff x h x m\\nhx\\nm\\n/H22841\\n=\\n=\\n∑\\n−\\n0\\n399\\nRecall from our explanation of Figs. 3.29 and 3.30 that the procedure consists of (1) \\nrotating (flipping) \\nh\\n by \\n180\\n°\\n,\\n [see Fig. 4.27(c)], (2) translating the resulting function \\nby an amount \\nx\\n [F\\nig. 4.27(d)], and (3) for \\neach\\n value \\nx\\n of translation, computing the \\nentire sum of products in the right side of the equation. In terms of Fig. 4.27, this', metadata={'source': 'imagepro.pdf', 'page': 255}),\n",
       " Document(page_content='means multiplying the function in Fig. 4.27(a) by the function in Fig. 4.27(d) for \\neach\\n \\nvalue of \\nx\\n. The displacement \\nx\\n ranges over all values required to completely slide \\nh\\n \\nacross \\nf\\n. Figure\\n \\n4.27(e) shows the convolution of these two functions. As you know, \\nconvolution is a function of the displacement variable\\n, \\nx\\n, and the range of \\nx\\n required \\nin this example to completely slide \\nh\\n past \\nf\\n is from 0 to 799.\\nThe function products \\nare elementwise products, \\nas deﬁned in Section 2.6. \\nWe will discuss efﬁcient \\nways for computing the \\nDFT in Section 4.11.\\nDIP4E_GLOBAL_Print_Ready.indb   254\\n6/16/2017   2:05:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 255}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n255\\nIf we use the DFT and the convolution theorem to try to obtain the same result \\nas in the left column of Fig. 4.27, we must take into account the periodicity inher-\\nent in the expression for the DFT. This is equivalent to convolving the two periodic \\nfunctions in Figs. 4.27(f) and (g) (i.e., as Eqs. (4-46) and (4-47) indicate, the func-\\ntions their transforms have implied periodicity). The convolution procedure is the \\nsame as we just discussed, but the two functions now are periodic. Proceeding with \\nthese two functions as in the previous paragraph would yield the result in Fig. 4.27(j), \\nwhich obviously is incorrect. Because we are convolving two periodic functions, the \\nconvolution itself is periodic. The closeness of the periods in Fig. 4.27 is such that \\nf\\n(\\nm\\n)\\nm\\nm\\n3\\n3\\n200 400\\n0\\n0 200\\n400\\nf\\n(\\nm\\n)\\n2\\nm\\nm\\n200\\n400\\n0\\n2\\n0 200 400\\nh\\n(\\nm\\n)\\nh\\n(\\nm\\n)\\nm\\nm\\n200 400\\n0\\n0 200 400\\nh\\n(\\n/H11002\\nm\\n)\\nh\\n(\\n/H11002\\nm\\n)\\nm\\nm\\n200 400\\n0\\n0 200 400\\nx\\nx\\nh', metadata={'source': 'imagepro.pdf', 'page': 256}),\n",
       " Document(page_content='0\\n0 200 400\\nx\\nx\\nh\\n(\\nx\\n \\n/H11002\\n \\nm\\n)\\nh\\n(\\nx\\n \\n/H11002\\n \\nm\\n)\\nx\\nx\\nRange of\\nFourier transform\\ncomputation\\n200 400 600 800\\n0\\n600\\n1200\\n600\\n1200\\n0 200 400\\n() ( )\\ng\\nfx\\n/H22841\\n() ( )\\ng\\nfx\\n/H22841\\nb\\na\\nc\\ne\\nd\\nf\\nh\\ng\\ni\\nj\\nFIGURE 4.27\\nLeft column:  \\nSpatial  \\nconvolution \\ncomputed with \\nEq. (3-35), using \\nthe approach \\ndiscussed in  \\nSection 3.4.  \\nRight column: \\nCircular  \\nconvolution. The \\nsolid line in (j) \\nis the result we \\nwould obtain \\nusing the DFT, \\nor, equivalently, \\nEq. (4-48). This \\nerroneous result \\ncan be remedied \\nby using zero  \\npadding.\\nDIP4E_GLOBAL_Print_Ready.indb   255\\n6/16/2017   2:05:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 256}),\n",
       " Document(page_content='256\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nthey interfere with each other to cause what is commonly referred to as \\nwraparound \\nerror\\n. According to the convolution theorem, if we had computed the DFT of the \\ntwo 400-point functions, \\nf\\n and \\nh\\n, multiplied the two transforms, and then computed \\nthe inverse DFT, we would have obtained the erroneous 400-point segment of the \\nperiodic convolution shown as a solid line in Fig. 4.27(j) (remember the limits of the \\n1-D DFT are \\nu\\n=−\\n012\\n1\\n,,, , ) .\\n…\\nM\\n This is also the result we would obtain if we used \\nEq.\\n (4-48) [the 1-D equivalent of Eq. (4-94)] to compute one period of the circular \\nconvolution.\\nFortunately, the solution to the wraparound error problem is simple. Consider \\ntwo functions, \\nfx\\n()\\n and \\nhx\\n()\\n composed of \\nA\\n and \\nB\\n samples\\n, respectively. It can be \\nshown (Brigham [1988]) that if we append zeros to both functions so that they have \\nthe same length, denoted by \\nP\\n, then wraparound is avoided by choosing', metadata={'source': 'imagepro.pdf', 'page': 257}),\n",
       " Document(page_content='PAB\\n≥+\\n−\\n1  \\n(4-97)\\nIn our example, each function has 400 points, so the minimum value we could use is \\nP\\n=\\n799,\\n which implies that we would append 399 zeros to the trailing edge of each \\nfunction.\\n This procedure is called \\nzero padding\\n, as we discussed in Section 3.4. As \\nan exercise, you should convince yourself that if the periods of the functions in Figs. \\n4.27(f) and (g) were lengthened by appending to each period at least 399 zeros, the \\nresult would be a periodic convolution in which each period is identical to the cor-\\nrect result in Fig. 4.27(e). Using the DFT via the convolution theorem would result \\nin a 799-point spatial function identical to Fig. 4.27(e). The conclusion, then, is that \\nto obtain the same convolution result between the “straight” representation of the \\nconvolution equation approach in Chapter 3, and the DFT approach, functions in \\nthe latter must be padded prior to computing their transforms.', metadata={'source': 'imagepro.pdf', 'page': 257}),\n",
       " Document(page_content='Visualizing a similar example in 2-D is more difﬁcult, but we would arrive at the \\nsame conclusion regarding wraparound error and the need for appending zeros to \\nthe functions. Let \\nfx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n be two image arrays of sizes \\nAB\\n×\\n and \\nCD\\n×\\n \\npixels\\n, respectively. Wraparound error in their circular convolution can be avoided \\nby padding these functions with zeros, as follows:\\n \\nfx y\\nfx\\ny x A\\ny B\\nAxP ByQ\\np\\n(,)\\n(,)\\n=\\n≤≤ − ≤≤−\\n≤≤ ≤≤\\n⎧\\n⎨\\n⎩\\n01\\n01\\n0\\n  and  \\n  or  \\n \\n(4-98)\\nand\\n \\nhx y\\nhx\\ny x C\\ny D\\nCxP Dy Q\\np\\n(,)\\n(,)\\n=\\n≤≤− ≤≤ −\\n≤≤ ≤≤\\n⎧\\n⎨\\n⎩\\n01\\n01\\n0\\n  and  \\n  or  \\n \\n(4-99)\\nwith\\n \\nPA C\\n≥+\\n−\\n1  \\n(4-100)\\nand\\nThe padding zeros could \\nbe appended also at \\nthe beginning of the \\nfunctions, or they could \\nbe divided between the \\nbeginning and end of the \\nfunctions. It is simpler to \\nappend them at the end.\\nWe use zero-padding \\nhere for simplicity. Recall \\nfrom the discussion of \\nFig. 3.39 that replicate \\nand mirror padding \\ngenerally yield better \\nresults.', metadata={'source': 'imagepro.pdf', 'page': 257}),\n",
       " Document(page_content='results.\\nDIP4E_GLOBAL_Print_Ready.indb   256\\n6/16/2017   2:05:30 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 257}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n257\\n \\nQBD\\n≥+\\n−\\n1  \\n(4-101)\\nThe resulting padded images are of size \\nPQ\\n×\\n.\\n If both arrays are of the same size, \\nMN\\n×\\n,\\n then we require that \\nP\\nM\\n≥−\\n21\\n and \\nQN\\n≥−\\n21\\n.\\n As a rule, DFT algorithms \\ntend to execute faster with arrays of even size\\n, so it is good practice to select \\nP\\n and \\nQ\\n as the smallest even integers that satisfy the preceding equations. If the two arrays \\nare of the same size, this means that \\nP\\n and \\nQ\\n are selected as:\\n \\nPM\\n=\\n2  \\n(4-102)\\nand\\n \\nQN\\n=\\n2  \\n(4-103)\\nFigure 4.31 in the next section illustrates the effects of wraparound error on images. \\nT\\nhe two functions in Figs. 4.27(a) and (b) conveniently become zero before the \\nend of the sampling interval. If one or both of the functions were not zero at the end \\nof the interval, then a discontinuity would be created when zeros were appended \\nto the function to eliminate wraparound error. This is analogous to multiplying a', metadata={'source': 'imagepro.pdf', 'page': 258}),\n",
       " Document(page_content='function by a box, which in the frequency domain would imply convolution of the \\noriginal transform with a sinc function (see Example 4.1). This, in turn, would create \\nso-called \\nfrequency leakage\\n, caused by the high-frequency components of the sinc \\nfunction. Leakage produces a blocky effect on images. Although leakage can never \\nbe totally eliminated, it can be reduced signiﬁcantly by multiplying the sampled \\nfunction by another function that tapers smoothly to near zero at both ends of the \\nsampled record. This idea is to dampen the sharp transitions (and thus the high fre-\\nquency components) of the box. This approach, called \\nwindowing\\n or \\napodizing\\n, is an \\nimportant consideration when ﬁdelity in image reconstruction (as in high-deﬁnition \\ngraphics) is desired. \\nSUMMARY OF 2-D DISCRETE FOURIER TRANSFORM PROPERTIES\\nTable 4.3 summarizes the principal DFT definitions introduced in this chapter. We \\nwill discuss the separability property in Section', metadata={'source': 'imagepro.pdf', 'page': 258}),\n",
       " Document(page_content='4.11, where we also show how to \\nobtain the inverse DFT using a forward transform algorithm.\\n Correlation will be \\ndiscussed in detail Chapter 12.\\nTable 4.4 summarizes some important DFT pairs. Although our focus is on dis-\\ncrete functions, the last two entries in the table are Fourier transform pairs that can \\nbe derived only for continuous variables (note the use of continuous variable nota-\\ntion).We include them here because, with proper interpretation, they are quite use-\\nful in digital image processing. The differentiation pair can be used to derive the fre-\\nquency-domain equivalent of the Laplacian deﬁned in Eq. (3-50) (see Problem 4.52). \\nThe Gaussian pair is discussed in Section 4.7. Tables\\n \\n4.1, 4.3 and 4.4 provide a sum-\\nmary of properties useful when working with the DFT\\n. Many of these properties \\nare key elements in the development of the material in the rest of this chapter, and \\nsome are used in subsequent chapters.\\nA simple apodizing \\nfunction is a triangle, cen-', metadata={'source': 'imagepro.pdf', 'page': 258}),\n",
       " Document(page_content='tered on the data record, \\nwhich tapers to 0 at both \\nends of the record. This is \\ncalled a \\nBartlett window\\n. \\nOther common windows \\nare the Gaussian, the \\nHamming\\n and the \\nHann\\n \\nwindows. \\nDIP4E_GLOBAL_Print_Ready.indb   257\\n6/16/2017   2:05:31 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 258}),\n",
       " Document(page_content='258\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nName\\nExpression(s)\\n1) Discrete Fourier \\ntransform (DFT) of\\nfx y\\n(,\\n)\\nFf\\nx\\ny\\ne\\nju x M y N\\ny\\nN\\nx\\nM\\n(,) (,)\\n()\\nuv\\nv\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\np\\n2) Inverse discrete  \\nFourier transform  \\n(IDFT) of \\nF\\n(,\\n)\\nuv\\nfx y\\nMN\\nFe\\nju x M y N\\nN M\\n(,)\\n(,)\\n()\\n=\\n+\\n=\\n−\\n=\\n−\\n∑ ∑\\n1\\n2\\n0\\n1\\n0\\n1\\nuv\\nv\\nv u\\np\\n3) Spectrum\\nFR I R F I F\\n(,) (,) (,)\\n( ) ; ( )\\nuv uv uv\\n=+\\n⎡\\n⎣\\n⎤\\n⎦\\n==\\n22\\n12\\nReal  Imag\\n4) Phase angle\\nf\\n(,) t a n\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n−\\n1\\nI\\nR\\n5) Polar representation\\nFF e\\nj\\n(,) (,)\\n(,)\\nuv uv\\nuv\\n=\\nf\\n6) Power spectrum\\nPF\\n(,) (,)\\nuv uv\\n=\\n2\\n7) Average value\\nf\\nMN\\nfx y\\nMN\\nF\\ny\\nN\\nx\\nM\\n==\\n=\\n−\\n=\\n−\\n∑ ∑\\n11\\n00\\n0\\n1\\n0\\n1\\n(,) (,)\\n8)\\nPeriodicity (\\nk\\n1\\n and  \\nk\\n2\\n are integers)\\nFF k M F k N\\nFk\\nk N\\nfx y fx k M y\\n(,) ( ,) (, )\\n(, )\\n(,) ( ,\\nuv u v uv\\nuv\\n=+ = +\\n=++\\n=+\\n12\\n12\\n1\\n)\\n)( , )\\n(,)\\n=+\\n=+ +\\nfx y k N\\nfx k M y k N\\n2\\n12\\n9) Convolution\\n((\\n,\\n)\\n(\\n,\\n)\\n)( , )\\nff\\nm\\nn\\nh\\nx\\nm\\ny\\nn\\nhx y\\nn\\nN\\nm\\nM\\n/H22841\\n=\\n−−\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n10) Correlation\\n((\\n,\\n)\\n(\\n,\\n)\\n)( , )\\n*\\nff\\nm\\nn\\nh', metadata={'source': 'imagepro.pdf', 'page': 259}),\n",
       " Document(page_content=')\\n)( , )\\n*\\nff\\nm\\nn\\nh\\nx\\nm\\ny\\nn\\nhx y\\nn\\nN\\nm\\nM\\n/H22845\\n=\\n++\\n=\\n−\\n=\\n−\\n∑ ∑\\n0\\n1\\n0\\n1\\n11) Separability\\nThe 2-D DFT can be computed by computing 1-D DFT \\ntransforms along the rows (columns) of the image, followed \\nby 1-D transforms along the columns (rows) of the result. \\nSee Section 4.11.\\n12) Obtaining the IDFT \\nusing a DFT  \\nalgorithm\\nMNf x y F e\\nju x M y N\\nN M\\n** ( )\\n(,) (,)\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\nv u\\n2\\n0\\n1\\n0\\n1\\np\\n \\nThis equation indicates that inputting \\nF\\n*\\n(,)\\nuv\\n into an \\nalgorithm that computes the forward transform (right side \\nof above equation) yields \\nMNf x y\\n*\\n(,) .\\n Taking the complex \\nconjugate and dividing by \\nMN\\n gives the desired inverse\\n. See \\nSection 4.11.\\nTABLE \\n4.3\\nSummary of DFT \\ndeﬁnitions and \\ncorresponding \\nexpressions. \\nDIP4E_GLOBAL_Print_Ready.indb   258\\n6/16/2017   2:05:32 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 259}),\n",
       " Document(page_content='4.6\\n  \\nSome Properties of the 2-D DFT and IDFT\\n    \\n259\\nTABLE \\n4.4\\nSummmary of \\nDFT pairs. The  \\nclosed-form \\nexpressions in 12 \\nand 13 are valid \\nonly for  \\ncontinuous  \\nvariables. They \\ncan be used with \\ndiscrete variables \\nby sampling the  \\ncontinuous  \\nexpressions. \\nName\\nDFT Pairs\\n1) Symmetry \\nproperties\\nSee Table 4.1\\n2) Linearity\\na f xy b f xy a F b F\\n12 12\\n(,) (,) (,) (,)\\n+⇔+\\nuv uv\\n3) Translation \\n(general)\\nfx y e\\nFu u\\nfx x y y F e\\nj uxM yN\\nj\\n(,)\\n( , )\\n(,) ( , )\\n()\\n2\\n00\\n00\\n2\\n0\\np\\np\\n+\\n−\\n⇔−−\\n−− ⇔\\nv\\n0\\nvv\\nuv\\n(\\n()\\nux M y N\\n0\\n+\\nv\\n0\\n4) Translation \\nto center of \\nthe frequency \\nrectangle, \\n(,)\\nMN\\n22\\nfx y F M N\\nfx M N F\\nxy\\n(,) ( ) ( , )\\n(,) ( , ) ( )\\n−⇔\\n−−\\n−− ⇔ −\\n+\\n+\\n12 2\\n22 1\\nuv\\nyu\\nv\\nuv\\n5) Rotation\\nfr F\\nrx y y x\\n(, ) ( , )\\ntan (\\n)t\\na\\nn\\n(\\n)\\nuu v wu\\nuvw\\n+⇔ +\\n=+ = =+ =\\n−−\\n00\\n22 1 22 1\\nuv\\nv u\\n6) Convolution \\ntheorem\\n†\\nfF\\nH\\nfh x y M N F H\\nhx y\\n/H22841\\n/H22841\\n)( , )\\n() (\\n() ( , )\\n() ( , ) ) ( , )\\n⇔\\n⇔\\n[]\\ni\\ni\\nuv\\nuv\\n1\\n7) Correlation \\ntheorem\\n†\\n() ( , ) ( ) ( , )\\n() ( , ) ( ) ( ) ( , )\\n*\\n*', metadata={'source': 'imagepro.pdf', 'page': 260}),\n",
       " Document(page_content='*\\n*\\nfh x y F H\\nfh x y M N FH\\n/H22845\\n/H22845\\n⇔\\n⇔\\n[]\\ni\\ni\\nuv\\nuv\\n1\\n8) Discrete unit \\nimpulse\\nd\\n(,\\n)\\nxy\\n⇔\\n1\\n1\\n⇔\\nMN\\nd\\n(,)\\nuv\\n9) Rectangle\\nrec\\nab a b\\na\\na\\nb\\nb\\ne\\njab\\n,\\nsin( )\\n()\\nsin( )\\n()\\n()\\n[]\\n⇔\\n−+\\np\\np\\np\\np\\np\\nu\\nu\\nv\\nv\\nuv\\n10) Sine\\nsin(\\n) ( , ) ( , )\\n22\\n2\\n0 0\\n00 00\\npp d d\\nu v\\nuu vv uu vv\\nxM yN\\njMN\\n+⇔ + + − − −\\n[]\\n11) Cosine\\ncos(\\n) ( , ) ( , )\\n22\\n1\\n2\\n0 0\\n00 00\\npp d d\\nu v uu vv uu vv\\nxM yN\\n+⇔ + + + − −\\n[]\\nThe following Fourier transform pairs are derivable only for continuous variables, denoted \\nas before by \\nt\\n and \\nz\\n for spatial variables and by \\nm\\n and \\nn\\n for frequency variables. These \\nresults can be used for DFT work by sampling the continuous forms\\n.\\n12) Differentiation \\n(the expressions \\non the right \\nassume that \\nf\\n(,\\n) .\\n±±\\n/H11009/H11009\\n=\\n0\\nab ab\\n∂\\n∂\\n∂\\n∂\\n⇔\\n∂\\n∂\\n⇔\\ntz\\nft z j j F\\nft z\\nt\\nj\\nmn\\nmn\\nm\\nm\\nm\\n(, ) ( ) ( ) ( , )\\n(, )\\n()\\n22\\n2\\npm pn m n\\npm\\nF F\\nft z\\nz\\njF\\nn\\nm\\nn\\n(,) ;\\n(, )\\n() ( , )\\nmn p n mn\\n∂\\n∂\\n⇔\\n2\\n13) Gaussian\\nAe A e A\\ntz\\n2\\n22\\n2\\n22 2 2\\n2 2 2\\nps\\nps\\nm n s\\n−+ − +\\n⇔\\n() ( )', metadata={'source': 'imagepro.pdf', 'page': 260}),\n",
       " Document(page_content='−+ − +\\n⇔\\n() ( )\\n(  is a constant)\\n†\\n Assumes that \\nfx y\\n(,\\n)\\n and \\nhxy\\n(,\\n)\\n have been properly padded. Convolution is associative, commutative, and \\ndistributive\\n. Correlation is distributive (see Table 3.5). The products are elementwise products (see Section 2.6).\\nDIP4E_GLOBAL_Print_Ready.indb   259\\n6/16/2017   2:05:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 260}),\n",
       " Document(page_content='260\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.7 THE BASICS OF FILTERING IN THE FREQUENCY DOMAIN  \\nIn this section, we lay the groundwork for all the filtering techniques discussed in the \\nremainder of the chapter. \\nADDITIONAL CHARACTERISTICS OF THE FREQUENCY DOMAIN\\nWe begin by observing in Eq. (4-67) that \\neach\\n term of \\nF\\n(,\\n)\\nuv\\n contains \\nall\\n values of \\nfx y\\n(,\\n) ,\\n modified by the values of the exponential terms. Thus, with the exception \\nof trivial cases\\n, it is usually impossible to make direct associations between specific \\ncomponents of an image and its transform. However, some general statements can \\nbe made about the relationship between the frequency components of the Fourier \\ntransform and spatial features of an image. For instance, because frequency is direct-\\nly related to spatial rates of change, it is not difficult intuitively to associate frequen-\\ncies in the Fourier transform with patterns of intensity variations in an image. We', metadata={'source': 'imagepro.pdf', 'page': 261}),\n",
       " Document(page_content='showed in Section 4.6 that the slowest varying frequency component \\n()\\nuv\\n==\\n0  \\nis proportional to the average intensity of an image\\n. As we move away from the \\norigin of the transform, the low frequencies correspond to the slowly varying inten-\\nsity components of an image. In an image of a room, for example, these might cor-\\nrespond to smooth intensity variations on the walls and floor. As we move further \\naway from the origin, the higher frequencies begin to correspond to faster and faster \\nintensity changes in the image. These are the edges of objects and other components \\nof an image characterized by abrupt changes in intensity.\\nFiltering techniques in the frequency domain are based on modifying the Fourier \\ntransform to achieve a speciﬁc objective, and then computing the inverse DFT to get \\nus back to the spatial domain, as introduced in Section 2.6. It follows from Eq. (4-87) \\nthat the two components of the transform to which we have access are the transform', metadata={'source': 'imagepro.pdf', 'page': 261}),\n",
       " Document(page_content='magnitude (spectrum) and the phase angle. We learned in Section 4.6 that visual \\nanalysis of the phase component generally is not very useful. The spectrum, however, \\nprovides some useful guidelines as to the gross intensity characteristics of the image \\nfrom which the spectrum was generated. For example, consider Fig. 4.28(a), which \\nis a scanning electron microscope image of an integrated circuit, magniﬁed approxi-\\nmately 2500 times. \\nAside from the interesting construction of the device itself, we note two principal \\nfeatures in this image: strong edges that run approximately at \\n±° ,\\n45\\n and two white, \\noxide protrusions resulting from thermally induced failure\\n. The Fourier spectrum \\nin Fig. 4.28(b) shows prominent components along the \\n±°\\n45\\n directions that corre-\\nspond to the edges just mentioned.\\n Looking carefully along the vertical axis in Fig. \\n4.28(b), we see a vertical component of the transform that is off-axis, slightly to the', metadata={'source': 'imagepro.pdf', 'page': 261}),\n",
       " Document(page_content='left. This component was caused by the edges of the oxide protrusions. Note how the \\nangle of the frequency component with respect to the vertical axis corresponds to \\nthe inclination (with respect to the horizontal axis of the image) of the long white \\nelement. Note also the zeros in the vertical frequency component, corresponding to \\nthe narrow vertical span of the oxide protrusions.\\nThese are typical of the types of associations we can make in general between \\nthe frequency and spatial domains. As we will show later in this chapter, even these \\ntypes of gross associations, coupled with the relationships mentioned previously \\n4.7\\nDIP4E_GLOBAL_Print_Ready.indb   260\\n6/16/2017   2:05:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 261}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n261\\nbetween frequency content and rate of change of intensity levels in an image, can \\nlead to some very useful results. We will show in Section 4.8 the effects of modifying \\nvarious frequency ranges in the transform of Fig. 4.28(a). \\nFREQUENCY DOMAIN FILTERING FUNDAMENTALS\\nFiltering in the frequency domain consists of modifying the Fourier transform of an \\nimage, then computing the inverse transform to obtain the spatial domain represen-\\ntation of the processed result. Thus, given (a padded) digital image, \\nfx y\\n(,\\n) ,\\n of size \\nPQ\\n×\\n pixels, the basic filtering equation in which we are interested has the form:\\n \\ngxy\\nH F\\n(,\\n)\\n(,)(,)\\n=\\n[]\\n{}\\n−\\nReal\\n/H5219\\n1\\nuv uv\\n \\n(4-104)\\nwhere \\n/H5219\\n−\\n1\\n is the IDFT, \\nF\\n(,\\n)\\nuv\\n is the DFT of the input image, \\nfx y\\n(,\\n) ,\\n \\nH\\n(,\\n)\\nuv\\n is a \\nfilter transfer function \\n(which we often call just a \\nfilter\\n or \\nfilter function\\n),\\n and \\ngxy\\n(,\\n)\\n \\nis the \\nfiltered\\n (\\noutput\\n) \\nimage\\n.', metadata={'source': 'imagepro.pdf', 'page': 262}),\n",
       " Document(page_content='output\\n) \\nimage\\n.\\n Functions \\nF\\n, \\nH\\n, and \\ng\\n are arrays of size \\nPQ\\n×\\n,\\n the same \\nas the padded input image\\n. The product \\nHF\\n(,\\n) (,)\\nuv uv\\n is formed using elementwise \\nmultiplication,\\n as defined in Section 2.6. The filter transfer function modifies the \\ntransform of the input image to yield the processed output, \\ngxy\\n(,\\n) .\\n The task of speci-\\nfying \\nH\\n(,\\n)\\nuv\\n is simplified considerably by using functions that are symmetric about \\ntheir center\\n, which requires that \\nF\\n(,\\n)\\nuv\\n be centered also. As explained in Section 4.6, \\nthis is accomplished by multiplying the input image by \\n()\\n−\\n+\\n1\\nxy\\n prior to computing \\nits transform.\\n†\\n†\\n Some software implementations of the 2-D DFT (e.g., MATLAB) do not center the transform. This implies \\nthat ﬁlter functions must be arranged to correspond to the same data format as the uncentered transform (i.e., \\nwith the origin at the top left). The net result is that ﬁlter transfer functions are more difﬁcult to generate and', metadata={'source': 'imagepro.pdf', 'page': 262}),\n",
       " Document(page_content='display. We use centering in our discussions to aid in visualization, which is crucial in developing a clear under-\\nstanding of ﬁltering concepts. Either method can be used in practice, provided that consistency is maintained. \\nIf \\nH\\n is real and  \\nsymmetric and \\nf\\n is real \\n(as is typically the case), \\nthen the IDFT in Eq. \\n(4-104) should yield \\nreal quantities in theory. \\nIn practice, the inverse \\noften contains para-\\nsitic complex terms from \\nroundoff error and other \\ncomputational inaccura-\\ncies. Thus, it is customary \\nto take the real part of \\nthe IDFT to form \\ng\\n.\\nb a\\nFIGURE 4.28\\n (a) SEM image of a damaged integrated circuit. (b) Fourier spectrum of (a).  \\n(Original image courtesy of Dr. J. M. Hudak, Brockhouse Institute for Materials Research, \\nMcMaster University, Hamilton, Ontario, Canada.) \\nDIP4E_GLOBAL_Print_Ready.indb   261\\n6/16/2017   2:05:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 262}),\n",
       " Document(page_content='262\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nWe are now in a position to consider ﬁltering in  detail. One of the simplest ﬁlter \\ntransfer functions we can construct is a function \\nH\\n(,\\n)\\nuv\\n that is 0 at the center of \\nthe (centered) transform,\\n and 1’s elsewhere. This ﬁlter would reject the dc term and \\n“pass” (i.e., leave unchanged) all other terms of \\nF\\n(,\\n)\\nuv\\n when we form the product \\nHF\\n(,\\n) (,) .\\nuv uv\\n We know from property 7 in Table 4.3 that the dc term is responsible \\nfor the average intensity of an image\\n, so setting it to zero will reduce the average \\nintensity of the output image to zero. Figure 4.29 shows the result of this operation \\nusing Eq. (4-104). As expected, the image became much darker. An average of zero \\nimplies the existence of negative intensities. Therefore, although it illustrates the \\nprinciple, Fig. 4.29 is not a true representation of the original, as all negative intensi-\\nties were clipped (set to 0) by the display.', metadata={'source': 'imagepro.pdf', 'page': 263}),\n",
       " Document(page_content='As noted earlier, low frequencies in the transform are related to slowly varying \\nintensity components in an image, such as the walls of a room or a cloudless sky in \\nan outdoor scene. On the other hand, high frequencies are caused by sharp transi-\\ntions in intensity, such as edges and noise. Therefore, we would expect that a func-\\ntion \\nH\\n(,\\n)\\nuv\\n that attenuates high frequencies while passing low frequencies (called a \\nlo\\nwpass ﬁlter\\n, as noted before) would blur an image, while a ﬁlter with the opposite \\nproperty (called a \\nhighpass ﬁlter\\n) would enhance sharp detail, but cause a reduction \\nin contrast in the image. Figure 4.30 illustrates these effects. For example, the ﬁrst \\ncolumn of this ﬁgure shows a lowpass ﬁlter transfer function and the corresponding \\nﬁltered image. The second column shows similar results for a highpass ﬁlter. Note \\nthe similarity between Figs. 4.30(e) and Fig. 4.29. The reason is that the highpass', metadata={'source': 'imagepro.pdf', 'page': 263}),\n",
       " Document(page_content='ﬁlter function shown eliminates the dc term, resulting in the same basic effect that \\nled to Fig. 4.29. As illustrated in the third column, adding a small constant to the \\nﬁlter does not affect sharpening appreciably, but it does prevent elimination of the \\ndc term and thus preserves tonality.\\nEquation (4-104) involves the product of two functions in the frequency domain \\nwhich, by the convolution theorem, implies convolution in the spatial domain. We \\nknow from the discussion in Section 4.6 that we can expect wraparound error if \\nthe functions in question are not padded. Figure 4.31 shows what happens when \\nFIGURE 4.29\\nResult of ﬁlter-\\ning the image in \\nFig. 4.28(a) with \\na ﬁlter transfer \\nfunction that sets \\nto 0 the dc term, \\nFP Q\\n(, ) ,\\n22\\n \\nin the centered \\nF\\nourier transform, \\nwhile leaving all \\nother transform \\nterms unchanged.\\nDIP4E_GLOBAL_Print_Ready.indb   262\\n6/16/2017   2:05:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 263}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n263\\nv\\nv\\nu\\nu\\na\\nH\\n(\\nu\\n, \\nv\\n)\\nH\\n(\\nu\\n, \\nv\\n)\\nM\\n/\\n2\\nM\\n/\\n2\\nN\\n/\\n2\\nN\\n/\\n2\\nH\\n(\\nu\\n, \\nv\\n)\\nv\\nu\\nM\\n/\\n2\\nN\\n/\\n2\\nb a\\nc\\ne d\\nf\\nFIGURE 4.30\\n Top row: Frequency domain ﬁlter transfer functions of (a) a lowpass ﬁlter, (b) a highpass ﬁlter, and (c) \\nan offset highpass ﬁlter. Bottom row: Corresponding ﬁltered images obtained using Eq. (4-104). The offset in (c) is \\na\\n=\\n08\\n5\\n.,\\n and the height of \\nH\\n(,\\n)\\nuv\\n is 1. Compare (f) with Fig. 4.28(a). \\nb a\\nc\\nFIGURE 4.31\\n (a) A simple image. (b) Result of blurring with a Gaussian lowpass ﬁlter without padding. (c) Result of \\nlowpass ﬁltering with zero padding. Compare the vertical edges in (b) and (c). \\nDIP4E_GLOBAL_Print_Ready.indb   263\\n6/16/2017   2:05:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 264}),\n",
       " Document(page_content='264\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nFIGURE 4.32\\n (a) Image periodicity without image padding. (b) Periodicity after padding with 0’s (black). The dashed \\nareas in the center correspond to the image in Fig. 4.31(a). Periodicity is inherent when using the DFT. (The thin \\nwhite lines in both images are superimposed for clarity; they are not part of the data.) \\nwe apply Eq. (4-104) without padding. Figure 4.31(a) shows a simple image, and \\nFig. 4.31(b) is the result of lowpass ﬁltering the image with a Gaussian lowpass ﬁlter \\nof the form shown in Fig. 4.30(a). As expected, the image is blurred. However, the \\nblurring is not uniform; the top white edge is blurred, but the sides are not. Pad-\\nding the input image with zeros according to Eqs. (4-98) and (4-99) before applying \\nEq. (4-104) resulted in the ﬁltered image in Fig. 4.31(c). This result is as expected, \\nwith a uniform dark border resulting from zero padding (see Fig. 3.33 for an expla-', metadata={'source': 'imagepro.pdf', 'page': 265}),\n",
       " Document(page_content='nation of this effect).\\nFigure 4.32 illustrates the reason for the discrepancy between Figs. 4.31(b) and (c). \\nThe dashed area in Fig. 4.32(a) corresponds to the image in Fig. 4.31(a). The other \\ncopies of the image are due to the implied periodicity of the image (and its trans-\\nform) implicit when we use the DFT, as explained in Section 4.6. Imagine convolving \\nthe spatial representation of the blurring ﬁlter (i.e., the corresponding spatial ker-\\nnel) with this image. When the kernel is centered on the top of the dashed image, it \\nwill encompass part of the image and also part of the bottom of the periodic image \\nimmediately above it. When a dark and a light region reside under the ﬁlter, the \\nresult is a mid-gray, blurred output. However, when the kernel is centered on the top \\nright side of the image, it will encompass only light areas in the image and its right \\nregion. Because the average of a constant value is that same value, ﬁltering will have', metadata={'source': 'imagepro.pdf', 'page': 265}),\n",
       " Document(page_content='no effect in this area, giving the result in Fig. 4.31(b). Padding the image with 0’s cre-\\nates a uniform border around each image of the periodic sequence, as Fig. 4.32(b) \\nshows. Convolving the blurring kernel with the padded “mosaic” of Fig. 4.32(b) gives \\nthe correct result in Fig. 4.31(c). You can see from this example that failure to pad an \\nimage prior to ﬁltering can lead to unexpected results. \\nThus far, the discussion has centered on padding the input image. However, \\nEq. (4-104) also involves a ﬁlter transfer function that can be speciﬁed either in the \\nDIP4E_GLOBAL_Print_Ready.indb   264\\n6/16/2017   2:05:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 265}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n265\\nspatial or in the frequency domain. But padding is done in the spatial domain, which \\nraises an important question about the relationship between \\nspatial\\n padding and \\nﬁlter functions speciﬁed directly in the frequency domain.\\nIt would be reasonable to conclude that the way to handle padding of a frequency \\ndomain transfer function is to construct the function the same size as the unpad-\\nded image, compute the IDFT of the function to obtain the corresponding spatial \\nrepresentation, pad that representation in the spatial domain, and then compute its \\nDFT to return to the frequency domain. The 1-D example in Fig. 4.33 illustrates the \\npitfalls in this approach. \\nFigure 4.33(a) shows a 1-D ideal lowpass ﬁlter transfer function in the frequency \\ndomain. The function is real and has even symmetry, so we know from property 8 \\nin Table 4.1 that its IDFT will be real and symmetric also. Figure 4.33(b) shows the', metadata={'source': 'imagepro.pdf', 'page': 266}),\n",
       " Document(page_content='result of multiplying the elements of the transfer function by \\n()\\n−\\n1\\nu\\n and computing \\nits IDFT to obtain the corresponding spatial ﬁlter kernel. The result is shown in \\nFig. 4.33(b). It is evident in this ﬁgure that the extremes of this spatial function are \\nnot zero. Zero-padding the function would create two discontinuities, as Fig. 4.33(c) \\nshows. To return to the frequency domain, we compute the forward DFT of the \\nspatial, padded function. As Fig. 4.33(d) shows, the discontinuities in the padded \\nfunction caused ringing in its frequency domain counterpart. \\nPadding the two ends of \\na function is the same \\nas padding one end, \\nprovided that the total \\nnumber of zeros is the \\nsame.\\n0.01\\n0.02\\n0.03\\n0.04\\n0 128 256 384 511\\n0 128 255\\n0\\n1.2\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n/H11002\\n0.2\\n/H11002\\n0.01\\n0 128 255\\n0 128 256 384 511\\n1.2\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n/H11002\\n0.2\\n0.01\\n0.02\\n0.03\\n0.04\\n0\\n/H11002\\n0.01\\nb\\na\\nd\\nc\\nFIGURE 4.33\\n(a) Filter transfer \\nfunction speciﬁed in \\nthe (centered)  \\nfrequency domain.', metadata={'source': 'imagepro.pdf', 'page': 266}),\n",
       " Document(page_content='frequency domain. \\n(b) Spatial  \\nrepresentation (ﬁlter \\nkernel) obtained by  \\ncomputing the IDFT \\nof (a).  \\n(c) Result of  \\npadding (b) to twice \\nits length (note the \\ndiscontinuities).  \\n(d) Corresponding \\nﬁlter in the frequen-\\ncy domain obtained \\nby computing the \\nDFT of (c). Note the \\nringing caused by \\nthe discontinuities \\nin (c). Part (b) of the \\nﬁgure is below (a), \\nand (d) is below (c).\\nDIP4E_GLOBAL_Print_Ready.indb   265\\n6/16/2017   2:05:46 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 266}),\n",
       " Document(page_content='266\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe preceding results tell us that we cannot pad the spatial representation of a \\nfrequency domain transfer function in order to avoid wraparound error. Our objec-\\ntive is to work with speciﬁed ﬁlter shapes in the frequency domain without having to \\nbe concerned with truncation issues. An alternative is to pad images and then create \\nthe desired ﬁlter transfer function directly in the frequency domain, this function \\nbeing of the same size as the padded images (remember, images and ﬁlter transfer \\nfunctions must be of the same size when using the DFT). Of course, this will result \\nin wraparound error because no padding is used for the ﬁlter transfer function, but \\nthis error is mitigated signiﬁcantly by the separation provided by padding the image, \\nand it is preferable to ringing. Smooth transfer functions (such as those in Fig. 4.30) \\npresent even less of a problem. Speciﬁcally, then, the approach we will follow in this', metadata={'source': 'imagepro.pdf', 'page': 267}),\n",
       " Document(page_content='chapter is to pad images to size \\nPQ\\n×\\n and construct ﬁlter transfer functions of the \\nsame dimensions directly in the frequenc\\ny domain. As explained earlier, \\nP\\n and \\nQ\\n \\nare given by Eqs. (4-100) and (4-101).\\nWe conclude this section by analyzing the phase angle of ﬁltered images. We can \\nexpress the DFT in terms of its real and imaginary parts: \\nFRj I\\n(,\\n) (,) (,) .\\nuv uv uv\\n=+\\n \\nEquation (4-104) then becomes\\n \\ngxy H R j H I\\n(\\n,) (,)(,) (,) (,)\\n=+\\n[]\\n−\\n/H5219\\n1\\nuv uv uv uv\\n \\n(4-105)\\nThe phase angle is computed as the arctangent of the ratio of the imaginary and the \\nreal parts of a complex number [see Eq.\\n (4-88)]. Because \\nH\\n(,\\n)\\nuv\\n multiplies both \\nR\\n and \\nI\\n,\\n it will cancel out when this ratio is formed. Filters that affect the real and \\nimaginary parts equally, and thus have no effect on the phase angle, are appropri-\\nately called \\nzero-phase-shift\\n filters. These are the only types of filters considered in \\nthis chapter.', metadata={'source': 'imagepro.pdf', 'page': 267}),\n",
       " Document(page_content='this chapter. \\nThe importance of the phase angle in determining the spatial structure of an \\nimage was vividly illustrated in Fig. 4.26. Thus, it should be no surprise that even \\nsmall changes in the phase angle can have dramatic (and usually undesirable) effects \\non the ﬁltered output. Figures 4.34(b) and (c) illustrate the effect of changing the \\nphase angle array of the DFT of Fig. 4.34(a) (the \\nF\\n(,)\\nuv\\n term was not changed in \\neither case).\\n Figure 4.34(b) was obtained by multiplying the phase angle, \\nf\\n(,\\n) ,\\nuv\\n in \\nEq.\\n (4-86) by \\n−\\n1\\n and computing the IDFT.  The net result is a reﬂection of every pixel \\nin the image about both coordinate axes\\n. Figure 4.34(c) was obtained by multiply-\\ning the phase term by 0.25 and computing the IDFT. Even a scale change rendered \\nthe image almost unrecognizable. These two results illustrate the advantage of using \\nfrequency-domain ﬁlters that do not alter the phase angle.\\nSUMMARY OF STEPS FOR FILTERING IN THE FREQUENCY DOMAIN', metadata={'source': 'imagepro.pdf', 'page': 267}),\n",
       " Document(page_content='The process of filtering in the frequency domain can be summarized as follows:\\n1. \\nGiven an input image \\nfx y\\n(,\\n)\\n of size \\nMN\\n×\\n,\\n obtain the padding sizes \\nP\\n and \\nQ\\n \\nusing Eqs\\n. (4-102) and (4-103); that is, \\nPM\\n=\\n2  and \\nQN\\n=\\n2.\\nDIP4E_GLOBAL_Print_Ready.indb   266\\n6/16/2017   2:05:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 267}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n267\\n2. \\nForm a padded\\n†\\n image \\nfx y\\np\\n(,)\\n of size \\nPQ\\n×\\n using zero-, mirror-, or replicate \\npadding (see F\\nig. 3.39\\n \\nfor a comparison of padding methods).\\n3. \\nMultiply \\nfx y\\np\\n(,)\\n by \\n()\\n−\\n1\\nxy\\n+\\n to center the Fourier transform on the \\nPQ\\n×\\n fre-\\nquenc\\ny rectangle.\\n4. \\nCompute the DFT, \\nF\\n(,\\n) ,\\nuv\\n of the image from Step 3.\\n5. \\nConstruct a real, symmetric ﬁlter transfer function, \\nH\\n(,\\n) ,\\nuv\\n of size \\nPQ\\n×\\n with \\ncenter at \\n(, ) .\\nPQ\\n22\\n6. \\nForm the product \\nGH F\\n(,\\n) (,) (,)\\nuv uv uv\\n=\\n using elementwise multiplication; that \\ni\\ns, \\nGik HikFik\\n(,\\n) (, ) (, )\\n=\\n for \\niM\\n=−\\n012\\n1\\n,,, ,\\n…\\n and \\nk\\n=−\\n012\\n1\\n,,, , .\\n…\\nN\\n7. \\nObtain the ﬁltered image (of size \\nP\\nQ\\n×\\n) by computing the IDFT of \\nG\\n(,\\n) :\\nuv\\n \\ngx y\\nG\\np\\nxy\\n(,)\\n(,) ( )\\n=\\n{}\\n⎡\\n⎣\\n⎤\\n⎦\\n−\\n−+\\nQR\\nreal\\n/H5219\\n1\\n1\\nuv\\n8. \\nObtain the ﬁnal ﬁltered result, \\ngxy\\n(,\\n) ,\\n of the same size as the input image, by \\nextracting the \\nMN\\n×\\n region from the top, left quadrant of \\ngx y\\np', metadata={'source': 'imagepro.pdf', 'page': 268}),\n",
       " Document(page_content='gx y\\np\\n(,) .\\n \\nW\\ne will discuss the construction of filter transfer functions (Step 5) in the following \\nsections of this chapter. In theory, the IDFT in Step 7 should be real because \\nfx y\\n(,\\n)\\n \\nis real and \\nH\\n(,\\n)\\nuv\\n is real and symmetric. However, parasitic complex terms in the \\nIDFT resulting from computational inaccuracies are not uncommon.\\n Taking the real \\npart of the result takes care of that. Multiplication by \\n()\\n−\\n+\\n1\\nxy\\n cancels out the multi-\\nplication by this factor in Step 3.\\n†\\n  Sometimes we omit padding when doing “quick” experiments to get an idea of ﬁlter performance, or when \\ntrying to determine quantitative relationships between spatial features and their effect on frequency domain \\ncomponents, particularly in band and notch ﬁltering, as explained later in Section 4.10 and in Chapter 5. \\nSee Section 2.6 for a \\ndeﬁnition of elementwise \\noperations.\\nb a\\nc\\nFIGURE 4.34\\n (a) Original image. (b) Image obtained by multiplying the phase angle array by \\n−\\n1', metadata={'source': 'imagepro.pdf', 'page': 268}),\n",
       " Document(page_content='−\\n1\\n in Eq. (4-86) and \\ncomputing the IDFT\\n. (c) Result of multiplying the phase angle by 0.25 and computing the IDFT. The magnitude of \\nthe transform, \\nF\\n(,) ,\\nuv\\n used in (b) and (c) was the same.\\nDIP4E_GLOBAL_Print_Ready.indb   267\\n6/16/2017   2:05:50 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 268}),\n",
       " Document(page_content='268\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nFigure 4.35 illustrates the preceding steps using zero padding. The ﬁgure legend \\nexplains the source of each image. If enlarged, Fig. 4.35(c) would show black dots \\ninterleaved in the image because negative intensities, resulting from the multiplica-\\ntion of \\nf\\np\\n by \\n() ,\\n−\\n+\\n1\\nxy\\n are clipped at 0 by the display. Note in Fig. 4.35(h) the charac-\\nteristic dark border of  by lowpass ﬁltered images obtained using zero padding.\\nCORRESPONDENCE BETWEEN FILTERING IN THE SPATIAL AND  \\nFREQUENCY DOMAINS \\nAs mentioned several times before, the link between filtering in the spatial and fre-\\nquency domains is the convolution theorem. Earlier in this section, we defined fil-\\ntering in the frequency domain as the elementwise product of a filter transfer func-\\ntion, \\nH\\n(,\\n) ,\\nuv\\n and \\nF\\n(,\\n) ,\\nuv\\n the Fourier transform of the input image. Given \\nH\\n(,\\n) ,\\nuv\\n \\nsuppose that we want to find its equivalent kernel in the spatial domain.', metadata={'source': 'imagepro.pdf', 'page': 269}),\n",
       " Document(page_content='If we let \\nfx y x y\\n(,\\n) (,) ,\\n=\\nd\\n it follows from Table 4.4 that \\nF\\n(,\\n) .\\nuv\\n=\\n1\\n Then, from Eq. (4-104), \\nthe filtered output is \\n/H5219\\n−\\n{}\\n1\\nH\\n(,).\\nuv\\n This expression as the inverse transform of the \\nfrequenc\\ny domain filter transfer function, which is the corresponding kernel in the \\nSee Section 2.6 for a \\ndeﬁnition of elementwise \\noperations.\\nb a\\nc\\ne d\\nf\\nh\\ng\\nFIGURE 4.35\\n(a) An \\nMN\\n×\\n \\nimage\\n, \\nf\\n.  \\n(b) Padded image\\n, \\nf\\np\\n of size \\nPQ\\n×\\n. \\n(c) Result of \\nmultiplying \\nf\\np\\n by \\n() .\\n−\\n+\\n1\\nxy\\n \\n(d) Spectrum of \\nF\\n. (e) Centered \\nGaussian lowpass \\nﬁlter transfer \\nfunction,\\n \\nH\\n, of size \\nPQ\\n×\\n.  \\n(f) Spectrum of \\nthe product \\nHF\\n. \\n(g) Image \\ng\\np\\n, the \\nreal part of the \\nIDFT of \\nHF\\n,\\n mul-\\ntiplied by \\n() .\\n−\\n+\\n1\\nxy\\n  \\n(h) Final result, \\ng\\n, obtained by \\nextracting the ﬁrst \\nM\\n rows and \\nN\\n \\ncolumns of \\ng\\np\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   268\\n6/16/2017   2:05:53 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 269}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n269\\nspatial domain. Conversely, it follows from a similar analysis and the convolution \\ntheorem that, given a spatial filter kernel, we obtain its frequency domain repre-\\nsentation by taking the forward Fourier transform of the kernel. Therefore, the two \\nfilters form a Fourier transform pair:\\n \\nhxy H\\n(,\\n) (,)\\n⇔\\nuv\\n \\n(4-106)\\nwhere \\nhxy\\n(,\\n)\\n is the spatial kernel. Because this kernel can be obtained from the \\nresponse of a frequenc\\ny domain filter to an impulse, \\nhxy\\n(,\\n)\\n sometimes is referred to \\nas the \\nimpulse response\\n of \\nH\\n(,\\n) .\\nuv\\n Also, because all quantities in a discrete imple-\\nmentation of Eq.\\n (4-106) are finite, such filters are called \\nfinite impulse response\\n \\n(FIR) filters. These are the only types of linear spatial filters considered in this book.\\nWe discussed spatial convolution in Section 3.4,\\n \\nand its implementation in \\nEq.\\n (3-35), which involved convolving functions of different sizes. When we use the', metadata={'source': 'imagepro.pdf', 'page': 270}),\n",
       " Document(page_content='DFT to compute the transforms used in the convolution theorem, it is implied that \\nwe are convolving periodic functions of the \\nsame\\n size, as explained in Fig. 4.27. For \\nthis reason, as explained earlier, Eq. (4-94) is referred to as \\ncircular convolution\\n.\\nWhen computational speed, cost, and size are important parameters, spatial con-\\nvolution ﬁltering using Eq. (3-35) is well suited for small kernels using hardware \\nand/or ﬁrmware, as explained in Section 4.1. However, when working with general-\\npurpose machines, frequency-domain methods in which the DFT is computed using \\na fast Fourier transform (FFT) algorithm can be hundreds of times faster than using \\nspatial convolution, depending on the size of the kernels used, as you saw in Fig. 4.2. \\nWe will discuss the FFT and its computational advantages in Section 4.11.\\nFiltering concepts are more intuitive in the frequency domain, and ﬁlter design \\noften is easier there. One way to take advantage of the properties of both domains', metadata={'source': 'imagepro.pdf', 'page': 270}),\n",
       " Document(page_content='is to specify a ﬁlter in the frequency domain, compute its IDFT, and then use the \\nproperties of the resulting, full-size spatial kernel as a guide for constructing smaller \\nkernels. This is illustrated next (keep in mind that the Fourier transform and its \\ninverse are linear processes (see Problem 4.24), so the discussion is limited to linear \\nﬁltering). In Example 4.15, we illustrate the converse, in which a spatial kernel is \\ngiven, and we obtain its full-size frequency domain representation. This approach is \\nuseful for analyzing the behavior of small spatial kernels in the frequency domain. \\nFrequency domain ﬁlters can be used as guides for specifying the coefﬁcients of \\nsome of the small kernels we discussed in Chapter 3. Filters based on Gaussian func-\\ntions are of particular interest because, as noted in Table 4.4, both the forward and \\ninverse Fourier transforms of a Gaussian function are real Gaussian functions. We', metadata={'source': 'imagepro.pdf', 'page': 270}),\n",
       " Document(page_content='limit the discussion to 1-D to illustrate the underlying principles. Two-dimensional \\nGaussian transfer functions are discussed later in this chapter.\\nLet \\nHu\\n()\\n denote the 1-D frequency domain Gaussian transfer function\\n \\nHu A e\\nu\\n()\\n=\\n−\\n22\\n2\\ns\\n \\n(4-107)\\nwhere \\ns\\n is the standard deviation of the Gaussian curve. The kernel in the spatial \\ndomain is obtained by taking the inverse DFT of \\nHu\\n()\\n (see Problem\\n \\n4.48):\\n \\nhx A e\\nx\\n()\\n=\\n−\\n2\\n2\\n22 2\\nps\\nps\\n \\n(4-108)\\nAs mentioned in Table \\n4.4, the forward and \\ninverse Fourier trans-\\nforms of Gaussians are \\nvalid only for continuous \\nvariables. To use discrete \\nformulations, we sample \\nthe continuous forms.\\nDIP4E_GLOBAL_Print_Ready.indb   269\\n6/16/2017   2:05:54 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 270}),\n",
       " Document(page_content='270\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThese two equations are important for two reasons: (1) They are a Fourier trans-\\nform pair, both components of which are Gaussian \\nand\\n real. This facilitates analysis \\nbecause we do not have to be concerned with complex numbers. In addition, Gauss-\\nian curves are intuitive and easy to manipulate. (2) The functions behave recipro-\\ncally. When \\nHu\\n()\\n has a broad profile (large value of \\ns\\n),\\n \\nhx\\n()\\n has a narrow profile, \\nand vice versa.\\n In fact, as \\ns\\n approaches infinity, \\nHu\\n()\\n tends toward a constant func-\\ntion and \\nhx\\n()\\n tends toward an impulse, which implies no filtering in either domain.\\nF\\nigures 4.36(a) and (b) show plots of a Gaussian lowpass ﬁlter transfer function \\nin the frequency domain and the corresponding function in the spatial domain. Sup-\\npose that we want to use the shape of \\nhx\\n()\\n in Fig. 4.36(b) as a guide for specifying \\nthe coefﬁcients of a small kernel in the spatial domain.', metadata={'source': 'imagepro.pdf', 'page': 271}),\n",
       " Document(page_content='The key characteristic of the \\nfunction in Fig. 4.36(b) is that all its values are positive. Thus, we conclude that we \\ncan implement lowpass ﬁltering in the spatial domain by using a kernel with all posi-\\ntive coefﬁcients (as we did in Section 3.5). For reference, Fig. 4.36(b) also shows two \\nof the kernels discussed in that section. Note the reciprocal relationship between \\nthe width of the Gaussian functions, as discussed in the previous paragraph. The nar-\\nrower the frequency domain function, the more it will attenuate the low frequencies, \\nresulting in increased blurring. In the spatial domain, this means that a larger kernel \\nmust be used to increase blurring, as we illustrated in Example 3.11.\\nAs you know from Section 3.7, we can construct a highpass ﬁlter from a lowpass \\nﬁlter by subtracting a lowpass function from a constant. We working with Gauss-\\nian functions, we can gain a little more control over ﬁlter function shape by using \\na so-called \\ndifference of Gaussians', metadata={'source': 'imagepro.pdf', 'page': 271}),\n",
       " Document(page_content=', which involves two lowpass functions. In the \\nfrequency domain, this becomes\\n \\nHu A e B e\\nuu\\n()\\n//\\n=−\\n−−\\n2\\n1\\n22\\n2\\n2\\n22\\nss\\n \\n(4-109)\\nwith \\nAB\\n≥\\n and \\nss\\n12\\n>\\n. The corresponding function in the spatial domain is\\nH\\n(\\nu\\n)\\nuu\\nx\\nx\\nH\\n(\\nu\\n)\\nh\\n(\\nx\\n)\\n1\\n16\\n––\\n/H11003\\nh\\n(\\nx\\n)\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n8\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n/H11002\\n1\\n0\\n/H11002\\n1\\n0\\n/H11002\\n1\\n4\\n/H11002\\n1\\n0\\n/H11002\\n1\\n0\\n1\\n2\\n1\\n2\\n1\\n9\\n––\\n/H11003\\n4\\n2\\n1\\n2\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\nb\\na\\nd\\nc\\nFIGURE 4.36\\n(a) A 1-D Gaussian \\nlowpass transfer \\nfunction in the \\nfrequency domain. \\n(b) Corresponding \\nkernel in the spatial \\ndomain. (c) Gauss-\\nian highpass trans-\\nfer function in the \\nfrequency domain. \\n(d) Corresponding \\nkernel. The small \\n2-D kernels shown \\nare kernels we used \\nin Chapter 3. \\nDIP4E_GLOBAL_Print_Ready.indb   270\\n6/16/2017   2:05:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 271}),\n",
       " Document(page_content='4.7\\n  \\nThe Basics of Filtering in the Frequency Domain\\n    \\n271\\n \\nhx A e\\nB e\\nxx\\n()\\n=−\\n−−\\n22\\n1\\n2\\n2\\n2\\n2\\n1\\n22 2\\n2\\n22\\nps\\nps\\nps\\nps\\n \\n(4-110)\\nFigures 4.36(c) and (d) show plots of these two equations. We note again the reci-\\nprocity in width,\\n but the most important feature here is that \\nhx\\n()\\n has a positive cen-\\nter term with negative terms on either side\\n. The small kernels shown in Fig. 4.36(d), \\nwhich we used in Chapter 3 for sharpening, “capture” this property, and thus illus-\\ntrate how knowledge of frequency domain filtering can be used as the basis for \\nchoosing coefficients of spatial kernels.\\nAlthough we have gone through signiﬁcant effort to get here, be assured that it is \\nimpossible to truly understand ﬁltering in the frequency domain without the foun-\\ndation we have just established. In practice, the frequency domain can be viewed as \\na “laboratory” in which we take advantage of the correspondence between frequen-', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='cy content and image appearance. As will be demonstrated numerous times later in \\nthis chapter, some tasks that would be exceptionally difﬁcult to formulate direct-\\nly in the spatial domain become almost trivial in the frequency domain. Once we \\nhave selected a speciﬁc ﬁlter transfer function via experimentation in the frequency \\ndomain, we have the option of implementing the ﬁlter directly in that domain using \\nthe FFT, or we can take the IDFT of the transfer function to obtain the equivalent \\nspatial domain function. As we showed in Fig. 4.36, one approach is to specify a \\nsmall spatial kernel that attempts to capture the “essence” of the \\nfull\\n ﬁlter function \\nin the spatial domain. A more formal approach is to design a 2-D digital ﬁlter by \\nusing approximations based on mathematical or statistical criteria, as we discussed \\nin Section 3.7. \\nEXAMPLE 4.15 :  Obtaining a frequency domain transfer function from a spatial kernel.', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='In this example, we start with a spatial kernel and show how to generate its corresponding ﬁlter trans-\\nfer function in the frequency domain. Then, we compare the ﬁltering results obtained using frequency \\ndomain and spatial techniques. This type of analysis is useful when one wishes to compare the perfor-\\nmance of a given kernel against one or more “full” ﬁlter candidates in the frequency domain, or to gain a \\ndeeper understanding about the performance of a kernel in the spatial domain. To keep matters simple, \\nwe use the \\n33\\n×\\n vertical Sobel kernel from Fig. 3.50(e). Figure 4.37(a) shows a \\n600 600\\n×\\n-p\\nixel\\n image, \\nfx y\\n(,\\n) ,\\n that we wish to ﬁlter, and Fig. 4.37(b) shows its spectrum.\\nF\\nigure 4.38(a) shows the Sobel kernel, \\nhxy\\n(,\\n)\\n (the perspective plot is explained below). Because \\nthe input image is of size \\n600 600\\n×\\n pixels and the kernel is of size \\n33\\n×\\n,\\n we avoid wraparound error in \\nthe frequenc\\ny domain by padding \\nf\\n and \\nh\\n with zeros to size \\n602 602\\n×', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='602 602\\n×\\n pixels, according to Eqs. (4-100) \\nand (4-101).\\n At ﬁrst glance, the Sobel kernel appears to exhibit odd symmetry. However, its ﬁrst element \\nis not 0, as required by Eq. (4-81). To convert the kernel to the smallest size that will satisfy Eq. (4-83), \\nwe have to add to it a leading row and column of 0’s, which turns it into an array of size \\n44\\n×\\n.\\n We can \\nembed this array into a larger array of zeros and still maintain its odd symmetry if the larger array is of \\neven dimensions (as is the \\n44\\n×\\n kernel) \\nand\\n their centers coincide\\n, as explained in Example 4.10. The \\npreceding comments are an important aspect of ﬁlter generation. If we preserve the odd symmetry with \\nrespect to the padded array in forming \\nhx y\\np\\n(,) ,\\n we know from property 9 in Table 4.1 that \\nH\\n(,\\n)\\nuv\\n will \\nbe purely imaginary\\n. As we show at the end of this example, this will yield results that are identical to \\nﬁltering the image spatially using the original kernel \\nhxy\\n(,\\n) .', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='hxy\\n(,\\n) .\\n If the symmetry were not preserved, the \\nresults would no longer be the same\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   271\\n6/16/2017   2:05:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 272}),\n",
       " Document(page_content='272\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe procedure used to generate \\nH\\n(,\\n)\\nuv\\n is: (1) multiply \\nhx y\\np\\n(,)\\n by \\n()\\n−\\n+\\n1\\nxy\\n to center the frequency \\ndomain ﬁlter; (2) compute the forward DFT of the result in (1) to generate \\nH\\n(,\\n) ;\\nuv\\n (3) set the real \\npart of \\nH\\n(,\\n)\\nuv\\n to 0 to account for parasitic real parts (we know that \\nH\\n has to be purely imaginary \\nbecause \\nh\\np\\n is real and odd); and (4) multiply the result by \\n() .\\n−\\n+\\n1\\nu\\nv\\n This last step reverses the multiplica-\\ntion of \\nH\\n(,\\n)\\nuv\\n by \\n() ,\\n−\\n+\\n1\\nu\\nv\\n which is implicit when \\nhxy\\n(,\\n)\\n was manually placed in the center of \\nhx y\\np\\n(,) .\\n \\nF\\nigure 4.38(a) shows a perspective plot of \\nH\\n(,\\n) ,\\nuv\\n and Fig. 4.38(b) shows \\nH\\n(,\\n)\\nuv\\n as an image. Note \\nthe antisymmetry in this image about its center\\n, a result of \\nH\\n(,\\n)\\nuv\\n being odd. Function \\nH\\n(,\\n)\\nuv\\n is used \\nas any other frequenc\\ny domain ﬁlter transfer function. Figure 4.38(c) is the result of using the ﬁlter', metadata={'source': 'imagepro.pdf', 'page': 273}),\n",
       " Document(page_content='transfer function just obtained to ﬁlter the image in Fig. 4.37(a) in the frequency domain, using the step-\\nby-step ﬁltering procedure outlined earlier. As expected from a derivative ﬁlter, edges were enhanced \\nand all the constant intensity areas were reduced to zero (the grayish tone is due to scaling for display). \\nFigure 4.38(d) shows the result of ﬁltering the same image in the spatial domain with the Sobel kernel \\nhxy\\n(,\\n) ,\\n using the procedure discussed in Section 3.6. The results are identical.\\n4.8  IMAGE SMOOTHING USING LOWPASS FREQUENCY DOMAIN  \\nFILTERS  \\nThe remainder of this chapter deals with various filtering techniques in the frequency \\ndomain, beginning with lowpass filters. Edges and other sharp intensity transitions \\n(such as noise) in an image contribute significantly to the high frequency content \\nof its Fourier transform. Hence, smoothing (blurring) is achieved in the frequency \\ndomain by high-frequency attenuation; that is, by \\nlowpass', metadata={'source': 'imagepro.pdf', 'page': 273}),\n",
       " Document(page_content='lowpass\\n filtering. In this section, \\nwe consider three types of lowpass filters: \\nideal\\n, \\nButterworth\\n, and \\nGaussian\\n. These \\nthree categories cover the range from very sharp (ideal) to very smooth (Gaussian) \\nfiltering. The shape of a Butterworth filter is controlled by a parameter called the \\nfilter order\\n. For large values of this parameter, the Butterworth filter approaches \\nthe ideal filter. For lower values, the Butterworth filter is more like a Gaussian filter. \\nThus, the Butterworth filter provides a transition between two “extreme\\ns.” \\nAll filter-\\ning in this section follows the procedure outlined in the previous section, so all filter \\ntransfer functions, \\nH\\n(,\\n) ,\\nuv\\n are understood to be of size \\nPQ\\n×\\n;\\n that is, the discrete \\n4.8\\nb a\\nFIGURE 4.37\\n(a) Image of a \\nbuilding, and  \\n(b) its Fourier \\nspectrum.\\nDIP4E_GLOBAL_Print_Ready.indb   272\\n6/16/2017   2:05:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 273}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n273\\n/H11002\\n1\\n/H11002\\n2\\n/H11002\\n1\\n0\\n0\\n0\\n1\\n2\\n1\\nb a\\nd c\\nFIGURE 4.38\\n(a) A spatial \\nkernel and per-\\nspective plot of \\nits corresponding \\nfrequency domain \\nﬁlter transfer \\nfunction.  \\n(b) Transfer  \\nfunction shown as \\nan image.  \\n(c) Result of  \\nﬁltering \\nFig. 4.37(a) in the \\nfrequency domain \\nwith the transfer \\nfunction in (b).  \\n(d) Result of \\nﬁltering the same \\nimage in the  \\nspatial domain \\nwith the kernel \\nin (a). The results \\nare identical. \\nfrequency variables are in the range \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nP\\n and \\nv\\n=−\\n012\\n1\\n,,, , ,\\n…\\nQ\\n \\nwhere \\nP\\n and \\nQ\\n are the padded sizes given by Eqs\\n. (4-100) and (4-101).\\nIDEAL LOWPASS FILTERS\\nA 2-D lowpass filter that passes without attenuation all frequencies within a circle of \\nradius from the origin, and “cuts off” all frequencies outside this, circle is called an \\nideal lowpass filter\\n (ILPF); it is specified by the transfer function\\n \\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n1\\n0\\n0\\n0', metadata={'source': 'imagepro.pdf', 'page': 274}),\n",
       " Document(page_content='uv\\n=\\n⎧\\n⎨\\n⎩\\n1\\n0\\n0\\n0\\nif \\nif \\n≤\\n>\\n \\n(4-111)\\nwhere \\nD\\n0\\n is a positive constant, and \\nD\\n(,\\n)\\nuv\\n is the distance between a point \\n(,)\\nuv\\n in \\nthe frequenc\\ny domain and the center of the \\nPQ\\n×\\n frequency rectangle; that is,\\n \\nDu P Q\\n(,)\\n/\\nuv\\nv\\n=−\\n()\\n+−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n12\\n \\n(4-112)\\nDIP4E_GLOBAL_Print_Ready.indb   273\\n6/16/2017   2:06:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 274}),\n",
       " Document(page_content='274\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nwhere, as before, \\nP\\n and \\nQ\\n are the padded sizes from Eqs. (4-102) and (4-103). \\nFigure 4.39(a) shows a perspective plot of transfer function \\nH\\n(,\\n)\\nuv\\n and Fig. 4.39(b) \\nshows it displayed as an image\\n. As mentioned in Section 4.3, the name \\nideal\\n indicates \\nthat all frequencies on or inside a circle of radius \\nD\\n0\\n are passed without attenuation, \\nwhereas all frequencies outside the circle are completely attenuated (filtered out). \\nThe ideal lowpass filter transfer function is radially symmetric about the origin. This \\nmeans that it is defined completely by a radial cross section, as Fig. 4.39(c) shows. A \\n2-D representation of the filter is obtained by rotating the cross section 360°.\\nFor an ILPF cross section, the point of transition between the values \\nH\\n(,\\n)\\nuv\\n=\\n1 \\nand \\nH\\n(,\\n)\\nuv\\n=\\n0\\n is called the\\n cutoff frequency\\n.\\n In Fig. 4.39, the cutoff frequency is \\nD\\n0\\n. \\nT', metadata={'source': 'imagepro.pdf', 'page': 275}),\n",
       " Document(page_content='D\\n0\\n. \\nT\\nhe sharp cutoff frequency of an ILPF cannot be realized with electronic compo-\\nnents, although they certainly can be simulated in a computer (subject to the con-\\nstrain that the fastest possible transition is limited by the distance between pixels). \\nThe lowpass ﬁlters in this chapter are compared by studying their behavior as a \\nfunction of the same cutoff frequencies. One way to establish standard cutoff fre-\\nquency loci using circles that enclose speciﬁed amounts of total \\nimage power\\n \\nP\\nT\\n, \\nwhich we obtain by summing the components of the power spectrum of the padded \\nimages at each point \\n(,) ,\\nuv\\n for \\nu\\n=−\\n012\\n1\\n,,, ,\\n…\\nP\\n and \\nv\\n=−\\n012\\n1\\n,,, , ;\\n…\\nQ\\n that is,\\n \\nPP\\nT\\nQ\\nu\\nP\\n=\\n=\\n−\\n=\\n−\\n∑ ∑\\n(,)\\nuv\\nv\\n0\\n1\\n0\\n1\\n \\n(4-113)\\nwhere \\nP\\n(,\\n)\\nuv\\n is given by Eq. (4-89). If the DFT has been centered, a circle of radius \\nD\\n0\\n with origin at the center of the frequency rectangle encloses \\na\\n percent of the \\npower\\n, where\\n \\na\\n=\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n∑ ∑\\n100\\nPP\\nT\\nu\\n(,)\\nuv\\nv\\n \\n(4-114)', metadata={'source': 'imagepro.pdf', 'page': 275}),\n",
       " Document(page_content='(,)\\nuv\\nv\\n \\n(4-114)\\nand the summation is over values of \\n(,)\\nuv\\n that lie inside the circle or on its boundary.\\nF\\nigures 4.40(a) and (b) show a test pattern image and its spectrum. The cir-\\ncles superimposed on the spectrum have radii of 10, 30, 60, 160, and 460 pixels, \\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nD\\n0\\n1\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nb a\\nc\\nFIGURE 4.39\\n (a) Perspective plot of an ideal lowpass-ﬁlter transfer function. (b) Function displayed as an image. \\n \\n(c) Radial cross section. \\nDIP4E_GLOBAL_Print_Ready.indb   274\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 275}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n275\\nrespectively, and enclosed the percentages of total power listed in the ﬁgure caption. \\nThe spectrum falls off rapidly, with close to 87% of the total power being enclosed \\nby a relatively small circle of radius 10. The signiﬁcance of this will become evident \\nin the following example.\\nEXAMPLE 4.16 :  Image smoothing in the frequency domain using lowpass ﬁlters.\\nFigure 4.41 shows the results of applying ILPFs with cutoff frequencies at the radii shown in Fig. 4.40(b). \\nFigure 4.41(b) is useless for all practical purposes, unless the objective of blurring is to eliminate all \\ndetail in the image, except the “blobs” representing the largest objects. The severe blurring in this image \\nis a clear indication that most of the sharp detail information in the image is contained in the 13% power \\nremoved by the ﬁlter. As the ﬁlter radius increases, less and less power is removed, resulting in less blur-', metadata={'source': 'imagepro.pdf', 'page': 276}),\n",
       " Document(page_content='ring. Note that the images in Figs. 4.41(c) through (e) contain signiﬁcant “ringing,” which becomes ﬁner \\nin texture as the amount of high frequency content removed decreases. Ringing is visible even in the \\nimage in which only 2% of the total power was removed [Fig. 4.41(e)]. This ringing behavior is a char-\\nacteristic of ideal ﬁlters, as we have mentioned several times before. Finally, the result for \\na\\n=\\n99\\n4\\n.%\\n in \\nF\\nig. 4.41(f) shows very slight blurring and almost imperceptible ringing but, for the most part, this image \\nis close to the original. This indicates that little edge information is contained in the upper 0.6% of the \\nspectrum power removed by the ILPF.\\nIt is clear from this example that ideal lowpass ﬁltering is not practical. However, it is useful to study \\nthe behavior of ILPFs as part of our development of ﬁltering concepts. Also, as shown in the discussion \\nthat follows, some interesting insight is gained by attempting to explain the ringing property of ILPFs', metadata={'source': 'imagepro.pdf', 'page': 276}),\n",
       " Document(page_content='in the spatial domain.\\nb a\\nFIGURE 4.40\\n (a) Test pattern of size \\n688 688\\n×\\n pixels, and (b) its spectrum. The spectrum is dou-\\nble the image size as a result of padding\\n, but is shown half size to ﬁt. The circles have radii of \\n10, 30, 60, 160, and 460 pixels with respect to the full-size spectrum. The radii enclose 86.9, 92.8, \\n95.1, 97.6, and 99.4% of the padded image power, respectively.\\nDIP4E_GLOBAL_Print_Ready.indb   275\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 276}),\n",
       " Document(page_content='276\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nThe blurring and ringing properties of ILPFs can be explained using the convolu-\\ntion theorem. Figure 4.42(a) shows an image of a frequency-domain ILPF transfer \\nfunction of radius 15 and size \\n1000 1000\\n×\\n pixels. Figure 4.42(b) is the spatial repre-\\nsentation,\\n \\nhxy\\n(,\\n) ,\\n of the ILPF, obtained by taking the IDFT of (a) (note the ringing). \\nF\\nigure 4.42(c) shows the intensity proﬁle of a line passing through the center of (b).\\nThis proﬁle resembles a sinc function.\\n†\\n Filtering in the spatial domain is done by \\nconvolving the function in Fig. 4.42(b) with an image. Imagine each pixel in an image \\nas being a discrete impulse whose strength is proportional to the intensity of the \\nimage at that location. Convolving this sinc-like function with an impulse copies (i.e., \\nshifts the origin of) the function to the location of the impulse. That is, convolution \\n†', metadata={'source': 'imagepro.pdf', 'page': 277}),\n",
       " Document(page_content='†\\n Although this proﬁle resembles a sinc function, the transform of an ILPF is actually a Bessel function whose \\nderivation is beyond the scope of this discussion. The important point to keep in mind is that the inverse propor-\\ntionality between the “width” of the ﬁlter function in the frequency domain, and the “spread” of the width of the \\nlobes in the spatial function, still holds. \\nb a\\nc\\ne d\\nf\\nFIGURE 4.41\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using ILPFs with cutoff frequencies \\nset at radii values 10,\\n 30, 60, 160, and 460, as shown in Fig. 4.40(b). The power removed by these ﬁlters was 13.1, 7.2, \\n4.9, 2.4, and 0.6% of the total, respectively. We used mirror padding to avoid the black borders characteristic of zero \\npadding, as illustrated in Fig. 4.31(c).\\nDIP4E_GLOBAL_Print_Ready.indb   276\\n6/16/2017   2:06:04 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 277}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n277\\nmakes a copy of the function in Fig. 4.42(b) centered on each pixel location in the \\nimage. The center lobe of this spatial function is the principal cause of blurring, while \\nthe outer, smaller lobes are mainly responsible for ringing. Because the “spread” of \\nthe spatial function is inversely proportional to the radius of \\nH\\n(,\\n) ,\\nuv\\n the larger \\nD\\n0\\n \\nbecomes (i,e, the more frequencies that are passed), the more the spatial function \\napproaches an impulse which, in the limit, causes no blurring at all when convolved \\nwith the image. The converse happens as \\nD\\n0\\n becomes smaller. This type of recipro-\\ncal behavior should be routine to you by now. In the next two sections, we show that \\nit is possible to achieve blurring with little or no ringing, an important objective in \\nlowpass ﬁltering. \\nGAUSSIAN LOWPASS FILTERS\\nGaussian lowpass filter (GLPF) transfer functions have the form\\n \\nHe\\nD\\n(,)\\n(,) /\\nuv\\nuv\\n=\\n−\\n22\\n2\\ns', metadata={'source': 'imagepro.pdf', 'page': 278}),\n",
       " Document(page_content='uv\\nuv\\n=\\n−\\n22\\n2\\ns\\n \\n(4-115)\\nwhere, as in Eq. (4-112), \\nD\\n(,\\n)\\nuv\\n is the distance from the center of the \\nPQ\\n×\\n fre-\\nquenc\\ny rectangle to any point, \\n(,)\\nuv\\n, contained by the rectangle. Unlike our earlier \\nexpressions for Gaussian functions\\n, we do not use a multiplying constant here in \\norder to be consistent with the filters discussed in this and later sections, whose \\nhighest value is 1. As before, \\ns\\n is a measure of spread about the center. By letting \\ns\\n=\\nD\\n0\\n,\\n we can express the Gaussian transfer function in the same notation as other \\nfunctions in this section:\\n \\nHe\\nDD\\n(,)\\n(,) /\\nuv\\nuv\\n=\\n−\\n2\\n0\\n2\\n2\\n \\n(4-116)\\nwhere \\nD\\n0\\n is the cutoff frequency. When \\nDD\\n(,\\n) ,\\nuv\\n=\\n0\\n the GLPF transfer function is \\ndown to 0.607 of its maximum value of 1.0.\\nFrom Table 4.4, we know that the inverse Fourier transform of a frequency-\\ndomain Gaussian function is Gaussian also. This means that a spatial Gaussian ﬁlter \\nkernel, obtained by computing the IDFT of Eq. (4-115) or (4-116), will have no', metadata={'source': 'imagepro.pdf', 'page': 278}),\n",
       " Document(page_content='ringing. As property 13 of Table 4.4 shows, the same inverse relationship explained \\nearlier for ILPFs is true also of GLPFs. Narrow Gaussian transfer functions in the \\nfrequency domain imply broader kernel functions in the spatial domain, and vice \\nb a\\nc\\nFIGURE 4.42\\n  \\n(a) Frequency  \\ndomain ILPF \\ntransfer function. \\n(b) Corresponding \\nspatial domain  \\nkernel function.  \\n(c) Intensity proﬁle \\nof a horizontal line \\nthrough the center \\nof (b).\\nDIP4E_GLOBAL_Print_Ready.indb   277\\n6/16/2017   2:06:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 278}),\n",
       " Document(page_content='278\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nversa. Figure 4.43 shows a perspective plot, image display, and radial cross sections \\nof a GLPF transfer function.\\nEXAMPLE 4.17 :  Image smoothing in the frequency domain using Gaussian lowpass ﬁlters.\\nFigure 4.44 shows the results of applying the GLPF of Eq. (4-116) to Fig. 4.44(a), with \\nD\\n0\\n equal to the ﬁve \\nradii in Fig. 4.40(b). Compared to the results obtained with an ILPF (Fig. 4.41), we note a smooth transi-\\ntion in blurring as a function of increasing cutoff frequency. The GLPF achieved slightly less smoothing \\nthan the ILPF. The key difference is that we are assured of no ringing when using a GLPF. This is an \\nimportant consideration in practice, especially in situations in which any type of artifact is unacceptable, \\nas in medical imaging. In cases where more control of the transition between low and high frequencies', metadata={'source': 'imagepro.pdf', 'page': 279}),\n",
       " Document(page_content='about the cutoff frequency are needed, the Butterworth lowpass ﬁlter discussed next presents a more \\nsuitable choice. The price of this additional control over the ﬁlter proﬁle is the possibility of ringing, as \\nyou will see shortly.\\nBUTTERWORTH LOWPASS FILTERS\\nThe transfer function of a Butterworth lowpass filter (BLPF) of order \\nn\\n, with cutoff \\nfrequency at a distance \\nD\\n0\\n from the center of the frequency rectangle, is defined as\\n \\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\n \\n(4-117)\\nwhere \\nD\\n(,\\n)\\nuv\\n is given by Eq. (4-112). Figure 4.45 shows a perspective plot, image \\ndisplay\\n, and radial cross sections of the BLPF function. Comparing the cross section \\nplots in Figs. 4.39, 4.43, and 4.45, we see that the BLPF function can be controlled to \\napproach the characteristics of the ILPF using higher values of \\nn\\n, and the GLPF for \\nlower values of \\nn\\n, while providing a smooth transition in from low to high frequen-', metadata={'source': 'imagepro.pdf', 'page': 279}),\n",
       " Document(page_content='cies. Thus, we can use a BLPF to approach the sharpness of an ILPF function with \\nconsiderably less ringing. \\nu\\nv\\n1.0\\n0.607\\nD\\n0\\n \\n/H11005 \\n10\\nD\\n0\\n \\n/H11005 \\n20\\nD\\n0\\n \\n/H11005 \\n40\\nD\\n0\\n \\n/H11005 \\n60\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\n0\\nb a\\nc\\nFIGURE 4.43\\n (a) Perspective plot of a GLPF transfer function. (b) Function displayed as an image. (c) Radial cross \\nsections for various values of \\nD\\n0\\n.\\nDIP4E_GLOBAL_Print_Ready.indb   278\\n6/16/2017   2:06:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 279}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n279\\nb a\\nc\\ne d\\nf\\nFIGURE 4.44\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using GLPFs with cutoff frequen-\\ncies at the radii shown in F\\nig. 4.40. Compare with Fig. 4.41. We used mirror padding to avoid the black borders \\ncharacteristic of zero padding.\\n0.5\\nD\\n0\\nn \\n/H11005 \\n1\\nn \\n/H11005 \\n2\\nn \\n/H11005 \\n3\\nn \\n/H11005 \\n4\\n1.0\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nv\\nu\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nb a\\nc\\nFIGURE 4.45\\n (a) Perspective plot of a Butterworth lowpass-ﬁlter transfer function. (b) Function displayed as an image. \\n(c) Radial cross sections of BLPFs of orders 1 through 4. \\nDIP4E_GLOBAL_Print_Ready.indb   279\\n6/16/2017   2:06:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 280}),\n",
       " Document(page_content='280\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nc\\ne d\\nf\\nFIGURE 4.46\\n (a) Original image of size \\n688 688\\n×\\n pixels. (b)–(f) Results of ﬁltering using BLPFs with cutoff frequen-\\ncies at the radii shown in F\\nig. 4.40 and \\nn\\n=\\n22\\n5\\n..\\n Compare with Figs. 4.41 and 4.44. We used mirror padding to avoid \\nthe black borders characteristic of zero padding\\n.\\nEXAMPLE 4.18 :  Image smoothing using a Butterworth lowpass ﬁlter.\\nFigures 4.46(b)-(f) show the results of applying the BLPF of Eq. (4-117) to Fig. 4.46(a), with cutoff \\nfrequencies equal to the ﬁve radii in Fig. 4.40(b), and with \\nn\\n=\\n22\\n5\\n..\\n The results in terms of blurring are \\nbetween the results obtained with using ILPFs and GLPFs\\n. For example, compare Fig. 4.46(b), with \\nFigs. 4.41(b) and 4.44(b). The degree of blurring with the BLPF was less than with the ILPF, but more \\nthan with the GLPF. \\nThe spatial domain kernel obtainable from a BLPF of order 1 has no ringing.', metadata={'source': 'imagepro.pdf', 'page': 281}),\n",
       " Document(page_content='Generally, ringing is imperceptible in ﬁlters of order 2 or 3, but can become sig-\\nniﬁcant in ﬁlters of higher orders. Figure 4.47 shows a comparison between the spa-\\ntial representation (i.e., spatial kernels) corresponding to BLPFs of various orders \\n(using a cutoff frequency of 5 in all cases). Shown also is the intensity proﬁle along \\nThe kernels in Figs. 4.47(a)  \\nthrough (d) were obtained \\nusing the procedure out-\\nlined in the explanation of \\nFig. 4.42.\\nDIP4E_GLOBAL_Print_Ready.indb   280\\n6/16/2017   2:06:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 281}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n281\\nb\\na\\nd\\nc\\nf\\nh\\ne\\ng\\nFIGURE 4.47\\n (a)–(d) Spatial representations (i.e., spatial kernels) corresponding to BLPF transfer functions of size \\n1000 1000\\n×\\n pixels, cut-off frequency of 5, and order 1, 2, 5, and 20, respectively. (e)–(h) Corresponding intensity \\nproﬁles through the center of the ﬁlter functions\\n. \\na horizontal scan line through the center of each spatial kernel. The kernel corre-\\nsponding to the BLPF of order 1 [see Fig. 4.47(a)] has neither ringing nor negative \\nvalues. The kernel corresponding to a BLPF of order 2 does show mild ringing and \\nsmall negative values, but they certainly are less pronounced than would be the case \\nfor an ILPF. As the remaining images show, ringing becomes signiﬁcant for higher-\\norder ﬁlters. A BLPF of order 20 has a spatial kernel that exhibits ringing charac-\\nteristics similar to those of the ILPF (in the limit, both ﬁlters are identical). BLPFs', metadata={'source': 'imagepro.pdf', 'page': 282}),\n",
       " Document(page_content='of orders 2 to 3 are a good compromise between effective lowpass ﬁltering and \\nacceptable spatial-domain ringing. Table 4.5 summarizes the lowpass ﬁlter transfer \\nfunctions discussed in this section.\\nADDITIONAL EXAMPLES OF LOWPASS FILTERING\\nIn the following discussion, we show several practical applications of lowpass filter-\\ning in the frequency domain. The first example is from the field of machine per-\\nception with application to character recognition; the second is from the printing \\nand publishing industry; and the third is related to processing satellite and aerial \\nimages. Similar results can be obtained using the lowpass spatial filtering techniques \\ndiscussed in Section 3.5. We use GLPFs in all examples for consistency, but simi-\\nlar results can be obtained using BLPFs. Keep in mind that images are padded to \\ndouble size for filtering, as indicated by Eqs. (4-102) and (4-103), and filter transfer \\nfunctions have to match padded-image size. The values of \\nD\\n0', metadata={'source': 'imagepro.pdf', 'page': 282}),\n",
       " Document(page_content='D\\n0\\n used in the following \\nexamples reflect this doubled filter size.\\nDIP4E_GLOBAL_Print_Ready.indb   281\\n6/16/2017   2:06:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 282}),\n",
       " Document(page_content='282\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nIdeal\\nGaussian\\nButterworth\\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n≤\\n>\\n⎧\\n⎨\\n⎩\\n1\\n0\\n0\\n0\\nif \\nif \\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=\\n−\\n2\\n0\\n2\\n2\\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\nTABLE \\n4.5\\nLowpass ﬁlter transfer functions. \\nD\\n0\\n is the cutoff frequency, and \\nn\\n is the order of the Butterworth ﬁlter.\\nFigure 4.48 shows a sample of text of low resolution. One encounters text like \\nthis, for example, in fax transmissions, duplicated material, and historical records. \\nThis particular sample is free of additional difﬁculties like smudges, creases, and \\ntorn sections. The magniﬁed section in Fig. 4.48(a) shows that the characters in this \\ndocument have distorted shapes due to lack of resolution, and many of the charac-\\nters are broken. Although humans ﬁll these gaps visually without difﬁculty, machine \\nrecognition systems have real difﬁculties reading broken characters. One approach', metadata={'source': 'imagepro.pdf', 'page': 283}),\n",
       " Document(page_content='for handling this problem is to bridge small gaps in the input image by blurring \\nit. Figure 4.48(b) shows how well characters can be “repaired” by this simple pro-\\ncess using a Gaussian lowpass ﬁlter with \\nD\\n0\\n120\\n=\\n.\\n It is typical to follow the type of \\n“repair”\\n just described with additional processing, such as thresholding and thinning, \\nto yield cleaner characters. We will discuss thinning in Chapter 9 and thresholding \\nin Chapter 10.\\nLowpass ﬁltering is a staple in the printing and publishing industry, where it is \\nused for numerous preprocessing functions, including unsharp masking, as discussed \\nin Section 3.6. “Cosmetic” processing is another use of lowpass ﬁltering prior to print-\\ning. Figure 4.49 shows an application of lowpass ﬁltering for producing a smoother, \\nsofter-looking result from a sharp original. For human faces, the typical objective is \\nto reduce the sharpness of ﬁne skin lines and small blemishes. The magniﬁed sec-', metadata={'source': 'imagepro.pdf', 'page': 283}),\n",
       " Document(page_content='tions in Figs. 4.49(b) and (c) clearly show a signiﬁcant reduction in ﬁne skin lines \\naround the subject’s eyes. In fact, the smoothed images look quite soft and pleasing.\\nFigure 4.50 shows two applications of lowpass ﬁltering on the same image, but \\nwith totally different objectives. Figure 4.50(a) is an \\n808 754\\n×\\n segment of a very high \\nWe will cover unsharp \\nmasking in the frequency \\ndomain in Section 4.9.\\nb a\\nFIGURE 4.48\\n(a) Sample text \\nof low resolution \\n(note the broken \\ncharacters in the  \\nmagniﬁed view). \\n(b) Result of \\nﬁltering with a \\nGLPF,  \\nshowing that gaps \\nin the broken \\ncharacters were \\njoined. \\nDIP4E_GLOBAL_Print_Ready.indb   282\\n6/16/2017   2:06:11 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 283}),\n",
       " Document(page_content='4.8\\n  \\nImage Smoothing Using Lowpass Frequency Domain Filters\\n    \\n283\\nb a\\nc\\nFIGURE 4.49\\n (a) Original \\n785 732\\n×\\n image. (b) Result of ﬁltering using a GLPF with \\nD\\n0\\n150\\n=\\n.\\n (c) Result of ﬁltering \\nusing a GLPF with \\nD\\n0\\n130\\n=\\n.\\n Note the reduction in ﬁne skin lines in the magniﬁed sections in (b) and (c). \\nresolution radiometer (VHRR) image showing part of the Gulf of Mexico (dark) \\nand Florida (light) (note the horizontal sensor scan lines). The boundaries between \\nbodies of water were caused by loop currents. This image is illustrative of remotely \\nsensed images in which sensors have the tendency to produce pronounced scan lines \\nalong the direction in which the scene is being scanned. (See Example 4.24 for an \\nb a\\nc\\nFIGURE 4.50\\n (a) \\n808 754\\n×\\n satellite image showing prominent horizontal scan lines. (b) Result of ﬁltering using a \\nGLPF with \\nD\\n0\\n50\\n=\\n. (c) Result of using a GLPF with \\nD\\n0\\n20\\n=\\n. (Original image courtesy of NOAA.) \\nDIP4E_GLOBAL_Print_Ready.indb   283', metadata={'source': 'imagepro.pdf', 'page': 284}),\n",
       " Document(page_content='6/16/2017   2:06:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 284}),\n",
       " Document(page_content='284\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nillustration of imaging conditions that can lead for such degradations.) Lowpass ﬁl-\\ntering is a crude (but simple) way to reduce the effect of these lines, as Fig. 4.50(b) \\nshows (we consider more effective approaches in Sections 4.10 and 5.4). This image \\nwas obtained using a GLFP with \\nD\\n0\\n50\\n=\\n.\\n The reduction in the effect of the scan \\nlines in the smoothed image can simplify the detection of macro features\\n, such as the \\ninterface boundaries between ocean currents.\\nFigure 4.50(c) shows the result of signiﬁcantly more aggressive Gaussian lowpass \\nﬁltering with \\nD\\n0\\n20\\n=\\n.\\n Here, the objective is to blur out as much detail as possible \\nwhile leaving large features recognizable\\n. For instance, this type of ﬁltering could be \\npart of a preprocessing stage for an image analysis system that searches for features \\nin an image bank. An example of such features could be lakes of a given size, such', metadata={'source': 'imagepro.pdf', 'page': 285}),\n",
       " Document(page_content='as Lake Okeechobee in the lower eastern region of Florida, shown in Fig. 4.50(c) as \\na nearly round dark region surrounded by a lighter region. Lowpass ﬁltering helps \\nto simplify the analysis by averaging out features smaller than the ones of interest.\\n4.9 IMAGE SHARPENING USING HIGHPASS FILTERS  \\nWe showed in the previous section that an image can be smoothed by attenuating \\nthe high-frequency components of its Fourier transform. Because edges and other \\nabrupt changes in intensities are associated with high-frequency components, image \\nsharpening can be achieved in the frequency domain by highpass filtering, which \\nattenuates low-frequencies components without disturbing high-frequencies in the \\nFourier transform. As in Section 4.8, we consider only zero-phase-shift filters that \\nare radially symmetric. All filtering in this section is based on the procedure outlined \\nin Section 4.7, so all images are assumed be padded to size \\nPQ\\n×\\n [see Eqs. (4-102) \\nand (4-103)],', metadata={'source': 'imagepro.pdf', 'page': 285}),\n",
       " Document(page_content='and (4-103)],\\n and filter transfer functions, \\nH\\n(,\\n) ,\\nuv\\n are understood to be centered, \\ndiscrete functions of size \\nPQ\\n×\\n. \\nIDEAL, GAUSSIAN, AND BUTTERWORTH HIGHPASS FILTERS FROM \\nLOWPASS FILTERS\\nAs was the case with kernels in the spatial domain (see Section 3.7), subtracting a \\nlowpass filter transfer function from 1 yields the corresponding highpass filter trans-\\nfer function in the frequency domain:\\n \\nHH\\nHP\\nLP\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-118)\\nwhere \\nH\\nLP\\n(,)\\nuv\\n is the transfer function of a lowpass filter. Thus, it follows from \\nEq.\\n (4-111) that an ideal highpass filter (IHPF) transfer function is given by\\n \\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n0\\n1\\n0\\n0\\nif \\nif \\n≤\\n>\\n \\n(4-119)\\nwhere, as before, \\nD\\n(,\\n)\\nuv\\n is the distance from the center of the \\nPQ\\n×\\n frequency rect-\\nangle\\n, as given in Eq. (4-112). Similarly, it follows from Eq. (4-116) that the transfer \\nfunction of a Gaussian highpass filter (GHPF) transfer function is given by\\n4.9\\nIn some applications of', metadata={'source': 'imagepro.pdf', 'page': 285}),\n",
       " Document(page_content='highpass ﬁltering, it is \\nadvantageous to enhance \\nthe high-frequencies of \\nthe Fourier transform.\\nDIP4E_GLOBAL_Print_Ready.indb   284\\n6/16/2017   2:06:12 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 285}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n285\\n  \\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n1\\n2\\n0\\n2\\n2\\n \\n(4-120)\\nand, from Eq. (4-117), that the transfer function of a Butterworth highpass filter \\n(BHPF) is\\n \\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\n \\n(4-121)\\nFigure 4.51 shows 3-D plots, image representations, and radial cross sections for \\nthe preceding transfer functions\\n. As before, we see that the BHPF transfer function \\nin the third row of the ﬁgure represents a transition between the sharpness of the \\nIHPF and the broad smoothness of the GHPF transfer function.\\nIt follows from Eq. (4-118) that the spatial kernel corresponding to a highpass \\nﬁlter transfer function in the frequency domain is given by\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\n1\\nH\\n(\\nu, \\nv\\n)\\nD\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nH\\n(\\nu, \\nv\\n)\\nu\\nv\\nu\\nv\\nu\\nv\\nb a\\nc\\ne d\\nf\\nh\\ng\\ni\\nFIGURE 4.51\\nTop row:  \\nPerspective plot, \\nimage, and, radial \\ncross section of \\nan IHPF transfer \\nfunction. Middle \\nand bottom \\nrows: The same', metadata={'source': 'imagepro.pdf', 'page': 286}),\n",
       " Document(page_content='rows: The same \\nsequence for \\nGHPF and BHPF \\ntransfer functions. \\n(The thin image \\nborders were \\nadded for clarity. \\nThey are not part \\nof the data.)\\nDIP4E_GLOBAL_Print_Ready.indb   285\\n6/16/2017   2:06:16 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 286}),\n",
       " Document(page_content='286\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n \\nhx y H\\nH\\nxy\\nh xy\\nHP\\nHP\\nLP\\nLP\\n(,) (,)\\n(,)\\n(,) (,)\\n=\\n[]\\n=−\\n[]\\n=−\\n−\\n−\\n/H5219\\n/H5219\\n1\\n1\\n1\\nuv\\nuv\\nd\\n \\n(4-122)\\nwhere we used the fact that the IDFT of 1 in the frequency domain is a unit impulse \\nin the spatial domain (see \\nTable 4.4). This equation is precisely the foundation for \\nthe discussion in\\n \\nSection 3.7, in which we showed how to construct a highpass kernel \\nby subtracting a lowpass kernel from a unit impulse\\n.\\nFigure 4.52 shows highpass spatial kernels constructed in just this manner, using \\nEq. (4-122) with ILPF, GLPF, and BLPF transfer functions (the values of \\nM\\n, \\nN\\n, and \\nD\\n0\\n used in this ﬁgure are the same as those we used for Fig. 4.42, and the BLPF is of \\norder 2). Figure 4.52(a) shows the resulting ideal highpass kernel obtained using Eq. \\n(4-122), and Fig. 4.52(b) is a horizontal intensity proﬁle through the center of the ker-', metadata={'source': 'imagepro.pdf', 'page': 287}),\n",
       " Document(page_content='nel. The center element of the proﬁle is a unit impulse, visible as a bright dot in the \\ncenter of Fig. 4.52(a). Note that this highpass kernel has the same ringing properties \\nillustrated in Fig. 4.42(b) for its corresponding lowpass counterpart. As you will see \\nshortly, ringing is just as objectionable as before, but this time in images sharpened \\nwith ideal highpass ﬁlters. The other images and proﬁles in Fig. 4.52 are for Gaussian \\nand Butterworth kernels. We know from Fig. 4.51 that GHPF transfer functions in \\nthe frequency domain tend to have a broader “skirt” than Butterworth functions of \\ncomparable size and cutoff frequency. Thus, we would expect Butterworth spatial \\nRecall that a unit impulse \\nin the spatial domain is \\nan array of 0’s with a 1 in \\nthe center.\\nb a\\nc\\ne d\\nf\\nFIGURE 4.52\\n (a)–(c): Ideal, Gaussian, and Butterworth highpass spatial kernels obtained from \\nIHPF, GHPF, and BHPF frequency-domain transfer functions. (The thin image borders are', metadata={'source': 'imagepro.pdf', 'page': 287}),\n",
       " Document(page_content='not part of the data.) (d)–(f): Horizontal intensity proﬁles through the centers of the kernels.\\nDIP4E_GLOBAL_Print_Ready.indb   286\\n6/16/2017   2:06:17 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 287}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n287\\nkernels to be “broader” than comparable Gaussian kernels, a fact that is conﬁrmed \\nby the images and their proﬁles in Figs. 4.52. Table 4.6 summarizes the three highpass \\nﬁlter transfer functions discussed in the preceding paragraphs.\\nEXAMPLE 4.19 :  Highpass ﬁltering of the character test pattern.\\nThe ﬁrst row of Fig. 4.53 shows the result of ﬁltering the test pattern in Fig. 4.37(a) using IHPF, GHPF, and \\nBHPF transfer functions with \\nD\\n0\\n60\\n=\\n [see Fig. 4.37(b)] and \\nn\\n=\\n2\\n for the Butterworth ﬁlter. We know \\nfrom Chapter 3 that highpass ﬁltering produces images with negative values\\n. The images in Fig.  4.53 are \\nnot scaled, so the negative values are clipped by the display at 0 (black). The key objective of highpass \\nﬁltering is to sharpen. Also, because the highpass ﬁlters used here set the DC term to zero, the images \\nhave essentially no tonality, as explained earlier in connection with Fig. 4.30.', metadata={'source': 'imagepro.pdf', 'page': 288}),\n",
       " Document(page_content='Our main objective in this example is to compare the behavior of the three highpass ﬁlters. As \\nFig. 4.53(a) shows, the ideal highpass ﬁlter produced results with severe distortions caused by ringing. \\nFor example, the blotches inside the strokes of the large letter “a” are ringing artifacts. By comparison, \\nneither Figs. 4.53(b) or (c) have such distortions. With reference to Fig. 4.37(b), the ﬁlters removed or \\nattenuated approximately 95% of the image energy. As you know, removing the lower frequencies of an \\nimage reduces its gray-level content signiﬁcantly, leaving mostly edges and other sharp transitions, as is \\nevident in Fig. 4.53. The details you see in the ﬁrst row of the ﬁgure are contained in only the upper 5% \\nof the image energy.\\nThe second row, obtained with \\nD\\n0\\n160\\n=\\n,\\n is more interesting. The remaining energy of those images \\nis about 2.5%,\\n or half, the energy of the images in the ﬁrst row. However, the difference in ﬁne detail', metadata={'source': 'imagepro.pdf', 'page': 288}),\n",
       " Document(page_content='is striking. See, for example, how much cleaner the boundary of the large “a” is now, especially in the \\nGaussian and Butterworth results. The same is true for all other details, down to the smallest objects. \\nThis is the type of result that is considered acceptable when detection of edges and boundaries is impor-\\ntant.\\nFigure 4.54 shows the images in the second row of Fig. 4.53, scaled using Eqs. (2-31) and (2-32) to \\ndisplay the full intensity range of both positive and negative intensities. The ringing in Fig. 4.54(a) shows \\nthe inadequacy of ideal highpass ﬁlters. In contrast, notice the smoothness of the background on the \\nother two images, and the crispness of their edges.\\nEXAMPLE 4.20 :  Using highpass ﬁltering and thresholding for image enhancement.\\nFigure 4.55(a) is a \\n962 1026\\n×\\n image of a thumbprint in which smudges (a typical problem) are evident. \\nA key step in automated ﬁngerprint recognition is enhancement of print ridges and the reduction of \\nsmudges', metadata={'source': 'imagepro.pdf', 'page': 288}),\n",
       " Document(page_content='smudges\\n. In this example, we use highpass ﬁltering to enhance the ridges and reduce the effects of \\nIdeal\\nGaussian\\nButterworth\\nH\\nDD\\nDD\\n(,\\n)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n⎧\\n⎨\\n⎩\\n0\\n1\\n0\\n0\\nif \\nif \\n≤\\n>\\nHe\\nDD\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n1\\n2\\n0\\n2\\n2\\nH\\nDD\\nn\\n(,)\\n(,)\\nuv\\nuv\\n=\\n+\\n[]\\n1\\n1\\n0\\n2\\nTABLE \\n4.6\\nHighpass ﬁlter transfer functions. \\nD\\n0\\n is the cutoff frequency and \\nn\\n is the order of the Butterworth transfer function.\\nDIP4E_GLOBAL_Print_Ready.indb   287\\n6/16/2017   2:06:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 288}),\n",
       " Document(page_content='288\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nb a\\nc\\ne d\\nf\\nFIGURE 4.53\\n Top row: The image from Fig. 4.40(a) ﬁltered with IHPF, GHPF, and BHPF transfer functions using \\nD\\n0\\n60\\n=\\n in all cases (\\nn\\n=\\n2 for the BHPF). Second row: Same sequence, but using \\nD\\n0\\n160\\n=\\n.\\nb a\\nc\\nFIGURE 4.54\\n The images from the second row of Fig. 4.53 scaled using Eqs. (2-31) and (2-32) to show both positive \\nand negative values.\\nDIP4E_GLOBAL_Print_Ready.indb   288\\n6/16/2017   2:06:18 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 289}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n289\\nsmudging. Enhancement of the ridges is accomplished by the fact that their boundaries are character-\\nized by high frequencies, which are unchanged by a highpass ﬁlter. On the other hand, the ﬁlter reduces \\nlow frequency components, which correspond to slowly varying intensities in the image, such as the \\nbackground and smudges. Thus, enhancement is achieved by reducing the effect of all features except \\nthose with high frequencies, which are the features of interest in this case.\\nFigure 4.55(b) is the result of using a Butterworth highpass ﬁlter of order 4 with a cutoff frequency \\nof 50. A fourth-order ﬁlter provides a sharp (but smooth) transition from low to high frequencies, with \\nﬁltering characteristics between an ideal and a Gaussian ﬁlter. The cutoff frequency chosen is about 5% \\nof the long dimension of the image. The idea is for \\nD\\n0\\n to be close to the origin so that low frequencies are', metadata={'source': 'imagepro.pdf', 'page': 290}),\n",
       " Document(page_content='attenuated but not completely eliminated, except for the DC term which is set to 0, so that tonality dif-\\nferences between the ridges and background are not lost completely. Choosing a value for \\nD\\n0\\n between \\n5% and 10% of the long dimension of the image is a good starting point. Choosing a large value of \\nD\\n0\\n would highlight ﬁne detail to such an extent that the deﬁnition of the ridges would be affected. As \\nexpected, the highpass ﬁltered image has negative values, which are shown as black by the display.\\nA simple approach for highlighting sharp features in a highpass-ﬁltered image is to threshold it by set-\\nting to black (0) all negative values and to white (1) the remaining values. Figure 4.55(c) shows the result \\nof this operation. Note how the ridges are clear, and how the effect of the smudges has been reduced \\nconsiderably. In fact, ridges that are barely visible in the top, right section of the image in Fig. 4.55(a) are', metadata={'source': 'imagepro.pdf', 'page': 290}),\n",
       " Document(page_content='nicely enhanced in Fig. 4.55(c). An automated algorithm would ﬁnd it much easier to follow the ridges \\non this image than it would on the original. \\nTHE LAPLACIAN IN THE FREQUENCY DOMAIN\\nIn Section 3.6, we used the Laplacian for image sharpening in the spatial domain. In \\nthis section, we revisit the Laplacian and show that it yields equivalent results using \\nfrequency domain techniques. It can be shown (see Problem 4.52) that the Laplacian \\ncan be implemented in the frequency domain using the filter transfer function\\n \\nHu\\n(,\\n) ( )\\nuv v\\n=− +\\n4\\n22 2\\np\\n \\n(4-123)\\nb a\\nc\\nFIGURE 4.55\\n (a) Smudged thumbprint. (b) Result of highpass ﬁltering (a). (c) Result of thresholding (b). (Original \\nimage courtesy of the U.S. National Institute of Standards and Technology.) \\nDIP4E_GLOBAL_Print_Ready.indb   289\\n6/16/2017   2:06:19 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 290}),\n",
       " Document(page_content='290\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor, with respect to the center of the frequency rectangle, using the transfer function\\n \\nHu\\nP Q\\nD\\n(,)\\n(,)\\nuv\\nv\\nuv\\n=− −\\n()\\n+−\\n()\\n⎡\\n⎣\\n⎤\\n⎦\\n=−\\n42 2\\n4\\n2\\n22\\n22\\np\\np\\n \\n(4-124)\\nwhere \\nD\\n(,\\n)\\nuv\\n is the distance function defined in Eq. (4-112). Using this transfer \\nfunction,\\n the Laplacian of an image, \\nfx y\\n(,\\n) ,\\n is obtained in the familiar manner:\\n \\n/H11612\\n21\\nfx y H F\\n(,) (,)(,)\\n=\\n[]\\n−\\n/H5219\\nuv uv\\n \\n(4-125)\\nwhere \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) .\\n As in Eq. (3-54), enhancement is implemented \\nusing the equation \\n \\ngxy f xy c f xy\\n(,\\n) (,) (,)\\n=+ ∇\\n2\\n \\n(4-126)\\nHere, \\nc\\n=−\\n1\\n because \\nH\\n(,\\n)\\nuv\\n is negative. In Chapter 3, \\nfx y\\n(,\\n)\\n and \\n/H11612\\n2\\nfx y\\n(,)\\n had \\ncomparable values\\n. However, computing \\n/H11612\\n2\\nfx y\\n(,)\\n with Eq. (4-125) introduces DFT \\nscaling factors that can be several orders of magnitude larger than the maximum \\nvalue of \\nf\\n.\\n Thus, the differences between \\nf\\n and its Laplacian must be brought into', metadata={'source': 'imagepro.pdf', 'page': 291}),\n",
       " Document(page_content='comparable ranges. The easiest way to handle this problem is to normalize the val-\\nues of \\nfx y\\n(,\\n)\\n to the range \\n[, ]\\n01\\n (before computing its DFT) and divide \\n/H11612\\n2\\nfx y\\n(,)\\n by \\nits maximum value\\n, which will bring it to the approximate range \\n[, ] .\\n−\\n11\\n (Remember, \\nthe Laplacian has negative values\\n.) Equation (4-126) can then be used.\\nWe can write Eq. (4-126) directly in the frequency domain as\\n  \\ngxy F H F\\nHF\\n(\\n,) (,) (,)(,)\\n(,) (,)\\n=−\\n{}\\n=−\\n[]\\n{}\\n=\\n−\\n−\\n−\\n/H5219\\n/H5219\\n/H5219\\n1\\n1\\n1\\n1\\n1\\nuv uv uv\\nuv uv\\n+ +\\n⎡\\n⎣\\n⎤\\n⎦\\n{}\\n4\\n22\\np\\nDF\\n(,) (,)\\nuv uv\\n \\n(4-127)\\nAlthough this result is elegant, it has the same scaling issues just mentioned, com-\\npounded by the fact that the normalizing factor is not as easily computed.\\n For this \\nreason, Eq. (4-126) is the preferred implementation in the frequency domain, with \\n/H11612\\n2\\nfx y\\n(,)\\n computed using Eq. (4-125) and scaled using the approach mentioned in \\nthe previous paragraph.', metadata={'source': 'imagepro.pdf', 'page': 291}),\n",
       " Document(page_content='EXAMPLE 4.21 :  Image sharpening in the frequency domain using the Laplacian.\\nFigure 4.56(a) is the same as Fig. 3.46(a), and Fig. 4.56(b) shows the result of using Eq. (4-126), in which \\nthe Laplacian was computed in the frequency domain using Eq. (4-125). Scaling was done as described \\nin connection with Eq. (4-126). We see by comparing Figs. 4.56(b) and 3.46(d) that the frequency-domain \\nresult is superior. The image in Fig. 4.56(b) is much sharper, and shows details that are barely visible in \\n3.46(d), which was obtained using the Laplacian kernel in Fig. 3.45(b), with a \\n−\\n8\\n in the center. The sig-\\nniﬁcant improvement achieved in the frequenc\\ny domain is not unexpected. The spatial Laplacian kernel \\nDIP4E_GLOBAL_Print_Ready.indb   290\\n6/16/2017   2:06:20 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 291}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n291\\nencompasses a very small neighborhood, while the formulation in Eqs. (4-125) and (4-126) encompasses \\nthe entire image.\\nUNSHARP MASKING, HIGH-BOOST FILTERING, AND HIGH- \\nFREQUENCY-EMPHASIS FILTERING\\nIn this section, we discuss frequency domain formulations of the unsharp mask-\\ning and high-boost filtering image sharpening techniques introduced in Section 3.6. \\nUsing frequency domain methods, the mask defined in Eq. (3-55) is given by \\n \\ngx y f x y f x y\\nmask\\nLP\\n(,) (,) (,)\\n=−\\n \\n(4-128)\\nwith\\n \\nfx y H F\\nLP\\nLP\\n(,) (,)(,)\\n=\\n[]\\n−\\n/H5219\\n1\\nuv uv\\n \\n(4-129)\\nwhere \\nH\\nLP\\n(,)\\nuv\\n is a lowpass filter transfer function, and \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) .\\n \\nHere\\n, \\nfx y\\nLP\\n(,)\\n is a smoothed image analogous to \\nfx y\\n(,)\\n in Eq. (3-55). Then, as in \\nEq.\\n (3-56),\\n \\ngxy f xy k g xy\\n(,\\n) (,) (,)\\n=+\\nmask\\n \\n(4-130)\\nThis expression defines \\nunsharp masking\\n when \\nk\\n=\\n1\\n and high-boost filtering when \\nk\\n>\\n1.', metadata={'source': 'imagepro.pdf', 'page': 292}),\n",
       " Document(page_content='k\\n>\\n1.\\n Using the preceding results, we can express Eq. (4-130) entirely in terms of \\nfrequenc\\ny domain computations involving a lowpass filter:\\n \\ngxy\\nk H F\\n(,\\n)\\n(,) (,)\\n=+ −\\n[]\\n{}\\n−\\n/H5219\\n1\\n11\\nQR\\nLP\\nuv uv\\n \\n(4-131)\\nb a\\nFIGURE 4.56\\n(a) Original, \\nblurry image.  \\n(b) Image \\nenhanced using \\nthe Laplacian in \\nthe frequency  \\ndomain.  \\nCompare with \\nFig. 3.46(d). \\n(Original image \\ncourtesy of \\nNASA.)\\nDIP4E_GLOBAL_Print_Ready.indb   291\\n6/16/2017   2:06:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 292}),\n",
       " Document(page_content='292\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nWe can express this result in terms of a highpass filter using Eq. (4-118):\\n \\ngxy\\nk H F\\nP\\n(,)\\n(,) (,)\\n=+\\n[]\\n{}\\n−\\n/H5219\\n1\\n1\\nH\\nuv uv\\n \\n(4-132)\\nThe expression contained within the square brackets is called a \\nhigh-frequency-\\nemphasis filter transfer function\\n.\\n As noted earlier, highpass filters set the dc term \\nto zero, thus reducing the average intensity in the filtered image to 0. The high-fre-\\nquency-emphasis filter does not have this problem because of the 1 that is added to \\nthe highpass filter transfer function. Constant \\nk\\n gives control over the proportion of \\nhigh frequencies that influences the final result. A slightly more general formulation \\nof high-frequency-emphasis filtering is the expression\\n \\ngxy k kH F\\n(,\\n)\\n(,) (,)\\n=+\\n[]\\n{}\\n−\\n/H5219\\n1\\n12 H P\\nuv uv\\n \\n(4-133)\\nwhere \\nk\\n1\\n0\\n≥\\n offsets the value the transfer function so as not to zero-out the dc term \\n[see F\\nig. 4.30(c)], and \\nk\\n2\\n0\\n>', metadata={'source': 'imagepro.pdf', 'page': 293}),\n",
       " Document(page_content='k\\n2\\n0\\n>\\n controls the contribution of high frequencies.\\nEXAMPLE 4.22 :  Image enhancement using high-frequency-emphasis ﬁltering.\\nFigure 4.57(a) shows a \\n503 720\\n×\\n-p\\nixel\\n chest X-ray image with a narrow range of intensity levels. The \\nobjective of this example is to enhance the image using high-frequenc\\ny-emphasis ﬁltering. X-rays can-\\nnot be focused in the same manner that optical lenses can, and the resulting images generally tend to be \\nslightly blurred. Because the intensities in this particular image are biased toward the dark end of the \\nb a\\nd c\\nFIGURE 4.57\\n(a) A chest X-ray.\\n(b) Result of  \\nﬁltering with a \\nGHPF function.  \\n(c) Result of \\nhigh-frequency-\\nemphasis ﬁltering \\nusing the same \\nGHPF. (d) Result \\nof performing  \\nhistogram  \\nequalization on (c). \\n(Original image \\ncourtesy of Dr. \\nThomas R. Gest, \\nDivision of  \\nAnatomical  \\nSciences,  \\nUniversity of \\nMichigan Medical \\nSchool.)\\nDIP4E_GLOBAL_Print_Ready.indb   292\\n6/16/2017   2:06:22 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 293}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n293\\ngray scale, we also take this opportunity to give an example of how spatial domain processing can be \\nused to complement frequency-domain ﬁltering.\\nImage artifacts, such as ringing, are unacceptable in medical image processing, so we use a Gaussian \\nhighpass ﬁlter transfer function. Because the spatial representation of a GHPF function is Gaussian also, \\nwe know that ringing will not be an issue. The value chosen for \\nD\\n0\\n should provide enough ﬁltering to \\nsharpen boundaries while at the same time not over-sharpening minute details (such as noise). We used \\nD\\n0\\n70\\n=\\n,\\n approximately 10% of the long image dimension, but other similar values would work also. \\nF\\nigure 4.57(b) is the result of highpass ﬁltering the original image (scaled as the images in Fig. 4.54). As \\nexpected, the image is rather featureless, but the important boundaries (e.g., the edges of the ribs) are', metadata={'source': 'imagepro.pdf', 'page': 294}),\n",
       " Document(page_content='clearly delineated. Figure 4.57(c) shows the advantage of high-frequency-emphasis ﬁltering, where we \\nused Eq. (4-133) with \\nk\\n1\\n05\\n=\\n.\\n and \\nk\\n2\\n07 5\\n=\\n..\\n Although the image is still dark, the gray-level tonality has \\nbeen restored,\\n with the added advantage of sharper features. \\nAs we discussed in Section 3.3, an image characterized by intensity levels in a narrow range of the \\ngray scale is an ideal candidate for histogram equalization. As Fig. 4.57(d) shows, this was indeed an \\nappropriate method to further enhance the image. Note the clarity of the bone structure and other \\ndetails that simply are not visible in any of the other three images. The ﬁnal enhanced image is a little \\nnoisy, but this is typical of X-ray images when their gray scale is expanded. The result obtained using a \\ncombination of high-frequency-emphasis and histogram equalization is superior to the result that would \\nbe obtained by using either method alone.\\nHOMOMORPHIC FILTERING', metadata={'source': 'imagepro.pdf', 'page': 294}),\n",
       " Document(page_content='The illumination-reflectance model introduced in Section 2.3 can be used to develop \\na frequency domain procedure for improving the appearance of an image by simul-\\ntaneous intensity range compression and contrast enhancement. From the discus-\\nsion in that section, an image \\nfx y\\n(,\\n)\\n can be expressed as the product of its illumina-\\ntion,\\n \\nixy\\n(,\\n) ,\\n and reflectance, \\nrxy\\n(,\\n) ,\\n components:\\n \\nfx y i x y rx y\\n(,\\n) (,) (,)\\n=\\n \\n(4-134)\\nThis equation cannot be used directly to operate on the frequency components of \\nillumination and reflectance because the F\\nourier transform of a product is not the \\nproduct of the transforms:\\n \\n/H5219/H5219 /H5219\\nfx\\ny i x y rx y\\n(,) (,) (,)\\n[] [ ] [ ]\\n≠\\n \\n(4-135)\\nHowever, suppose that we define\\n \\nzxy f xy\\nix\\ny rxy\\n(,) l n (,)\\nln ( , ) ln ( , )\\n=\\n=+\\n \\n(4-136)\\nThen,\\n \\n/H5219/H5219\\n/H5219/H5219\\nzx\\ny f xy\\nix\\nyr x y\\n(,) l n (,)\\nln ( , ) ln ( , )\\n[]\\n=\\n[]\\n=\\n[]\\n+\\n[]\\n \\n(4-137)\\nIf \\nf\\n(\\nx\\n, \\ny\\n) has any zero \\nvalues, a 1 must be added \\nto the image to avoid', metadata={'source': 'imagepro.pdf', 'page': 294}),\n",
       " Document(page_content='having to deal with ln(0). \\nThe 1 is then subtracted \\nfrom the ﬁnal result.\\nDIP4E_GLOBAL_Print_Ready.indb   293\\n6/16/2017   2:06:23 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 294}),\n",
       " Document(page_content='294\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nor\\n \\nZF F\\nir\\n(,) (,) (,)\\nuv uv uv\\n=+\\n \\n(4-138)\\nwhere \\nF\\ni\\n(,)\\nuv\\n and \\nF\\nr\\n(,)\\nuv\\n are the Fourier transforms of \\nln ( , )\\nix\\ny\\n and \\nln ( , ),\\nrx\\ny\\n \\nrespectively\\n.\\nWe can ﬁlter \\nZ\\n(,\\n)\\nuv\\n using a ﬁlter transfer function \\nH\\n(,\\n)\\nuv\\n so that\\n \\nSH Z\\nHF\\nHF\\nir\\n(,) (,) (,)\\n(,) (,) (,) (,)\\nuv uv uv\\nuv uv uv uv\\n=\\n=+\\n \\n(4-139)\\nThe filtered image in the spatial domain is then\\n \\nsxy S\\nHF\\nHF\\nir\\n(,) (,)\\n(,) (,) (,) (\\n,)\\n=\\n[]\\n=\\n[]\\n+\\n[]\\n−\\n−−\\n/H5219\\n/H5219/H5219\\n1\\n11\\nuv\\nuv uv\\nuv uv\\n \\n(4-140)\\nBy defining\\n \\n′\\n=\\n[]\\n−\\nix y H F\\ni\\n(,) (,) (,)\\n/H5219\\n1\\nuv uv\\n \\n(4-141)\\nand\\n \\n′\\n=\\n[]\\n−\\nrx y H F\\nr\\n(,) (,) (,)\\n/H5219\\n1\\nuv uv\\n \\n(4-142)\\nwe can express Eq. (4-140) in the form\\n \\nsxy i xy r xy\\n(,\\n) (,) (,)\\n=\\n′\\n+\\n′\\n \\n(4-143)\\nFinally, because \\nzxy\\n(,\\n)\\n was formed by taking the natural logarithm of the input \\nimage\\n, we reverse the process by taking the exponential of the ﬁltered result to form \\nthe output image:\\n \\ngxy e\\nee\\nix\\ny rx y\\nsx y\\nix y rx y\\n(,)\\n(,) (', metadata={'source': 'imagepro.pdf', 'page': 295}),\n",
       " Document(page_content='ix y rx y\\n(,)\\n(,) (\\n,)\\n(,)\\n(,) (,)\\n=\\n=\\n=\\n′′\\n00\\n \\n(4-144)\\nwhere\\n \\nix y e\\nix y\\n0\\n(,)\\n(,)\\n=\\n′\\n \\n(4-145)\\nand\\n \\nrx y e\\nrx y\\n0\\n(,)\\n(,)\\n=\\n′\\n \\n(4-146)\\nare the illumination and reflectance components of the output (processed) image.\\nF\\nigure 4.58 is a summary of the ﬁltering approach just derived. This method is \\nbased on a special case of a class of systems known as \\nhomomorphic systems\\n. In this \\nparticular application, the key to the approach is the separation of the illumination \\nDIP4E_GLOBAL_Print_Ready.indb   294\\n6/16/2017   2:06:25 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 295}),\n",
       " Document(page_content='4.9\\n  \\nImage Sharpening Using Highpass Filters\\n    \\n295\\nand reﬂectance components achieved in the form shown in Eq. (4-138). The \\nhomo-\\nmorphic ﬁlter transfer function\\n, \\nH\\n(,\\n) ,\\nuv\\n then can operate on these components sepa-\\nrately\\n, as indicated by Eq. (4-139).\\nThe illumination component of an image generally is characterized by slow spa-\\ntial variations, while the reﬂectance component tends to vary abruptly, particularly \\nat the junctions of dissimilar objects. These characteristics lead to associating the low \\nfrequencies of the Fourier transform of the logarithm of an image with illumination, \\nand the high frequencies with reﬂectance. Although these associations are rough \\napproximations, they can be used to advantage in image ﬁltering, as illustrated in \\nExample 4.23.\\nA good deal of control can be gained over the illumination and reﬂectance com-\\nponents with a homomorphic ﬁlter. This control requires speciﬁcation of a ﬁlter \\ntransfer function \\nH\\n(,\\n)\\nuv', metadata={'source': 'imagepro.pdf', 'page': 296}),\n",
       " Document(page_content='H\\n(,\\n)\\nuv\\n that affects the low- and high-frequency components of \\nthe F\\nourier transform in different, controllable ways. Figure 4.59 shows a cross sec-\\ntion of such a function. If the parameters \\ng\\nL\\n and \\ng\\nH\\n are chosen so that \\ng\\nL\\n<\\n1\\n and \\ng\\nH\\n≥\\n1,\\n the ﬁlter function in Fig. 4.59 will attenuate the contribution made by the \\nlow frequencies (illumination) and amplify the contribution made by high frequen-\\ncies (reﬂectance).\\n The net result is simultaneous dynamic range compression and \\ncontrast enhancement.\\nThe shape of the function in Fig. 4.59 can be approximated using a highpass ﬁlter \\ntransfer function. For example, using a slightly modiﬁed form of the GHPF function \\nyields the homomorphic function\\nHe\\nHL\\ncD D\\nL\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n()\\n−\\n⎡\\n⎣\\n⎤\\n⎦\\n+\\n−\\ngg\\ng\\n1\\n2\\n0\\n2\\n(4-147)\\nwhere \\nD\\n(,\\n)\\nuv\\n is defined in Eq. (4-112) and constant \\nc\\n controls the sharpness of the \\nslope of the function as it transitions between \\ng\\nL\\n and \\ng\\nH\\n.\\n This filter transfer function', metadata={'source': 'imagepro.pdf', 'page': 296}),\n",
       " Document(page_content='is similar to the high-frequenc\\ny-emphasis function discussed in the previous section.\\nA BHPF function would \\nwork well too, with the \\nadded advantage of more \\ncontrol over the sharp-\\nness of the transition \\nbetween \\ng\\nL\\n \\nand\\n g\\nH\\n. The \\ndisadvantage is the  \\npossibility of ringing for \\nhigh values of \\nn\\n.\\nln\\nexp\\nDFT\\n(DFT)\\n/H11002\\n1\\nH\\n(u, \\nv\\n)\\ng\\n(\\nx\\n, \\ny\\n)\\nf\\n(\\nx\\n, \\ny\\n)\\nFIGURE 4.58\\nSummary of steps \\nin homomorphic \\nﬁltering.\\ng\\nH\\ng\\nL\\n(,)\\nD\\nuv\\n(,)\\nH\\nuv\\nFIGURE 4.59\\nRadial cross  \\nsection of a  \\nhomomorphic \\nﬁlter transfer \\nfunction..\\nDIP4E_GLOBAL_Print_Ready.indb   295\\n6/16/2017   2:06:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 296}),\n",
       " Document(page_content='296\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nEXAMPLE 4.23 :  Homomorphic ﬁltering.\\nFigure 4.60(a) shows a full body PET (Positron Emission Tomography) scan of size \\n1162 746\\n×\\n pixels. \\nT\\nhe image is slightly blurred and many of its low-intensity features are obscured by the high intensity of \\nthe “hot spots” dominating the dynamic range of the display. (These hot spots were caused by a tumor in \\nthe brain and one in the lungs.) Figure 4.60(b) was obtained by homomorphic ﬁltering Fig. 4.60(a) using \\nthe ﬁlter transfer function in Eq. (4-147) with \\ng\\nL\\n=\\n04\\n.,\\n \\ng\\nH\\n=\\n30\\n.,\\n \\nc\\n=\\n5,\\n and \\nD\\n0\\n20\\n=\\n.\\n A radial cross sec-\\ntion of this function looks just like F\\nig. 4.59, but with a much sharper slope, and the transition between \\nlow and high frequencies much closer to the origin.\\nNote in Fig. 4.60(b) how much sharper the hot spots, the brain, and the skeleton are in the processed', metadata={'source': 'imagepro.pdf', 'page': 297}),\n",
       " Document(page_content='image, and how much more detail is visible in this image, including, for example, some of the organs, the \\nshoulders, and the pelvis region. By reducing the effects of the dominant illumination components (the \\nhot spots), it became possible for the dynamic range of the display to allow lower intensities to become \\nmore visible. Similarly, because the high frequencies are enhanced by homomorphic ﬁltering, the reﬂec-\\ntance components of the image (edge information) were sharpened considerably. The enhanced image \\nin Fig. 4.60(b) is a signiﬁcant improvement over the original. \\n4.10  SELECTIVE FILTERING  \\nThe filters discussed in the previous two sections operate over the entire frequency \\nrectangle. There are applications in which it is of interest to process specific bands of \\nfrequencies or small regions of the frequency rectangle. Filters in the first category \\n4.10\\nb a\\nFIGURE 4.60\\n(a) Full body PET \\nscan. (b) Image \\nenhanced using \\nhomomorphic \\nﬁltering. (Original \\nimage courtesy', metadata={'source': 'imagepro.pdf', 'page': 297}),\n",
       " Document(page_content='image courtesy \\nof Dr. Michael E. \\nCasey, CTI Pet \\nSystems.)\\nDIP4E_GLOBAL_Print_Ready.indb   296\\n6/16/2017   2:06:26 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 297}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n297\\nare called \\nband filters. \\nIf frequencies in the band are filtered out, the band filter is \\ncalled a \\nbandreject\\n filter; similarly, if the frequencies are passed, the filter is called \\na \\nbandpass\\n filter. Filters in the second category are called \\nnotch filters\\n. These filters \\nare further qualified as being \\nnotch reject\\n or \\nnotch pass\\n filters, depending on whether \\nfrequencies in the notch areas are rejected or passed.\\nBANDREJECT AND BANDPASS FILTERS\\nAs you learned in\\n \\nSection 3.7, bandpass and bandreject filter transfer functions in \\nthe frequenc\\ny domain can be constructed by combining lowpass and highpass filter \\ntransfer functions, with the latter also being derivable from lowpass functions (see \\nFig. 3.52). In other words, lowpass filter transfer functions are the basis for forming \\nhighpass, bandreject, and bandpass filter functions. Furthermore, a bandpass filter', metadata={'source': 'imagepro.pdf', 'page': 298}),\n",
       " Document(page_content='transfer function is obtained from a bandreject function in the same manner that we \\nobtained a highpass from a lowpass transfer function:\\n \\nHH\\nBP\\nBR\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-148)\\nFigure 4.61(a) shows how to construct an ideal bandreject ﬁlter (IBRF) transfer \\nfunction.\\n It consists of an ILPF and an IHPF function with different cutoff frequen-\\ncies. When dealing with bandpass functions, the parameters of interest are the width, \\nW\\n, and the center, \\nC\\n0\\n,\\n of the band. An equation for the IBRF function is easily \\nobtained by inspection from F\\nig, 4.61(a), as the leftmost entry in Table 4.7 shows. \\nThe key requirements of a bandpass transfer function are: (1) the values of the func-\\ntion must be in the range \\n[, ] ;\\n01\\n (2) the value of the function must be zero at a dis-\\ntance \\nC\\n0\\n from the origin (center) of the function; and (3) we must be able to specify \\na value for \\nW\\n. Clearly, the IBRF function just developed satisﬁes these requirements.', metadata={'source': 'imagepro.pdf', 'page': 298}),\n",
       " Document(page_content='Adding lowpass and highpass transfer functions to form Gaussian and Butter-\\nworth bandreject functions presents some difﬁculties. For example, Fig. 4.61(b) \\nshows a bandpass function formed as the sum of lowpass and highpass Gaussian \\nfunctions with different cutoff points. Two problems are immediately obvious: we \\nhave no direct control over \\nW\\n, and the value of \\nH\\n(,\\n)\\nuv\\n is not 0 at \\nC\\n0\\n.\\n We could \\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\nW\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\n1.0\\n0\\nC\\n(,)\\nH\\nuv\\n(,)\\nD\\nuv\\nb\\na\\nc\\nd\\nFIGURE 4.61\\n Radial cross sections. (a) Ideal bandreject ﬁlter transfer function. (b) Bandreject transfer function formed \\nby the sum of Gaussian lowpass and highpass ﬁlter functions. (The minimum is not 0 and does not align with \\nC\\n0\\n.)\\n  \\n(c) Radial plot of Eq.\\n (4-149). (The minimum is 0 and is properly aligned with \\nC\\n0\\n,\\n but the value at the origin is \\nnot 1.) (d) Radial plot of Eq.\\n (4-150); this Gaussian-shape plot meets all the requirements of a bandreject ﬁlter', metadata={'source': 'imagepro.pdf', 'page': 298}),\n",
       " Document(page_content='transfer function.\\nDIP4E_GLOBAL_Print_Ready.indb   297\\n6/16/2017   2:06:27 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 298}),\n",
       " Document(page_content='298\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\noffset the function and scale it so that values fall in the range \\n[, ] ,\\n01\\n but ﬁnding an \\nanalytical solution for the point where the lowpass and highpass Gaussian functions \\nintersect is impossible\\n, and this intersection would be required to solve for the cutoff \\npoints in terms of \\nC\\n0\\n.\\n The only alternatives are trial-and-error or numerical methods.\\nF\\nortunately, instead of adding lowpass and highpass transfer function, an alterna-\\ntive is to modify the expressions for the Gaussian and Butterworth highpass transfer \\nfunctions so that they will satisfy the three requirements stated earlier. We illustrate \\nthe procedure for a Gaussian function. In this case, we begin by changing the point \\nat which \\nH\\n(,)\\nuv\\n=\\n0 from \\nD\\n(,\\n)\\nuv\\n=\\n0 to \\nDC\\n(,\\n)\\nuv\\n=\\n0\\n in Eq. (4-120):\\n \\nHe\\nDC\\nW\\n(,)\\n(,)\\nuv\\nuv\\n=−\\n−\\n−\\n()\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n0\\n2\\n2\\n \\n(4-149)\\nA plot of this function [Fig. 4.61(c)] shows that, below \\nC\\n0\\n,', metadata={'source': 'imagepro.pdf', 'page': 299}),\n",
       " Document(page_content='C\\n0\\n,\\n the function behaves as a \\nlowpass Gaussian function,\\n at \\nC\\n0\\n the function will always be 0, and for values higher \\nthan \\nC\\n0\\n the function behaves as a highpass Gaussian function. Parameter \\nW\\n is pro-\\nportional to the standard deviation and thus controls the “width” of the band. The \\nonly problem remaining is that the function is not always 1 at the origin. A simple \\nmodification of Eq. (4-149) removes this shortcoming:\\n \\nHe\\nDC\\nDW\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=−\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n0\\n2\\n2\\n \\n(4-150)\\nNow, the exponent is infinite when \\nD\\n(,\\n) ,\\nuv\\n=\\n0\\n which makes the exponential term go \\nto zero and \\nH\\n(,\\n)\\nuv\\n=\\n1\\n at the origin, as desired. In this modification of Eq. (4-149), \\nthe basic Gaussian shape is preserved and the three requirements stated earlier are \\nsatisfied.\\n Figure 4.61(d) shows a plot of Eq. (4-150). A similar analysis leads to the \\nform of a Butterworth bandreject filter transfer function shown in Table 4.7.', metadata={'source': 'imagepro.pdf', 'page': 299}),\n",
       " Document(page_content='Figure 4.62 shows perspective plots of the ﬁlter transfer functions just discussed. \\nAt ﬁrst glance the Gaussian and Butterworth functions appear to be about the same, \\nbut, as before, the behavior of the Butterworth function is between the ideal and \\nGaussian functions. As Fig. 4.63 shows, this is easier to see by viewing the three ﬁlter \\nThe overall ratio in this \\nequation is squared so \\nthat, as the distance \\nincreases, Eqs. (4-149) \\nand (4-150) behave  \\napproximately the same.\\nIdeal (IBRF)\\nGaussian (GBRF)\\nButterworth (BBRF)\\nH\\nC\\nW\\nDC\\nW\\n(,)\\n(,)\\nuv\\nuv\\n=\\n−+\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n0\\n22\\n1\\n00\\nif \\notherwise\\n≤≤\\nHe\\nDC\\nDW\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=−\\n−\\n−\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n1\\n2\\n0\\n2\\n2\\nH\\nDW\\nDC\\nn\\n(,)\\n(,)\\n(,)\\nuv\\nuv\\nuv\\n=\\n+\\n−\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n1\\n1\\n2\\n0\\n2\\n2\\nTABLE \\n4.7\\nBandreject ﬁlter transfer functions. \\nC\\n0\\n is the center of the band, \\nW\\n is the width of the band, and \\nD\\n(,\\n)\\nuv\\n is the dis-\\ntance from the center of the transfer function to a point \\n(,)\\nuv\\n in the frequency rectangle.\\nDIP4E_GLOBAL_Print_Ready.indb   298', metadata={'source': 'imagepro.pdf', 'page': 299}),\n",
       " Document(page_content='6/16/2017   2:06:29 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 299}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n299\\nfunctions as images. Increasing the order of the Butterworth function would bring it \\ncloser to the ideal bandreject transfer function. \\nNOTCH FILTERS\\nNotch filters are the most useful of the selective filters. A notch filter rejects (or \\npasses) frequencies in a predefined neighborhood of the frequency rectangle. Zero-\\nphase-shift filters must be symmetric about the origin (center of the frequency \\nrectangle), so a notch filter transfer function with center at \\n(,)\\nuv\\n00\\n must have a \\ncorresponding notch at location \\n(,) .\\n−−\\nuv\\n00\\n \\nNotch reject\\n filter transfer functions are \\nconstructed as products of highpass filter transfer functions whose centers have \\nbeen translated to the centers of the notches. The general form is:\\n \\nHH H\\nk\\nk\\nQ\\nk\\nNR\\n(,) (,) (,)\\nuv uv uv\\n=\\n=\\n−\\n∏\\n1\\n \\n(4-151)\\nwhere \\nH\\nk\\n(,)\\nuv\\n and \\nH\\nk\\n−\\n(,)\\nuv\\n are highpass filter transfer functions whose centers are \\nat \\n(,)\\nuv\\nkk\\n and \\n(,) ,\\n−−\\nuv\\nkk', metadata={'source': 'imagepro.pdf', 'page': 300}),\n",
       " Document(page_content='(,) ,\\n−−\\nuv\\nkk\\n respectively. These centers are specified with respect to \\nthe center of the frequency rectangle, \\nMN\\n22\\n,,\\n()\\n where, as usual, \\nM\\n and \\nN\\n are the \\nb a\\nc\\nFIGURE 4.62\\n Perspective plots of (a) ideal, (b) modiﬁed Gaussian, and (c) modiﬁed Butterworth (of order 1) bandre-\\nject ﬁlter transfer functions from Table 4.7. All transfer functions are of size \\n512 512\\n×\\n elements, with \\nC\\n0\\n128\\n=\\n and \\nW\\n=\\n60.\\nu\\nv\\n(,)\\nHuv\\nu\\nv\\nHuv\\nu\\nv\\n(,)\\nHuv\\n(,)\\nb a\\nc\\nFIGURE 4.63\\n(a) The ideal,  \\n(b) Gaussian, and \\n(c) Butterworth \\nbandpass transfer \\nfunctions from \\nFig. 4.62, shown \\nas images. (The \\nthin border lines \\nare not part of the \\nimage data.) \\nDIP4E_GLOBAL_Print_Ready.indb   299\\n6/16/2017   2:06:34 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 300}),\n",
       " Document(page_content='300\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nnumber of rows and columns in the input image. Thus, the distance computations for \\neach filter transfer function are given by\\n \\nDu M u N\\nkk k\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n=− − + − −\\n⎡\\n⎣\\n⎤\\n⎦\\n22\\n22\\n12\\n \\n(4-152)\\nand\\n \\nDu M u N\\nkk k\\n−\\n=− + + − +\\n⎡\\n⎣\\n⎤\\n⎦\\n(,) (\\n) ( )\\n/\\nuv\\nv v\\n22\\n22\\n12\\n \\n(4-153)\\nFor example, the following is a Butterworth notch reject filter transfer function of \\norder \\nn\\n,\\n containing three notch pairs:\\n \\nH\\nDD\\nDD\\nkk\\nn\\nk\\nkk\\nn\\nNR\\n(,)\\n(,\\n)(\\n,\\n)\\nuv\\nuv\\nuv\\n=\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n+\\n[]\\n⎡\\n⎣\\n⎢\\n=\\n−\\n∏\\n1\\n1\\n1\\n1\\n0\\n1\\n3\\n0\\n⎢ ⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n \\n(4-154)\\nwhere \\nD\\nk\\n(,)\\nuv\\n and \\nD\\nk\\n−\\n(,)\\nuv\\n are given by Eqs. (4-152) and (4-153). The constant \\nD\\nk\\n0\\n \\nis the same for each pair of notches, but it can be different for different pairs. Other \\nnotch reject filter functions are constructed in the same manner, depending on the \\nhighpass filter function chosen. As with the filters discussed earlier, a notch pass', metadata={'source': 'imagepro.pdf', 'page': 301}),\n",
       " Document(page_content='filter transfer function is obtained from a notch reject function using the expression\\n \\nHH\\nNP\\nNR\\n(,) (,)\\nuv uv\\n=−\\n1\\n \\n(4-155)\\nAs the next two examples show, one of the principal applications of notch ﬁlter-\\ning is for selectively modifying local regions of the DFT\\n. Often, this type of pro-\\ncessing is done interactively, working directly with DFTs obtained without padding. \\nThe advantages of working interactively with actual DFTs (as opposed to having to \\n“translate” from padded to actual frequency values) generally outweigh any wrap-\\naround errors that may result from not using padding in the ﬁltering process. If nec-\\nessary, after an acceptable solution is obtained, a ﬁnal result using padding can be \\ngenerated by adjusting all ﬁlter parameters to compensate for the padded DFT size. \\nThe following two examples were done without padding. To get an idea of how DFT \\nvalues change as a function of padding, see Problem 4.42.', metadata={'source': 'imagepro.pdf', 'page': 301}),\n",
       " Document(page_content='EXAMPLE 4.24 :  Using notch ﬁltering to remove moiré patterns from digitized printed media images.\\nFigure 4.64(a) is the scanned newspaper image used in Fig. 4.21, showing a prominent moiré pattern, and \\nFig. 4.64(b) is its spectrum. The Fourier transform of a pure sine, which is a periodic function, is a pair of \\nconjugate symmetric impulses (see Table 4.4). The symmetric “impulse-like” bursts in Fig. 4.64(b) are a \\nresult of the near periodicity of the moiré pattern. We can attenuate these bursts by using notch ﬁltering. \\nFigure 4.64(c) shows the result of multiplying the DFT of Fig. 4.64(a) by a Butterworth notch reject \\ntransfer function with \\nD\\n0\\n9\\n=\\n and \\nn\\n=\\n4\\n for all notch pairs (the centers of the notches are coincide with \\nthe centers of the black circular regions in the ﬁgure).\\n The value of the radius was selected (by visual \\ninspection of the spectrum) to encompass the energy bursts completely, and the value of \\nn\\n was selected', metadata={'source': 'imagepro.pdf', 'page': 301}),\n",
       " Document(page_content='n\\n was selected \\nto produce notches with sharp transitions. The locations of the center of the notches were determined \\nDIP4E_GLOBAL_Print_Ready.indb   300\\n6/16/2017   2:06:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 301}),\n",
       " Document(page_content='4.10\\n  \\nSelective Filtering\\n    \\n301\\ninteractively from the spectrum. Figure 4.64(d) shows the result obtained with this ﬁlter transfer func-\\ntion, using the ﬁltering procedure outlined in Section 4.7. The improvement is signiﬁcant, considering \\nthe low resolution and degree of degradation of the original image. \\nb a\\nd c\\nFIGURE 4.64\\n (a) Sampled \\nnewspaper  \\nimage showing a \\nmoiré pattern.  \\n(b) Spectrum.  \\n(c) Fourier  \\ntransform \\nmultiplied by \\na Butterworth \\nnotch reject ﬁlter \\ntransfer function. \\n(d) Filtered image. \\nDIP4E_GLOBAL_Print_Ready.indb   301\\n6/16/2017   2:06:35 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 302}),\n",
       " Document(page_content='302\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nEXAMPLE 4.25 :  Using notch ﬁltering to remove periodic interference.\\nFigure 4.65(a) shows an image of part of the rings surrounding the planet Saturn. This image was cap-\\ntured by \\nCassini\\n, the ﬁrst spacecraft to enter the planet’s orbit. The nearly sinusoidal pattern visible in \\nthe image was caused by an AC signal superimposed on the camera video signal just prior to digitizing \\nthe image. This was an unexpected problem that corrupted some images from the mission. Fortunately, \\nthis type of interference is fairly easy to correct by postprocessing. One approach is to use notch ﬁltering. \\nFigure 4.65(b) shows the DFT spectrum. Careful analysis of the vertical axis reveals a series of \\nsmall bursts of energy near the origin which correspond to the nearly sinusoidal interference. A simple \\napproach is to use a narrow notch rectangle ﬁlter starting with the lowest frequency burst, and extending', metadata={'source': 'imagepro.pdf', 'page': 303}),\n",
       " Document(page_content='for the remainder of the vertical axis. Figure 4.65(c) shows the transfer function of such a ﬁlter (white \\nrepresents 1 and black 0). Figure 4.65(d) shows the result of processing the corrupted image with this \\nﬁlter. This result is a signiﬁcant improvement over the original image.\\nTo obtain and image of just the interference pattern, we isolated the frequencies in the vertical axis \\nusing a notch pass transfer function, obtained by subtracting the notch reject function from 1 [see \\nFig. 4.66(a)]. T\\nhen, as Fig. 4.66(b) shows, the IDFT of the ﬁltered image is the spatial interference pattern.\\nb a\\nd c\\nFIGURE 4.65\\n(a) Image of  \\nSaturn rings \\nshowing nearly \\nperiodic  \\ninterference.  \\n(b) Spectrum. \\n(The bursts of \\nenergy in the  \\nvertical axis \\nnear the origin \\ncorrespond to \\nthe interference \\npattern).  \\n(c) A vertical \\nnotch reject ﬁlter \\ntransfer function.  \\n(d) Result of \\nﬁltering.  \\n(The thin black \\nborder in (c) is \\nnot part of the \\ndata.) (Original \\nimage courtesy', metadata={'source': 'imagepro.pdf', 'page': 303}),\n",
       " Document(page_content='image courtesy \\nof Dr. Robert A. \\nWest, NASA/\\nJPL.) \\nDIP4E_GLOBAL_Print_Ready.indb   302\\n6/16/2017   2:06:36 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 303}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n303\\n4.11 THE FAST FOURIER TRANSFORM \\nWe have focused attention thus far on theoretical concepts and on examples of fil-\\ntering in the frequency domain. One thing that should be clear by now is that com-\\nputational requirements in this area of image processing are not trivial. Thus, it is \\nimportant to develop a basic understanding of methods by which Fourier transform \\ncomputations can be simplified and speeded up. This section deals with these issues. \\nSEPARABILITY OF THE 2-D DFT\\nAs mentioned in Table 4.3, the 2-D DFT is separable into 1-D transforms. We can \\nwrite Eq. (4-67) as\\n \\nFef x y e\\nFx e\\nju x M\\ny\\nN\\nx\\nM\\njy N\\nju x M\\n(,)\\n(,)\\n(,)\\nuv\\nv\\nv\\n=\\n=\\n−\\n=\\n−\\n=\\n−\\n−\\n−\\n∑ ∑\\n2\\n0\\n1\\n0\\n1\\n2\\n2\\npp\\np\\nx x\\nM\\n=\\n−\\n∑\\n0\\n1\\n \\n(4-156)\\nwhere\\n \\nFx fx ye\\ny\\nN\\njy N\\n(,) (,)\\nv\\nv\\n=\\n=\\n−\\n−\\n∑\\n0\\n1\\n2\\np\\n \\n(4-157)\\nFor one value of \\nx,\\n and for \\nv\\n=−\\n012\\n1\\n,,, , ,\\n…\\nN\\n we see that \\nFx\\n(,\\n)\\nv\\n is the 1-D DFT of \\none\\n row of \\nfx y\\n(,\\n) .\\n By varying \\nx\\n from 0 to \\nM\\n−', metadata={'source': 'imagepro.pdf', 'page': 304}),\n",
       " Document(page_content='x\\n from 0 to \\nM\\n−\\n1 in Eq. (4-157), we compute a set of \\n1-D DFTs for all rows of \\nfx y\\n(,\\n) .\\n The computations in Eq. (4-156) similarly are 1-D \\ntransforms of the columns of \\nFx\\n(,\\n) .\\nv\\n Thus, we conclude that the 2-D DFT of \\nfx y\\n(,\\n)\\n \\ncan be obtained by computing the 1-D transform of each row of \\nfx y\\n(,\\n)\\n and then \\ncomputing the 1-D transform along each column of the result.\\n This is an important \\nsimplification because we have to deal only with one variable at a time. A similar \\ndevelopment applies to computing the 2-D IDFT using the 1-D IDFT. However, \\nas we show in the following section, we can compute the IDFT using an algorithm \\n4.11\\nWe could have formu-\\nlated the preceding \\ntwo equations to show \\nthat a 2-D DFT can be \\nobtained by computing \\nthe 1-D DFT of each \\ncolumn\\n of the input \\nimage followed by 1-D \\ncomputations on the \\nrows of the result. \\nb a\\nFIGURE 4.66\\n(a) Notch pass \\nﬁlter function \\nused to isolate \\nthe vertical axis \\nof the DFT of Fig. \\n4.65(a).', metadata={'source': 'imagepro.pdf', 'page': 304}),\n",
       " Document(page_content='4.65(a).  \\n(b) Spatial pattern \\nobtained by  \\ncomputing the \\nIDFT of (a). \\nDIP4E_GLOBAL_Print_Ready.indb   303\\n6/16/2017   2:06:37 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 304}),\n",
       " Document(page_content='304\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ndesigned to compute the forward DFT, so all 2-D Fourier transform computations \\nare reduced to multiple passes of a 1-D algorithm designed for computing the 1-D \\nDFT.\\nCOMPUTING THE IDFT USING A DFT ALGORITHM\\nTaking the complex conjugate of both sides of Eq. (4-68) and multiplying the results \\nby \\nMN\\n yields\\n \\nMNf x y F e\\nju x My N\\nN\\nu\\nM\\n** ( )\\n(,) (,)\\n=\\n−+\\n=\\n−\\n=\\n−\\n∑ ∑\\nuv\\nv\\nv\\n2\\n0\\n1\\n0\\n1\\np\\n \\n(4-158)\\nBut, we recognize the form of the right side of this result as the DFT of \\nF\\n*\\n(,) .\\nuv\\n There-\\nfore\\n, Eq. (4-158) indicates that if we substitute \\nF\\n*\\n(,)\\nuv\\n into an algorithm designed to \\ncompute the 2-D forward F\\nourier transform, the result will be \\nMNf x y\\n*\\n(,) .\\n Taking \\nthe complex conjugate and dividing this result by \\nMN\\n yields \\nfx y\\n(,\\n) ,\\n which is the \\ninverse of \\nF\\n(,\\n) .\\nuv\\n \\nComputing the 2-D inverse from a 2-D forward DFT algorithm that is based on', metadata={'source': 'imagepro.pdf', 'page': 305}),\n",
       " Document(page_content='successive passes of 1-D transforms (as in the previous section) is a frequent source \\nof confusion involving the complex conjugates and multiplication by a constant,\\n nei-\\nther of which is done in the 1-D algorithms. The key concept to keep in mind is that \\nwe simply input \\nF\\n*\\n(,)\\nuv\\n into whatever forward algorithm we have. The result will be \\nMNf x y\\n*\\n(,) .\\n All we have to do with this result to obtain \\nfx y\\n(,\\n)\\n is to take its complex \\nconjugate and divide it by the constant \\nMN\\n.\\n Of course, when \\nfx y\\n(,\\n)\\n is real, as typi-\\ncally is the case\\n, then \\nfx y f x y\\n*\\n(,) (,) .\\n=\\nTHE FAST FOURIER TRANSFORM (FFT)\\nWork in the frequency domain would not be practical if we had to implement \\nEqs. (4-67) and (4-68) directly. Brute-force implementation of these equations \\nrequires on the order of \\nMN\\n()\\n2\\n multiplications and additions. For images of moder-\\nate size (say, \\n2048 2048\\n×\\n pixels), this means on the order of 17 trillion multiplica-\\ntions and additions for just one 2-D DFT', metadata={'source': 'imagepro.pdf', 'page': 305}),\n",
       " Document(page_content=', excluding the exponentials, which could be \\ncomputed once and stored in a look-up table. Without the discovery of the \\nfast Fou-\\nrier transform\\n (FFT), which reduces computations to the order of \\nMN MN\\nlog\\n2\\n mul-\\ntiplications and additions, it is safe to say that the material presented in this chapter \\nwould be of little practical value. The computational reductions afforded by the FFT \\nare impressive indeed. For example, computing the 2-D FFT of a \\n2048 2048\\n×\\n image \\nwould require on the order of 92 million multiplication and additions\\n, which is a \\nsignificant reduction from the one trillion computations mentioned above.\\nAlthough the FFT is a topic covered extensively in the literature on signal pro-\\ncessing, this subject matter is of such signiﬁcance in our work that this chapter would \\nbe incomplete if we did not provide an introduction explaining why the FFT works \\nas it does. The algorithm we selected to accomplish this objective is the so-called \\nsuccessive-doubling method', metadata={'source': 'imagepro.pdf', 'page': 305}),\n",
       " Document(page_content=', which was the original algorithm that led to the birth \\nof an entire industry. This particular algorithm assumes that the number of samples \\nis an integer power of 2, but this is not a general requirement of other approaches \\nDIP4E_GLOBAL_Print_Ready.indb   304\\n6/16/2017   2:06:38 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 305}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n305\\n(Brigham [1988]).We know from the previous section that 2-D DFTs can be imple-\\nmented by successive passes of the 1-D transform, so we need to focus only on the \\nFFT of one variable.\\nIn derivations of the FFT, it is customary to express Eq. (4-44) in the form\\n \\nFu f xW\\nM\\nux\\nx\\nM\\n()\\n=\\n()\\n=\\n−\\n∑\\n0\\n1\\n \\n(4-159)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nM\\n where\\n \\nWe\\nM\\njM\\n=\\n−\\n2\\np\\n \\n(4-160)\\nand \\nM\\n is assumed to be of the form\\n \\nM\\np\\n=\\n2  \\n(4-161)\\nwhere \\np\\n is a positive integer\\n. Then it follows that \\nM\\n can be expressed as\\n \\nMK\\n=\\n2  \\n(4-162)\\nwith \\nK\\n being a positive integer also\\n. Substituting Eq. (4-162) into Eq. (4-159) yields\\n \\nFu fxW\\nfx\\nW fx W\\nK\\nux\\nx\\nK\\nK\\nux\\nx\\nK\\nK\\nux\\n() ()\\n() ( )\\n=\\n=+ +\\n=\\n−\\n()\\n=\\n−\\n+\\n∑\\n∑\\n2\\n0\\n21\\n2\\n2\\n0\\n1\\n2\\n2\\n22\\n1\\n1 1\\n0\\n1\\n()\\n=\\n−\\n∑\\nx\\nK\\n \\n(4-163)\\nHowever, it can be shown using Eq. (4-160) that \\nWW\\nK\\nux\\nK\\nux\\n2\\n2\\n=\\n,\\n so Eq. (4-163) can be \\nwritten as\\n \\nFu f x W f x W W\\nK\\nux\\nK\\nux\\nx\\nK\\nK\\nu\\nx\\nK\\n() ( ) ( )\\n=+ +\\n=\\n−\\n=\\n−\\n∑\\n∑\\n22 1\\n0\\n1\\n2\\n0\\n1\\n \\n(4-164)\\nDefining', metadata={'source': 'imagepro.pdf', 'page': 306}),\n",
       " Document(page_content='(4-164)\\nDefining\\n \\nFu f x W\\nK\\nux\\nx\\nK\\neven\\n() ( )\\n=\\n=\\n−\\n∑\\n2\\n0\\n1\\n \\n(4-165)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nK\\n and\\n \\nFu f x W\\nK\\nux\\nx\\nK\\nodd\\n() ( )\\n=+\\n=\\n−\\n∑\\n21\\n0\\n1\\n \\n(4-166)\\nfor \\nu\\n=−\\n012\\n1\\n,,, , ,\\n…\\nK\\n reduces Eq. (4-164) to\\n \\nFu F u F uW\\nK\\nu\\n() () ()\\n=+\\neven\\nodd 2\\n \\n(4-167)\\nDIP4E_GLOBAL_Print_Ready.indb   305\\n6/16/2017   2:06:39 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 306}),\n",
       " Document(page_content='306\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nAlso, because \\nWW\\nM\\nuK\\nK\\nu\\n+\\n=\\n and \\nWW\\nK\\nuK\\nK\\nu\\n22\\n+\\n=−\\n, it follows that\\n \\nFu K F u F uW\\nK\\nu\\n( ) () ()\\n+= −\\neven\\nodd 2\\n \\n(4-168)\\nAnalysis of Eqs. (4-165) through (4-168) reveals some important (and surprising) \\nproperties of these expressions\\n. An \\nM\\n-point DFT can be computed by dividing the \\noriginal expression into two parts, as indicated in Eqs. (4-167) and (4-168). Comput-\\ning the first half of \\nFu\\n()\\n requires evaluation of the two \\n()\\nM\\n2 -point\\n transforms giv-\\nen in Eqs\\n. (4-165) and (4-166). The resulting values of \\nFu\\neven\\n()\\n and \\nFu\\nodd\\n()\\n are then \\nsubstituted into Eq.\\n (4-167) to obtain \\nFu\\n()\\n for \\nu\\n=−\\n012 2 1\\n,,, , ( ) .\\n…\\nM\\n The other \\nhalf then follows directly from Eq.\\n (4-168) \\nwithout\\n additional transform evaluations.\\n It is of interest to examine the computational implications of the preceding pro-\\ncedure. Let \\n/H5258\\n()\\np\\n and \\n/H5246\\n()\\np\\n represent the number of complex multiplications and', metadata={'source': 'imagepro.pdf', 'page': 307}),\n",
       " Document(page_content='additions\\n, respectively, required to implement the method. As before, the number \\nof samples is \\n2\\np\\n,\\n where \\np\\n is a positive integer\\n. Suppose ﬁrst that \\np\\n=\\n1\\n so that the \\nnumber of samples is two\\n. A two-point transform requires the evaluation of \\nF\\n()\\n;\\n0  \\nthen \\nF\\n()\\n1\\n follows from Eq. (4-168). To obtain \\nF\\n()\\n0\\n requires computing \\nF\\neven\\n()\\n0\\n and \\nF\\nodd\\n() .\\n0\\n In this case \\nK\\n=\\n1\\n and Eqs. (4-165) and (4-166) are one-point transforms. \\nHowever\\n, because the DFT of a single sample point is the sample itself, no multipli-\\ncations or additions are required to obtain \\nF\\neven\\n()\\n0\\n and \\nF\\nodd\\n() .\\n0\\n One multiplication \\nof \\nF\\nodd\\n()\\n0\\n by \\nW\\n2\\n0\\n and one addition yields \\nF\\n()\\n0\\n from Eq. (4-167). Then \\nF\\n()\\n1\\n follows \\nfrom Eq.\\n (4-168) with one more addition (subtraction is considered to be the same \\nas addition). Because \\nFW\\nodd\\n()\\n0\\n2\\n0\\n has been computed already, the total number of \\noperations required for a two-point transform consists of \\n/H5258\\n()\\n11\\n=', metadata={'source': 'imagepro.pdf', 'page': 307}),\n",
       " Document(page_content='/H5258\\n()\\n11\\n=\\n multiplication \\nand \\n/H5246\\n()\\n12\\n=\\n additions.\\nT\\nhe next allowed value for \\np\\n is 2. According to the preceding development, a four-\\npoint transform can be divided into two parts. The ﬁrst half of \\nFu\\n()\\n requires evaluation \\nof two\\n, two-point transforms, as given in Eqs. (4-165) and (4-166) for \\nK\\n=\\n2.\\n A two-point \\ntransform requires \\n/H5258\\n()\\n1\\n multiplications and \\n/H5246\\n()\\n1\\n additions. Therefore, evaluation of \\nthese two equations requires a total of \\n21\\n/H5258\\n()\\n multiplications and \\n21\\n/H5246\\n()\\n additions. Two \\nfurther multiplications and additions are necessary to obtain \\nF\\n()\\n0\\n and \\nF\\n()\\n1\\n from Eq. \\n(4-167).\\n Because \\nFu W\\nK\\nu\\nodd\\n()\\n2\\n has been computed already for \\nu\\n=\\n{}\\n01\\n,,\\n two more \\nadditions give \\nF\\n()\\n2\\n and \\nF\\n()\\n.\\n3\\n The total is then \\n/H5258/H5258\\n()\\n()\\n22 1 2\\n=+\\n and \\n/H5246/H5246\\n()\\n() .\\n22 1 4\\n=+\\nWhen \\np\\n is equal to 3,\\n two four-point transforms are needed to evaluate \\nFu\\neven\\n()\\n \\nand \\nFu\\nodd\\n() .\\n They require \\n22\\n/H5258\\n()', metadata={'source': 'imagepro.pdf', 'page': 307}),\n",
       " Document(page_content='22\\n/H5258\\n()\\n multiplications and \\n22\\n/H5246\\n()\\n additions. Four more \\nmultiplications and eight more additions yield the complete transform.\\n The total \\nthen is then \\n/H5258/H5258\\n()\\n()\\n32 24\\n=+\\n multiplication and \\n/H5246/H5246\\n()\\n()\\n32 28\\n=+\\n additions.\\nContinuing this argument for any positive integer \\np\\n leads to recursive expressions \\nfor the number of multiplications and additions required to implement the FFT\\n:\\n \\n/H5258/H5258\\n()\\n( )\\npp p\\np\\n=− +\\n−\\n21 2 1\\n1\\n≥\\n \\n(4-169)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   306\\n6/16/2017   2:06:43 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 307}),\n",
       " Document(page_content='4.11\\n  \\nThe Fast Fourier Transform\\n    \\n307\\n \\n/H5246/H5246\\n()\\n( )\\npp p\\np\\n=− +\\n21 2 1\\n≥\\n \\n(4-170)\\nwhere \\n/H5258\\n()\\n00\\n=\\n and \\n/H5246\\n()\\n00\\n=\\n because the transform of a single point does not \\nrequire any multiplication or additions\\n.\\nThe method just developed is called the \\nsuccessive doubling FFT algorithm\\n \\nbecause it is based on computing a two-point transform from two one-point trans-\\nforms, a four-point transform from two two-point transforms, and so on, for any \\nM\\n \\nequal to an integer power of 2. It is left as an exercise (see Problem 4.63) to show \\nthat\\n \\n/H5258\\n() l o g\\npM M\\n=\\n1\\n2\\n2\\n \\n(4-171)\\nand\\n \\n/H5246\\n()\\nl o g\\nnM M\\n=\\n2\\n \\n(4-172)\\nwhere \\nM\\np\\n=\\n2.\\nThe computational advantage of the FFT over a direct implementation of the 1-D \\nDFT is deﬁned as\\n \\nCM\\nM\\nMM\\nM\\nM\\n()\\nlog\\nlog\\n=\\n=\\n2\\n2\\n2\\n \\n(4-173)\\nwhere \\nM\\n2\\n is the number of operations required for a “brute force” implementation \\nof the 1-D DFT. Because it is assumed that \\nM\\np\\n=\\n2,\\n we can write Eq. (4-173) in \\nterms of \\np\\n:\\n \\nCp\\np\\np\\n()', metadata={'source': 'imagepro.pdf', 'page': 308}),\n",
       " Document(page_content='p\\n:\\n \\nCp\\np\\np\\n()\\n=\\n2\\n \\n(4-174)\\nA plot of this function (Fig. 4.67) shows that the computational advantage increases \\nrapidly as a function of \\np\\n.\\n For example, when \\np\\n=\\n15\\n (32,768 points), the FFT has \\nnearly a 2,200 to 1 advantage over a brute-force implementation of the DFT\\n. Thus, \\nwe would expect that the FFT can be computed nearly 2,200 times faster than the \\nDFT on the same machine. As you learned in Section 4.1, the FFT also offers signifi-\\ncant computational advantages over spatial filtering, with the cross-over between \\nthe two approaches being for relatively small kernels.\\nThere are many excellent sources that cover details of the FFT so we will not \\ndwell on this topic further (see, for example, Brigham [1988]). Most comprehensive \\nsignal and image processing software packages contain generalized implementa-\\ntions of the FFT that do not require the number of points to be an integer power \\nDIP4E_GLOBAL_Print_Ready.indb   307\\n6/16/2017   2:06:44 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 308}),\n",
       " Document(page_content='308\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n1\\nC\\n(\\np\\n)\\n0\\n600\\n1200\\n1800\\n2400\\n234567\\np\\n8 9 10 11 12 13 14 15\\n2\\n()\\np\\nCp\\np\\n=\\nFIGURE 4.67\\nComputational \\nadvantage of the \\nFFT over a direct \\nimplementation \\nof the 1-D DFT. \\nThe number of \\nsamples is \\nM\\np\\n=\\n2.\\n \\nT\\nhe computational \\nadvantage increases \\nrapidly as a  \\nfunction of \\np\\n. \\nof 2 (at the expense of slightly less efﬁcient computation). Free FFT programs also \\nare readily available, principally over the internet.\\nSummary, References, and Further Reading\\n \\nThe material in this chapter is a progression from sampling to the Fourier transform, and then to ﬁltering in the \\nfrequency domain. Some of the concepts, such as the sampling theorem, make very little sense if not explained in \\nthe context of the frequency domain. The same is true of effects such as aliasing. Thus, the material developed in \\nthe preceding sections is a solid foundation for understanding the fundamentals of 2-D digital signal processing. We', metadata={'source': 'imagepro.pdf', 'page': 309}),\n",
       " Document(page_content='took special care to develop the material starting with basic principles, so that any reader with a modest mathemati-\\ncal background would be in a position not only to absorb the material, but also to apply it.\\nFor complementary reading on the 1-D and 2-D continuous Fourier transforms, see the books by Bracewell \\n[1995, 2003]. These two books, together with Castleman [1996], Petrou and Petrou [2010], Brigham [1988], and \\nSmith [2003], provide additional background for the material in Sections 4.2 through 4.6. Sampling phenomena \\nsuch as aliasing and moiré patterns are topics amply illustrated in books on computer graphics, as exempliﬁed by \\nHughes and Andries [2013]. For additional general background on the material in Sections 4.7 through 4.11 see \\nHall [1979], Jain [1989], Castleman [1996], and Pratt [2014]. For details on the software aspects of many of the ex-\\namples in this chapter, see Gonzalez, Woods, and Eddins [2009].\\nProblems', metadata={'source': 'imagepro.pdf', 'page': 309}),\n",
       " Document(page_content='Problems\\n \\nSolutions to the problems marked with an asterisk (*) are in the DIP4E Student Support Package (consult the book \\nwebsite: www.ImageProcessingPlace.com)\\n4.1 \\nAnswer the following:\\n(a) * \\nGive an equation similar to Eq. (4-10), but \\nfor an impulse located at \\ntt\\n=\\n0\\n.\\n(b) \\nRepeat for Eq. (4-15).\\n(c) * \\nIs it correct to say that \\ndd\\n()\\n()\\nta at\\n−= −\\n in \\ngeneral? Explain.\\n4.2 \\nRepeat Example 4.1, but using the function \\nft A\\n()\\n=\\n for \\n0\\n≤<\\ntT\\n and \\nft\\n()\\n=\\n0\\n for all other \\nvalues of \\nt\\n.\\n Explain the reason for any differences \\nbetween your results and the results in the exam-\\nple.\\n4.3 \\nWhat is the convolution of two, 1-D impulses: \\n(a) * \\nd\\n()\\nt\\n and \\nd\\n()\\n?\\ntt\\n−\\n0\\n(b) \\nd\\n()\\ntt\\n−\\n0\\n and \\nd\\n() ?\\ntt\\n+\\n0\\nDIP4E_GLOBAL_Print_Ready.indb   308\\n6/16/2017   2:06:45 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 309}),\n",
       " Document(page_content='Problems\\n    \\n309\\n4.4 * \\nUse the sifting property of the impulse to show \\nthat convolving a 1-D continuous function,\\n \\nft\\n()\\n,\\n \\nwith an impulse located at \\nt\\n0\\n shifts the function \\nso that its origin is moved to the location of the \\nimpulse (if the impulse is at the origin, the func-\\ntion is not shifted).\\n4.5 * \\nWith reference to Fig. 4.9, give a graphical illustra-\\ntion of an aliased pair of functions that are not \\nperiodic\\n.\\n4.6 \\nWith reference to Fig. 4.11:\\n(a) * \\nRedraw the ﬁgure, showing what the dots \\nwould look like for a sampling rate that \\nexceeds the Nyquist rate slightly\\n.\\n(b) \\nWhat is the \\nappro\\nximate\\n sampling rate repre-\\nsented by the large dots in Fig. 4.11?\\n(c) \\nApproximately\\n,\\n what would be the lowest \\nsampling rate that you would use so that (1) \\nthe Nyquist rate is satisﬁed, and (2) the sam-\\nples look like a sine wave?\\n4.7 \\nA function, \\nft\\n()\\n,\\n is formed by the sum of three \\nfunctions\\n, \\nft A t\\n1\\n( ) sin( ),\\n=\\np\\n \\nft B t\\n2\\n4\\n( ) sin( ),\\n=\\np\\n \\nand \\nft C t\\n3', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='=\\np\\n \\nand \\nft C t\\n3\\n8\\n( ) cos( ).\\n=\\np\\n(a) \\nAssuming that the functions extend to inﬁn-\\nity in both directions\\n, what is the highest fre-\\nquency of \\nft\\n()\\n?\\n (\\nHint:\\n Start by ﬁnding the \\nperiod of the sum of the three functions\\n.)\\n(b) * \\nWhat is the Nyquist rate corresponding to \\nyour result in (a)? (Give a numerical answer\\n.)\\n(c) \\nAt what rate would you sample \\nft\\n()\\n so that \\nperfect recovery of the function from its \\nsamples is possible?\\n4.8 * \\nShow that \\n/H5219\\n{}\\n() ,\\net\\njt t\\n2\\n0\\n0\\np\\ndm\\n=−\\n where \\nt\\n0\\n is a con-\\nstant. (\\nHint:\\n Study Example 4.2.) \\n4.9 \\nShow that the following expressions are true. \\n(\\nHint:\\n Make use of the solution to Problem 4.8):\\n(a) * \\n/H5219\\ncos( ) ( ) ( )\\n2\\n1\\n2\\n00 0\\npm d m m d m m\\nt\\n{}\\n=− + +\\n[]\\n(b) \\n/H5219\\nsin( ) ( ) ( )\\n2\\n1\\n2\\n00 0\\npm d m m d m m\\nt\\nj\\n{}\\n=− − +\\n[]\\n4.10 \\nConsider the function \\nft n t\\n(\\n) sin( ),\\n=\\n2\\np\\n where \\nn\\n is an integer\\n. Its Fourier transform, \\nF\\n()\\n,\\nm\\n is \\npurely imaginary (see Problem 4.9).\\n Because the \\ntransform, \\n/tildenosp\\nF\\n()', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='/tildenosp\\nF\\n()\\n,\\nm\\n of sampled data consists of peri-\\nodic copies of \\nF\\n()\\n,\\nm\\n it follows that \\n/tildenosp\\nF\\n()\\nm\\n will also \\nbe purely imaginary\\n. Draw a diagram similar to \\nFig. 4.6, and answer the following questions based \\non your diagram (assume that sampling starts at \\nt\\n=\\n0)\\n.\\n(a) * \\nWhat is the period of \\nft\\n()\\n?\\n(b) * \\nWhat is the frequency of \\nft\\n()\\n?\\n(c) * \\nWhat would the sampled function and its \\nF\\nourier transform look like in general if \\nft\\n()\\n \\nis sampled at a rate higher than the Nyquist \\nrate?\\n(d) \\nWhat would the sampled function look like \\nin general if \\nft\\n()\\n is sampled at a rate lower \\nthan the Nyquist rate?\\n(e) \\nWhat would the sampled function look like \\nif \\nft\\n()\\n is sampled at the Nyquist rate, with \\nsamples taken at \\ntT T\\n=\\n02\\n,,\\n,\\n±±\\n/H9004/H9004\\n…\\n ?\\n4.11 * \\nProve the validity of the convolution theorem of \\none continuous variable\\n, as given in Eqs. (4-25) \\nand (4-26).\\n4.12 \\nWe explained in the paragraph after Eq. (4-36) that', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='arbitrarily limiting the duration of a band-limit-\\ned function by multiplying it by a box function \\nwould cause the function to cease being band \\nlimited.\\n Show graphically why this is so by limit-\\ning the duration of the function \\nft\\nt\\n()\\nc o s ( )\\n=\\n2\\n0\\npm\\n \\n[the F\\nourier transform of this function is given in \\nProblem 4.9(a)]. (\\nHint:\\n The transform of a box \\nfunction is given in Example 4.1. Use that result \\nin your solution, and also the fact that convolu-\\ntion of a function with an impulse shifts the func-\\ntion to the location of the impulse, in the sense \\ndiscussed in the solution of Problem 4.4.)\\n4.13 * \\nComplete the steps that led from Eq. (4-37) to \\nEq.\\n (4-38).\\n4.14 \\nShow that \\n/tildenosp\\nF\\n()\\nm\\n in Eq. (4-40) is inﬁnitely periodic \\nin both directions\\n, with period \\n1\\n/H9004\\nT\\n.\\n4.15 \\nDo the following:\\n(a) \\nShow that Eqs. (4-42) and (4-43) are a Fou-\\nrier transform pair:\\n \\nfF\\nnm\\n⇔\\n.\\n(b) * \\nShow that Eqs. (4-44) and (4-45) also are a \\nF\\nourier transform pair: \\nfx Fu\\n()\\n() .', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='fx Fu\\n()\\n() .\\n⇔\\nYou will need the following orthogonality prop-\\nerty in both parts of this problem:\\n \\nee\\nMr u\\njr x M\\nx\\nM\\nju x M\\n2\\n0\\n1\\n2\\n0\\npp\\n=\\n−\\n−\\n∑\\n=\\n=\\n⎧\\n⎨\\n⎩\\nif \\notherwise\\nDIP4E_GLOBAL_Print_Ready.indb   309\\n6/16/2017   2:06:47 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 310}),\n",
       " Document(page_content='310\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\n4.16 \\nShow that both \\nFu\\n()\\n and \\nfx\\n()\\n in Eqs. (4-44) and \\n(4-45) are inﬁnitely periodic with period \\nM\\n;\\n that is, \\nFu Fu k M\\n()\\n( )\\n=+\\n and \\nfx fx M\\n()\\n( ) ,\\n=+\\n where \\nk\\n \\nis an integer\\n. [See Eqs. (4-46) and (4-47).]\\n4.17 \\nDemonstrate the validity of the translation (shift) \\nproperties of the following 1-D\\n, discrete Fourier \\ntransform pairs. (\\nHint:\\n It is easier in part (b) to \\nwork with the IDFT.)\\n(a) * \\nfxe Fu u\\nju x M\\n()\\n( )\\n2\\n0\\n0\\np\\n⇔−\\n(b) \\nfx x Fue\\nju x M\\n() ( )\\n−⇔\\n−\\n0\\n2\\n0\\np\\n4.18 \\nShow that the 1-D convolution theorem given in \\nEqs\\n. (4-25) and (4-26) also holds for discrete vari-\\nables, but with the right side of Eq. (4-26) multi-\\nplied by \\n1\\nM\\n. That is, show that\\n(a) * \\n \\n() ( ) () ( ) ,\\nfh x F H u\\n/H22841\\n⇔\\ni\\n and\\n(b) \\n() ( ) ( ) ( )\\nfhx\\nM\\nFH u\\ni\\n⇔\\n1\\n/H22841\\n \\n4.19 * \\nExtend the expression for 1-D convolution [see \\nEq.\\n (4-24)] to two continuous variables. Use \\nt\\n and \\nz', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content='t\\n and \\nz\\n for the variables on the left side of the expression \\nand \\na\\n and \\nb\\n for the variables in the 2-D integral. \\n4.20 \\nUse the sifting property of the 2-D impulse to \\nshow that convolution of a 2-D continuous func-\\ntion,\\n \\nft z\\n(,\\n) ,\\n with an impulse shifts the function \\nso that its origin is located at the location of the \\nimpulse\\n. (If the impulse is at the origin, the func-\\ntion is copied exactly as it was.) (\\nHint:\\n Study the \\nsolution to Problem 4.4).\\n4.21 \\nThe image on the left in the ﬁgure below consists \\nof alternating stripes of black/white\\n, each stripe \\nbeing two pixels wide. The image on the right \\nis the Fourier spectrum of the image on the left, \\nshowing the dc term and the frequency terms cor-\\nresponding to the stripes. (Remember, the spec-\\ntrum is symmetric so all components, other than \\nthe dc term, appear in two symmetric locations.)\\n(a) * \\nSuppose that the stripes of an image of the \\nsame size are four pixels wide\\n. Sketch what', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content='. Sketch what \\nthe spectrum of the image would look like, \\nincluding only the dc term and the two high-\\nest-value frequency terms, which correspond \\nto the two spikes in the spectrum above.\\n(b) \\nWhy are the components of the spectrum \\nlimited to the horizontal axis?\\n(c) \\nWhat would the spectrum look like for an \\nimage of the same size but having stripes that \\nare one pixel wide? Explain the reason for \\nyour answer\\n.\\n(d) \\nAre the dc terms in (a) and (c) the same, or \\nare they different? Explain.\\n4.22 \\nA high-technology company specializes in devel-\\noping imaging systems for digitizing images of \\ncommercial cloth.\\n The company has a new order \\nfor 1,000 systems for digitizing cloth consisting of \\nrepeating black and white vertical stripes, each \\nof width 2 cm. Optical and mechanical engineers \\nhave already designed the front-end optics and \\nmechanical positioning mechanisms so that you \\nare guaranteed that every image your system digi-\\ntizes starts with a complete black vertical stripe', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content='and ends with a complete white stripe. Every \\nimage acquired will contain exactly 250 vertical \\nstripes. Noise and optical distortions are negligi-\\nble. Having learned of your success in taking an \\nimage processing course, the company employs \\nyou to specify the resolution of the imaging chip \\nto be used in the new system. The optics can be \\nadjusted to project the ﬁeld of view accurately \\nonto the area deﬁned by the size of the chip you \\nspecify. Your design will be implemented in hun-\\ndreds of locations, so cost is an important consid-\\neration. What resolution chip (in terms of number \\nof imaging elements per horizontal line) would \\nyou specify to avoid aliasing? \\n4.23 * \\nWe know from the discussion in Section 4.5 that \\nzooming or shrinking a digital image generally \\ncauses aliasing\\n. Give an example of an image that \\nwould be free of aliasing if it were zoomed by \\npixel replication.\\n4.24 \\nWith reference to the discussion on linearity in \\nSection 2.6,\\n demonstrate that\\n(a) *', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content='(a) * \\nThe 2-D continuous Fourier transform is a \\nlinear operator\\n.\\n(b) \\nThe 2-D DFT is a linear operator also.\\nDIP4E_GLOBAL_Print_Ready.indb   310\\n6/16/2017   2:06:49 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 311}),\n",
       " Document(page_content='Problems\\n    \\n311\\n4.25 \\nWith reference to Eqs. (4-59) and (4-60), show the \\nvalidity of the following translation (shift) prop-\\nerties of 2-D\\n, \\ncontinuous\\n Fourier transform pairs. \\n(\\nHint:\\n Study the solutions to Problem 4.11.)\\n(a) * \\nft ze\\nF\\njt z\\n(, )\\n( , )\\n()\\n2\\n00\\n00\\npm n\\nmm nn\\n+\\n⇔−−\\n(b) \\nft t z z F e\\njt z\\n(, ) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nmn\\npm n\\n4.26 \\nShow the validity of the following 2-D \\ncontinuous \\nF\\nourier transform pairs.\\n(a) * \\nd\\n(,\\n)\\ntz\\n⇔\\n1\\n(b) * \\n1\\n⇔\\ndmn\\n(,)\\n(c) * \\nd\\npm n\\n(, )\\n()\\ntt zz e\\njt z\\n−− ⇔\\n−+\\n00\\n2\\n00\\n(d) \\net z\\njt t z z\\n2\\n00\\n00\\np\\ndm n\\n()\\n(,)\\n+\\n⇔−−\\n(e) * \\ncos( )\\n22\\n00\\npm pn\\ntz\\n+⇔\\n() ( , )( , )\\n12\\n00 00\\ndm m n n dm m n n\\n−− + ++\\n[]\\n(f) \\nsin( )\\n22\\n00\\npm pn\\ntz\\n+⇔\\n() ( , ) ( , )\\n12\\n00 00\\nj\\ndm m n n dm m n n\\n−− − ++\\n[]\\n4.27 \\nWith reference to Eqs. (4-71) and (4-72), dem-\\nonstrate the validity of the following translation \\n(shifting) properties of 2-D\\n, \\ndiscrete\\n Fourier trans-\\nform pairs from Table 4.4. (\\nHint:\\n Study the solu-\\ntions to Problem 4.17.) \\n(a) \\nfx y e', metadata={'source': 'imagepro.pdf', 'page': 312}),\n",
       " Document(page_content='(a) \\nfx y e\\nFu u\\njx M y N\\n(,)\\n( , )\\n()\\n2\\n00\\n00\\np\\nuv\\nvv\\n+\\n⇔− −\\n(b) * \\nfx x y y F e\\njx M y N\\n(,) ( , )\\n()\\n−− ⇔\\n−+\\n00\\n2\\n00\\nuv\\nuv\\np\\n4.28 \\nShow the validity of the following 2-D \\ndiscrete \\nF\\nourier transform pairs from Table 4.4: \\n(a) * \\nd\\n(,\\n)\\nxy\\n⇔\\n1\\n(b) * \\n1\\n⇔\\nMN\\nd\\n(,)\\nuv\\n(c) \\nd\\np\\n(,)\\n()\\nxx yy e\\nj u xM yN\\n−− ⇔\\n−+\\n00\\n2\\n00\\nv\\n(d) * \\neM\\nN\\nu\\nu\\nj uxM yN\\n2\\n00\\n0\\np\\nd\\n()\\n(,)\\n+\\n⇔− −\\nv\\n0\\nvv\\n(e) \\ncos(\\n)\\n22\\n00\\npm pn\\nxM yN\\n+⇔\\n() ( , ) ( , )\\nMN u u u\\n2\\n00 00\\ndm d\\n++ + − −\\n[]\\nvv vv\\n(f) * \\nsin(\\n)\\n22\\n00\\npm pn\\nxM yN\\n+⇔\\n() ( , ) ( , )\\njMN u u u\\n2\\n00 00\\ndm d\\n++ − − −\\n[]\\nvv vv\\n4.29 \\nYou are given a “canned” program that computes \\nthe 2-D\\n, DFT pair. However, it is not known \\nin which of the two equations the \\n1\\nMN\\n term \\nis included or if it was split as two constants\\n, \\n1\\nMN\\n,\\n in front of both the forward and inverse \\ntransforms\\n. How can you ﬁnd where the term(s) \\nis (are) included if this information is not avail-\\nable in the documentation?\\n4.30 \\nWhat is period and frequency of each of following', metadata={'source': 'imagepro.pdf', 'page': 312}),\n",
       " Document(page_content='digital sequences (\\nHint:\\n \\nThink of these as square \\nwaves.)\\n(a) * \\n0 1 0 1 0 1 0 1 . . .\\n(b) \\n0 0 1 0 0 1 0 0 1 . . . .\\n(c) \\n0 0 1 1 0 0 1 1 0 0 1 1 . . .\\n4.31 \\nWith reference to the 1-D sequences in Example \\n4.10:\\n(a) * \\nWhen \\nM\\n is even,\\n why is the point at \\nM\\n2\\n in \\nan even sequence always arbitrary?\\n(b) \\nWhen \\nM\\n is even,\\n why is the point at \\nM\\n2\\n in \\nan odd sequence always 0?\\n4.32 \\nWe mentioned in Example 4.10 that embedding a \\n2-D array of even (odd) dimensions into a larger \\narray of zeros of even (odd) dimensions keeps the \\nsymmetry of the original array\\n, provided that the \\ncenters coincide. Show that this is true also for \\nthe following 1-D arrays (i.e., show that the larger \\narrays have the same symmetry as the smaller \\narrays). For arrays of even length, use arrays of \\n0’s ten elements long. For arrays of odd lengths, \\nuse arrays of 0’s nine elements long.\\n(a) * \\na\\nbccb\\n,,,,\\n{}\\n(b) \\n00\\n,,, , ,\\n−−\\n{}\\nbc c b\\n(c) \\nabcdcb\\n,,,,,\\n{}\\n(d) \\n0, , , ,\\n−−\\n{}\\nb ccb\\n4.33', metadata={'source': 'imagepro.pdf', 'page': 312}),\n",
       " Document(page_content='−−\\n{}\\nb ccb\\n4.33 \\nIn Example 4.10 we showed a Sobel kernel \\nembedded in a ﬁeld of zeros\\n. The kernel is of size \\n33\\n×\\n and its structure appears to be odd. However, \\nits ﬁrst element is \\n−\\n1,\\n and we know that in order \\nto be odd,\\n the ﬁrst (top, left) element a 2-D array \\nmust be zero. Show the smallest ﬁeld of zeros in \\nwhich you can embed the Sobel kernel so that it \\nsatisﬁes the condition of oddness.\\n4.34 \\nDo the following:\\n(a) * \\nShow that the \\n66\\n×\\n array in Example 4.10 is \\nodd.\\n(b) \\nWhat would happen if the minus signs are \\nchanged to pluses?\\n(c) \\nExplain why, as stated at the end of the exam-\\nple\\n, adding to the array another row of 0’s on \\nthe top and column of 0’s to the left would \\ngive a result that is neither even nor odd.\\n(d) \\nSuppose that the row is added to the bot-\\nDIP4E_GLOBAL_Print_Ready.indb   311\\n6/16/2017   2:06:51 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 312}),\n",
       " Document(page_content='312\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\ntom and the column to the right? Would that \\nchange your answer in (c)?\\n4.35 \\nThe following problems are related to the proper-\\nties in \\nTable 4.1.\\n(a) * \\nDemonstrate the validity of property 2.\\n(b) * \\nDemonstrate the validity of property 4.\\n(c) \\nDemonstrate the validity of property 5.\\n(d) * \\nDemonstrate the validity of property 7.\\n(e) \\nDemonstrate the validity of property 9.\\n4.36 \\nYou know from Table 4.3 that the dc term, \\nF\\n(,\\n) ,\\n00\\n \\nof a DFT is proportional to the average value of \\nits corresponding spatial image\\n. Assume that the \\nimage is of size \\nMN\\n×\\n.\\n Suppose that you pad the \\nimage with zeros to size \\nPQ\\n×\\n,\\n where \\nP\\n and \\nQ\\n \\nare given in Eqs. (4-102) and (4-103). Let \\nF\\np\\n(,)\\n00\\n \\ndenote the dc term of the DFT of the padded \\nfunction.\\n(a) * \\nWhat is the ratio of the average values of the \\noriginal and padded images?\\n(b) \\nIs \\nFF\\np\\n(,) (,) ?\\n00 00\\n=\\n Support your answer \\nmathematically\\n.\\n4.37', metadata={'source': 'imagepro.pdf', 'page': 313}),\n",
       " Document(page_content='.\\n4.37 \\nDemonstrate the validity of the periodicity prop-\\nerties (entry 8) in \\nTable 4.3. \\n4.38 \\nWith reference to the 2-D discrete convolution \\ntheorem in Eqs\\n. (4-95) and (4-96) (entry 6 in \\nTable 4.4), show that\\n(a) \\n \\n(( )\\n(\\n,\\n)\\n)( , )\\nfF\\nH\\nhx y\\n/H22841\\n⇔\\ni\\nuv\\n(b) * \\n \\n() ( , ) ( ) ( ) ( , )\\nfhx y M N F H\\ni\\n⇔\\n[]\\n1\\n/H22841\\nuv\\n(\\nHint:\\n Study the solution to Problem 4.18.)\\n4.39 \\nWith reference to the 2-D discrete correlation \\ntheorem (entry 7 in \\nTable 4.4), show that\\n(a) * \\n \\n(( )\\n(\\n,\\n)\\n)( , )\\n*\\nfF H\\nhx y\\n/H22845\\n⇔\\ni\\nuv\\n(b) \\n \\n() ( , ) ( ) ( ) ( , )\\n*\\nfhx y M N F H\\ni\\n⇔\\n[]\\n1\\n/H22845\\nuv\\n4.40 * \\nDemonstrate validity of the differentiation pairs \\nin entry 12 of \\nTable 4.4. \\n4.41 \\nWe discussed in Section 4.6 the need for image \\npadding when ﬁltering in the frequenc\\ny domain. \\nWe showed in that section that images could be \\npadded by appending zeros to the ends of rows \\nand columns in the image (see the following \\nimage, on the left). Do you think it would make a', metadata={'source': 'imagepro.pdf', 'page': 313}),\n",
       " Document(page_content='difference if we centered the image and surround-\\ned it by a border of zeros instead (see image on \\nthe right), but without changing the total number \\nof zeros used? Explain.\\n4.42 * \\nThe two Fourier spectra shown are of the same \\nimage\\n. The spectrum on the left corresponds to \\nthe original image, and the spectrum on the right \\nwas obtained after the image was padded with \\nzeros. Explain the signiﬁcant increase in signal \\nstrength along the vertical and horizontal axes of \\nthe spectrum shown on the right.\\n4.43 \\nConsider the images shown. The image on the \\nright was obtained by:\\n (a) multiplying the image \\non the left by \\n() ;\\n−\\n+\\n1\\nxy\\n (b) computing the DFT; (c) \\ntaking the complex conjugate of the transform; \\n(d) computing the inverse DFT; and (e) multiply-\\ning the real part of the result by \\n() .\\n−\\n+\\n1\\nxy\\n Explain \\n(mathematically) why the image on the right \\nappears as it does.\\nDIP4E_GLOBAL_Print_Ready.indb   312\\n6/16/2017   2:06:52 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 313}),\n",
       " Document(page_content='Problems\\n    \\n313\\n4.44 * \\nThe image in Fig. 4.34(b) was obtained by mul-\\ntiplying by \\n−\\n1\\n the phase angle of the image in \\nF\\nig. 4.34(a), and then computing the IDFT. With \\nreference to Eq. (4-86) and entry 5 in Table 4.1, \\nexplain why this operation caused the image to be \\nreﬂected about both coordinate axes.\\n4.45 \\nIn Fig. 4.34(b) we saw that multiplying the phase \\nangle by \\n−\\n1\\n ﬂipped the image with respect to both \\ncoordinate axes\\n. \\nSuppose that instead we multi-\\nplied the magnitude of the transform by \\n−\\n1\\n and \\nthen took the inverse DFT using the equation:\\n \\ngxy F e\\nj\\n(,) (,) .\\n(,)\\n=−\\n{}\\n−\\n/H5219\\n1\\nuv\\nuv\\nf\\n(a) * \\nWhat would be the difference between the \\ntwo images \\ngxy\\n(,\\n)\\n and \\nfx y\\n(,\\n) ?\\n [Remember, \\nF\\n(,\\n)\\nuv\\n is the DFT of \\nfx y\\n(,\\n) . ]\\n(b) \\nAssuming that they are both 8-bit images, \\nwhat would \\ngxy\\n(,\\n)\\n look like in terms of \\nfx y\\n(,\\n)\\n if we scaled the intensity values of \\ngxy\\n(,\\n)\\n using Eqs. (2-31) and (2-32), with \\nK\\n=\\n255\\n?\\n4.46', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='K\\n=\\n255\\n?\\n4.46 \\nWhat is the source of the nearly periodic bright \\nspots on the horizontal axis of F\\nig. 4.40(b)?\\n4.47 * \\nConsider a \\n33\\n×\\n spatial kernel that averages \\nthe four closest neighbors of a point \\n(,) ,\\nxy\\n but \\nexcludes the point itself from the average\\n.\\n(a) \\nFind the equivalent ﬁlter transfer function, \\nH\\n(,\\n) ,\\nuv\\n in the frequency domain.\\n(b) \\nShow that your result is a lowpass ﬁlter trans-\\nfer function.\\n4.48 * \\nA continuous Gaussian lowpass ﬁlter in the con-\\ntinuous frequenc\\ny domain has the transfer func-\\ntion\\nHA e\\n(,)\\n()\\nmn\\nmn s\\n=\\n−+\\n22 2\\n2\\nShow that the corresponding ﬁlter kernel in the \\ncontinuous spatial domain is\\nhtz A e\\ntz\\n(, )\\n()\\n=\\n−+\\n2\\n22\\n22 2 2\\nps\\nps\\n4.49 \\nGiven an image of size \\nMN\\n×\\n,\\n you are asked to \\nperform an experiment that consists of repeat-\\nedly lowpass ﬁltering the image in the frequenc\\ny \\ndomain using a Gaussian lowpass ﬁlter transfer \\nfunction with a cutoff frequency, \\nD\\n0\\n.\\n You may \\nignore computational round-off errors\\n. \\n(a) * \\nLet \\nK', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='. \\n(a) * \\nLet \\nK\\n denote the number of applications of \\nthe ﬁlter\\n. Can you predict (without doing the \\nexperiment) what the result (image) will be \\nfor a sufﬁciently large value of \\nK\\n? If so, what \\nis that result?\\n(b) \\nLet \\nc\\nmin\\n denote the smallest positive num-\\nber representable in the machine in which \\nthe proposed experiment will be conducted \\n(any number \\n<\\nc\\nmin\\n is automatically set to 0). \\nDerive an expression (in terms of \\nc\\nmin\\n)\\n for \\nthe minimum value of \\nK\\n that will guarantee \\nthe result that you predicted in (a).\\n \\n4.50 \\nAs explained in Section 3.6, ﬁrst-order deriva-\\ntives can be approximated by the spatial differ\\n-\\nences \\ngf x y x f x y f x y\\nx\\n=∂ ∂ = + −\\n(, ) ( , ) (, )\\n1\\n and \\ng f xy y f xy f xy\\ny\\n=∂ ∂ = + −\\n(, ) (, ) (, ) .\\n1\\n(a) \\nFind the equivalent ﬁlter transfer func-\\ntions \\nH\\nx\\n(,)\\nuv\\n and \\nH\\ny\\n(,)\\nuv\\n in the frequency \\ndomain.\\n(b) \\nShow that these are highpass ﬁlter transfer \\nfunctions\\n.\\n(\\nHint:\\n Study the solution to Problem 4.47.)\\n4.51', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='4.51 \\nFind the equivalent frequency-domain ﬁlter \\ntransfer function for the Laplacian kernel shown \\nin F\\nig. 3.45(a). Show that your result behaves as a \\nhighpass ﬁlter transfer function. (\\nHint:\\n Study the \\nsolution to Problem 4.47.)\\n4.52 \\nDo the following:\\n(a) \\nShow that the Laplacian of a continuous \\nfunction \\nft z\\n(,\\n)\\n of two continuous variables, \\nt\\n and \\nz\\n,\\n satisﬁes the following Fourier trans-\\nform pair:\\n/H11612\\n22\\n2\\n2\\n4\\nft z F\\n(, ) ( ) ( , )\\n⇔− +\\npm n m n\\n(\\nHint:\\n See Eq.\\n (3-50) and study entry 12 in \\nTable 4.4.)\\n(b) * \\nThe result in (a) is valid only for continuous \\nvariables\\n. How would you implement the \\ncontinuous frequency domain transfer func-\\ntion \\nH\\n(,\\n) ( )\\nmn p m n\\n=− +\\n4\\n22 2\\n for discrete \\nvariables?\\n(c) \\nAs you saw in Example 4.21, the Laplacian \\nresult in the frequenc\\ny domain was similar to \\nthe result in Fig. 3.46(d), which was obtained \\nusing a spatial kernel with a center coefﬁ-\\ncient equal to \\n−\\n8.\\n Explain why the frequency', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='domain result was not similar instead to the \\nDIP4E_GLOBAL_Print_Ready.indb   313\\n6/16/2017   2:06:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 314}),\n",
       " Document(page_content='314\\n    \\nChapter\\n \\n4\\n  \\nFiltering in the Frequency Domain\\nresult in Fig. 3.46(c), which was obtained \\nusing a kernel with a center coefﬁcient of \\n−\\n4.\\n4.53 * \\nCan you think of a way to use the Fourier trans-\\nform to compute (or partially compute) the \\nmagnitude of the gradient [Eq.\\n (3-58)] for use in \\nimage differentiation? If your answer is yes, give \\na method to do it. If your answer is no, explain \\nwhy.\\n4.54 \\nAs explained in Eq. (4-118), it is possible to obtain \\nthe transfer function of a highpass ﬁlter from the \\ntransfer function of a lowpass ﬁlter by subtract-\\ning the latter from 1.\\n What is the highpass spatial \\nkernel corresponding to the lowpass Gaussian \\ntransfer function given in Problem 4.48?\\n4.55 \\nEach spatial highpass kernel in Fig. \\n4.52\\n has a \\nstrong spike in the center. Explain the source of \\nthis spikes.\\n4.56 * \\nShow how the Butterworth highpass ﬁlter trans-\\nfer function in Eq.\\n (4-121) follows from its low-\\npass counterpart in Eq. (4-117).\\n4.57', metadata={'source': 'imagepro.pdf', 'page': 315}),\n",
       " Document(page_content='4.57 \\nConsider the hand X-ray images shown below. \\nT\\nhe image on the right was obtained by lowpass\\n(Original image courtesy of Dr. Thomas R. Gest, Division \\nof Anatomical Sciences, University of Michigan Medical \\nSchool.) \\nﬁltering the image on the left with a Gaussian \\nlowpass ﬁlter, and then highpass ﬁltering the \\nresult with a Gaussian highpass ﬁlter. The images \\nare of size \\n420 344\\n×\\n pixels and \\nD\\n0\\n25\\n=\\n was used \\nfor both ﬁlter transfer functions\\n.\\n(a) * \\nExplain why the center part of the ﬁnger ring \\nin the ﬁgure on the right appears so bright \\nand solid,\\n considering that the dominant \\ncharacteristic of the ﬁltered image consists \\nof edges of the ﬁngers and wrist bones, with \\ndarker areas in between. In other words, \\nwould you not expect the highpass ﬁlter to \\nrender the constant area inside the ring as \\ndark, since a highpass ﬁlter eliminates the dc \\nterm and reduces low frequencies?\\n(b) \\n Do you think the result would have been dif-', metadata={'source': 'imagepro.pdf', 'page': 315}),\n",
       " Document(page_content='ferent if the order of the ﬁltering process had \\nbeen reversed?\\n4.58 \\nConsider the sequence of images shown below. \\nT\\nhe image on the top left is a segment of an X-ray \\nimage of a commercial printed circuit board. The \\nimages following it are, respectively, the results of \\nsubjecting the image to 1, 10, and 100 passes of a \\nGaussian highpass ﬁlter with \\nD\\n0\\n30\\n=\\n.\\n The images \\nare of size \\n330 334\\n×\\n pixels, with each pixel being \\nrepresented by 8 bits of gray\\n. The images were \\nscaled for display, but this has no effect on the \\nproblem statement.\\n(Original image courtesy of Mr. Joseph E. Pascente, Lixi, Inc.)\\n(a) \\nIt appears from the images that changes will \\ncease to take place after a ﬁnite number of \\npasses\\n. Show whether or not this is the case. \\nYou may ignore computational round-off \\nerrors. Let \\nc\\nmin\\n denote the smallest positive \\nnumber representable in the machine in \\nwhich the computations are conducted. \\n(b) \\nIf you determined in (a) that changes would', metadata={'source': 'imagepro.pdf', 'page': 315}),\n",
       " Document(page_content='cease after a ﬁnite number of iterations\\n, \\ndetermine the minimum value of that num-\\nber.\\n(\\nHint: \\nStudy the solution to Problem 4.49.)\\n4.59 \\nAs illustrated in Fig. 4.57, combining high-fre-\\nquenc\\ny emphasis and histogram equalization is \\nDIP4E_GLOBAL_Print_Ready.indb   314\\n6/16/2017   2:06:55 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 315}),\n",
       " Document(page_content='Problems\\n    \\n315\\nan effective method for achieving edge sharpen-\\ning and contrast enhancement.\\n(a) * \\nShow whether or not it matters which pro-\\ncess is applied ﬁrst.\\n(b) \\nIf the order does matter, give a rationale for \\nusing one or the other method ﬁrst.\\n4.60 \\nUse a Butterworth highpass ﬁlter to construct a \\nhomomorphic ﬁlter transfer function that has the \\nsame general shape as the function in F\\nig. 4.59.\\n4.61 \\nSuppose that you are given a set of images gener-\\nated by an experiment dealing with the analysis of \\nstellar events\\n. Each image contains a set of bright, \\nwidely scattered dots corresponding to stars in \\na sparsely occupied region of the universe. The \\nproblem is that the stars are barely visible as a \\nresult of superimposed illumination from atmo-\\nspheric dispersion. If these images are modeled as \\nthe product of a constant illumination component \\nwith a set of impulses, give an enhancement pro-\\ncedure based on homomorphic ﬁltering designed', metadata={'source': 'imagepro.pdf', 'page': 316}),\n",
       " Document(page_content='to bring out the image components due to the \\nstars themselves.\\n4.62 \\nHow would you generate an image of only the \\ninterference pattern visible in F\\nig. 4.64(a)?\\n4.63 * \\nShow the validity of Eqs. (4-171) and (4-172). \\n(\\nHint:\\n \\nUse proof by induction.)\\n4.64 \\nA skilled medical technician is assigned the job of \\ninspecting a set of images generated by an elec-\\ntron microscope experiment.\\n In order to simplify \\nthe inspection task, the technician decides to use \\ndigital image enhancement and, to this end, exam-\\nines a set of representative images and ﬁnds the \\nfollowing problems: (1) bright, isolated dots that \\nare of no interest; (2) lack of sharpness; (3) not \\nenough contrast in some images; and (4) shifts \\nin the average intensity to values other than \\nA\\n0\\n, \\nwhich is the average value required to perform \\ncorrectly certain intensity measurements\\n. The \\ntechnician wants to correct these problems and \\nthen display in white all intensities in a band \\nbetween intensities \\nI\\n1\\n and \\nI\\n2\\n,', metadata={'source': 'imagepro.pdf', 'page': 316}),\n",
       " Document(page_content='I\\n1\\n and \\nI\\n2\\n,\\n while keeping nor-\\nmal tonality in the remaining intensities\\n. Propose \\na sequence of processing steps that the technician \\ncan follow to achieve the desired goal. You may \\nuse techniques from both Chapters 3 and 4.\\nDIP4E_GLOBAL_Print_Ready.indb   315\\n6/16/2017   2:06:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 316}),\n",
       " Document(page_content='DIP4E_GLOBAL_Print_Ready.indb   4\\n6/16/2017   2:01:57 PMThis page intentionally left blank\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 317}),\n",
       " Document(page_content='3175\\nImage Restoration \\nand Reconstruction\\nPreview\\nAs in image enhancement, the principal goal of restoration techniques is to improve an image in some \\npredeﬁned sense. Although there are areas of overlap, image enhancement is largely a subjective pro-\\ncess, while image restoration is for the most part an objective process. Restoration attempts to recover \\nan image that has been degraded by using a priori knowledge of the degradation phenomenon. Thus, \\nrestoration techniques are oriented toward modeling the degradation and applying the inverse process \\nin order to recover the original image. In this chapter, we consider linear, space invariant restoration \\nmodels that are applicable in a variety of restoration situations. We also discuss fundamental tech-\\nniques of image reconstruction from projections, and their application to computed tomography (CT), \\none of the most important commercial applications of image processing, especially in health care.', metadata={'source': 'imagepro.pdf', 'page': 318}),\n",
       " Document(page_content='Upon completion of this chapter, readers should:\\n Be familiar with the characteristics of various \\nnoise models used in image processing, and \\nhow to estimate from image data the param-\\neters that deﬁne those models.\\n Be familiar with linear, nonlinear, and adap-\\ntive spatial ﬁlters used to restore (denoise) \\nimages that have been degraded only by noise.\\n Know how to apply notch ﬁltering in the fre-\\nquency domain for removing periodic noise \\nin an image.\\n Understand the foundation of linear, space \\ninvariant system concepts, and how they can \\nbe applied in formulating image restoration \\nsolutions in the frequency domain.\\n Be familiar with direct inverse ﬁltering and its \\nlimitations.\\n Understand minimum mean-square-error (Wie-\\nner) ﬁltering and its advantages over direct \\ninverse ﬁltering.\\n Understand constrained, least-squares ﬁlter-\\ning.\\n Be familiar with the fundamentals of image \\nreconstruction from projections, and their \\napplication to computed tomography.', metadata={'source': 'imagepro.pdf', 'page': 318}),\n",
       " Document(page_content='Things which we see are not themselves what we see . . .  \\nIt remains completely unknown to us what the objects may be \\nby themselves and apart from the receptivity of our senses. \\nWe know only but our manner of perceiving them.\\nImmanuel Kant\\nDIP4E_GLOBAL_Print_Ready.indb   317\\n6/16/2017   2:06:56 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 318}),\n",
       " Document(page_content='318\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\n5.1 A MODEL OF THE IMAGE DEGRADATION/RESTORATION  \\nPROCESS  \\nIn this chapter, we model image degradation as an operator \\n/H5108\\n that, together with an \\nadditive noise term,\\n operates on an input image \\nfx y\\n(,\\n)\\n to produce a degraded image \\ngxy\\n(,\\n)\\n (see Fig. 5.1). Given \\ngxy\\n(,\\n) ,\\n some knowledge about \\n/H5108\\n,\\n and some knowledge \\nabout the additive noise term \\nh\\n(,\\n) ,\\nxy\\n the objective of restoration is to obtain an \\nestimate \\nˆ\\n(,\\n)\\nfx y\\n of the original image. We want the estimate to be as close as possible \\nto the original image and,\\n in general, the more we know about \\n/H5108\\n and \\nh\\n,\\n the closer \\nˆ\\n(,\\n)\\nfx y\\n will be to \\nfx y\\n(,\\n) .\\n \\nW\\ne will show in Section 5.5 that, if \\n/H5108\\n is a linear, position-invariant operator, then \\nthe degraded image is given in the spatial domain by\\n \\ngxy h f xy xy\\n(,\\n) ( ) (,) (,)\\n=+\\n/H22841\\nh\\n \\n(5-1)\\nwhere \\nhxy\\n(,\\n)', metadata={'source': 'imagepro.pdf', 'page': 319}),\n",
       " Document(page_content='where \\nhxy\\n(,\\n)\\n is the spatial representation of the degradation function. As in Chapters \\n3 and 4,\\n the symbol “\\n/H22841\\n” indicates convolution. It follows from the convolution theorem \\nthat the equivalent of Eq. (5-1) in the frequency domain is\\n \\nGH F N\\n(,\\n) (,) (,) (,)\\nuv uv uv uv\\n=+\\n \\n(5-2)\\nwhere the terms in capital letters are the Fourier transforms of the corresponding \\nterms in Eq.\\n (5-1). These two equations are the foundation for most of the restora-\\ntion material in this chapter.\\nIn the following three sections, we work only with degradations caused by noise. \\nBeginning in Section 5.5 we look at several methods for image restoration in the \\npresence of both \\n/H5108\\n and \\nh\\n.\\n5.2 NOISE MODELS  \\nThe principal sources of noise in digital images arise during image acquisition and/or \\ntransmission. The performance of imaging sensors is affected by a variety of environ-\\nmental factors during image acquisition, and by the quality of the sensing elements', metadata={'source': 'imagepro.pdf', 'page': 319}),\n",
       " Document(page_content='themselves. For instance, in acquiring images with a CCD camera, light levels and \\nsensor temperature are major factors affecting the amount of noise in the resulting \\nimage. Images are corrupted during transmission principally by interference in the \\ntransmission channel. For example, an image transmitted using a wireless network \\nmight be corrupted by lightning or other atmospheric disturbance. \\n5.1\\n5.2\\nDegradation\\nDEGRADATION\\nRESTORATION\\nRestoration\\nfilter(s)\\nf\\n(\\nx\\n,\\ny\\n)\\ng\\n(\\nx\\n,\\ny\\n)\\nf\\n(\\nx\\n,\\ny\\n)\\nˆ\\nNoise\\nh\\n(\\nx\\n,\\ny\\n)\\n/H11001\\n/H5108\\nFIGURE 5.1\\nA model of the \\nimage  \\ndegradation/ \\nrestoration  \\nprocess. \\nDIP4E_GLOBAL_Print_Ready.indb   318\\n6/16/2017   2:06:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 319}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n319\\nSPATIAL AND FREQUENCY PROPERTIES OF NOISE\\nRelevant to our discussion are parameters that define the spatial characteristics of \\nnoise, and whether the noise is correlated with the image. Frequency properties refer \\nto the frequency content of noise in the Fourier (frequency) domain discussed in \\ndetail in Chapter 4. For example, when the Fourier spectrum of noise is constant, the \\nnoise is called \\nwhite noise\\n. This terminology is a carryover from the physical prop-\\nerties of white light, which contains all frequencies in the visible spectrum in equal \\nproportions.\\nWith the exception of spatially periodic noise, we assume in this chapter that \\nnoise is independent of spatial coordinates, and that it is uncorrelated with respect \\nto the image itself (that is, there is no correlation between pixel values and the values \\nof noise components). Although these assumptions are at least partially invalid in', metadata={'source': 'imagepro.pdf', 'page': 320}),\n",
       " Document(page_content='some applications (quantum-limited imaging, such as in X-ray and nuclear-medicine \\nimaging, is a good example), the complexities of dealing with spatially dependent \\nand correlated noise are beyond the scope of our discussion.\\nSOME IMPORTANT NOISE PROBABILITY DENSITY FUNCTIONS\\nIn the discussion that follows, we shall be concerned with the statistical behavior of \\nthe intensity values in the noise component of the model in Fig. 5.1. These may be \\nconsidered random variables, characterized by a probability density function (PDF), \\nas noted briefly as noted earlier. The noise component of the model in Fig. 5.1 is an \\nimage, \\nh\\n(,\\n) ,\\nxy\\n of the same size as the input image. We create a noise image for simu-\\nlation purposes by generating an array whose intensity values are random numbers \\nwith a specified probability density function.\\n This approach is true for all the PDFs \\nto be discussed shortly, with the exception of salt-and-pepper noise, which is applied', metadata={'source': 'imagepro.pdf', 'page': 320}),\n",
       " Document(page_content='differently. The following are among the most common noise PDFs found in image \\nprocessing applications.\\nGaussian Noise\\nBecause of its mathematical tractability in both the spatial and frequency domains, \\nGaussian noise models are used frequently in practice. In fact, this tractability is so \\nconvenient that it often results in Gaussian models being used in situations in which \\nthey are marginally applicable at best. \\nThe PDF of a \\nGaussian\\n random variable, \\nz\\n, is deﬁned by the following familiar \\nexpression:\\n \\npz e\\nz\\nzz\\n()\\n()\\n=−\\n−\\n−\\n1\\n2\\n2\\n2\\n2\\nps\\ns\\n/H11009/H11009\\n<<\\n \\n(5-3)\\nwhere \\nz\\n represents intensity\\n, \\nz\\n is the mean (average) value of \\nz\\n,\\n and \\ns\\n is its standard \\ndeviation.\\n Figure 5.2(a) shows a plot of this function. We know that for a Gaussian \\nrandom variable, the probability that values of \\nz\\n are in the range \\nz\\n±\\ns\\n is approxi-\\nmately 0.68;\\n the probability is about 0.95 that the values of \\nz\\n are in the range \\nz\\n±\\n2\\ns\\n.\\nYou may ﬁnd it helpful \\nto take a look at the', metadata={'source': 'imagepro.pdf', 'page': 320}),\n",
       " Document(page_content='Tutorials section of the \\nbook website for a brief \\nreview of probability.\\nDIP4E_GLOBAL_Print_Ready.indb   319\\n6/16/2017   2:06:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 320}),\n",
       " Document(page_content='320\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nRayleigh Noise\\nThe PDF of \\nRayleigh\\n noise is given by\\n \\npz\\nb\\nza e za\\nza\\nzab\\n()\\n()\\n()\\n=\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−−\\n2\\n0\\n2\\n≥\\n<\\n \\n(5-4)\\nThe mean and variance of \\nz\\n when this random variable is characterized by a Ray-\\nleigh PDF are\\n \\nza b\\n=+\\np\\n4  \\n(5-5)\\nand\\n \\ns\\np\\n2\\n4\\n4\\n=\\n−\\n()\\nb\\n \\n(5-6)\\nFigure 5.2(b) shows a plot of the Rayleigh density. Note the displacement from the \\norigin,\\n and the fact that the basic shape of the density is skewed to the right. The \\nRayleigh density can be quite useful for modeling the shape of skewed histograms.\\nz\\nRayleigh\\np\\n(\\nz\\n)\\nK\\nErlang (Gamma)\\nz\\n(\\nb\\n \\n/H11002\\n 1)\\n/\\na\\nz\\np\\n(\\nz\\n)\\nz\\na\\nExponential\\np\\n(\\nz\\n)\\nP\\np\\nSalt-and-\\npepper\\np\\n(\\nz\\n)\\n1\\n2\\nps\\n0.607\\n2\\nps\\n_\\nz\\n/H11002\\n \\ns\\n_\\nz\\n/H11001\\n \\ns\\n_\\nz\\np\\n(\\nz\\n)\\n2\\nb\\n0.607\\nz\\na\\nb\\n2\\na\\n \\n/H11001\\na\\n(\\nb\\n \\n/H11002\\n 1)\\nb\\n/H11002\\n1\\n(\\nb\\n \\n/H11002\\n 1)!\\nK\\n \\n/H11005\\ne\\n/H11002\\n(\\nb\\n/H11002\\n1)\\nUniform\\nz\\nab\\np\\n(\\nz\\n)\\n1\\nb\\n \\n/H11002\\n \\na\\nGaussian\\nP\\ns\\n0\\n21\\nk\\n−\\nV\\n1( )\\nsp\\nPP\\n−+\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.2', metadata={'source': 'imagepro.pdf', 'page': 321}),\n",
       " Document(page_content='c\\ne\\nd\\nf\\nFIGURE 5.2\\n Some important probability density functions.\\nDIP4E_GLOBAL_Print_Ready.indb   320\\n6/16/2017   2:06:58 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 321}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n321\\nErlang (Gamma) Noise\\nThe PDF of Erlang noise is\\n \\npz\\naz\\nb\\nez\\nz\\nbb\\naz\\n()\\n() !\\n=\\n−\\n<\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−\\n−\\n1\\n1\\n0\\n00\\n≥\\n \\n(5-7)\\nwhere the parameters are such that \\nab\\n>\\n, \\nb\\n is a positive integer\\n, and “!” indicates \\nfactorial. The mean and variance of \\nz\\n are\\n \\nz\\nb\\na\\n=\\n \\n(5-8)\\nand\\n \\ns\\n2\\n2\\n=\\nb\\na\\n \\n(5-9)\\nFigure 5.2(c) shows a plot of this density. Although Eq. (5-9) often is referred to as \\nthe \\ngamma\\n density\\n, strictly speaking this is correct only when the denominator is \\nthe gamma function, \\n/H9003\\n()\\n.\\nb\\n When the denominator is as shown, the density is more \\nappropriately called the \\nErlang\\n density\\n.\\nExponential Noise\\nThe PDF of \\nexponential\\n noise is given by \\n \\npz\\nae\\nz\\nz\\naz\\n()\\n=\\n<\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n−\\n≥\\n0\\n00\\n \\n(5-10)\\nwhere \\na\\n>\\n0. The mean and variance of \\nz\\n are\\n \\nz\\na\\n=\\n1\\n \\n(5-11)\\nand\\n \\ns\\n2\\n2\\n1\\n=\\na\\n \\n(5-12)\\nNote that this PDF is a special case of the Erlang PDF with \\nb\\n=\\n1.\\n Figure 5.2(d) \\nshows a plot of the exponential density function.\\nUniform Noise\\nThe PDF of', metadata={'source': 'imagepro.pdf', 'page': 322}),\n",
       " Document(page_content='The PDF of \\nuniform\\n noise is\\n \\npz\\nba\\nazb\\n()\\n=\\n−\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\n1\\n0\\n≤≤\\notherwise\\n \\n(5-13)\\nDIP4E_GLOBAL_Print_Ready.indb   321\\n6/16/2017   2:07:00 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 322}),\n",
       " Document(page_content='322\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nThe mean and variance of \\nz\\n are\\n \\nz\\nab\\n=\\n+\\n2\\n \\n(5-14)\\nand\\n \\ns\\n2\\n2\\n12\\n=\\n−\\n()\\nba\\n \\n(5-15)\\nFigure 5.2(e) shows a plot of the uniform density.\\nSalt-and-Pepper Noise\\nIf \\nk\\n represents the number of bits used to represent the intensity values in a digital \\nimage, then the range of possible intensity values for that image is \\n[, ]\\n02 1\\nk\\n−\\n (e.g., \\n[, ]\\n0 255\\n for an 8-bit image). The PDF of \\nsalt-and-pepper\\n noise is given by\\n \\npz\\nPz\\nPz\\nPP\\nz V\\ns\\nk\\np\\nsp\\n()\\n()\\n=\\n=−\\n=\\n−+ =\\n⎧\\n⎨\\n⎪\\n⎩\\n⎪\\nfor \\nfor \\nfor \\n21\\n0\\n1\\n \\n(5-16)\\nwhere \\nV\\n is any integer value in the range \\n02 1\\n<<\\n−\\nV\\nk\\n.\\nLet \\nh\\n(,\\n)\\nxy\\n denote a salt-and-pepper noise image, whose intensity values satisfy \\nEq.\\n (5-16). Given an image, \\nfx y\\n(,\\n) ,\\n of the same size as \\nh\\n(,\\n) ,\\nxy\\n we corrupt it with salt-\\nand-pepper noise by assigning a 0 to all locations in \\nf\\n where a 0 occurs in \\nh\\n.\\n Similarly, \\nwe assign a value of \\n21\\nk\\n−\\n to all location in \\nf\\n where that value appears in \\nh', metadata={'source': 'imagepro.pdf', 'page': 323}),\n",
       " Document(page_content='h\\n.\\n Finally, \\nwe leave unchanged all location in \\nf\\n where \\nV\\n occurs in \\nh\\n.\\nIf neither \\nP\\ns\\n nor \\nP\\np\\n is zero, and especially if they are equal, noise values satisfy-\\ning Eq. (5-16) will be white \\n()\\n21\\nk\\n−\\n or black (0), and will resemble salt and pepper \\ngranules distributed randomly over the image;\\n hence the name of this type of noise. \\nOther names you will ﬁnd used in the literature are \\nbipolar impulse noise\\n (\\nunipolar\\n \\nif either \\nP\\ns\\n or \\nP\\np\\n is 0), \\ndata-drop-out noise\\n, and \\nspike noise\\n. We use the terms impulse \\nand salt-and-pepper noise interchangeably. \\nThe probability, \\nP\\n, that a pixel is corrupted by salt or pepper noise is \\nP\\nPP\\nsp\\n=+\\n. \\nIt is common terminology to refer to \\nP\\n as the \\nnoise density\\n.\\n If, for example, \\nP\\ns\\n=\\n00 2\\n.  \\nand \\nP\\np\\n=\\n00 1\\n.,\\n then \\nP\\n=\\n00\\n3\\n.\\n and we say that approximately 2% of the pixels in an \\nimage are corrupted by salt noise\\n, 1% are corrupted by pepper noise, and the noise', metadata={'source': 'imagepro.pdf', 'page': 323}),\n",
       " Document(page_content='density is 3%, meaning that approximately 3% of the pixels in the image are cor-\\nrupted by salt-and-pepper noise.\\nAlthough, as you have seen, salt-and-pepper noise is speciﬁed by the probability \\nof each, and not by the mean and variance, we include the latter here for complete-\\nness. The mean of salt-and-pepper noise is given by\\n \\nzP K P P P\\nps p\\nk\\ns\\n=+ − − + −\\n() ( ) ( )\\n01 2 1\\n \\n(5-17)\\nand the variance by\\nWhen image intensities \\nare scaled to the range \\n[0, 1], we replace by 1 the \\nvalue of salt in this equa-\\ntion. \\nV\\n then becomes a \\nfractional value in the \\nopen interval (0, 1). \\nDIP4E_GLOBAL_Print_Ready.indb   322\\n6/16/2017   2:07:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 323}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n323\\n \\ns\\n22 2\\n2\\n01 2\\n1\\n=− + − −− + −\\n() ( ) ( ) ( )\\nzP K z P P P\\nps\\np\\nk\\ns\\n \\n(5-18)\\nwhere we have included 0 as a value explicit in both equations to indicate that the \\nvalue of pepper noise is assumed to be zero\\n.\\nAs a group, the preceding PDFs provide useful tools for modeling a broad range \\nof noise corruption situations found in practice. For example, Gaussian noise arises \\nin an image due to factors such as electronic circuit noise and sensor noise caused by \\npoor illumination and/or high temperature. The Rayleigh density is helpful in char-\\nacterizing noise phenomena in range imaging. The exponential and gamma densities \\nﬁnd application in laser imaging. Impulse noise is found in situations where quick \\ntransients, such as faulty switching, take place during imaging. The uniform density \\nis perhaps the least descriptive of practical situations. However, the uniform density \\nis quite useful as the basis for numerous random number generators that are used', metadata={'source': 'imagepro.pdf', 'page': 324}),\n",
       " Document(page_content='extensively in simulations (Gonzalez, Woods, and Eddins [2009]).\\nEXAMPLE 5.1 :  Noisy images and their histograms.\\nFigure 5.3 shows a test pattern used for illustrating the noise models just discussed. This is a suitable pat-\\ntern to use because it is composed of simple, constant areas that span the gray scale from black to near \\nwhite in only three increments. This facilitates visual analysis of the characteristics of the various noise \\ncomponents added to an image. \\nFigure 5.4 shows the test pattern after addition of the six types of noise in Fig. 5.2. Below each image \\nis the histogram computed directly from that image. The parameters of the noise were chosen in each \\ncase so that the histogram corresponding to the three intensity levels in the test pattern would start to \\nmerge. This made the noise quite visible, without obscuring the basic structure of the underlying image.\\nWe see a close correspondence in comparing the histograms in Fig. 5.4 with the PDFs in Fig. 5.2.', metadata={'source': 'imagepro.pdf', 'page': 324}),\n",
       " Document(page_content='The histogram for the salt-and-pepper example does not contain a speciﬁc peak for \\nV\\n because, as you \\nwill recall, \\nV\\n is used only during the creation of the noise image to leave values in the original image \\nunchanged. Of course, in addition to the salt and pepper peaks, there are peaks for the other intensi-\\nties in the image. With the exception of slightly different overall intensity, it is difﬁcult to differentiate \\nFIGURE 5.3\\nTest pattern used \\nto illustrate the \\ncharacteristics of \\nthe PDFs from \\nFig. 5.2.\\nDIP4E_GLOBAL_Print_Ready.indb   323\\n6/16/2017   2:07:01 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 324}),\n",
       " Document(page_content='324\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nc\\ne\\nd\\nf\\nFIGURE 5.4\\n Images and histograms resulting from adding Gaussian, Rayleigh, and Erlanga noise to the image in \\nFig. 5.3.\\nvisually between the ﬁrst ﬁve images in Fig. 5.4, even though their histograms are signiﬁcantly different. \\nThe salt-and-pepper appearance of the image in Fig. 5.4(i) is the only one that is visually indicative of \\nthe type of noise causing the degradation.\\nPERIODIC NOISE\\nPeriodic noise in images typically arises from electrical or electromechanical inter-\\nference during image acquisition. This is the only type of spatially dependent noise \\nwe will consider in this chapter. As we will discuss in Section 5.4, periodic noise can \\nbe reduced significantly via frequency domain filtering. For example, consider the \\nimage in Fig. 5.5(a). This image is corrupted by additive (spatial) sinusoidal noise. \\nThe Fourier transform of a pure sinusoid is a pair of conjugate impulses\\n†\\n located at', metadata={'source': 'imagepro.pdf', 'page': 325}),\n",
       " Document(page_content='†\\n located at \\n†  Be careful not to confuse the term \\nimpulse\\n in the frequency domain with the use of the same term in impulse \\nnoise discussed earlier, which is in the spatial domain.\\nDIP4E_GLOBAL_Print_Ready.indb   324\\n6/16/2017   2:07:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 325}),\n",
       " Document(page_content='5.2\\n  \\nNoise Models\\n    \\n325\\nh\\ng\\ni\\nk\\nl\\nj\\nFIGURE 5.4 \\n(\\ncontinued\\n) Images and histograms resulting from adding exponential, uniform, and salt-and-pepper noise \\nto the image in Fig. 5.3. In the salt-and-pepper histogram, the peaks in the origin (zero intensity) and at the far end \\nof the scale are shown displaced slightly so that they do not blend with the page background.\\nthe conjugate frequencies of the sine wave (see Table 4.4). Thus, if the amplitude of \\na sine wave in the spatial domain is strong enough, we would expect to see in the \\nspectrum of the image a pair of impulses for each sine wave in the image. As shown \\nin Fig. 5.5(b), this is indeed the case. Eliminating or reducing these impulses in the \\nfrequency domain will eliminate or reduce the sinusoidal noise in the spatial domain. \\nWe will have much more to say in Section 5.4 about this and other examples of peri-\\nodic noise.\\nESTIMATING NOISE PARAMETERS', metadata={'source': 'imagepro.pdf', 'page': 326}),\n",
       " Document(page_content='The parameters of periodic noise typically are estimated by inspection of the Fourier \\nspectrum. Periodic noise tends to produce frequency spikes that often can be detect-\\ned even by visual analysis. Another approach is to attempt to infer the periodicity \\nDIP4E_GLOBAL_Print_Ready.indb   325\\n6/16/2017   2:07:02 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 326}),\n",
       " Document(page_content='326\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.5\\n(a) Image  \\ncorrupted by  \\nadditive  \\nsinusoidal noise. \\n(b) Spectrum \\nshowing two  \\nconjugate  \\nimpulses caused \\nby the sine wave.  \\n(Original  \\nimage courtesy of \\nNASA.) \\nof noise components directly from the image, but this is possible only in simplis-\\ntic cases. Automated analysis is possible in situations in which the noise spikes are \\neither exceptionally pronounced, or when knowledge is available about the general \\nlocation of the frequency components of the interference (see Section 5.4).\\nThe parameters of noise PDFs may be known partially from sensor speciﬁcations, \\nbut it is often necessary to estimate them for a particular imaging arrangement. If \\nthe imaging system is available, one simple way to study the characteristics of system \\nnoise is to capture a set of “ﬂat” images. For example, in the case of an optical sen-', metadata={'source': 'imagepro.pdf', 'page': 327}),\n",
       " Document(page_content='sor, this is as simple as imaging a solid gray board that is illuminated uniformly. The \\nresulting images typically are good indicators of system noise.\\nWhen only images already generated by a sensor are available, it is often possible \\nto estimate the parameters of the PDF from small patches of reasonably constant \\nbackground intensity. For example, the vertical strips shown in Fig. 5.6 were cropped \\nfrom the Gaussian, Rayleigh, and uniform images in Fig. 5.4. The histograms shown \\nwere calculated using image data from these small strips. The histograms in Fig. 5.4 \\nthat correspond to the histograms in Fig. 5.6 are the ones in the middle of the group \\nof three in Figs. 5.4(d), (e), and (k).We see that the shapes of these histograms cor-\\nrespond quite closely to the shapes of the corresponding histograms in Fig. 5.6. Their \\nheights are different due to scaling, but the shapes are unmistakably similar.\\nThe simplest use of the data from the image strips is for calculating the mean and', metadata={'source': 'imagepro.pdf', 'page': 327}),\n",
       " Document(page_content='variance of intensity levels. Consider a strip (subimage) denoted by \\nS\\n, and let \\npz\\nSi\\n() ,\\niL\\n=−\\n012 1\\n,,, , ,\\n…\\n denote the probability estimates (normalized histogram values) \\nof the intensities of the pixels in \\nS\\n,\\n where \\nL\\n is the number of possible intensities in \\nthe entire image (e.g., 256 for an 8-bit image). As in Eqs. (2-69) and (2-70), we esti-\\nmate the mean and variance of the pixel values in \\nS\\n as follows:\\n \\nzz p z\\niS i\\ni\\nL\\n=\\n=\\n−\\n∑\\n()\\n0\\n1\\n \\n(5-19)\\nand\\nDIP4E_GLOBAL_Print_Ready.indb   326\\n6/16/2017   2:07:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 327}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial  Filtering\\n    \\n327\\nb a\\nc\\nFIGURE 5.6\\n Histograms computed using small strips (shown as inserts) from (a) the Gaussian, (b) the Rayleigh, and \\n(c) the uniform noisy images in Fig. 5.4.\\n \\ns\\n22\\n0\\n1\\n=−\\n=\\n−\\n∑\\n() ( )\\nzz p z\\niS i\\ni\\nL\\n \\n(5-20)\\nThe shape of the histogram identifies the closest PDF match. If the shape is approxi-\\nmately Gaussian,\\n then the mean and variance are all we need because the Gaussian \\nPDF is specified completely by these two parameters. For the other shapes discussed \\nearlier, we use the mean and variance to solve for the parameters \\na\\n and \\nb\\n. Impulse \\nnoise is handled differently because the estimate needed is of the actual probability \\nof occurrence of white and black pixels. Obtaining this estimate requires that both \\nblack and white pixels be visible, so a mid-gray, relatively constant area is needed in \\nthe image in order to be able to compute a meaningful histogram of the noise. The', metadata={'source': 'imagepro.pdf', 'page': 328}),\n",
       " Document(page_content='heights of the peaks corresponding to black and white pixels are the estimates of \\nP\\na\\nand \\nP\\nb\\n in Eq. (5-16).\\n5.3  RESTORATION IN THE PRESENCE OF NOISE ONLY—SPATIAL  \\nFILTERING  \\nWhen an image is degraded only by additive noise, Eqs. (5-1) and (5-2) become\\n \\ngxy f xy xy\\n(,\\n) (,) (,)\\n=+\\nh\\n \\n(5-21)\\nand\\n \\nGF N\\n(,\\n) (,) (,)\\nuv uv uv\\n=+\\n \\n(5-22)\\nThe noise terms generally are unknown, so subtracting them from \\ngxy\\n(,\\n)\\n \\n[( , ) ]\\nG\\nuv\\n \\nto obtain \\nfx y\\n(,\\n)\\n \\n[( ,) ]\\nF\\nuv\\n typically is not an option. In the case of periodic noise, \\n5.3\\nDIP4E_GLOBAL_Print_Ready.indb   327\\n6/16/2017   2:07:03 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 328}),\n",
       " Document(page_content='328\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nsometimes it is possible to estimate \\nN\\n(,\\n)\\nuv\\n from the spectrum of \\nG\\n(,\\n) ,\\nuv\\n as noted \\nin Section 5.2.\\n In this case \\nN\\n(,\\n)\\nuv\\n can be subtracted from \\nG\\n(,\\n)\\nuv\\n to obtain an esti-\\nmate of the original image\\n, but this type of knowledge is the exception, rather than \\nthe rule.\\nSpatial ﬁltering is the method of choice for estimating \\nfx y\\n(,\\n)\\n [i.e., \\ndenoising\\n  \\nimage \\ngxy\\n(,\\n)\\n] in situations when only additive random noise is present. Spatial ﬁl-\\ntering was discussed in detail in Chapter 3.\\n With the exception of the nature of the \\ncomputation performed by a speciﬁc ﬁlter, the mechanics for implementing all the \\nﬁlters that follow are exactly as discussed in Sections 3.4 through 3.7.\\nMEAN FILTERS\\nIn this section, we discuss briefly the noise-reduction capabilities of the spatial filters \\nintroduced in Section 3.5 and develop several other filters whose performance is in', metadata={'source': 'imagepro.pdf', 'page': 329}),\n",
       " Document(page_content='many cases superior to the filters discussed in that section.\\nArithmetic Mean Filter\\nThe \\narithmetic mean filter\\n is the simplest of the mean filters (the arithmetic mean \\nfilter is the same as the box filter we discussed in Chapter 3). Let \\nS\\nxy\\n represent the \\nset of coordinates in a rectangular subimage window (neighborhood) of size \\nmn\\n×\\n, \\ncentered on point \\n(,) .\\nxy\\n The arithmetic mean filter computes the average value of \\nthe corrupted image\\n, \\ngxy\\n(,\\n) ,\\n in the area defined by \\nS\\nxy\\n.\\n The value of the restored \\nimage \\nˆ\\nf\\n at point \\n(,)\\nxy\\n is the arithmetic mean computed using the pixels in the \\nregion defined by \\nS\\nxy\\n.In other words,\\n \\nˆ\\n(,\\n)(\\n,\\n)\\n(, )\\nfx y\\nmn\\ngrc\\nrc S\\nxy\\n=\\n∈\\n∑\\n1\\n \\n(5-23)\\nwhere, as in Eq. (2-43), \\nr\\n and \\nc\\n are the row and column coordinates of the pixels \\ncontained in the neighborhood \\nS\\nxy\\n.\\n This operation can be implemented using a spa-\\ntial kernel of size \\nmn\\n×\\n in which all coefficients have value \\n1\\nmn\\n.\\n A mean filter', metadata={'source': 'imagepro.pdf', 'page': 329}),\n",
       " Document(page_content='.\\n A mean filter \\nsmooths local variations in an image\\n, and noise is reduced as a result of blurring.\\nGeometric Mean Filter\\nAn image restored using a \\ngeometric\\n \\nmean\\n \\nfilter\\n is given by the expression\\n \\nˆ\\n(,\\n)( , )\\n(, )\\nfx y g r c\\nrc S\\nmn\\nxy\\n=\\n⎡\\n⎣\\n⎢\\n⎢\\n⎤\\n⎦\\n⎥\\n⎥\\n∈\\n∏\\n1\\n \\n(5-24)\\nwhere \\n/H9016\\n indicates multiplication. Here, \\neac\\nh\\n restored pixel is given by the product of \\nall\\n the pixels in the subimage area, raised to the power \\n1\\nmn\\n.\\n As Example 5.2 below \\nillustrates\\n, a geometric mean filter achieves smoothing comparable to an arithmetic \\nmean filter, but it tends to lose less image detail in the process.\\nWe assume that \\nm\\n and \\nn\\n are odd integers. The \\nsize\\n of a mean ﬁlter is \\nthe same as the size of \\nneighborhood \\nS\\nxy\\n; that \\nis, \\nm \\n/H11003 \\nn\\n. \\nDIP4E_GLOBAL_Print_Ready.indb   328\\n6/16/2017   2:07:05 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 329}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n329\\nHarmonic Mean Filter\\nThe \\nharmonic mean\\n filtering operation is given by the expression\\n \\nˆ\\n(,)\\n(, )\\n(, )\\nfx y\\nmn\\ngrc\\nrc S\\nxy\\n=\\n∈\\n∑\\n1\\n \\n(5-25)\\nThe harmonic mean filter works well for salt noise, but fails for pepper noise. It does \\nwell also with other types of noise like Gaussian noise\\n.\\nContraharmonic Mean Filter\\nThe \\ncontraharmonic\\n \\nmean\\n \\nfilter\\n yields a restored image based on the expression\\n \\nˆ\\n(,)\\n(, )\\n(, )\\n(, )\\n(, )\\nfx y\\ngrc\\ngrc\\nQ\\nrc S\\nQ\\nrc S\\nxy\\nxy\\n=\\n+\\n∈\\n∈\\n∑\\n∑\\n1\\n \\n(5-26)\\nwhere \\nQ\\n is called the \\nor\\nder\\n of the filter. This filter is well suited for reducing or vir-\\ntually eliminating the effects of salt-and-pepper noise. For positive values of \\nQ\\n, the \\nfilter eliminates pepper noise. For negative values of \\nQ,\\n it eliminates salt noise. It \\ncannot do both simultaneously. Note that the contraharmonic filter reduces to the \\narithmetic mean filter if \\nQ\\n=\\n0,\\n and to the harmonic mean filter if \\nQ\\n=−\\n1.', metadata={'source': 'imagepro.pdf', 'page': 330}),\n",
       " Document(page_content='Q\\n=−\\n1.\\nEXAMPLE 5.2 :  Image denoising using spatial mean ﬁlters.\\nFigure 5.7(a) shows an 8-bit X-ray image of a circuit board, and Fig. 5.7(b) shows the same image, but \\ncorrupted with additive Gaussian noise of zero mean and variance of 400. For this type of image, this is \\na signiﬁcant level of noise. Figures 5.7(c) and (d) show, respectively, the result of ﬁltering the noisy image \\nwith an arithmetic mean ﬁlter of size \\n33\\n×\\n and a geometric mean ﬁlter of the same size. Although both \\nﬁlters did a reasonable job of attenuating the contribution due to noise\\n, the geometric mean ﬁlter did \\nnot blur the image as much as the arithmetic ﬁlter. For instance, the connector ﬁngers at the top of the \\nimage are sharper in Fig. 5.7(d) than in (c). The same is true in other parts of the image.\\nFigure 5.8(a) shows the same circuit image, but corrupted now by pepper noise with probability of', metadata={'source': 'imagepro.pdf', 'page': 330}),\n",
       " Document(page_content='0.1. Similarly, Fig. 5.8(b) shows the image corrupted by salt noise with the same probability. Figure 5.8(c) \\nshows the result of ﬁltering Fig. 5.8(a) using a contraharmonic mean ﬁlter with \\nQ\\n=\\n15\\n.,\\n and Fig. 5.8(d) \\nshows the result of ﬁltering F\\nig. 5.8(b) with \\nQ\\n=−\\n15\\n..\\n Both ﬁlters did a good job of reducing the effect of \\nthe noise\\n. The positive-order ﬁlter did a better job of cleaning the background, at the expense of slightly \\nthinning and blurring the dark areas. The opposite was true of the negative order ﬁlter.\\nIn general, the arithmetic and geometric mean ﬁlters (particularly the latter) are well suited for ran-\\ndom noise like Gaussian or uniform noise. The contraharmonic ﬁlter is well suited for impulse noise, but \\nit has the disadvantage that it must be known whether the noise is dark or light in order to select the \\nproper sign for \\nQ\\n. The results of choosing the wrong sign for \\nQ\\n can be disastrous, as Fig. 5.9 shows. Some', metadata={'source': 'imagepro.pdf', 'page': 330}),\n",
       " Document(page_content='of the ﬁlters discussed in the following sections eliminate this shortcoming.\\nDIP4E_GLOBAL_Print_Ready.indb   329\\n6/16/2017   2:07:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 330}),\n",
       " Document(page_content='330\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nORDER-STATISTIC FILTERS\\nWe introduced order-statistic filters in Section 3.6. We now expand the discussion \\nin that section and introduce some additional order-statistic filters. As noted in Sec-\\ntion 3.6, order-statistic filters are spatial filters whose response is based on ordering \\n(ranking) the values of the pixels contained in the neighborhood encompassed by \\nthe filter. The ranking result determines the response of the filter.\\nMedian Filter\\nThe best-known order-statistic filter in image processing is the \\nmedian filter\\n, which, \\nas its name implies, replaces the value of a pixel by the median of the intensity levels \\nin a predefined neighborhood of that pixel:\\n \\nˆ\\n(,\\n) ( ,)\\n(, )\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\nmedian\\n \\n(5-27)\\nwhere, as before, \\nS\\nxy\\n is a subimage (neighborhood) centered on point \\n(,) .\\nxy\\n The val-\\nue of the pixel at \\n(,)\\nxy\\n is included in the computation of the median. Median filters \\nb a\\nd c\\nFIGURE 5.7', metadata={'source': 'imagepro.pdf', 'page': 331}),\n",
       " Document(page_content='b a\\nd c\\nFIGURE 5.7\\n(a) X-ray image \\nof circuit board. \\n(b) Image  \\ncorrupted by  \\nadditive Gaussian \\nnoise. (c) Result \\nof ﬁltering with \\nan arithmetic \\nmean ﬁlter of size \\n33\\n×\\n. (d) Result \\nof ﬁltering with a \\ngeometric mean \\nﬁlter of the same \\nsize\\n. (Original \\nimage courtesy of \\nMr. Joseph E.  \\nPascente, Lixi, \\nInc.)\\nDIP4E_GLOBAL_Print_Ready.indb   330\\n6/16/2017   2:07:06 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 331}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n331\\nb a\\nd c\\nFIGURE 5.8\\n(a) Image  \\ncorrupted by \\npepper noise with \\na probability of \\n0.1. (b) Image \\ncorrupted by salt \\nnoise with the \\nsame  \\nprobability.  \\n(c) Result of  \\nﬁltering (a) with \\na \\n33\\n×\\n contra-\\nharmonic ﬁlter \\nQ\\n=\\n15\\n..\\n (d) Result \\nof ﬁltering (b) \\nwith \\nQ\\n=−\\n15\\n..\\n \\nb a\\n \\nFIGURE 5.9\\nResults of  \\nselecting the \\nwrong sign in  \\ncontraharmonic \\nﬁltering.  \\n(a) Result of \\nﬁltering Fig. 5.8(a) \\nwith a  \\ncontraharmonic \\nﬁlter of size \\n33\\n×\\n \\nand \\nQ\\n=−\\n15\\n..\\n  \\n(b) Result of  \\nﬁltering F\\nig. 5.8(b) \\nusing \\nQ\\n=\\n15\\n..\\n \\nDIP4E_GLOBAL_Print_Ready.indb   331\\n6/16/2017   2:07:07 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 332}),\n",
       " Document(page_content='332\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nare quite popular because, for certain types of random noise, they provide excellent \\nnoise-reduction capabilities, with considerably less blurring than linear smoothing \\nfilters of similar size. Median filters are particularly effective in the presence of both \\nbipolar and unipolar impulse noise, as Example 5.3 below shows. Computation of \\nthe median and implementation of this filter are discussed in Section 3.6.\\nMax and Min Filters\\nAlthough the median filter is by far the order-statistic filter most used in image pro-\\ncessing, it is by no means the only one. The median represents the 50th percentile of \\na ranked set of numbers, but you will recall from basic statistics that ranking lends \\nitself to many other possibilities. For example, using the 100th percentile results in \\nthe so-called \\nmax filter\\n, given by\\n \\nˆ\\n(\\n, ) max ( , )\\n(,)\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\n \\n(5-28)', metadata={'source': 'imagepro.pdf', 'page': 333}),\n",
       " Document(page_content='xy\\n=\\n{}\\n∈\\n \\n(5-28)\\nThis filter is useful for finding the brightest points in an image or for eroding dark \\nregions adjacent to bright areas\\n. Also, because pepper noise has very low values, it \\nis reduced by this filter as a result of the max selection process in the subimage area \\nS\\nxy\\n.\\nThe 0th percentile ﬁlter is the \\nmin ﬁlter\\n:\\n \\nˆ\\n(,\\n) m i n ( ,)\\n(, )\\nfx y g r c\\nrc S\\nxy\\n=\\n{}\\n∈\\n \\n(5-29)\\nThis filter is useful for finding the darkest points in an image or for eroding light \\nregions adjacent to dark areas\\n. Also, it reduces salt noise as a result of the min opera-\\ntion.\\nMidpoint Filter\\nThe \\nmidpoint filter\\n computes the midpoint between the maximum and minimum \\nvalues in the area encompassed by the filter:\\n \\nˆ\\n( , ) m a x (, ) m i n (, )\\n(, )\\n(, )\\nfx y\\ng r c g r c\\nrc S\\nrc S\\nxy\\nxy\\n=\\n{}\\n+\\n{}\\n⎡\\n⎣\\n⎢\\n⎤\\n⎦\\n⎥\\n∈\\n∈\\n1\\n2\\n \\n(5-30)\\nNote that this filter combines order statistics and averaging. It works best for ran-\\ndomly distributed noise\\n, like Gaussian or uniform noise.', metadata={'source': 'imagepro.pdf', 'page': 333}),\n",
       " Document(page_content='Alpha-Trimmed Mean Filter\\nSuppose that we delete the \\nd\\n2\\n lowest and the \\nd\\n2\\n highest intensity values of \\ngrc\\n(,\\n)\\n \\nin the neighborhood \\nS\\nxy\\n.\\n Let \\ngr c\\nR\\n(, )\\n represent the remaining \\nmn d\\n−\\n pixels in \\nS\\nxy\\n. \\nA filter formed by averaging these remaining pixels is called an \\nalpha-trimmed mean \\nfilter\\n.\\n The form of this filter is \\nDIP4E_GLOBAL_Print_Ready.indb   332\\n6/16/2017   2:07:08 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 333}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n333\\n \\nˆ\\n(,\\n)(\\n,\\n)\\n(, )\\nfx y\\nmn d\\ngr c\\nR\\nrc S\\nxy\\n=\\n−\\n∈\\n∑\\n1\\n \\n(5-31)\\nwhere the value of \\nd\\n can range from 0 to \\nmn\\n−\\n1.\\n When \\nd\\n=\\n0\\n the alpha-trimmed fil-\\nter reduces to the arithmetic mean filter discussed earlier\\n. If we choose \\ndm n\\n=−\\n1,\\n \\nthe filter becomes a median filter\\n. For other values of \\nd\\n, the alpha-trimmed filter is \\nuseful in situations involving multiple types of noise, such as a combination of salt-\\nand-pepper and Gaussian noise.\\nEXAMPLE 5.3 :  Image denoising using order-statistic ﬁlters.\\nFigure 5.10(a) shows the circuit board image corrupted by salt-and-pepper noise with probabilities \\nP\\nP\\nsp\\n==\\n01\\n..\\n Figure 5.10(b) shows the result of median ﬁltering with a ﬁlter of size \\n33\\n×\\n.\\n The improve-\\nment over F\\nig. 5.10(a) is signiﬁcant, but several noise points still are visible. A second pass [on the im-', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='age in Fig. 5.10(b)] with the median ﬁlter removed most of these points, leaving only few, barely visible \\nnoise points. These were removed with a third pass of the ﬁlter. These results are good examples of the \\npower of median ﬁltering in handling impulse-like additive noise. Keep in mind that repeated passes \\nof a median ﬁlter will blur the image, so it is desirable to keep the number of passes as low as possible. \\nFigure 5.11(a) shows the result of applying the max ﬁlter to the pepper noise image of Fig. 5.8(a). The \\nﬁlter did a reasonable job of removing the pepper noise, but we note that it also removed (set to a light \\nintensity level) some dark pixels from the borders of the dark objects. Figure 5.11(b) shows the result \\nof applying the min ﬁlter to the image in Fig. 5.8(b). In this case, the min ﬁlter did a better job than the \\nmax ﬁlter on noise removal, but it removed some white points around the border of light objects. These', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='made the light objects smaller and some of the dark objects larger (like the connector ﬁngers in the top \\nof the image) because white points around these objects were set to a dark level.\\nThe alpha-trimmed ﬁlter is illustrated next. Figure 5.12(a) shows the circuit board image corrupted \\nthis time by additive, uniform noise of variance 800 and zero mean. This is a high level of noise corrup-\\ntion that is made worse by further addition of salt-and-pepper noise with \\nPP\\nsp\\n==\\n01\\n.,\\n as Fig. 5.12(b) \\nshows\\n. The high level of noise in this image warrants use of larger ﬁlters. Figures 5.12(c) through (f) show \\nthe results, respectively, obtained using arithmetic mean, geometric mean, median, and alpha-trimmed \\nmean (with \\nd\\n=\\n6\\n) ﬁlters of size \\n55\\n×\\n.\\n As expected, the arithmetic and geometric mean ﬁlters (especially \\nthe latter) did not do well because of the presence of impulse noise\\n. The median and alpha-trimmed', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='ﬁlters performed much better, with the alpha-trimmed ﬁlter giving slightly better noise reduction. For \\nexample, note in Fig. 5.12(f) that the fourth connector ﬁnger from the top left is slightly smoother in \\nthe alpha-trimmed result. This is not unexpected because, for a high value of \\nd\\n, the alpha-trimmed ﬁlter \\napproaches the performance of the median ﬁlter, but still retains some smoothing capabilities.\\nADAPTIVE FILTERS\\nOnce selected, the filters discussed thus far are applied to an image without regard \\nfor how image characteristics vary from one point to another. In this section, we \\ntake a look at two \\nadaptive\\n filters whose behavior changes based on statistical char-\\nacteristics of the image inside the filter region defined by the \\nmn\\n×\\n rectangular \\nneighborhood \\nS\\nxy\\n.\\n As the following discussion shows, adaptive filters are capable \\nof performance superior to that of the filters discussed thus far\\n. The price paid for \\nDIP4E_GLOBAL_Print_Ready.indb   333', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='6/16/2017   2:07:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 334}),\n",
       " Document(page_content='334\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nb a\\nFIGURE 5.11\\n(a) Result of \\nﬁltering Fig. 5.8(a) \\nwith a max ﬁlter \\nof size \\n33\\n×\\n. \\n(b) Result of  \\nﬁltering F\\nig. 5.8(b) \\nwith a min ﬁlter of \\nthe same size. \\nb a\\nd c\\nFIGURE 5.10\\n(a) Image  \\ncorrupted by salt-\\nand- pepper noise \\nwith probabilities \\nPP\\nsp\\n==\\n01\\n..\\n \\n(b) Result of one \\npass with a medi-\\nan ﬁlter of size \\n33\\n×\\n. (c) Result \\nof processing (b) \\nwith this ﬁlter\\n.  \\n(d) Result of  \\nprocessing (c) \\nwith the same \\nﬁlter. \\nDIP4E_GLOBAL_Print_Ready.indb   334\\n6/16/2017   2:07:09 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 335}),\n",
       " Document(page_content='5.3\\n  \\nRestoration in the Presence of Noise Only—Spatial Filtering\\n    \\n335\\nb a\\nd c\\nf e\\nFIGURE 5.12\\n(a) Image  \\ncorrupted by \\nadditive uniform \\nnoise. (b) Image \\nadditionally  \\ncorrupted by \\nadditive salt-and-\\npepper noise. \\n(c)-(f) Image (b) \\nﬁltered with a \\n55\\n×\\n: \\n(c) arithmetic \\nmean ﬁlter;\\n  \\n(d) geometric \\nmean ﬁlter;  \\n(e) median ﬁlter; \\n(f) alpha-trimmed \\nmean ﬁlter, with \\nd\\n=\\n6. \\nDIP4E_GLOBAL_Print_Ready.indb   335\\n6/16/2017   2:07:10 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 336}),\n",
       " Document(page_content='336\\n    \\nChapter\\n \\n5\\n  \\nImage Restoration and Reconstruction\\nimproved filtering power is an increase in filter complexity. Keep in mind that we \\nstill are dealing with the case in which the degraded image is equal to the original \\nimage plus noise. No other types of degradations are being considered yet.\\nAdaptive, Local Noise Reduction Filter\\nThe simplest statistical measures of a random variable are its mean and variance. \\nThese are reasonable parameters on which to base an adaptive filter because they \\nare quantities closely related to the appearance of an image. The mean gives a mea-\\nsure of average intensity in the region over which the mean is computed, and the \\nvariance gives a measure of image contrast in that region.\\nOur ﬁlter is to operate on a neighborhood, \\nS\\nxy\\n,\\n centered on coordinates \\n(,) .\\nxy\\nThe response of the ﬁlter at \\n(,)\\nxy\\n is to be based on the following quantities: \\ngxy\\n(,\\n) ,\\n \\nthe value of the noisy image at \\n(,) ;\\nxy\\n \\ns\\nh\\n2\\n,\\n the variance of the noise; \\nz', metadata={'source': 'imagepro.pdf', 'page': 337}),\n",
       " Document(page_content='z\\nS\\nxy\\n,\\n the local \\naverage intensity of the pixels in \\nS\\nxy\\n;\\n and \\ns\\nS\\nxy\\n2\\n,\\n the local variance of the intensities of \\npixels in \\nS\\nxy\\n. We want the behavior of the ﬁlter to be as follows:\\n1. \\nIf \\ns\\nh\\n2\\n is zero, the ﬁlter should return simply the value of \\ng \\nat \\n(,) .\\nxy\\n This is the \\ntrivial,\\n zero-noise case in which \\ng\\n is equal to \\nf\\n at \\n(,) .\\nxy\\n2.\\n \\nIf the local variance \\ns\\nS\\nxy\\n2\\n is high relative to \\ns\\nh\\n2\\n,\\n the ﬁlter should return a value \\nclose to \\ng\\n at \\n(,) .\\nxy\\n A high local variance typically is associated with edges, and \\nthese should be preserved.\\n3. \\nIf the two variances are equal, we want the ﬁlter to return the arithmetic mean \\nvalue of the pixels in \\nS\\nxy\\n.\\n This condition occurs when the local area has the same \\nproperties as the overall image\\n, and local noise is to be reduced by averaging.\\nAn adaptive expression for obtaining \\nˆ\\n(,\\n)\\nfx y\\n based on these assumptions may be \\nwritten as\\n \\nˆ\\n(,) (,) (,)\\nfx y g x y g x\\nyz\\nS\\nS\\nxy\\nxy\\n=− −\\n⎡\\n⎣\\n⎤\\n⎦\\ns\\ns\\nh\\n2\\n2', metadata={'source': 'imagepro.pdf', 'page': 337}),\n",
       " Document(page_content='⎡\\n⎣\\n⎤\\n⎦\\ns\\ns\\nh\\n2\\n2\\n \\n(5-32)\\nThe only quantity that needs to be known a priori is \\ns\\nh\\n2\\n,\\n the variance of the noise \\ncorrupting image \\nfx y\\n(,\\n) .\\n This is a constant that can be estimated from sample noisy \\nimages using Eq.\\n (3-26). The other parameters are computed from the pixels in \\nneighborhood \\nS\\nxy\\n using Eqs. (3-27) and (3-28).\\nAn assumption in Eq. (5-32) is that the ratio of the two variances does not exceed 1, \\nwhich implies that \\nss\\nh\\n22\\n≤\\nS\\nxy\\n.\\n The noise in our model is additive and position indepen-\\ndent,\\n so this is a reasonable assumption to make because \\nS\\nxy\\n is a subset of \\ngxy\\n(,\\n) .\\n \\nHowever\\n, we seldom have exact knowledge of \\ns\\nh\\n2\\n.\\n Therefore, it is possible for this \\ncondition to be violated in practice\\n. For that reason, a test should be built into an \\nimplementation of Eq. (5-32) so that the ratio is set to 1 if the condition \\nss\\nh\\n22\\n>\\nS\\nxy\\n \\noccurs. This makes this ﬁlter nonlinear. However, it prevents nonsensical results (i.e.,', metadata={'source': 'imagepro.pdf', 'page': 337}),\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents[:30], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2122eae49d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Roger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, \\nRhonda Knighton, Don P . Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, \\nMichael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David \\nK. Wehe, and Robert A. West. We also wish to acknowledge other individuals and \\norganizations cited in the captions of numerous ﬁgures throughout the book for \\ntheir permission to use that material. \\nWe also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for \\ntheir support and signiﬁcant patience during the production of the book.\\nR.C.G.\\nR.E.W.\\nDIP4E_GLOBAL_Print_Ready.indb   12\\n6/16/2017   2:01:57 PM\\nwww.EBooksWorld.ir', metadata={'source': 'imagepro.pdf', 'page': 13})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Department of Agriculture and Farmers Welfare\"\n",
    "result=db.similarity_search(query)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM model using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LLama-3 LLM Model\n",
    "llm=Ollama(model=\"llama3\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design ChatPrompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step bt step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>                                          \n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Introduction || Create Stuff Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "- A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "[Learn more](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002122EAE49D0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever= db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers Chains\n",
    "- Retrievers chain: This chain takes in user inquiry, which is then passed to the retriver to fetch relevant documents. Those documents(and original inputs) are then passed to an LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking Any Query now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"\"\"image processing with an introduction to techniques for image \n",
    "pattern classification\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I would answer your question as follows:\n",
      "\n",
      "The companion website www.ImageProcessingPlace.com offers additional support in a number of important areas. \n",
      "\n",
      "For the Student or Independent Reader, the site contains tutorials on topics relevant to the material in the book. One specific tutorial that could be helpful for image pattern classification is found in the Tutorials section.\n",
      "\n",
      "This edition of Digital Image Processing is a major revision of the book and continues to provide an introduction to basic concepts and methodologies applicable to digital image processing. To achieve these objectives, the authors focused on material that they believe is fundamental and whose scope of application is not limited to the solution of specialized problems.\n",
      "\n",
      "One possible area for exploring image pattern classification could be in the Wavelet and Other Image Transforms chapter (Chapter 7) where various transforms such as Fourier-Related Transforms, Walsh-Hadamard Transforms, Slant Transform, Haar Transform, and Wavelet Transforms are discussed. These transforms can be used to classify patterns or features within an image.\n",
      "\n",
      "Another possible area for exploring image pattern classification could be in the Image Compression and Watermarking chapter (Chapter 8) where various coding techniques such as Huffman Coding, Golomb Coding, Arithmetic Coding, LZW Coding, Run-length Coding, Symbol-based Coding, Bit-plane Coding, Block Transform Coding, Predictive Coding, Wavelet Coding, and Digital Image Watermarking are discussed. These coding techniques can be used to compress or watermark images and potentially classify patterns within an image.\n",
      "\n",
      "For the Instructor, the site contains classroom presentation materials in modifiable PowerPoint format. This could be useful for creating lecture slides that cover topics related to image pattern classification.\n",
      "\n",
      "For the Practitioner, the site contains additional specialized topics such as links to commercial sites and selected new references. These resources may provide practical applications or real-world examples of image pattern classification techniques.\n",
      "\n",
      "I hope this answer is helpful!\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
